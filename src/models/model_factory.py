"""
Model factory for creating different architectures for comparison experiments.
"""

import torch
from torch.utils.checkpoint import checkpoint
import timm
import torchvision.models as models
from .swin.swin_transformer_model import SwinTransformerModel
from .model_wrapper import ModelWrapper
from .segmentation_wrapper import SegmentationModelWrapper
from .resnet_encoder import ResNetFeatureExtractor, ResNetSegmentationWrapper
from .deit_encoder import DeiTFeatureExtractor, DeiTSegmentationWrapper
from .heads import LinearClassificationHead, UperNetHead


def create_model(config):
    """
    Create model based on configuration type.

    Args:
        config: MODEL_CONFIG dictionary

    Returns:
        ModelWrapper containing the model
    """
    model_type = config["type"]

    if model_type == "swin":
        return create_swin_model(config)
    elif model_type == "vit":
        return create_vit_model(config)
    elif model_type == "resnet":
        return create_resnet_model(config)
    else:
        raise ValueError(f"Unknown model type: {model_type}")


def create_swin_model(config):
    """Create Swin Transformer model (existing implementation)."""
    encoder = SwinTransformerModel(
        img_size=224,  # Fixed for ImageNet
        patch_size=config["patch_size"],
        embedding_dim=config["embed_dim"],
        depths=config["depths"],
        num_heads=config["num_heads"],
        window_size=config["window_size"],
        mlp_ratio=config["mlp_ratio"],
        dropout_rate=config["dropout"],
        attention_dropout_rate=config["attention_dropout"],
        projection_dropout_rate=config["projection_dropout"],
        drop_path_rate=config["drop_path_rate"],
        use_shifted_window=config["use_shifted_window"],
        use_relative_bias=config["use_relative_bias"],
        use_absolute_pos_embed=config["use_absolute_pos_embed"],
        use_hierarchical_merge=config["use_hierarchical_merge"],
        use_gradient_checkpointing=config.get("use_gradient_checkpointing", False),
    )

    pred_head = LinearClassificationHead(
        num_features=encoder.num_features,
        num_classes=1000,  # ImageNet
    )

    return ModelWrapper(
        encoder=encoder,
        pred_head=pred_head,
        freeze=False,  # From scratch training
    )


def create_vit_model(config):
    """Create ViT model using timm library with gradient checkpointing."""
    model = timm.create_model(
        "vit_tiny_patch16_224",
        pretrained=False,
        num_classes=config["num_classes"],
        img_size=config["img_size"],
        patch_size=config["patch_size"],
        embed_dim=config["embed_dim"],
        depth=config["depth"],
        num_heads=config["num_heads"],
        mlp_ratio=config["mlp_ratio"],
        drop_path_rate=0.1,  # Add some dropout for regularization
    )

    # Enable gradient checkpointing for memory efficiency if requested
    if config.get("use_gradient_checkpointing", False):
        if hasattr(model, "grad_checkpointing"):
            model.grad_checkpointing = True
        elif hasattr(model, "blocks"):
            # Manually enable gradient checkpointing for blocks
            original_forward = model.blocks.forward

            def checkpointed_forward(x):
                if model.training:
                    for block in model.blocks:
                        x = checkpoint(block, x, use_reentrant=False)
                    return x
                else:
                    return original_forward(x)

            model.blocks.forward = checkpointed_forward

    return model


def create_resnet_model(config):
    """Create ResNet model using torchvision with gradient checkpointing."""
    # Use layers config to choose ResNet variant
    layers = config.get("layers", [3, 4, 6, 3])

    if layers == [3, 4, 6, 3]:
        model = models.resnet50(pretrained=False, num_classes=config["num_classes"])
    elif layers == [3, 4, 23, 3]:
        model = models.resnet101(pretrained=False, num_classes=config["num_classes"])
    else:
        # Default to ResNet-50
        model = models.resnet50(pretrained=False, num_classes=config["num_classes"])

    # Enable gradient checkpointing for memory efficiency if requested
    if config.get("use_gradient_checkpointing", False):
        if (
            hasattr(model, "layer1")
            and hasattr(model, "layer2")
            and hasattr(model, "layer3")
            and hasattr(model, "layer4")
        ):
            # Wrap each ResNet layer with gradient checkpointing
            original_layer1_forward = model.layer1.forward
            original_layer2_forward = model.layer2.forward
            original_layer3_forward = model.layer3.forward
            original_layer4_forward = model.layer4.forward

            def checkpointed_layer1(x):
                return (
                    checkpoint(original_layer1_forward, x, use_reentrant=False)
                    if model.training
                    else original_layer1_forward(x)
                )

            def checkpointed_layer2(x):
                return (
                    checkpoint(original_layer2_forward, x, use_reentrant=False)
                    if model.training
                    else original_layer2_forward(x)
                )

            def checkpointed_layer3(x):
                return (
                    checkpoint(original_layer3_forward, x, use_reentrant=False)
                    if model.training
                    else original_layer3_forward(x)
                )

            def checkpointed_layer4(x):
                return (
                    checkpoint(original_layer4_forward, x, use_reentrant=False)
                    if model.training
                    else original_layer4_forward(x)
                )

            model.layer1.forward = checkpointed_layer1
            model.layer2.forward = checkpointed_layer2
            model.layer3.forward = checkpointed_layer3
            model.layer4.forward = checkpointed_layer4

    return model


def create_segmentation_model(swin_config, downstream_config, load_pretrained=True):
    """
    Create segmentation model with Swin encoder + UperNet head.
    
    Args:
        swin_config: SWIN_CONFIG dictionary from config
        downstream_config: DOWNSTREAM_CONFIG dictionary from config
        load_pretrained: If True, load ImageNet pretrained weights for encoder
    
    Returns:
        SegmentationModelWrapper containing encoder + segmentation head
    """
    from src.utils.load_weights import load_pretrained_reference, transfer_weights
    
    # Create Swin Transformer encoder
    encoder = SwinTransformerModel(
        img_size=swin_config["img_size"],  # 512 for ADE20K
        patch_size=swin_config["patch_size"],
        embedding_dim=swin_config["embed_dim"],
        depths=swin_config["depths"],
        num_heads=swin_config["num_heads"],
        window_size=swin_config["window_size"],
        mlp_ratio=swin_config["mlp_ratio"],
        dropout_rate=swin_config["dropout"],
        attention_dropout_rate=swin_config["attention_dropout"],
        projection_dropout_rate=swin_config["projection_dropout"],
        drop_path_rate=swin_config["drop_path_rate"],
        use_shifted_window=swin_config.get("use_shifted_window", True),
        use_relative_bias=swin_config.get("use_relative_bias", True),
        use_absolute_pos_embed=swin_config.get("use_absolute_pos_embed", False),
        use_hierarchical_merge=swin_config.get("use_hierarchical_merge", False),
        use_gradient_checkpointing=swin_config.get("use_gradient_checkpointing", False),
    )
    
    # Calculate in_channels for each stage
    # Swin-T: [96, 192, 384, 768]
    embed_dim = swin_config["embed_dim"]
    in_channels = [int(embed_dim * (2 ** i)) for i in range(len(swin_config["depths"]))]
    
    # Create UperNet segmentation head
    seg_head = UperNetHead(
        in_channels=in_channels,
        num_classes=downstream_config["num_classes"],
        channels=512,  # FPN channels (standard for UperNet)
        pool_scales=(1, 2, 3, 6),  # PPM scales from paper
        dropout=0.1,  # Dropout before classifier
    )
    
    # Combine encoder + head
    model = SegmentationModelWrapper(
        encoder=encoder,
        seg_head=seg_head,
        freeze_encoder=downstream_config.get("freeze_encoder", False),
    )
    
    # Load pretrained ImageNet weights for encoder
    if load_pretrained and downstream_config.get("use_pretrained", True):
        pretrained_model_name = "swin_tiny_patch4_window7_224"  # TIMM model name
        print(f"Loading pretrained weights from TIMM: {pretrained_model_name}")
        
        pretrained_model = load_pretrained_reference(
            model_name=pretrained_model_name,
            device="cpu",  # Load to CPU first, then move to device
        )
        
        if pretrained_model is not None:
            stats = transfer_weights(
                custom_model=model,
                pretrained_model=pretrained_model,
                encoder_only=True,  # Only transfer encoder weights
            )
            print(f"Weight transfer complete: {stats['transferred']} layers transferred, "
                  f"{stats['missing']} missing, {stats['size_mismatches']} size mismatches")
        else:
            print("Warning: Could not load pretrained weights. Training from scratch.")
    
    return model


def create_resnet_segmentation_model(resnet_config, downstream_config):
    """
    Create segmentation model with ResNet encoder + UperNet head.
    
    This function loads a pretrained ResNet-101/50 from torchvision and combines
    it with UperNet for semantic segmentation on ADE20K or similar datasets.
    
    Args:
        resnet_config: ResNet configuration dictionary with keys:
            - variant: 'resnet50' or 'resnet101' (default: 'resnet101')
            - pretrained: Whether to load ImageNet pretrained weights (default: True)
            - img_size: Input image size (default: 512)
        downstream_config: DOWNSTREAM_CONFIG dictionary with keys:
            - num_classes: Number of segmentation classes (e.g., 150 for ADE20K)
            - freeze_encoder: If True, freeze ResNet encoder weights
    
    Returns:
        ResNetSegmentationWrapper containing encoder + segmentation head
    """
    # Get ResNet variant
    variant = resnet_config.get("variant", "resnet101")
    pretrained = resnet_config.get("pretrained", True)
    img_size = resnet_config.get("img_size", 512)
    use_gradient_checkpointing = resnet_config.get("use_gradient_checkpointing", False)
    
    print(f"Creating ResNet segmentation model: {variant}, pretrained={pretrained}, "
          f"gradient_checkpointing={use_gradient_checkpointing}")
    
    # Create ResNet feature extractor
    encoder = ResNetFeatureExtractor(
        variant=variant,
        pretrained=pretrained,
        use_gradient_checkpointing=use_gradient_checkpointing,
    )
    encoder.set_img_size(img_size)
    
    # Get feature channels from encoder
    # ResNet-101/50: [256, 512, 1024, 2048]
    in_channels = encoder.out_channels
    
    print(f"ResNet encoder feature channels: {in_channels}")
    
    # Create UperNet segmentation head
    seg_head = UperNetHead(
        in_channels=in_channels,
        num_classes=downstream_config["num_classes"],
        channels=512,  # FPN channels (standard for UperNet)
        pool_scales=(1, 2, 3, 6),  # PPM scales from paper
        dropout=0.1,  # Dropout before classifier
    )
    
    # Combine encoder + head
    model = ResNetSegmentationWrapper(
        encoder=encoder,
        seg_head=seg_head,
        freeze_encoder=downstream_config.get("freeze_encoder", False),
    )
    
    # Print parameter counts
    param_counts = model.get_num_params()
    print(f"Model parameters: encoder={param_counts['encoder']:,}, "
          f"head={param_counts['head']:,}, total={param_counts['total']:,}, "
          f"trainable={param_counts['trainable']:,}")
    
    return model


def create_deit_segmentation_model(deit_config, downstream_config):
    """
    Create segmentation model with DeiT encoder + UperNet head.
    
    This function loads a pretrained DeiT from timm and combines it with
    UperNet for semantic segmentation. Since DeiT outputs single-scale features,
    MultiLevelNeck (bilinear interpolation) is used to create a pseudo-hierarchy
    compatible with UperNet, following the exact approach in mmsegmentation
    and the Swin Transformer paper (Table 3).
    
    Key: All feature levels have the SAME channel dimension (384 for DeiT-S),
    which matches the paper's 52M parameter count.
    
    Args:
        deit_config: DeiT configuration dictionary with keys:
            - variant: timm model name (default: 'deit_small_patch16_224')
            - pretrained: Whether to load ImageNet pretrained weights (default: True)
            - img_size: Input image size (default: 512)
            - extract_layers: Which transformer layers to extract (default: (2,5,8,11))
        downstream_config: DOWNSTREAM_CONFIG dictionary with keys:
            - num_classes: Number of segmentation classes (e.g., 150 for ADE20K)
            - freeze_encoder: If True, freeze DeiT encoder weights
    
    Returns:
        DeiTSegmentationWrapper containing encoder + segmentation head
    """
    # Get DeiT configuration
    variant = deit_config.get("variant", "deit_small_patch16_224")
    pretrained = deit_config.get("pretrained", True)
    img_size = deit_config.get("img_size", 512)
    use_gradient_checkpointing = deit_config.get("use_gradient_checkpointing", False)
    extract_layers = deit_config.get("extract_layers", (2, 5, 8, 11))
    
    print(f"Creating DeiT segmentation model: {variant}, pretrained={pretrained}, "
          f"gradient_checkpointing={use_gradient_checkpointing}")
    print(f"Extracting features from layers: {extract_layers}")
    
    # Create DeiT feature extractor with MultiLevelNeck (bilinear interpolation)
    encoder = DeiTFeatureExtractor(
        variant=variant,
        pretrained=pretrained,
        img_size=img_size,
        use_gradient_checkpointing=use_gradient_checkpointing,
        extract_layers=extract_layers,
    )
    
    # Get feature channels from encoder
    # DeiT with MultiLevelNeck: [384, 384, 384, 384] (same channels, matching paper)
    in_channels = encoder.out_channels
    
    print(f"DeiT encoder feature channels: {in_channels}")
    print(f"DeiT embed_dim={encoder.embed_dim}, patch_size={encoder.patch_size}")
    
    # Create UperNet segmentation head
    seg_head = UperNetHead(
        in_channels=in_channels,
        num_classes=downstream_config["num_classes"],
        channels=512,  # FPN channels (standard for UperNet)
        pool_scales=(1, 2, 3, 6),  # PPM scales from paper
        dropout=0.1,  # Dropout before classifier
    )
    
    # Combine encoder + head
    model = DeiTSegmentationWrapper(
        encoder=encoder,
        seg_head=seg_head,
        freeze_encoder=downstream_config.get("freeze_encoder", False),
    )
    
    # Print parameter counts
    param_counts = model.get_num_params()
    print(f"Model parameters: encoder={param_counts['encoder']:,}, "
          f"head={param_counts['head']:,}, total={param_counts['total']:,}, "
          f"trainable={param_counts['trainable']:,}")
    
    return model
