import torch
import torch.nn as nn
import torch.nn.functional as F

from .mlp import MLP
from .window_attention import WindowAttention
from .drop_path import DropPath
from .window_utils import create_image_mask, window_partition, window_reverse


class SwinTransformerBlock(nn.Module):
    """
    Swin Transformer Block: The fundamental building block of Swin Transformer.

    This module implements one complete Swin block that can operate in either:
    - Standard Window Multi-head Self Attention (W-MSA) mode
    - Shifted Window Multi-head Self Attention (SW-MSA) mode

    ┌────────────────────── SWIN BLOCK ARCHITECTURE ──────────────────────┐
    │                                                                      │
    │  Input: [B, H×W, C]                                                  │
    │      │                                                               │
    │      ├─────────────────────────┐                                     │
    │      │                         │  (Residual Connection 1)            │
    │      ▼                         │                                     │
    │  LayerNorm                     │                                     │
    │      │                         │                                     │
    │      ▼                         │                                     │
    │  Window / Shifted-Window       │                                     │
    │  Multi-head Self Attention     │                                     │
    │  (W-MSA or SW-MSA)             │                                     │
    │      │                         │                                     │
    │      ▼                         │                                     │
    │  DropPath (Stochastic Depth)   │                                     │
    │      │                         │                                     │
    │      ▼                         │                                     │
    │  ◄──(+)◄────────────────────────┘                                     │
    │      │                                                               │
    │      ├─────────────────────────┐                                     │
    │      │                         │  (Residual Connection 2)            │
    │      ▼                         │                                     │
    │  LayerNorm                     │                                     │
    │      │                         │                                     │
    │      ▼                         │                                     │
    │  MLP (FC → GELU → FC)          │                                     │
    │      │                         │                                     │
    │      ▼                         │                                     │
    │  DropPath (Stochastic Depth)   │                                     │
    │      │                         │                                     │
    │      ▼                         │                                     │
    │  ◄──(+)◄────────────────────────┘                                     │
    │      │                                                               │
    │      ▼                                                               │
    │  Output: [B, H×W, C]                                                 │
    │                                                                      │
    └──────────────────────────────────────────────────────────────────────┘

    Key Design Choices:
    1. **Pre-LayerNorm**: Normalization before attention/MLP (better gradient flow)
    2. **Residual Connections**: Enable deep networks (up to 100+ layers)
    3. **DropPath**: Stochastic depth regularization (drops entire layers randomly)
    4. **MLP Expansion**: Hidden dim = 4× input dim (standard in transformers)

    W-MSA vs SW-MSA:
    - **W-MSA (shift_size=0)**: Attention within fixed non-overlapping windows
    - **SW-MSA (shift_size>0)**: Attention within shifted windows for cross-window connection
    - **Alternating Pattern**: W-MSA → SW-MSA → W-MSA → SW-MSA (enables global modeling)
    """

    def __init__(
        self,
        dim: int,
        input_resolution: tuple,
        num_heads: int,
        window_size: int = 7,
        shift_size: int = 0,
        mlp_ratio: float = 4.0,
        dropout: float = 0.0,
        attention_dropout: float = 0.0,
        projection_dropout: float = 0.0,
        drop_path: float = 0.0,
        use_relative_bias: bool = True,  # Ablation flag: True for learned bias, False for zero bias
        use_absolute_pos_embed: bool = False,  # Ablation flag: True for absolute pos embed (ViT-style)
    ):
        """
        Initialize Swin Transformer Block.

        Args:
            dim: Feature dimension (C)
            input_resolution: (H, W) spatial resolution
            num_heads: Number of attention heads
            window_size: Window size for attention (default: 7×7)
            shift_size: Shift size for SW-MSA (0 for W-MSA, window_size//2 for SW-MSA)
            mlp_ratio: Ratio of MLP hidden dim to embedding dim
            dropout: Dropout rate
            attention_dropout: Attention dropout rate
            projection_dropout: Projection dropout rate
            drop_path: Stochastic depth rate (probability of dropping the layer)

        Notes:
            - For W-MSA blocks: shift_size = 0
            - For SW-MSA blocks: shift_size = window_size // 2
            - Input resolution must be divisible by window_size (after shifting)
        """
        super().__init__()

        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio

        # MMSegmentation approach: Use padding instead of falling back to global attention
        # If window size is larger than input resolution, use input resolution as window size
        # (this only happens at very small resolutions like 7x7 in stage 4)
        if min(self.input_resolution) <= self.window_size:
            self.window_size = min(self.input_resolution)
            self.shift_size = 0

        assert (
            0 <= self.shift_size < self.window_size
        ), f"shift_size must be in [0, window_size), got shift_size={self.shift_size}, window_size={self.window_size}"

        # Layer 1: LayerNorm → Window Attention → Residual
        self.norm1 = nn.LayerNorm(dim)

        self.attn = WindowAttention(
            dim=dim,
            window_size=(self.window_size, self.window_size),
            num_heads=num_heads,
            attn_dropout=attention_dropout,
            proj_dropout=projection_dropout,
            use_relative_bias=use_relative_bias,  # Pass ablation flag
            use_absolute_pos_embed=use_absolute_pos_embed,  # Pass ablation flag
        )

        # Stochastic depth (DropPath) for regularization
        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()

        # Layer 2: LayerNorm → MLP → Residual
        self.norm2 = nn.LayerNorm(dim)

        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, dropout=dropout)

        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through Swin Transformer Block.

        Args:
            x: Input tensor [B, H*W, C]

        Returns:
            Output tensor [B, H*W, C]

        Processing Flow (MMSegmentation-style with padding):
        1. Save input for residual connection
        2. Apply LayerNorm
        3. Reshape to 2D: [B, H*W, C] → [B, H, W, C]
        4. Pad feature maps to multiples of window_size (if needed)
        5. Apply cyclic shift (if SW-MSA)
        6. Partition into windows
        7. Apply window attention
        8. Merge windows back
        9. Reverse cyclic shift (if SW-MSA)
        10. Remove padding (if added)
        11. Reshape to sequence: [B, H, W, C] → [B, H*W, C]
        12. Add residual with DropPath
        13. Apply MLP block with residual connection
        """
        H, W = self.input_resolution
        B, L, C = x.shape

        assert L == H * W, f"Input feature has wrong size: {L} vs {H*W}"

        # Save input for residual connection
        shortcut = x

        # ─────────────────────────────────────────────────────────────
        # Block 1: LayerNorm → (Shifted) Window Attention → Residual
        # ─────────────────────────────────────────────────────────────

        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # Pad feature maps to multiples of window_size (MMSegmentation approach)
        # This ensures window_partition works correctly even when H/W are not divisible by window_size
        pad_l = pad_t = 0
        pad_r = (self.window_size - W % self.window_size) % self.window_size
        pad_b = (self.window_size - H % self.window_size) % self.window_size
        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))  # Pad (C, W, H) dimensions: (0, 0, left, right, top, bottom)
        _, Hp, Wp, _ = x.shape

        # Cyclic shift for SW-MSA
        if self.shift_size > 0:
            shifted_x = torch.roll(
                x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)
            )
            # Create attention mask dynamically for SW-MSA (using padded resolution)
            image_mask = create_image_mask(
                (Hp, Wp),  # Use padded resolution for mask
                self.window_size,
                self.shift_size,
                device=x.device,
            )
            mask_windows = window_partition(image_mask, self.window_size)
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(
                attn_mask != 0, float(-100.0)
            ).masked_fill(attn_mask == 0, float(0.0))
        else:
            shifted_x = x
            attn_mask = None

        # Partition windows: [B, Hp, Wp, C] → [B*num_windows, window_size, window_size, C]
        x_windows = window_partition(shifted_x, self.window_size)
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)

        # Apply window attention
        attn_windows = self.attn(x_windows, attn_mask)

        # Merge windows back: [B*num_windows, window_size*window_size, C] → [B, Hp, Wp, C]
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)

        # Reverse cyclic shift for SW-MSA
        if self.shift_size > 0:
            x = torch.roll(
                shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)
            )
        else:
            x = shifted_x

        # Remove padding (crop back to original resolution)
        if pad_r > 0 or pad_b > 0:
            x = x[:, :H, :W, :].contiguous()

        x = x.view(B, H * W, C)

        # Residual connection with stochastic depth
        x = shortcut + self.drop_path1(x)

        # ─────────────────────────────────────────────────────────────
        # Block 2: LayerNorm → MLP → Residual
        # ─────────────────────────────────────────────────────────────

        x = x + self.drop_path2(self.mlp(self.norm2(x)))

        return x
