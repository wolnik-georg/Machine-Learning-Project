2026-01-06 00:19:48,414 - INFO - Logging initialized. Log file: runs/run_261/training.log
2026-01-06 00:19:48,482 - INFO - Experiment directory: runs/run_261
2026-01-06 00:19:48,484 - INFO - Setting random seeds (seed: 42)...
2026-01-06 00:19:48,502 - INFO - ✅ All seeds set to 42 (deterministic=False)
2026-01-06 00:19:48,838 - INFO - Using GPU: NVIDIA GeForce RTX 4070
2026-01-06 00:19:48,839 - INFO - GPU memory: 11.6GB
2026-01-06 00:19:48,840 - INFO - CuDNN benchmark mode enabled
2026-01-06 00:19:48,841 - INFO - Encoder type: deit
2026-01-06 00:19:48,843 - INFO - Data config: {'dataset': 'ADE20K', 'use_batch_for_val': False, 'val_batch': 5, 'batch_size': 8, 'num_workers': 8, 'root': './datasets', 'img_size': 512, 'n_train': None, 'n_test': None, 'stratified': False}
2026-01-06 00:19:48,844 - INFO - Encoder config: {'type': 'deit', 'variant': 'deit_small_patch16_224', 'pretrained': True, 'img_size': 512, 'patch_size': 16, 'embed_dim': 384, 'extract_layers': (2, 5, 8, 11), 'use_gradient_checkpointing': True}
2026-01-06 00:19:48,845 - INFO - Downstream config: {'mode': 'from_scratch', 'head_type': 'upernet', 'num_classes': 150, 'hidden_dim': None, 'freeze_encoder': False, 'use_pretrained': True}
2026-01-06 00:19:48,847 - INFO - Training config: {'learning_rate': 6e-05, 'num_epochs': 160, 'warmup_epochs': 2, 'warmup_start_factor': 0.1, 'weight_decay': 0.01, 'mixed_precision': True, 'compile': False}
2026-01-06 00:19:48,848 - INFO - Loading ADE20K dataset...
2026-01-06 00:19:48,850 - INFO - Using user ADE20K dataset from /home/pml20/datasets/ade20k
2026-01-06 00:19:50,412 - INFO - Loaded ADE20K data from /home/pml20/datasets/ade20k: train=20210, val=2000, test=2000
2026-01-06 00:19:50,413 - INFO - Dataset limits applied
2026-01-06 00:19:50,417 - INFO - Dataset loaded: train=20210 (2527 batches), val=2000 (250 batches)
2026-01-06 00:19:50,418 - INFO - ============================================================
2026-01-06 00:19:50,419 - INFO - Segmentation Pipeline: DEIT_SMALL + UperNet (with deconv) on ADE20K
2026-01-06 00:19:50,420 - INFO - ============================================================
2026-01-06 00:19:50,421 - INFO - Creating deit_small_patch16_224 + UperNet segmentation model...
2026-01-06 00:19:50,422 - INFO - Using deconvolution layers for hierarchical feature extraction
2026-01-06 00:19:51,448 - INFO - Loading pretrained weights from Hugging Face hub (timm/deit_small_patch16_224.fb_in1k)
2026-01-06 00:19:51,784 - INFO - HTTP Request: HEAD https://huggingface.co/timm/deit_small_patch16_224.fb_in1k/resolve/main/model.safetensors "HTTP/1.1 302 Found"
2026-01-06 00:19:51,807 - INFO - [timm/deit_small_patch16_224.fb_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2026-01-06 00:19:51,905 - INFO - Resized position embedding: (14, 14) to (32, 32).
2026-01-06 00:19:52,756 - INFO - Model parameters:
2026-01-06 00:19:52,757 - INFO -   Encoder: 21,983,616
2026-01-06 00:19:52,758 - INFO -   Head: 29,187,222
2026-01-06 00:19:52,759 - INFO -   Total: 51,170,838
2026-01-06 00:19:52,760 - INFO -   Trainable: 51,170,838
2026-01-06 00:19:52,850 - INFO - Mixed precision: CUDA bf16 selected (hardware supported)
2026-01-06 00:19:52,851 - INFO - Optimizer: training all model parameters (encoder + head)
2026-01-06 00:19:52,855 - INFO - LR Scheduler: cosine with 2 warmup epochs
2026-01-06 00:19:52,856 - INFO - Learning rate: 6e-05, min_lr: 6.000000000000001e-07
2026-01-06 00:19:52,858 - INFO - Training configuration:
2026-01-06 00:19:52,859 - INFO -   Epochs: 160
2026-01-06 00:19:52,860 - INFO -   Warmup epochs: 2
2026-01-06 00:19:52,861 - INFO -   Learning rate: 6e-05
2026-01-06 00:19:52,862 - INFO -   Weight decay: 0.01
2026-01-06 00:19:52,863 - INFO -   Freeze encoder: False
2026-01-06 00:19:52,863 - INFO -   Num classes: 150
2026-01-06 00:19:52,864 - INFO -   Mixed precision: torch.bfloat16
2026-01-06 00:19:52,865 - INFO - Starting training...
2026-01-06 00:19:52,866 - INFO - Starting segmentation training...
2026-01-06 00:19:52,867 - INFO - Training for 160 epochs, 2527 batches/epoch
2026-01-06 00:21:23,609 - INFO -   Batch 50/2527: Loss = 3.3099
2026-01-06 00:21:46,898 - INFO -   Batch 100/2527: Loss = 2.4111
2026-01-06 00:22:09,904 - INFO -   Batch 150/2527: Loss = 2.0343
2026-01-06 00:22:32,980 - INFO -   Batch 200/2527: Loss = 2.0202
2026-01-06 00:22:56,145 - INFO -   Batch 250/2527: Loss = 1.8264
2026-01-06 00:23:19,307 - INFO -   Batch 300/2527: Loss = 2.2169
2026-01-06 00:23:42,409 - INFO -   Batch 350/2527: Loss = 1.5380
2026-01-06 00:24:05,530 - INFO -   Batch 400/2527: Loss = 2.2607
2026-01-06 00:24:28,712 - INFO -   Batch 450/2527: Loss = 2.2180
2026-01-06 00:24:51,899 - INFO -   Batch 500/2527: Loss = 2.4435
2026-01-06 00:25:15,559 - INFO -   Batch 550/2527: Loss = 2.3748
2026-01-06 00:25:38,809 - INFO -   Batch 600/2527: Loss = 1.2807
2026-01-06 00:26:02,079 - INFO -   Batch 650/2527: Loss = 1.7005
2026-01-06 00:26:25,230 - INFO -   Batch 700/2527: Loss = 1.8883
2026-01-06 00:26:48,400 - INFO -   Batch 750/2527: Loss = 1.4765
2026-01-06 00:27:11,456 - INFO -   Batch 800/2527: Loss = 1.9379
2026-01-06 00:27:34,582 - INFO -   Batch 850/2527: Loss = 1.1686
2026-01-06 00:27:57,685 - INFO -   Batch 900/2527: Loss = 1.8843
2026-01-06 00:28:21,211 - INFO -   Batch 950/2527: Loss = 1.2849
2026-01-06 00:28:44,519 - INFO -   Batch 1000/2527: Loss = 1.7327
2026-01-06 00:29:07,808 - INFO -   Batch 1050/2527: Loss = 2.2786
2026-01-06 00:29:31,149 - INFO -   Batch 1100/2527: Loss = 1.4399
2026-01-06 00:29:54,290 - INFO -   Batch 1150/2527: Loss = 1.1240
2026-01-06 00:30:17,434 - INFO -   Batch 1200/2527: Loss = 1.5158
2026-01-06 00:30:40,982 - INFO -   Batch 1250/2527: Loss = 1.3981
2026-01-06 00:31:04,262 - INFO -   Batch 1300/2527: Loss = 1.4353
2026-01-06 00:31:27,402 - INFO -   Batch 1350/2527: Loss = 1.3783
2026-01-06 00:31:50,749 - INFO -   Batch 1400/2527: Loss = 1.6324
2026-01-06 00:32:13,949 - INFO -   Batch 1450/2527: Loss = 1.5606
2026-01-06 00:32:37,143 - INFO -   Batch 1500/2527: Loss = 2.0413
2026-01-06 00:33:00,353 - INFO -   Batch 1550/2527: Loss = 1.2605
2026-01-06 00:33:23,609 - INFO -   Batch 1600/2527: Loss = 1.8239
2026-01-06 00:33:46,988 - INFO -   Batch 1650/2527: Loss = 1.2672
2026-01-06 00:34:10,211 - INFO -   Batch 1700/2527: Loss = 0.7975
2026-01-06 00:34:33,342 - INFO -   Batch 1750/2527: Loss = 1.3448
2026-01-06 00:34:56,595 - INFO -   Batch 1800/2527: Loss = 1.1576
2026-01-06 00:35:19,798 - INFO -   Batch 1850/2527: Loss = 1.2313
2026-01-06 00:35:43,057 - INFO -   Batch 1900/2527: Loss = 0.9627
2026-01-06 00:36:06,330 - INFO -   Batch 1950/2527: Loss = 1.5333
2026-01-06 00:36:29,968 - INFO -   Batch 2000/2527: Loss = 1.2391
2026-01-06 00:36:53,051 - INFO -   Batch 2050/2527: Loss = 1.4179
2026-01-06 00:37:16,312 - INFO -   Batch 2100/2527: Loss = 1.1771
2026-01-06 00:37:39,534 - INFO -   Batch 2150/2527: Loss = 1.1865
2026-01-06 00:38:02,637 - INFO -   Batch 2200/2527: Loss = 0.9187
2026-01-06 00:38:25,813 - INFO -   Batch 2250/2527: Loss = 1.3607
2026-01-06 00:38:48,996 - INFO -   Batch 2300/2527: Loss = 1.2418
2026-01-06 00:39:12,263 - INFO -   Batch 2350/2527: Loss = 1.9601
2026-01-06 00:39:35,912 - INFO -   Batch 2400/2527: Loss = 1.0790
2026-01-06 00:39:59,311 - INFO -   Batch 2450/2527: Loss = 1.2233
2026-01-06 00:40:22,450 - INFO -   Batch 2500/2527: Loss = 1.0354
2026-01-06 00:46:30,609 - INFO - Epoch 1/160: Train Loss: 1.6089, Val Loss: 1.1641, Val mIoU: 17.41%, Val PixelAcc: 71.86%
2026-01-06 00:46:30,611 - INFO -   → New best mIoU: 17.41%
2026-01-06 00:46:32,718 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 00:46:32,720 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 00:47:00,056 - INFO -   Batch 50/2527: Loss = 1.3994
2026-01-06 00:47:25,760 - INFO -   Batch 100/2527: Loss = 1.2959
2026-01-06 00:47:51,387 - INFO -   Batch 150/2527: Loss = 1.2457
2026-01-06 00:48:17,128 - INFO -   Batch 200/2527: Loss = 1.1578
2026-01-06 00:48:42,844 - INFO -   Batch 250/2527: Loss = 1.2970
2026-01-06 00:49:08,452 - INFO -   Batch 300/2527: Loss = 0.8879
2026-01-06 00:49:34,705 - INFO -   Batch 350/2527: Loss = 1.2770
2026-01-06 00:50:00,446 - INFO -   Batch 400/2527: Loss = 1.0628
2026-01-06 00:50:26,063 - INFO -   Batch 450/2527: Loss = 0.8233
2026-01-06 00:50:51,842 - INFO -   Batch 500/2527: Loss = 1.0766
2026-01-06 00:51:17,566 - INFO -   Batch 550/2527: Loss = 1.3067
2026-01-06 00:51:43,197 - INFO -   Batch 600/2527: Loss = 1.1681
2026-01-06 00:52:08,977 - INFO -   Batch 650/2527: Loss = 0.8156
2026-01-06 00:52:34,699 - INFO -   Batch 700/2527: Loss = 1.0991
2026-01-06 00:53:00,317 - INFO -   Batch 750/2527: Loss = 0.8965
2026-01-06 00:53:26,440 - INFO -   Batch 800/2527: Loss = 0.9824
2026-01-06 00:53:52,194 - INFO -   Batch 850/2527: Loss = 1.3986
2026-01-06 00:54:17,815 - INFO -   Batch 900/2527: Loss = 0.8736
2026-01-06 00:54:43,579 - INFO -   Batch 950/2527: Loss = 1.3795
2026-01-06 00:55:09,358 - INFO -   Batch 1000/2527: Loss = 1.0444
2026-01-06 00:55:35,335 - INFO -   Batch 1050/2527: Loss = 1.1576
2026-01-06 00:56:01,082 - INFO -   Batch 1100/2527: Loss = 1.3911
2026-01-06 00:56:26,798 - INFO -   Batch 1150/2527: Loss = 0.7639
2026-01-06 00:56:52,411 - INFO -   Batch 1200/2527: Loss = 0.6807
2026-01-06 00:57:18,209 - INFO -   Batch 1250/2527: Loss = 0.8302
2026-01-06 00:57:43,943 - INFO -   Batch 1300/2527: Loss = 1.1689
2026-01-06 00:58:09,580 - INFO -   Batch 1350/2527: Loss = 1.5142
2026-01-06 00:58:35,353 - INFO -   Batch 1400/2527: Loss = 0.7734
2026-01-06 00:59:01,132 - INFO -   Batch 1450/2527: Loss = 1.3567
2026-01-06 00:59:26,747 - INFO -   Batch 1500/2527: Loss = 1.4146
2026-01-06 00:59:52,518 - INFO -   Batch 1550/2527: Loss = 1.5809
2026-01-06 01:00:18,241 - INFO -   Batch 1600/2527: Loss = 0.9877
2026-01-06 01:00:43,857 - INFO -   Batch 1650/2527: Loss = 1.0296
2026-01-06 01:01:09,607 - INFO -   Batch 1700/2527: Loss = 1.0736
2026-01-06 01:01:35,345 - INFO -   Batch 1750/2527: Loss = 1.0285
2026-01-06 01:02:01,348 - INFO -   Batch 1800/2527: Loss = 0.7079
2026-01-06 01:02:27,099 - INFO -   Batch 1850/2527: Loss = 1.1741
2026-01-06 01:02:52,834 - INFO -   Batch 1900/2527: Loss = 0.9241
2026-01-06 01:03:18,436 - INFO -   Batch 1950/2527: Loss = 0.7547
2026-01-06 01:03:44,197 - INFO -   Batch 2000/2527: Loss = 0.9292
2026-01-06 01:04:09,931 - INFO -   Batch 2050/2527: Loss = 0.7385
2026-01-06 01:04:35,560 - INFO -   Batch 2100/2527: Loss = 1.0166
2026-01-06 01:05:01,308 - INFO -   Batch 2150/2527: Loss = 0.7816
2026-01-06 01:05:27,384 - INFO -   Batch 2200/2527: Loss = 0.6712
2026-01-06 01:05:52,995 - INFO -   Batch 2250/2527: Loss = 0.9401
2026-01-06 01:06:18,741 - INFO -   Batch 2300/2527: Loss = 0.8443
2026-01-06 01:06:44,480 - INFO -   Batch 2350/2527: Loss = 0.8365
2026-01-06 01:07:10,103 - INFO -   Batch 2400/2527: Loss = 1.0165
2026-01-06 01:07:35,902 - INFO -   Batch 2450/2527: Loss = 1.0175
2026-01-06 01:08:02,029 - INFO -   Batch 2500/2527: Loss = 0.6375
2026-01-06 01:12:42,729 - INFO - Epoch 2/160: Train Loss: 1.0425, Val Loss: 0.9232, Val mIoU: 27.11%, Val PixelAcc: 74.11%
2026-01-06 01:12:42,731 - INFO -   → New best mIoU: 27.11%
2026-01-06 01:12:44,991 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 01:12:44,993 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 01:13:09,915 - INFO -   Batch 50/2527: Loss = 0.8561
2026-01-06 01:13:32,824 - INFO -   Batch 100/2527: Loss = 0.9038
2026-01-06 01:13:55,759 - INFO -   Batch 150/2527: Loss = 0.7641
2026-01-06 01:14:18,703 - INFO -   Batch 200/2527: Loss = 0.9718
2026-01-06 01:14:41,672 - INFO -   Batch 250/2527: Loss = 0.6733
2026-01-06 01:15:04,625 - INFO -   Batch 300/2527: Loss = 0.6317
2026-01-06 01:15:27,585 - INFO -   Batch 350/2527: Loss = 0.9017
2026-01-06 01:15:50,558 - INFO -   Batch 400/2527: Loss = 0.5429
2026-01-06 01:16:13,546 - INFO -   Batch 450/2527: Loss = 0.5223
2026-01-06 01:16:36,500 - INFO -   Batch 500/2527: Loss = 0.9774
2026-01-06 01:16:59,465 - INFO -   Batch 550/2527: Loss = 0.4671
2026-01-06 01:17:22,425 - INFO -   Batch 600/2527: Loss = 0.9026
2026-01-06 01:17:45,420 - INFO -   Batch 650/2527: Loss = 1.0620
2026-01-06 01:18:08,736 - INFO -   Batch 700/2527: Loss = 0.6142
2026-01-06 01:18:31,710 - INFO -   Batch 750/2527: Loss = 0.9028
2026-01-06 01:18:54,671 - INFO -   Batch 800/2527: Loss = 0.8682
2026-01-06 01:19:17,628 - INFO -   Batch 850/2527: Loss = 0.5060
2026-01-06 01:19:40,595 - INFO -   Batch 900/2527: Loss = 0.8072
2026-01-06 01:20:03,541 - INFO -   Batch 950/2527: Loss = 1.0330
2026-01-06 01:20:27,770 - INFO -   Batch 1000/2527: Loss = 1.3557
2026-01-06 01:20:50,715 - INFO -   Batch 1050/2527: Loss = 0.5149
2026-01-06 01:21:13,695 - INFO -   Batch 1100/2527: Loss = 0.6285
2026-01-06 01:21:37,016 - INFO -   Batch 1150/2527: Loss = 0.8527
2026-01-06 01:21:59,978 - INFO -   Batch 1200/2527: Loss = 0.9905
2026-01-06 01:22:22,938 - INFO -   Batch 1250/2527: Loss = 0.7589
2026-01-06 01:22:45,898 - INFO -   Batch 1300/2527: Loss = 1.0929
2026-01-06 01:23:08,889 - INFO -   Batch 1350/2527: Loss = 0.8948
2026-01-06 01:23:31,840 - INFO -   Batch 1400/2527: Loss = 0.8514
2026-01-06 01:23:55,170 - INFO -   Batch 1450/2527: Loss = 0.6889
2026-01-06 01:24:18,159 - INFO -   Batch 1500/2527: Loss = 0.9781
2026-01-06 01:24:41,108 - INFO -   Batch 1550/2527: Loss = 0.6907
2026-01-06 01:25:04,062 - INFO -   Batch 1600/2527: Loss = 0.9601
2026-01-06 01:25:27,016 - INFO -   Batch 1650/2527: Loss = 0.7687
2026-01-06 01:25:50,009 - INFO -   Batch 1700/2527: Loss = 0.5462
2026-01-06 01:26:12,967 - INFO -   Batch 1750/2527: Loss = 0.8929
2026-01-06 01:26:35,926 - INFO -   Batch 1800/2527: Loss = 1.0400
2026-01-06 01:26:58,913 - INFO -   Batch 1850/2527: Loss = 0.6777
2026-01-06 01:27:21,864 - INFO -   Batch 1900/2527: Loss = 1.1480
2026-01-06 01:27:44,822 - INFO -   Batch 1950/2527: Loss = 1.0444
2026-01-06 01:28:07,768 - INFO -   Batch 2000/2527: Loss = 1.4095
2026-01-06 01:28:30,755 - INFO -   Batch 2050/2527: Loss = 0.6640
2026-01-06 01:28:53,713 - INFO -   Batch 2100/2527: Loss = 1.0131
2026-01-06 01:29:17,051 - INFO -   Batch 2150/2527: Loss = 0.6828
2026-01-06 01:29:40,023 - INFO -   Batch 2200/2527: Loss = 0.7522
2026-01-06 01:30:02,980 - INFO -   Batch 2250/2527: Loss = 0.6061
2026-01-06 01:30:25,932 - INFO -   Batch 2300/2527: Loss = 0.8251
2026-01-06 01:30:48,897 - INFO -   Batch 2350/2527: Loss = 0.5570
2026-01-06 01:31:11,872 - INFO -   Batch 2400/2527: Loss = 0.9258
2026-01-06 01:31:34,825 - INFO -   Batch 2450/2527: Loss = 0.6793
2026-01-06 01:31:57,790 - INFO -   Batch 2500/2527: Loss = 0.5859
2026-01-06 01:36:37,294 - INFO - Epoch 3/160: Train Loss: 0.8677, Val Loss: 0.8610, Val mIoU: 29.95%, Val PixelAcc: 74.85%
2026-01-06 01:36:37,296 - INFO -   → New best mIoU: 29.95%
2026-01-06 01:36:39,498 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 01:36:39,500 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 01:37:05,273 - INFO -   Batch 50/2527: Loss = 0.5893
2026-01-06 01:37:28,869 - INFO -   Batch 100/2527: Loss = 0.7803
2026-01-06 01:37:52,799 - INFO -   Batch 150/2527: Loss = 0.7173
2026-01-06 01:38:16,391 - INFO -   Batch 200/2527: Loss = 0.6927
2026-01-06 01:38:40,144 - INFO -   Batch 250/2527: Loss = 0.6718
2026-01-06 01:39:03,753 - INFO -   Batch 300/2527: Loss = 0.5438
2026-01-06 01:39:27,945 - INFO -   Batch 350/2527: Loss = 0.8123
2026-01-06 01:39:51,803 - INFO -   Batch 400/2527: Loss = 1.0220
2026-01-06 01:40:15,618 - INFO -   Batch 450/2527: Loss = 1.0103
2026-01-06 01:40:39,213 - INFO -   Batch 500/2527: Loss = 0.8125
2026-01-06 01:41:02,854 - INFO -   Batch 550/2527: Loss = 0.9646
2026-01-06 01:41:26,812 - INFO -   Batch 600/2527: Loss = 0.5922
2026-01-06 01:41:50,390 - INFO -   Batch 650/2527: Loss = 0.8576
2026-01-06 01:42:14,138 - INFO -   Batch 700/2527: Loss = 0.6743
2026-01-06 01:42:37,780 - INFO -   Batch 750/2527: Loss = 0.8525
2026-01-06 01:43:01,600 - INFO -   Batch 800/2527: Loss = 0.7446
2026-01-06 01:43:25,464 - INFO -   Batch 850/2527: Loss = 0.7452
2026-01-06 01:43:49,255 - INFO -   Batch 900/2527: Loss = 0.9062
2026-01-06 01:44:12,892 - INFO -   Batch 950/2527: Loss = 0.7959
2026-01-06 01:44:36,513 - INFO -   Batch 1000/2527: Loss = 0.5724
2026-01-06 01:45:00,482 - INFO -   Batch 1050/2527: Loss = 0.5569
2026-01-06 01:45:24,436 - INFO -   Batch 1100/2527: Loss = 0.9738
2026-01-06 01:45:48,200 - INFO -   Batch 1150/2527: Loss = 1.1782
2026-01-06 01:46:11,852 - INFO -   Batch 1200/2527: Loss = 0.8024
2026-01-06 01:46:35,648 - INFO -   Batch 1250/2527: Loss = 0.7415
2026-01-06 01:46:59,516 - INFO -   Batch 1300/2527: Loss = 0.6567
2026-01-06 01:47:23,302 - INFO -   Batch 1350/2527: Loss = 0.7588
2026-01-06 01:47:46,913 - INFO -   Batch 1400/2527: Loss = 0.7180
2026-01-06 01:48:10,551 - INFO -   Batch 1450/2527: Loss = 0.6157
2026-01-06 01:48:34,885 - INFO -   Batch 1500/2527: Loss = 0.7340
2026-01-06 01:48:58,467 - INFO -   Batch 1550/2527: Loss = 0.7524
2026-01-06 01:49:22,231 - INFO -   Batch 1600/2527: Loss = 0.7339
2026-01-06 01:49:45,841 - INFO -   Batch 1650/2527: Loss = 0.8011
2026-01-06 01:50:09,668 - INFO -   Batch 1700/2527: Loss = 0.7275
2026-01-06 01:50:33,541 - INFO -   Batch 1750/2527: Loss = 0.4632
2026-01-06 01:50:57,718 - INFO -   Batch 1800/2527: Loss = 0.9646
2026-01-06 01:51:21,313 - INFO -   Batch 1850/2527: Loss = 0.5790
2026-01-06 01:51:44,943 - INFO -   Batch 1900/2527: Loss = 0.5505
2026-01-06 01:52:08,934 - INFO -   Batch 1950/2527: Loss = 0.7184
2026-01-06 01:52:32,510 - INFO -   Batch 2000/2527: Loss = 0.5507
2026-01-06 01:52:56,282 - INFO -   Batch 2050/2527: Loss = 0.6290
2026-01-06 01:53:19,896 - INFO -   Batch 2100/2527: Loss = 0.5843
2026-01-06 01:53:43,736 - INFO -   Batch 2150/2527: Loss = 0.8760
2026-01-06 01:54:07,602 - INFO -   Batch 2200/2527: Loss = 0.5837
2026-01-06 01:54:31,464 - INFO -   Batch 2250/2527: Loss = 0.8159
2026-01-06 01:54:55,051 - INFO -   Batch 2300/2527: Loss = 0.8544
2026-01-06 01:55:18,687 - INFO -   Batch 2350/2527: Loss = 0.5895
2026-01-06 01:55:42,653 - INFO -   Batch 2400/2527: Loss = 0.6253
2026-01-06 01:56:06,236 - INFO -   Batch 2450/2527: Loss = 0.8545
2026-01-06 01:56:29,997 - INFO -   Batch 2500/2527: Loss = 0.8490
2026-01-06 02:01:26,442 - INFO - Epoch 4/160: Train Loss: 0.7089, Val Loss: 0.8422, Val mIoU: 32.86%, Val PixelAcc: 75.79%
2026-01-06 02:01:26,444 - INFO -   → New best mIoU: 32.86%
2026-01-06 02:01:28,620 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 02:01:28,622 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 02:01:54,835 - INFO -   Batch 50/2527: Loss = 0.4697
2026-01-06 02:02:19,974 - INFO -   Batch 100/2527: Loss = 0.5520
2026-01-06 02:02:44,936 - INFO -   Batch 150/2527: Loss = 0.3788
2026-01-06 02:03:09,776 - INFO -   Batch 200/2527: Loss = 0.5464
2026-01-06 02:03:34,605 - INFO -   Batch 250/2527: Loss = 0.4714
2026-01-06 02:03:59,639 - INFO -   Batch 300/2527: Loss = 0.5917
2026-01-06 02:04:24,487 - INFO -   Batch 350/2527: Loss = 0.5971
2026-01-06 02:04:49,705 - INFO -   Batch 400/2527: Loss = 0.5645
2026-01-06 02:05:14,831 - INFO -   Batch 450/2527: Loss = 0.8525
2026-01-06 02:05:39,819 - INFO -   Batch 500/2527: Loss = 0.7106
2026-01-06 02:06:04,644 - INFO -   Batch 550/2527: Loss = 0.4167
2026-01-06 02:06:29,490 - INFO -   Batch 600/2527: Loss = 0.4397
2026-01-06 02:06:54,212 - INFO -   Batch 650/2527: Loss = 1.0419
2026-01-06 02:07:19,439 - INFO -   Batch 700/2527: Loss = 0.4683
2026-01-06 02:07:44,601 - INFO -   Batch 750/2527: Loss = 0.5386
2026-01-06 02:08:09,574 - INFO -   Batch 800/2527: Loss = 0.5067
2026-01-06 02:08:34,409 - INFO -   Batch 850/2527: Loss = 0.4712
2026-01-06 02:08:59,246 - INFO -   Batch 900/2527: Loss = 0.6248
2026-01-06 02:09:24,278 - INFO -   Batch 950/2527: Loss = 0.3694
2026-01-06 02:09:49,155 - INFO -   Batch 1000/2527: Loss = 0.6207
2026-01-06 02:10:14,025 - INFO -   Batch 1050/2527: Loss = 0.6531
2026-01-06 02:10:39,168 - INFO -   Batch 1100/2527: Loss = 0.5984
2026-01-06 02:11:04,177 - INFO -   Batch 1150/2527: Loss = 0.5103
2026-01-06 02:11:28,996 - INFO -   Batch 1200/2527: Loss = 0.7644
2026-01-06 02:11:53,837 - INFO -   Batch 1250/2527: Loss = 0.5743
2026-01-06 02:12:18,546 - INFO -   Batch 1300/2527: Loss = 0.6888
2026-01-06 02:12:43,388 - INFO -   Batch 1350/2527: Loss = 0.9831
2026-01-06 02:13:08,551 - INFO -   Batch 1400/2527: Loss = 0.5361
2026-01-06 02:13:33,916 - INFO -   Batch 1450/2527: Loss = 0.6312
2026-01-06 02:13:58,744 - INFO -   Batch 1500/2527: Loss = 0.5585
2026-01-06 02:14:23,577 - INFO -   Batch 1550/2527: Loss = 0.5727
2026-01-06 02:14:48,614 - INFO -   Batch 1600/2527: Loss = 0.6649
2026-01-06 02:15:13,474 - INFO -   Batch 1650/2527: Loss = 0.7548
2026-01-06 02:15:38,298 - INFO -   Batch 1700/2527: Loss = 0.9023
2026-01-06 02:16:03,418 - INFO -   Batch 1750/2527: Loss = 0.5195
2026-01-06 02:16:28,392 - INFO -   Batch 1800/2527: Loss = 0.4656
2026-01-06 02:16:53,615 - INFO -   Batch 1850/2527: Loss = 0.7730
2026-01-06 02:17:18,539 - INFO -   Batch 1900/2527: Loss = 0.6959
2026-01-06 02:17:43,268 - INFO -   Batch 1950/2527: Loss = 0.5549
2026-01-06 02:18:08,142 - INFO -   Batch 2000/2527: Loss = 0.4165
2026-01-06 02:18:33,249 - INFO -   Batch 2050/2527: Loss = 0.5958
2026-01-06 02:18:58,241 - INFO -   Batch 2100/2527: Loss = 0.6044
2026-01-06 02:19:23,445 - INFO -   Batch 2150/2527: Loss = 0.4267
2026-01-06 02:19:48,267 - INFO -   Batch 2200/2527: Loss = 0.5872
2026-01-06 02:20:13,298 - INFO -   Batch 2250/2527: Loss = 0.9566
2026-01-06 02:20:38,148 - INFO -   Batch 2300/2527: Loss = 0.7158
2026-01-06 02:21:02,994 - INFO -   Batch 2350/2527: Loss = 0.6670
2026-01-06 02:21:28,111 - INFO -   Batch 2400/2527: Loss = 0.4822
2026-01-06 02:21:53,093 - INFO -   Batch 2450/2527: Loss = 0.7009
2026-01-06 02:22:17,945 - INFO -   Batch 2500/2527: Loss = 0.6375
2026-01-06 02:26:57,913 - INFO - Epoch 5/160: Train Loss: 0.6176, Val Loss: 0.8265, Val mIoU: 33.73%, Val PixelAcc: 76.39%
2026-01-06 02:26:57,915 - INFO -   → New best mIoU: 33.73%
2026-01-06 02:27:00,137 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 02:27:00,139 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 02:27:26,588 - INFO -   Batch 50/2527: Loss = 0.6854
2026-01-06 02:27:51,565 - INFO -   Batch 100/2527: Loss = 0.6205
2026-01-06 02:28:16,791 - INFO -   Batch 150/2527: Loss = 0.5429
2026-01-06 02:28:41,651 - INFO -   Batch 200/2527: Loss = 0.4518
2026-01-06 02:29:06,774 - INFO -   Batch 250/2527: Loss = 0.4860
2026-01-06 02:29:31,797 - INFO -   Batch 300/2527: Loss = 0.7783
2026-01-06 02:29:56,622 - INFO -   Batch 350/2527: Loss = 0.5499
2026-01-06 02:30:21,487 - INFO -   Batch 400/2527: Loss = 0.5381
2026-01-06 02:30:46,221 - INFO -   Batch 450/2527: Loss = 0.6098
2026-01-06 02:31:11,054 - INFO -   Batch 500/2527: Loss = 0.2914
2026-01-06 02:31:36,180 - INFO -   Batch 550/2527: Loss = 0.5386
2026-01-06 02:32:01,183 - INFO -   Batch 600/2527: Loss = 0.3147
2026-01-06 02:32:26,011 - INFO -   Batch 650/2527: Loss = 0.5025
2026-01-06 02:32:50,849 - INFO -   Batch 700/2527: Loss = 0.4622
2026-01-06 02:33:15,903 - INFO -   Batch 750/2527: Loss = 0.5819
2026-01-06 02:33:40,760 - INFO -   Batch 800/2527: Loss = 0.3425
2026-01-06 02:34:05,611 - INFO -   Batch 850/2527: Loss = 0.4937
2026-01-06 02:34:31,113 - INFO -   Batch 900/2527: Loss = 0.3501
2026-01-06 02:34:56,097 - INFO -   Batch 950/2527: Loss = 0.8112
2026-01-06 02:35:20,913 - INFO -   Batch 1000/2527: Loss = 0.6360
2026-01-06 02:35:45,758 - INFO -   Batch 1050/2527: Loss = 0.4901
2026-01-06 02:36:10,487 - INFO -   Batch 1100/2527: Loss = 0.4757
2026-01-06 02:36:35,328 - INFO -   Batch 1150/2527: Loss = 0.5072
2026-01-06 02:37:00,487 - INFO -   Batch 1200/2527: Loss = 0.5657
2026-01-06 02:37:25,467 - INFO -   Batch 1250/2527: Loss = 0.4950
2026-01-06 02:37:50,647 - INFO -   Batch 1300/2527: Loss = 0.4939
2026-01-06 02:38:15,513 - INFO -   Batch 1350/2527: Loss = 0.7016
2026-01-06 02:38:40,530 - INFO -   Batch 1400/2527: Loss = 0.5280
2026-01-06 02:39:05,368 - INFO -   Batch 1450/2527: Loss = 0.6255
2026-01-06 02:39:30,205 - INFO -   Batch 1500/2527: Loss = 0.3854
2026-01-06 02:39:55,341 - INFO -   Batch 1550/2527: Loss = 0.3891
2026-01-06 02:40:20,683 - INFO -   Batch 1600/2527: Loss = 0.4254
2026-01-06 02:40:45,524 - INFO -   Batch 1650/2527: Loss = 1.0832
2026-01-06 02:41:10,369 - INFO -   Batch 1700/2527: Loss = 0.4476
2026-01-06 02:41:35,092 - INFO -   Batch 1750/2527: Loss = 0.5723
2026-01-06 02:41:59,939 - INFO -   Batch 1800/2527: Loss = 0.6085
2026-01-06 02:42:25,047 - INFO -   Batch 1850/2527: Loss = 0.5595
2026-01-06 02:42:50,040 - INFO -   Batch 1900/2527: Loss = 0.5439
2026-01-06 02:43:14,860 - INFO -   Batch 1950/2527: Loss = 0.4572
2026-01-06 02:43:39,714 - INFO -   Batch 2000/2527: Loss = 0.5440
2026-01-06 02:44:04,734 - INFO -   Batch 2050/2527: Loss = 0.3193
2026-01-06 02:44:29,574 - INFO -   Batch 2100/2527: Loss = 0.4428
2026-01-06 02:44:54,411 - INFO -   Batch 2150/2527: Loss = 0.4953
2026-01-06 02:45:19,554 - INFO -   Batch 2200/2527: Loss = 0.7741
2026-01-06 02:45:44,523 - INFO -   Batch 2250/2527: Loss = 0.7510
2026-01-06 02:46:09,334 - INFO -   Batch 2300/2527: Loss = 0.4768
2026-01-06 02:46:34,537 - INFO -   Batch 2350/2527: Loss = 0.5098
2026-01-06 02:46:59,274 - INFO -   Batch 2400/2527: Loss = 0.3767
2026-01-06 02:47:24,103 - INFO -   Batch 2450/2527: Loss = 0.7985
2026-01-06 02:47:49,230 - INFO -   Batch 2500/2527: Loss = 0.8308
2026-01-06 02:52:42,214 - INFO - Epoch 6/160: Train Loss: 0.5373, Val Loss: 0.8315, Val mIoU: 36.91%, Val PixelAcc: 77.00%
2026-01-06 02:52:42,216 - INFO -   → New best mIoU: 36.91%
2026-01-06 02:52:44,544 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 02:52:44,546 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 02:53:09,513 - INFO -   Batch 50/2527: Loss = 0.5227
2026-01-06 02:53:32,427 - INFO -   Batch 100/2527: Loss = 0.4382
2026-01-06 02:53:55,357 - INFO -   Batch 150/2527: Loss = 0.4528
2026-01-06 02:54:18,294 - INFO -   Batch 200/2527: Loss = 0.5656
2026-01-06 02:54:41,597 - INFO -   Batch 250/2527: Loss = 0.3580
2026-01-06 02:55:04,546 - INFO -   Batch 300/2527: Loss = 0.3031
2026-01-06 02:55:27,505 - INFO -   Batch 350/2527: Loss = 0.4809
2026-01-06 02:55:50,492 - INFO -   Batch 400/2527: Loss = 0.7717
2026-01-06 02:56:13,475 - INFO -   Batch 450/2527: Loss = 0.6315
2026-01-06 02:56:36,435 - INFO -   Batch 500/2527: Loss = 0.4735
2026-01-06 02:56:59,802 - INFO -   Batch 550/2527: Loss = 0.4984
2026-01-06 02:57:22,762 - INFO -   Batch 600/2527: Loss = 0.4168
2026-01-06 02:57:45,722 - INFO -   Batch 650/2527: Loss = 0.4732
2026-01-06 02:58:08,686 - INFO -   Batch 700/2527: Loss = 0.4565
2026-01-06 02:58:31,675 - INFO -   Batch 750/2527: Loss = 0.3963
2026-01-06 02:58:54,633 - INFO -   Batch 800/2527: Loss = 0.4979
2026-01-06 02:59:17,613 - INFO -   Batch 850/2527: Loss = 0.5177
2026-01-06 02:59:40,581 - INFO -   Batch 900/2527: Loss = 0.3884
2026-01-06 03:00:03,549 - INFO -   Batch 950/2527: Loss = 0.3682
2026-01-06 03:00:26,515 - INFO -   Batch 1000/2527: Loss = 0.5501
2026-01-06 03:00:49,473 - INFO -   Batch 1050/2527: Loss = 0.4650
2026-01-06 03:01:12,438 - INFO -   Batch 1100/2527: Loss = 0.5230
2026-01-06 03:01:35,398 - INFO -   Batch 1150/2527: Loss = 0.5997
2026-01-06 03:01:58,350 - INFO -   Batch 1200/2527: Loss = 0.4144
2026-01-06 03:02:21,678 - INFO -   Batch 1250/2527: Loss = 0.5406
2026-01-06 03:02:44,640 - INFO -   Batch 1300/2527: Loss = 0.4507
2026-01-06 03:03:07,601 - INFO -   Batch 1350/2527: Loss = 0.7240
2026-01-06 03:03:30,558 - INFO -   Batch 1400/2527: Loss = 0.5353
2026-01-06 03:03:53,521 - INFO -   Batch 1450/2527: Loss = 0.4191
2026-01-06 03:04:16,473 - INFO -   Batch 1500/2527: Loss = 0.5186
2026-01-06 03:04:39,435 - INFO -   Batch 1550/2527: Loss = 0.4181
2026-01-06 03:05:02,394 - INFO -   Batch 1600/2527: Loss = 0.6974
2026-01-06 03:05:25,352 - INFO -   Batch 1650/2527: Loss = 0.6330
2026-01-06 03:05:48,692 - INFO -   Batch 1700/2527: Loss = 0.3991
2026-01-06 03:06:11,684 - INFO -   Batch 1750/2527: Loss = 0.3476
2026-01-06 03:06:34,647 - INFO -   Batch 1800/2527: Loss = 0.5356
2026-01-06 03:06:57,603 - INFO -   Batch 1850/2527: Loss = 0.4404
2026-01-06 03:07:20,564 - INFO -   Batch 1900/2527: Loss = 0.4368
2026-01-06 03:07:43,927 - INFO -   Batch 1950/2527: Loss = 0.3135
2026-01-06 03:08:06,884 - INFO -   Batch 2000/2527: Loss = 0.3999
2026-01-06 03:08:29,863 - INFO -   Batch 2050/2527: Loss = 0.3759
2026-01-06 03:08:52,823 - INFO -   Batch 2100/2527: Loss = 0.5669
2026-01-06 03:09:15,780 - INFO -   Batch 2150/2527: Loss = 0.3472
2026-01-06 03:09:38,740 - INFO -   Batch 2200/2527: Loss = 0.4641
2026-01-06 03:10:01,710 - INFO -   Batch 2250/2527: Loss = 0.5371
2026-01-06 03:10:24,671 - INFO -   Batch 2300/2527: Loss = 0.4249
2026-01-06 03:10:47,627 - INFO -   Batch 2350/2527: Loss = 0.3899
2026-01-06 03:11:10,577 - INFO -   Batch 2400/2527: Loss = 0.7126
2026-01-06 03:11:33,541 - INFO -   Batch 2450/2527: Loss = 0.4687
2026-01-06 03:11:56,499 - INFO -   Batch 2500/2527: Loss = 0.4525
2026-01-06 03:16:36,136 - INFO - Epoch 7/160: Train Loss: 0.4808, Val Loss: 0.8245, Val mIoU: 36.44%, Val PixelAcc: 77.22%
2026-01-06 03:17:01,430 - INFO -   Batch 50/2527: Loss = 0.6000
2026-01-06 03:17:25,005 - INFO -   Batch 100/2527: Loss = 0.3215
2026-01-06 03:17:48,943 - INFO -   Batch 150/2527: Loss = 0.4333
2026-01-06 03:18:12,865 - INFO -   Batch 200/2527: Loss = 0.5277
2026-01-06 03:18:36,613 - INFO -   Batch 250/2527: Loss = 0.3771
2026-01-06 03:19:00,240 - INFO -   Batch 300/2527: Loss = 0.2628
2026-01-06 03:19:24,058 - INFO -   Batch 350/2527: Loss = 0.3599
2026-01-06 03:19:47,934 - INFO -   Batch 400/2527: Loss = 0.5890
2026-01-06 03:20:11,750 - INFO -   Batch 450/2527: Loss = 0.4492
2026-01-06 03:20:35,353 - INFO -   Batch 500/2527: Loss = 0.3128
2026-01-06 03:20:58,980 - INFO -   Batch 550/2527: Loss = 0.4144
2026-01-06 03:21:23,302 - INFO -   Batch 600/2527: Loss = 0.2855
2026-01-06 03:21:46,904 - INFO -   Batch 650/2527: Loss = 0.3508
2026-01-06 03:22:10,707 - INFO -   Batch 700/2527: Loss = 0.4139
2026-01-06 03:22:34,320 - INFO -   Batch 750/2527: Loss = 0.4942
2026-01-06 03:22:58,159 - INFO -   Batch 800/2527: Loss = 0.4696
2026-01-06 03:23:22,028 - INFO -   Batch 850/2527: Loss = 0.3299
2026-01-06 03:23:46,194 - INFO -   Batch 900/2527: Loss = 0.3886
2026-01-06 03:24:09,797 - INFO -   Batch 950/2527: Loss = 0.4472
2026-01-06 03:24:33,420 - INFO -   Batch 1000/2527: Loss = 0.2957
2026-01-06 03:24:57,388 - INFO -   Batch 1050/2527: Loss = 0.5488
2026-01-06 03:25:20,971 - INFO -   Batch 1100/2527: Loss = 0.3214
2026-01-06 03:25:44,739 - INFO -   Batch 1150/2527: Loss = 0.3937
2026-01-06 03:26:08,345 - INFO -   Batch 1200/2527: Loss = 0.3007
2026-01-06 03:26:32,198 - INFO -   Batch 1250/2527: Loss = 0.2906
2026-01-06 03:26:56,066 - INFO -   Batch 1300/2527: Loss = 0.4017
2026-01-06 03:27:19,856 - INFO -   Batch 1350/2527: Loss = 0.3184
2026-01-06 03:27:43,482 - INFO -   Batch 1400/2527: Loss = 0.3975
2026-01-06 03:28:07,101 - INFO -   Batch 1450/2527: Loss = 0.4317
2026-01-06 03:28:31,077 - INFO -   Batch 1500/2527: Loss = 0.5407
2026-01-06 03:28:54,656 - INFO -   Batch 1550/2527: Loss = 0.7673
2026-01-06 03:29:18,410 - INFO -   Batch 1600/2527: Loss = 0.2984
2026-01-06 03:29:42,402 - INFO -   Batch 1650/2527: Loss = 0.3465
2026-01-06 03:30:06,215 - INFO -   Batch 1700/2527: Loss = 0.2879
2026-01-06 03:30:30,078 - INFO -   Batch 1750/2527: Loss = 0.4499
2026-01-06 03:30:53,859 - INFO -   Batch 1800/2527: Loss = 0.4223
2026-01-06 03:31:17,468 - INFO -   Batch 1850/2527: Loss = 0.2653
2026-01-06 03:31:41,098 - INFO -   Batch 1900/2527: Loss = 0.4342
2026-01-06 03:32:05,092 - INFO -   Batch 1950/2527: Loss = 0.5690
2026-01-06 03:32:28,696 - INFO -   Batch 2000/2527: Loss = 0.4037
2026-01-06 03:32:52,818 - INFO -   Batch 2050/2527: Loss = 0.4751
2026-01-06 03:33:16,449 - INFO -   Batch 2100/2527: Loss = 0.5014
2026-01-06 03:33:40,277 - INFO -   Batch 2150/2527: Loss = 0.3605
2026-01-06 03:34:04,171 - INFO -   Batch 2200/2527: Loss = 0.3848
2026-01-06 03:34:27,983 - INFO -   Batch 2250/2527: Loss = 0.3952
2026-01-06 03:34:51,585 - INFO -   Batch 2300/2527: Loss = 0.4598
2026-01-06 03:35:15,578 - INFO -   Batch 2350/2527: Loss = 0.4510
2026-01-06 03:35:39,528 - INFO -   Batch 2400/2527: Loss = 0.4852
2026-01-06 03:36:03,147 - INFO -   Batch 2450/2527: Loss = 0.3508
2026-01-06 03:36:26,903 - INFO -   Batch 2500/2527: Loss = 0.4903
2026-01-06 03:41:19,267 - INFO - Epoch 8/160: Train Loss: 0.4308, Val Loss: 0.8420, Val mIoU: 37.40%, Val PixelAcc: 77.46%
2026-01-06 03:41:19,269 - INFO -   → New best mIoU: 37.40%
2026-01-06 03:41:21,503 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 03:41:21,506 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 03:41:47,551 - INFO -   Batch 50/2527: Loss = 0.3820
2026-01-06 03:42:12,626 - INFO -   Batch 100/2527: Loss = 0.5039
2026-01-06 03:42:37,599 - INFO -   Batch 150/2527: Loss = 0.3361
2026-01-06 03:43:02,423 - INFO -   Batch 200/2527: Loss = 0.3493
2026-01-06 03:43:27,258 - INFO -   Batch 250/2527: Loss = 0.3952
2026-01-06 03:43:52,276 - INFO -   Batch 300/2527: Loss = 0.3088
2026-01-06 03:44:17,130 - INFO -   Batch 350/2527: Loss = 0.4110
2026-01-06 03:44:41,976 - INFO -   Batch 400/2527: Loss = 0.2809
2026-01-06 03:45:07,108 - INFO -   Batch 450/2527: Loss = 0.5096
2026-01-06 03:45:32,086 - INFO -   Batch 500/2527: Loss = 0.4654
2026-01-06 03:45:57,274 - INFO -   Batch 550/2527: Loss = 0.3331
2026-01-06 03:46:22,131 - INFO -   Batch 600/2527: Loss = 0.3422
2026-01-06 03:46:46,904 - INFO -   Batch 650/2527: Loss = 0.4256
2026-01-06 03:47:11,749 - INFO -   Batch 700/2527: Loss = 0.3788
2026-01-06 03:47:36,917 - INFO -   Batch 750/2527: Loss = 0.3835
2026-01-06 03:48:01,927 - INFO -   Batch 800/2527: Loss = 0.3556
2026-01-06 03:48:26,762 - INFO -   Batch 850/2527: Loss = 0.4032
2026-01-06 03:48:51,610 - INFO -   Batch 900/2527: Loss = 0.5168
2026-01-06 03:49:16,983 - INFO -   Batch 950/2527: Loss = 0.3177
2026-01-06 03:49:41,816 - INFO -   Batch 1000/2527: Loss = 0.3334
2026-01-06 03:50:06,667 - INFO -   Batch 1050/2527: Loss = 0.3909
2026-01-06 03:50:31,795 - INFO -   Batch 1100/2527: Loss = 0.3534
2026-01-06 03:50:56,789 - INFO -   Batch 1150/2527: Loss = 0.4112
2026-01-06 03:51:21,720 - INFO -   Batch 1200/2527: Loss = 0.3281
2026-01-06 03:51:46,963 - INFO -   Batch 1250/2527: Loss = 0.4858
2026-01-06 03:52:11,690 - INFO -   Batch 1300/2527: Loss = 0.5976
2026-01-06 03:52:36,594 - INFO -   Batch 1350/2527: Loss = 0.4199
2026-01-06 03:53:01,717 - INFO -   Batch 1400/2527: Loss = 0.4648
2026-01-06 03:53:26,692 - INFO -   Batch 1450/2527: Loss = 0.3536
2026-01-06 03:53:51,527 - INFO -   Batch 1500/2527: Loss = 0.5732
2026-01-06 03:54:16,380 - INFO -   Batch 1550/2527: Loss = 0.4994
2026-01-06 03:54:41,412 - INFO -   Batch 1600/2527: Loss = 0.3194
2026-01-06 03:55:06,289 - INFO -   Batch 1650/2527: Loss = 0.4022
2026-01-06 03:55:31,139 - INFO -   Batch 1700/2527: Loss = 0.3347
2026-01-06 03:55:56,265 - INFO -   Batch 1750/2527: Loss = 0.3193
2026-01-06 03:56:21,240 - INFO -   Batch 1800/2527: Loss = 0.3668
2026-01-06 03:56:46,088 - INFO -   Batch 1850/2527: Loss = 0.3305
2026-01-06 03:57:10,959 - INFO -   Batch 1900/2527: Loss = 0.2198
2026-01-06 03:57:35,688 - INFO -   Batch 1950/2527: Loss = 0.4259
2026-01-06 03:58:00,914 - INFO -   Batch 2000/2527: Loss = 0.5432
2026-01-06 03:58:26,048 - INFO -   Batch 2050/2527: Loss = 0.2931
2026-01-06 03:58:51,037 - INFO -   Batch 2100/2527: Loss = 0.4420
2026-01-06 03:59:15,891 - INFO -   Batch 2150/2527: Loss = 0.6968
2026-01-06 03:59:40,740 - INFO -   Batch 2200/2527: Loss = 0.3400
2026-01-06 04:00:05,773 - INFO -   Batch 2250/2527: Loss = 0.2609
2026-01-06 04:00:30,658 - INFO -   Batch 2300/2527: Loss = 0.2638
2026-01-06 04:00:55,510 - INFO -   Batch 2350/2527: Loss = 0.4869
2026-01-06 04:01:21,027 - INFO -   Batch 2400/2527: Loss = 0.3573
2026-01-06 04:01:46,016 - INFO -   Batch 2450/2527: Loss = 0.3783
2026-01-06 04:02:10,842 - INFO -   Batch 2500/2527: Loss = 0.6633
2026-01-06 04:06:51,647 - INFO - Epoch 9/160: Train Loss: 0.3960, Val Loss: 0.8370, Val mIoU: 38.07%, Val PixelAcc: 77.97%
2026-01-06 04:06:51,649 - INFO -   → New best mIoU: 38.07%
2026-01-06 04:06:53,926 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 04:06:53,929 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 04:07:20,413 - INFO -   Batch 50/2527: Loss = 0.3688
2026-01-06 04:07:45,405 - INFO -   Batch 100/2527: Loss = 0.4219
2026-01-06 04:08:10,215 - INFO -   Batch 150/2527: Loss = 0.3777
2026-01-06 04:08:35,407 - INFO -   Batch 200/2527: Loss = 0.2936
2026-01-06 04:09:00,532 - INFO -   Batch 250/2527: Loss = 0.2813
2026-01-06 04:09:25,503 - INFO -   Batch 300/2527: Loss = 0.3399
2026-01-06 04:09:50,319 - INFO -   Batch 350/2527: Loss = 0.2574
2026-01-06 04:10:15,254 - INFO -   Batch 400/2527: Loss = 0.4280
2026-01-06 04:10:40,030 - INFO -   Batch 450/2527: Loss = 0.3256
2026-01-06 04:11:04,880 - INFO -   Batch 500/2527: Loss = 0.3202
2026-01-06 04:11:30,027 - INFO -   Batch 550/2527: Loss = 0.4118
2026-01-06 04:11:55,046 - INFO -   Batch 600/2527: Loss = 0.4002
2026-01-06 04:12:19,877 - INFO -   Batch 650/2527: Loss = 0.2676
2026-01-06 04:12:44,740 - INFO -   Batch 700/2527: Loss = 0.4120
2026-01-06 04:13:09,912 - INFO -   Batch 750/2527: Loss = 0.2733
2026-01-06 04:13:34,744 - INFO -   Batch 800/2527: Loss = 0.3218
2026-01-06 04:13:59,645 - INFO -   Batch 850/2527: Loss = 0.2814
2026-01-06 04:14:25,180 - INFO -   Batch 900/2527: Loss = 0.4023
2026-01-06 04:14:50,293 - INFO -   Batch 950/2527: Loss = 0.4152
2026-01-06 04:15:15,161 - INFO -   Batch 1000/2527: Loss = 0.4845
2026-01-06 04:15:39,997 - INFO -   Batch 1050/2527: Loss = 0.3645
2026-01-06 04:16:04,738 - INFO -   Batch 1100/2527: Loss = 0.2955
2026-01-06 04:16:29,619 - INFO -   Batch 1150/2527: Loss = 0.2816
2026-01-06 04:16:54,740 - INFO -   Batch 1200/2527: Loss = 0.3866
2026-01-06 04:17:19,763 - INFO -   Batch 1250/2527: Loss = 0.4872
2026-01-06 04:17:44,587 - INFO -   Batch 1300/2527: Loss = 0.6237
2026-01-06 04:18:09,777 - INFO -   Batch 1350/2527: Loss = 0.2718
2026-01-06 04:18:34,858 - INFO -   Batch 1400/2527: Loss = 0.3438
2026-01-06 04:18:59,725 - INFO -   Batch 1450/2527: Loss = 0.3845
2026-01-06 04:19:24,524 - INFO -   Batch 1500/2527: Loss = 0.2696
2026-01-06 04:19:49,720 - INFO -   Batch 1550/2527: Loss = 0.3073
2026-01-06 04:20:15,086 - INFO -   Batch 1600/2527: Loss = 0.3412
2026-01-06 04:20:39,944 - INFO -   Batch 1650/2527: Loss = 0.4234
2026-01-06 04:21:04,791 - INFO -   Batch 1700/2527: Loss = 0.2300
2026-01-06 04:21:29,530 - INFO -   Batch 1750/2527: Loss = 0.3055
2026-01-06 04:21:54,380 - INFO -   Batch 1800/2527: Loss = 0.3418
2026-01-06 04:22:19,497 - INFO -   Batch 1850/2527: Loss = 0.4991
2026-01-06 04:22:44,460 - INFO -   Batch 1900/2527: Loss = 0.4284
2026-01-06 04:23:09,300 - INFO -   Batch 1950/2527: Loss = 0.5433
2026-01-06 04:23:34,126 - INFO -   Batch 2000/2527: Loss = 0.4090
2026-01-06 04:23:59,144 - INFO -   Batch 2050/2527: Loss = 0.3147
2026-01-06 04:24:24,006 - INFO -   Batch 2100/2527: Loss = 0.3945
2026-01-06 04:24:48,854 - INFO -   Batch 2150/2527: Loss = 0.3875
2026-01-06 04:25:14,013 - INFO -   Batch 2200/2527: Loss = 0.5443
2026-01-06 04:25:38,960 - INFO -   Batch 2250/2527: Loss = 0.3956
2026-01-06 04:26:03,786 - INFO -   Batch 2300/2527: Loss = 0.3140
2026-01-06 04:26:29,054 - INFO -   Batch 2350/2527: Loss = 0.3282
2026-01-06 04:26:53,789 - INFO -   Batch 2400/2527: Loss = 0.2672
2026-01-06 04:27:18,652 - INFO -   Batch 2450/2527: Loss = 0.4130
2026-01-06 04:27:43,813 - INFO -   Batch 2500/2527: Loss = 0.3165
2026-01-06 04:32:36,654 - INFO - Epoch 10/160: Train Loss: 0.3685, Val Loss: 0.8800, Val mIoU: 37.05%, Val PixelAcc: 77.32%
2026-01-06 04:32:38,931 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_10.pth
2026-01-06 04:32:38,934 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_10.pth
2026-01-06 04:33:03,965 - INFO -   Batch 50/2527: Loss = 0.3257
2026-01-06 04:33:26,879 - INFO -   Batch 100/2527: Loss = 0.3861
2026-01-06 04:33:49,812 - INFO -   Batch 150/2527: Loss = 0.3232
2026-01-06 04:34:12,776 - INFO -   Batch 200/2527: Loss = 0.3556
2026-01-06 04:34:35,722 - INFO -   Batch 250/2527: Loss = 0.2508
2026-01-06 04:34:58,679 - INFO -   Batch 300/2527: Loss = 0.2848
2026-01-06 04:35:21,635 - INFO -   Batch 350/2527: Loss = 0.2263
2026-01-06 04:35:44,997 - INFO -   Batch 400/2527: Loss = 0.2742
2026-01-06 04:36:07,988 - INFO -   Batch 450/2527: Loss = 0.2475
2026-01-06 04:36:30,952 - INFO -   Batch 500/2527: Loss = 0.3544
2026-01-06 04:36:53,919 - INFO -   Batch 550/2527: Loss = 0.4301
2026-01-06 04:37:16,878 - INFO -   Batch 600/2527: Loss = 0.2133
2026-01-06 04:37:39,872 - INFO -   Batch 650/2527: Loss = 0.3306
2026-01-06 04:38:02,836 - INFO -   Batch 700/2527: Loss = 0.3248
2026-01-06 04:38:25,792 - INFO -   Batch 750/2527: Loss = 0.3539
2026-01-06 04:38:48,751 - INFO -   Batch 800/2527: Loss = 0.3327
2026-01-06 04:39:12,090 - INFO -   Batch 850/2527: Loss = 0.2745
2026-01-06 04:39:35,057 - INFO -   Batch 900/2527: Loss = 0.4019
2026-01-06 04:39:58,024 - INFO -   Batch 950/2527: Loss = 0.2867
2026-01-06 04:40:21,044 - INFO -   Batch 1000/2527: Loss = 0.2604
2026-01-06 04:40:44,001 - INFO -   Batch 1050/2527: Loss = 0.3277
2026-01-06 04:41:06,956 - INFO -   Batch 1100/2527: Loss = 0.4323
2026-01-06 04:41:30,257 - INFO -   Batch 1150/2527: Loss = 0.5041
2026-01-06 04:41:53,208 - INFO -   Batch 1200/2527: Loss = 0.2817
2026-01-06 04:42:16,161 - INFO -   Batch 1250/2527: Loss = 0.3382
2026-01-06 04:42:39,110 - INFO -   Batch 1300/2527: Loss = 0.3213
2026-01-06 04:43:02,058 - INFO -   Batch 1350/2527: Loss = 0.3529
2026-01-06 04:43:25,013 - INFO -   Batch 1400/2527: Loss = 0.2036
2026-01-06 04:43:47,963 - INFO -   Batch 1450/2527: Loss = 0.2522
2026-01-06 04:44:10,926 - INFO -   Batch 1500/2527: Loss = 0.2613
2026-01-06 04:44:33,882 - INFO -   Batch 1550/2527: Loss = 0.3193
2026-01-06 04:44:56,843 - INFO -   Batch 1600/2527: Loss = 0.2567
2026-01-06 04:45:19,791 - INFO -   Batch 1650/2527: Loss = 0.2190
2026-01-06 04:45:42,767 - INFO -   Batch 1700/2527: Loss = 0.2689
2026-01-06 04:46:05,722 - INFO -   Batch 1750/2527: Loss = 0.3925
2026-01-06 04:46:28,683 - INFO -   Batch 1800/2527: Loss = 0.3414
2026-01-06 04:46:51,996 - INFO -   Batch 1850/2527: Loss = 0.3050
2026-01-06 04:47:14,952 - INFO -   Batch 1900/2527: Loss = 0.5083
2026-01-06 04:47:37,908 - INFO -   Batch 1950/2527: Loss = 0.4095
2026-01-06 04:48:00,871 - INFO -   Batch 2000/2527: Loss = 0.3002
2026-01-06 04:48:23,835 - INFO -   Batch 2050/2527: Loss = 0.2821
2026-01-06 04:48:46,818 - INFO -   Batch 2100/2527: Loss = 0.3570
2026-01-06 04:49:09,774 - INFO -   Batch 2150/2527: Loss = 0.3779
2026-01-06 04:49:32,728 - INFO -   Batch 2200/2527: Loss = 0.3480
2026-01-06 04:49:55,685 - INFO -   Batch 2250/2527: Loss = 0.1975
2026-01-06 04:50:19,021 - INFO -   Batch 2300/2527: Loss = 0.3891
2026-01-06 04:50:41,972 - INFO -   Batch 2350/2527: Loss = 0.2936
2026-01-06 04:51:04,930 - INFO -   Batch 2400/2527: Loss = 0.3006
2026-01-06 04:51:27,928 - INFO -   Batch 2450/2527: Loss = 0.3389
2026-01-06 04:51:50,887 - INFO -   Batch 2500/2527: Loss = 0.2740
2026-01-06 04:56:30,103 - INFO - Epoch 11/160: Train Loss: 0.3390, Val Loss: 0.8603, Val mIoU: 37.48%, Val PixelAcc: 77.73%
2026-01-06 04:56:55,894 - INFO -   Batch 50/2527: Loss = 0.2951
2026-01-06 04:57:19,500 - INFO -   Batch 100/2527: Loss = 0.2429
2026-01-06 04:57:43,429 - INFO -   Batch 150/2527: Loss = 0.2944
2026-01-06 04:58:07,002 - INFO -   Batch 200/2527: Loss = 0.3003
2026-01-06 04:58:30,751 - INFO -   Batch 250/2527: Loss = 0.3706
2026-01-06 04:58:54,355 - INFO -   Batch 300/2527: Loss = 0.3057
2026-01-06 04:59:18,171 - INFO -   Batch 350/2527: Loss = 0.2238
2026-01-06 04:59:42,035 - INFO -   Batch 400/2527: Loss = 0.2833
2026-01-06 05:00:05,820 - INFO -   Batch 450/2527: Loss = 0.2330
2026-01-06 05:00:29,464 - INFO -   Batch 500/2527: Loss = 0.3740
2026-01-06 05:00:53,085 - INFO -   Batch 550/2527: Loss = 0.2467
2026-01-06 05:01:17,048 - INFO -   Batch 600/2527: Loss = 0.2839
2026-01-06 05:01:40,629 - INFO -   Batch 650/2527: Loss = 0.3585
2026-01-06 05:02:04,389 - INFO -   Batch 700/2527: Loss = 0.3917
2026-01-06 05:02:28,002 - INFO -   Batch 750/2527: Loss = 0.3308
2026-01-06 05:02:52,198 - INFO -   Batch 800/2527: Loss = 0.2300
2026-01-06 05:03:16,075 - INFO -   Batch 850/2527: Loss = 0.3065
2026-01-06 05:03:39,862 - INFO -   Batch 900/2527: Loss = 0.2300
2026-01-06 05:04:03,464 - INFO -   Batch 950/2527: Loss = 0.1993
2026-01-06 05:04:27,098 - INFO -   Batch 1000/2527: Loss = 0.4105
2026-01-06 05:04:51,056 - INFO -   Batch 1050/2527: Loss = 0.2567
2026-01-06 05:05:14,659 - INFO -   Batch 1100/2527: Loss = 0.2738
2026-01-06 05:05:38,422 - INFO -   Batch 1150/2527: Loss = 0.2712
2026-01-06 05:06:02,396 - INFO -   Batch 1200/2527: Loss = 0.1747
2026-01-06 05:06:26,232 - INFO -   Batch 1250/2527: Loss = 0.1974
2026-01-06 05:06:50,090 - INFO -   Batch 1300/2527: Loss = 0.2391
2026-01-06 05:07:13,887 - INFO -   Batch 1350/2527: Loss = 0.3782
2026-01-06 05:07:37,488 - INFO -   Batch 1400/2527: Loss = 0.3349
2026-01-06 05:08:01,120 - INFO -   Batch 1450/2527: Loss = 0.3301
2026-01-06 05:08:25,461 - INFO -   Batch 1500/2527: Loss = 0.3358
2026-01-06 05:08:49,031 - INFO -   Batch 1550/2527: Loss = 0.2174
2026-01-06 05:09:12,788 - INFO -   Batch 1600/2527: Loss = 0.3718
2026-01-06 05:09:36,402 - INFO -   Batch 1650/2527: Loss = 0.3729
2026-01-06 05:10:00,225 - INFO -   Batch 1700/2527: Loss = 0.1926
2026-01-06 05:10:24,082 - INFO -   Batch 1750/2527: Loss = 0.2441
2026-01-06 05:10:47,870 - INFO -   Batch 1800/2527: Loss = 0.2561
2026-01-06 05:11:11,472 - INFO -   Batch 1850/2527: Loss = 0.3615
2026-01-06 05:11:35,130 - INFO -   Batch 1900/2527: Loss = 0.4014
2026-01-06 05:11:59,107 - INFO -   Batch 1950/2527: Loss = 0.2940
2026-01-06 05:12:22,692 - INFO -   Batch 2000/2527: Loss = 0.2398
2026-01-06 05:12:46,496 - INFO -   Batch 2050/2527: Loss = 0.3184
2026-01-06 05:13:10,127 - INFO -   Batch 2100/2527: Loss = 0.2885
2026-01-06 05:13:33,951 - INFO -   Batch 2150/2527: Loss = 0.2587
2026-01-06 05:13:57,832 - INFO -   Batch 2200/2527: Loss = 0.3239
2026-01-06 05:14:21,996 - INFO -   Batch 2250/2527: Loss = 0.2120
2026-01-06 05:14:45,630 - INFO -   Batch 2300/2527: Loss = 0.5032
2026-01-06 05:15:09,303 - INFO -   Batch 2350/2527: Loss = 0.3102
2026-01-06 05:15:33,303 - INFO -   Batch 2400/2527: Loss = 0.1796
2026-01-06 05:15:56,889 - INFO -   Batch 2450/2527: Loss = 0.3771
2026-01-06 05:16:20,669 - INFO -   Batch 2500/2527: Loss = 0.2903
2026-01-06 05:21:12,617 - INFO - Epoch 12/160: Train Loss: 0.3175, Val Loss: 0.9046, Val mIoU: 37.04%, Val PixelAcc: 77.21%
2026-01-06 05:21:38,761 - INFO -   Batch 50/2527: Loss = 0.3643
2026-01-06 05:22:04,217 - INFO -   Batch 100/2527: Loss = 0.3784
2026-01-06 05:22:29,190 - INFO -   Batch 150/2527: Loss = 0.3070
2026-01-06 05:22:54,006 - INFO -   Batch 200/2527: Loss = 0.2186
2026-01-06 05:23:18,840 - INFO -   Batch 250/2527: Loss = 0.1184
2026-01-06 05:23:43,868 - INFO -   Batch 300/2527: Loss = 0.2295
2026-01-06 05:24:08,695 - INFO -   Batch 350/2527: Loss = 0.3254
2026-01-06 05:24:33,916 - INFO -   Batch 400/2527: Loss = 0.2207
2026-01-06 05:24:59,057 - INFO -   Batch 450/2527: Loss = 0.3442
2026-01-06 05:25:24,041 - INFO -   Batch 500/2527: Loss = 0.2793
2026-01-06 05:25:48,853 - INFO -   Batch 550/2527: Loss = 0.2231
2026-01-06 05:26:13,683 - INFO -   Batch 600/2527: Loss = 0.3237
2026-01-06 05:26:38,420 - INFO -   Batch 650/2527: Loss = 0.2332
2026-01-06 05:27:03,260 - INFO -   Batch 700/2527: Loss = 0.2293
2026-01-06 05:27:28,382 - INFO -   Batch 750/2527: Loss = 0.3290
2026-01-06 05:27:53,348 - INFO -   Batch 800/2527: Loss = 0.3218
2026-01-06 05:28:18,165 - INFO -   Batch 850/2527: Loss = 0.2177
2026-01-06 05:28:43,016 - INFO -   Batch 900/2527: Loss = 0.5167
2026-01-06 05:29:08,059 - INFO -   Batch 950/2527: Loss = 0.3452
2026-01-06 05:29:32,893 - INFO -   Batch 1000/2527: Loss = 0.3634
2026-01-06 05:29:57,750 - INFO -   Batch 1050/2527: Loss = 0.2384
2026-01-06 05:30:22,868 - INFO -   Batch 1100/2527: Loss = 0.1774
2026-01-06 05:30:48,213 - INFO -   Batch 1150/2527: Loss = 0.3393
2026-01-06 05:31:13,038 - INFO -   Batch 1200/2527: Loss = 0.2626
2026-01-06 05:31:37,885 - INFO -   Batch 1250/2527: Loss = 0.2139
2026-01-06 05:32:02,611 - INFO -   Batch 1300/2527: Loss = 0.2293
2026-01-06 05:32:27,449 - INFO -   Batch 1350/2527: Loss = 0.2373
2026-01-06 05:32:52,559 - INFO -   Batch 1400/2527: Loss = 0.3130
2026-01-06 05:33:17,561 - INFO -   Batch 1450/2527: Loss = 0.2093
2026-01-06 05:33:42,395 - INFO -   Batch 1500/2527: Loss = 0.3112
2026-01-06 05:34:07,618 - INFO -   Batch 1550/2527: Loss = 0.2902
2026-01-06 05:34:32,642 - INFO -   Batch 1600/2527: Loss = 0.3153
2026-01-06 05:34:57,466 - INFO -   Batch 1650/2527: Loss = 0.1344
2026-01-06 05:35:22,299 - INFO -   Batch 1700/2527: Loss = 0.2996
2026-01-06 05:35:47,412 - INFO -   Batch 1750/2527: Loss = 0.5101
2026-01-06 05:36:12,381 - INFO -   Batch 1800/2527: Loss = 0.4503
2026-01-06 05:36:37,587 - INFO -   Batch 1850/2527: Loss = 0.2953
2026-01-06 05:37:02,462 - INFO -   Batch 1900/2527: Loss = 0.2695
2026-01-06 05:37:27,182 - INFO -   Batch 1950/2527: Loss = 0.5641
2026-01-06 05:37:52,020 - INFO -   Batch 2000/2527: Loss = 0.2923
2026-01-06 05:38:17,135 - INFO -   Batch 2050/2527: Loss = 0.3652
2026-01-06 05:38:42,215 - INFO -   Batch 2100/2527: Loss = 0.3269
2026-01-06 05:39:07,062 - INFO -   Batch 2150/2527: Loss = 0.4981
2026-01-06 05:39:31,976 - INFO -   Batch 2200/2527: Loss = 0.2324
2026-01-06 05:39:57,166 - INFO -   Batch 2250/2527: Loss = 0.3496
2026-01-06 05:40:22,165 - INFO -   Batch 2300/2527: Loss = 0.4150
2026-01-06 05:40:47,128 - INFO -   Batch 2350/2527: Loss = 0.3080
2026-01-06 05:41:12,395 - INFO -   Batch 2400/2527: Loss = 0.3249
2026-01-06 05:41:37,472 - INFO -   Batch 2450/2527: Loss = 0.3888
2026-01-06 05:42:02,324 - INFO -   Batch 2500/2527: Loss = 0.4722
2026-01-06 05:46:42,386 - INFO - Epoch 13/160: Train Loss: 0.3096, Val Loss: 0.8975, Val mIoU: 37.38%, Val PixelAcc: 77.89%
2026-01-06 05:47:08,652 - INFO -   Batch 50/2527: Loss = 0.4305
2026-01-06 05:47:33,984 - INFO -   Batch 100/2527: Loss = 0.2363
2026-01-06 05:47:58,763 - INFO -   Batch 150/2527: Loss = 0.2693
2026-01-06 05:48:23,589 - INFO -   Batch 200/2527: Loss = 0.2355
2026-01-06 05:48:48,694 - INFO -   Batch 250/2527: Loss = 0.1658
2026-01-06 05:49:13,702 - INFO -   Batch 300/2527: Loss = 0.2138
2026-01-06 05:49:38,514 - INFO -   Batch 350/2527: Loss = 0.2803
2026-01-06 05:50:03,350 - INFO -   Batch 400/2527: Loss = 0.2781
2026-01-06 05:50:28,099 - INFO -   Batch 450/2527: Loss = 0.2531
2026-01-06 05:50:53,295 - INFO -   Batch 500/2527: Loss = 0.3205
2026-01-06 05:51:18,444 - INFO -   Batch 550/2527: Loss = 0.2882
2026-01-06 05:51:43,416 - INFO -   Batch 600/2527: Loss = 0.2509
2026-01-06 05:52:08,236 - INFO -   Batch 650/2527: Loss = 0.2144
2026-01-06 05:52:33,079 - INFO -   Batch 700/2527: Loss = 0.2570
2026-01-06 05:52:58,131 - INFO -   Batch 750/2527: Loss = 0.3028
2026-01-06 05:53:23,337 - INFO -   Batch 800/2527: Loss = 0.3263
2026-01-06 05:53:48,174 - INFO -   Batch 850/2527: Loss = 0.2492
2026-01-06 05:54:13,273 - INFO -   Batch 900/2527: Loss = 0.2395
2026-01-06 05:54:38,268 - INFO -   Batch 950/2527: Loss = 0.3162
2026-01-06 05:55:03,107 - INFO -   Batch 1000/2527: Loss = 0.2098
2026-01-06 05:55:27,958 - INFO -   Batch 1050/2527: Loss = 0.2914
2026-01-06 05:55:52,687 - INFO -   Batch 1100/2527: Loss = 0.1875
2026-01-06 05:56:17,518 - INFO -   Batch 1150/2527: Loss = 0.1732
2026-01-06 05:56:42,633 - INFO -   Batch 1200/2527: Loss = 0.2336
2026-01-06 05:57:07,653 - INFO -   Batch 1250/2527: Loss = 0.2168
2026-01-06 05:57:32,469 - INFO -   Batch 1300/2527: Loss = 0.2635
2026-01-06 05:57:57,304 - INFO -   Batch 1350/2527: Loss = 0.2758
2026-01-06 05:58:22,357 - INFO -   Batch 1400/2527: Loss = 0.1423
2026-01-06 05:58:47,215 - INFO -   Batch 1450/2527: Loss = 0.3775
2026-01-06 05:59:12,412 - INFO -   Batch 1500/2527: Loss = 0.2141
2026-01-06 05:59:37,574 - INFO -   Batch 1550/2527: Loss = 0.2165
2026-01-06 06:00:02,553 - INFO -   Batch 1600/2527: Loss = 0.2524
2026-01-06 06:00:27,368 - INFO -   Batch 1650/2527: Loss = 0.3847
2026-01-06 06:00:52,211 - INFO -   Batch 1700/2527: Loss = 0.2017
2026-01-06 06:01:16,936 - INFO -   Batch 1750/2527: Loss = 0.1940
2026-01-06 06:01:41,781 - INFO -   Batch 1800/2527: Loss = 0.3164
2026-01-06 06:02:06,918 - INFO -   Batch 1850/2527: Loss = 0.3096
2026-01-06 06:02:31,915 - INFO -   Batch 1900/2527: Loss = 0.2663
2026-01-06 06:02:57,080 - INFO -   Batch 1950/2527: Loss = 0.2445
2026-01-06 06:03:21,966 - INFO -   Batch 2000/2527: Loss = 0.2945
2026-01-06 06:03:46,992 - INFO -   Batch 2050/2527: Loss = 0.3299
2026-01-06 06:04:11,838 - INFO -   Batch 2100/2527: Loss = 0.3375
2026-01-06 06:04:36,671 - INFO -   Batch 2150/2527: Loss = 0.3741
2026-01-06 06:05:01,792 - INFO -   Batch 2200/2527: Loss = 0.3328
2026-01-06 06:05:27,134 - INFO -   Batch 2250/2527: Loss = 0.2502
2026-01-06 06:05:51,984 - INFO -   Batch 2300/2527: Loss = 0.2446
2026-01-06 06:06:16,830 - INFO -   Batch 2350/2527: Loss = 0.4692
2026-01-06 06:06:41,560 - INFO -   Batch 2400/2527: Loss = 0.2006
2026-01-06 06:07:06,408 - INFO -   Batch 2450/2527: Loss = 0.3709
2026-01-06 06:07:31,529 - INFO -   Batch 2500/2527: Loss = 0.2796
2026-01-06 06:12:23,957 - INFO - Epoch 14/160: Train Loss: 0.2817, Val Loss: 0.8928, Val mIoU: 38.86%, Val PixelAcc: 78.42%
2026-01-06 06:12:23,959 - INFO -   → New best mIoU: 38.86%
2026-01-06 06:12:26,175 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 06:12:26,178 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 06:12:51,084 - INFO -   Batch 50/2527: Loss = 0.2597
2026-01-06 06:13:14,003 - INFO -   Batch 100/2527: Loss = 0.3874
2026-01-06 06:13:36,934 - INFO -   Batch 150/2527: Loss = 0.3350
2026-01-06 06:13:59,870 - INFO -   Batch 200/2527: Loss = 0.2479
2026-01-06 06:14:22,819 - INFO -   Batch 250/2527: Loss = 0.2614
2026-01-06 06:14:45,792 - INFO -   Batch 300/2527: Loss = 0.2549
2026-01-06 06:15:08,752 - INFO -   Batch 350/2527: Loss = 0.2510
2026-01-06 06:15:31,711 - INFO -   Batch 400/2527: Loss = 0.2425
2026-01-06 06:15:55,025 - INFO -   Batch 450/2527: Loss = 0.2545
2026-01-06 06:16:17,997 - INFO -   Batch 500/2527: Loss = 0.2379
2026-01-06 06:16:40,957 - INFO -   Batch 550/2527: Loss = 0.2385
2026-01-06 06:17:03,918 - INFO -   Batch 600/2527: Loss = 0.3926
2026-01-06 06:17:26,882 - INFO -   Batch 650/2527: Loss = 0.2509
2026-01-06 06:17:49,849 - INFO -   Batch 700/2527: Loss = 0.2730
2026-01-06 06:18:12,814 - INFO -   Batch 750/2527: Loss = 0.1555
2026-01-06 06:18:35,770 - INFO -   Batch 800/2527: Loss = 0.2324
2026-01-06 06:18:59,083 - INFO -   Batch 850/2527: Loss = 0.1837
2026-01-06 06:19:22,038 - INFO -   Batch 900/2527: Loss = 0.2682
2026-01-06 06:19:45,009 - INFO -   Batch 950/2527: Loss = 0.1947
2026-01-06 06:20:08,005 - INFO -   Batch 1000/2527: Loss = 0.2862
2026-01-06 06:20:30,969 - INFO -   Batch 1050/2527: Loss = 0.1670
2026-01-06 06:20:53,935 - INFO -   Batch 1100/2527: Loss = 0.2940
2026-01-06 06:21:17,263 - INFO -   Batch 1150/2527: Loss = 0.2789
2026-01-06 06:21:40,213 - INFO -   Batch 1200/2527: Loss = 0.3015
2026-01-06 06:22:03,165 - INFO -   Batch 1250/2527: Loss = 0.3161
2026-01-06 06:22:26,131 - INFO -   Batch 1300/2527: Loss = 0.2623
2026-01-06 06:22:49,127 - INFO -   Batch 1350/2527: Loss = 0.3259
2026-01-06 06:23:12,048 - INFO -   Batch 1400/2527: Loss = 0.2547
2026-01-06 06:23:35,382 - INFO -   Batch 1450/2527: Loss = 0.2712
2026-01-06 06:23:58,303 - INFO -   Batch 1500/2527: Loss = 0.2924
2026-01-06 06:24:21,239 - INFO -   Batch 1550/2527: Loss = 0.4026
2026-01-06 06:24:44,174 - INFO -   Batch 1600/2527: Loss = 0.3601
2026-01-06 06:25:07,300 - INFO -   Batch 1650/2527: Loss = 0.3094
2026-01-06 06:25:30,244 - INFO -   Batch 1700/2527: Loss = 0.3239
2026-01-06 06:25:53,188 - INFO -   Batch 1750/2527: Loss = 0.3028
2026-01-06 06:26:16,138 - INFO -   Batch 1800/2527: Loss = 0.3377
2026-01-06 06:26:39,106 - INFO -   Batch 1850/2527: Loss = 0.3049
2026-01-06 06:27:02,419 - INFO -   Batch 1900/2527: Loss = 0.3238
2026-01-06 06:27:25,365 - INFO -   Batch 1950/2527: Loss = 0.2452
2026-01-06 06:27:48,310 - INFO -   Batch 2000/2527: Loss = 0.3373
2026-01-06 06:28:11,270 - INFO -   Batch 2050/2527: Loss = 0.2376
2026-01-06 06:28:34,221 - INFO -   Batch 2100/2527: Loss = 0.2274
2026-01-06 06:28:57,173 - INFO -   Batch 2150/2527: Loss = 0.1805
2026-01-06 06:29:20,127 - INFO -   Batch 2200/2527: Loss = 0.2621
2026-01-06 06:29:43,097 - INFO -   Batch 2250/2527: Loss = 0.3861
2026-01-06 06:30:06,410 - INFO -   Batch 2300/2527: Loss = 0.2313
2026-01-06 06:30:29,358 - INFO -   Batch 2350/2527: Loss = 0.2734
2026-01-06 06:30:52,304 - INFO -   Batch 2400/2527: Loss = 0.2449
2026-01-06 06:31:15,261 - INFO -   Batch 2450/2527: Loss = 0.2452
2026-01-06 06:31:38,209 - INFO -   Batch 2500/2527: Loss = 0.3345
2026-01-06 06:36:17,757 - INFO - Epoch 15/160: Train Loss: 0.2818, Val Loss: 1.0343, Val mIoU: 36.04%, Val PixelAcc: 74.97%
2026-01-06 06:36:43,342 - INFO -   Batch 50/2527: Loss = 0.3520
2026-01-06 06:37:06,920 - INFO -   Batch 100/2527: Loss = 0.2306
2026-01-06 06:37:30,852 - INFO -   Batch 150/2527: Loss = 0.2968
2026-01-06 06:37:54,413 - INFO -   Batch 200/2527: Loss = 0.2040
2026-01-06 06:38:18,158 - INFO -   Batch 250/2527: Loss = 0.3906
2026-01-06 06:38:41,780 - INFO -   Batch 300/2527: Loss = 0.3625
2026-01-06 06:39:05,593 - INFO -   Batch 350/2527: Loss = 0.2492
2026-01-06 06:39:29,443 - INFO -   Batch 400/2527: Loss = 0.1839
2026-01-06 06:39:53,219 - INFO -   Batch 450/2527: Loss = 0.3133
2026-01-06 06:40:16,817 - INFO -   Batch 500/2527: Loss = 0.1958
2026-01-06 06:40:40,443 - INFO -   Batch 550/2527: Loss = 0.2020
2026-01-06 06:41:04,443 - INFO -   Batch 600/2527: Loss = 0.2082
2026-01-06 06:41:28,009 - INFO -   Batch 650/2527: Loss = 0.1853
2026-01-06 06:41:51,762 - INFO -   Batch 700/2527: Loss = 0.2741
2026-01-06 06:42:15,403 - INFO -   Batch 750/2527: Loss = 0.2850
2026-01-06 06:42:39,589 - INFO -   Batch 800/2527: Loss = 0.2135
2026-01-06 06:43:03,444 - INFO -   Batch 850/2527: Loss = 0.2037
2026-01-06 06:43:27,222 - INFO -   Batch 900/2527: Loss = 0.2331
2026-01-06 06:43:50,826 - INFO -   Batch 950/2527: Loss = 0.2836
2026-01-06 06:44:14,473 - INFO -   Batch 1000/2527: Loss = 0.2481
2026-01-06 06:44:38,463 - INFO -   Batch 1050/2527: Loss = 0.3108
2026-01-06 06:45:02,036 - INFO -   Batch 1100/2527: Loss = 0.2581
2026-01-06 06:45:25,798 - INFO -   Batch 1150/2527: Loss = 0.1875
2026-01-06 06:45:49,782 - INFO -   Batch 1200/2527: Loss = 0.3243
2026-01-06 06:46:13,632 - INFO -   Batch 1250/2527: Loss = 0.1926
2026-01-06 06:46:37,488 - INFO -   Batch 1300/2527: Loss = 0.2459
2026-01-06 06:47:01,276 - INFO -   Batch 1350/2527: Loss = 0.2695
2026-01-06 06:47:24,882 - INFO -   Batch 1400/2527: Loss = 0.2824
2026-01-06 06:47:48,513 - INFO -   Batch 1450/2527: Loss = 0.1966
2026-01-06 06:48:12,857 - INFO -   Batch 1500/2527: Loss = 0.3609
2026-01-06 06:48:36,436 - INFO -   Batch 1550/2527: Loss = 0.1439
2026-01-06 06:49:00,200 - INFO -   Batch 1600/2527: Loss = 0.2074
2026-01-06 06:49:23,847 - INFO -   Batch 1650/2527: Loss = 0.2311
2026-01-06 06:49:47,667 - INFO -   Batch 1700/2527: Loss = 0.3557
2026-01-06 06:50:11,528 - INFO -   Batch 1750/2527: Loss = 0.2644
2026-01-06 06:50:35,301 - INFO -   Batch 1800/2527: Loss = 0.2766
2026-01-06 06:50:58,887 - INFO -   Batch 1850/2527: Loss = 0.2714
2026-01-06 06:51:22,524 - INFO -   Batch 1900/2527: Loss = 0.3317
2026-01-06 06:51:46,451 - INFO -   Batch 1950/2527: Loss = 0.3111
2026-01-06 06:52:10,007 - INFO -   Batch 2000/2527: Loss = 0.2404
2026-01-06 06:52:33,758 - INFO -   Batch 2050/2527: Loss = 0.2733
2026-01-06 06:52:57,556 - INFO -   Batch 2100/2527: Loss = 0.2181
2026-01-06 06:53:21,355 - INFO -   Batch 2150/2527: Loss = 0.3340
2026-01-06 06:53:45,208 - INFO -   Batch 2200/2527: Loss = 0.3637
2026-01-06 06:54:09,360 - INFO -   Batch 2250/2527: Loss = 0.2785
2026-01-06 06:54:32,950 - INFO -   Batch 2300/2527: Loss = 0.2349
2026-01-06 06:54:56,560 - INFO -   Batch 2350/2527: Loss = 0.2767
2026-01-06 06:55:20,617 - INFO -   Batch 2400/2527: Loss = 0.2782
2026-01-06 06:55:44,195 - INFO -   Batch 2450/2527: Loss = 0.2322
2026-01-06 06:56:07,977 - INFO -   Batch 2500/2527: Loss = 0.2603
2026-01-06 07:01:00,378 - INFO - Epoch 16/160: Train Loss: 0.2621, Val Loss: 0.9153, Val mIoU: 38.46%, Val PixelAcc: 77.97%
2026-01-06 07:01:26,390 - INFO -   Batch 50/2527: Loss = 0.2203
2026-01-06 07:01:51,495 - INFO -   Batch 100/2527: Loss = 0.2001
2026-01-06 07:02:16,871 - INFO -   Batch 150/2527: Loss = 0.2072
2026-01-06 07:02:41,686 - INFO -   Batch 200/2527: Loss = 0.2055
2026-01-06 07:03:06,545 - INFO -   Batch 250/2527: Loss = 0.2778
2026-01-06 07:03:31,584 - INFO -   Batch 300/2527: Loss = 0.2892
2026-01-06 07:03:56,414 - INFO -   Batch 350/2527: Loss = 0.1795
2026-01-06 07:04:21,275 - INFO -   Batch 400/2527: Loss = 0.1696
2026-01-06 07:04:46,737 - INFO -   Batch 450/2527: Loss = 0.2516
2026-01-06 07:05:11,731 - INFO -   Batch 500/2527: Loss = 0.2482
2026-01-06 07:05:36,544 - INFO -   Batch 550/2527: Loss = 0.2022
2026-01-06 07:06:01,388 - INFO -   Batch 600/2527: Loss = 0.2101
2026-01-06 07:06:26,115 - INFO -   Batch 650/2527: Loss = 0.1814
2026-01-06 07:06:50,957 - INFO -   Batch 700/2527: Loss = 0.2721
2026-01-06 07:07:16,088 - INFO -   Batch 750/2527: Loss = 0.2238
2026-01-06 07:07:41,086 - INFO -   Batch 800/2527: Loss = 0.2154
2026-01-06 07:08:05,931 - INFO -   Batch 850/2527: Loss = 0.1781
2026-01-06 07:08:30,768 - INFO -   Batch 900/2527: Loss = 0.2429
2026-01-06 07:08:55,773 - INFO -   Batch 950/2527: Loss = 0.3217
2026-01-06 07:09:20,588 - INFO -   Batch 1000/2527: Loss = 0.1412
2026-01-06 07:09:45,460 - INFO -   Batch 1050/2527: Loss = 0.2842
2026-01-06 07:10:10,582 - INFO -   Batch 1100/2527: Loss = 0.1907
2026-01-06 07:10:35,964 - INFO -   Batch 1150/2527: Loss = 0.2668
2026-01-06 07:11:00,785 - INFO -   Batch 1200/2527: Loss = 0.1904
2026-01-06 07:11:25,630 - INFO -   Batch 1250/2527: Loss = 0.2170
2026-01-06 07:11:50,388 - INFO -   Batch 1300/2527: Loss = 0.3290
2026-01-06 07:12:15,249 - INFO -   Batch 1350/2527: Loss = 0.2051
2026-01-06 07:12:40,367 - INFO -   Batch 1400/2527: Loss = 0.2596
2026-01-06 07:13:05,355 - INFO -   Batch 1450/2527: Loss = 0.2270
2026-01-06 07:13:30,171 - INFO -   Batch 1500/2527: Loss = 0.3665
2026-01-06 07:13:55,021 - INFO -   Batch 1550/2527: Loss = 0.2518
2026-01-06 07:14:20,418 - INFO -   Batch 1600/2527: Loss = 0.2004
2026-01-06 07:14:45,270 - INFO -   Batch 1650/2527: Loss = 0.2109
2026-01-06 07:15:10,116 - INFO -   Batch 1700/2527: Loss = 0.2448
2026-01-06 07:15:35,243 - INFO -   Batch 1750/2527: Loss = 0.2779
2026-01-06 07:16:00,211 - INFO -   Batch 1800/2527: Loss = 0.2722
2026-01-06 07:16:25,048 - INFO -   Batch 1850/2527: Loss = 0.1897
2026-01-06 07:16:50,263 - INFO -   Batch 1900/2527: Loss = 0.2363
2026-01-06 07:17:15,023 - INFO -   Batch 1950/2527: Loss = 0.2317
2026-01-06 07:17:39,862 - INFO -   Batch 2000/2527: Loss = 0.2398
2026-01-06 07:18:04,995 - INFO -   Batch 2050/2527: Loss = 0.1535
2026-01-06 07:18:29,975 - INFO -   Batch 2100/2527: Loss = 0.3098
2026-01-06 07:18:54,792 - INFO -   Batch 2150/2527: Loss = 0.2204
2026-01-06 07:19:19,632 - INFO -   Batch 2200/2527: Loss = 0.2027
2026-01-06 07:19:44,670 - INFO -   Batch 2250/2527: Loss = 0.2090
2026-01-06 07:20:09,507 - INFO -   Batch 2300/2527: Loss = 0.1997
2026-01-06 07:20:34,353 - INFO -   Batch 2350/2527: Loss = 0.2190
2026-01-06 07:20:59,475 - INFO -   Batch 2400/2527: Loss = 0.2815
2026-01-06 07:21:24,445 - INFO -   Batch 2450/2527: Loss = 0.3064
2026-01-06 07:21:49,271 - INFO -   Batch 2500/2527: Loss = 0.2752
2026-01-06 07:26:30,059 - INFO - Epoch 17/160: Train Loss: 0.2431, Val Loss: 0.9116, Val mIoU: 38.92%, Val PixelAcc: 78.37%
2026-01-06 07:26:30,061 - INFO -   → New best mIoU: 38.92%
2026-01-06 07:26:32,269 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 07:26:32,271 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 07:26:58,511 - INFO -   Batch 50/2527: Loss = 0.2508
2026-01-06 07:27:23,863 - INFO -   Batch 100/2527: Loss = 0.2495
2026-01-06 07:27:48,669 - INFO -   Batch 150/2527: Loss = 0.1618
2026-01-06 07:28:13,522 - INFO -   Batch 200/2527: Loss = 0.1590
2026-01-06 07:28:38,637 - INFO -   Batch 250/2527: Loss = 0.1482
2026-01-06 07:29:03,634 - INFO -   Batch 300/2527: Loss = 0.2129
2026-01-06 07:29:28,457 - INFO -   Batch 350/2527: Loss = 0.3323
2026-01-06 07:29:53,304 - INFO -   Batch 400/2527: Loss = 0.2565
2026-01-06 07:30:18,046 - INFO -   Batch 450/2527: Loss = 0.2642
2026-01-06 07:30:43,251 - INFO -   Batch 500/2527: Loss = 0.2641
2026-01-06 07:31:08,368 - INFO -   Batch 550/2527: Loss = 0.3340
2026-01-06 07:31:33,345 - INFO -   Batch 600/2527: Loss = 0.1294
2026-01-06 07:31:58,202 - INFO -   Batch 650/2527: Loss = 0.2450
2026-01-06 07:32:23,046 - INFO -   Batch 700/2527: Loss = 0.2476
2026-01-06 07:32:48,075 - INFO -   Batch 750/2527: Loss = 0.2518
2026-01-06 07:33:13,284 - INFO -   Batch 800/2527: Loss = 0.1981
2026-01-06 07:33:38,158 - INFO -   Batch 850/2527: Loss = 0.1510
2026-01-06 07:34:03,272 - INFO -   Batch 900/2527: Loss = 0.2207
2026-01-06 07:34:28,253 - INFO -   Batch 950/2527: Loss = 0.1861
2026-01-06 07:34:53,074 - INFO -   Batch 1000/2527: Loss = 0.2324
2026-01-06 07:35:17,932 - INFO -   Batch 1050/2527: Loss = 0.2867
2026-01-06 07:35:42,669 - INFO -   Batch 1100/2527: Loss = 0.3089
2026-01-06 07:36:07,521 - INFO -   Batch 1150/2527: Loss = 0.2201
2026-01-06 07:36:32,644 - INFO -   Batch 1200/2527: Loss = 0.2506
2026-01-06 07:36:57,620 - INFO -   Batch 1250/2527: Loss = 0.1627
2026-01-06 07:37:22,464 - INFO -   Batch 1300/2527: Loss = 0.2811
2026-01-06 07:37:47,295 - INFO -   Batch 1350/2527: Loss = 0.2130
2026-01-06 07:38:12,313 - INFO -   Batch 1400/2527: Loss = 0.2900
2026-01-06 07:38:37,151 - INFO -   Batch 1450/2527: Loss = 0.2178
2026-01-06 07:39:02,008 - INFO -   Batch 1500/2527: Loss = 0.2551
2026-01-06 07:39:27,497 - INFO -   Batch 1550/2527: Loss = 0.3826
2026-01-06 07:39:52,482 - INFO -   Batch 1600/2527: Loss = 0.3113
2026-01-06 07:40:17,319 - INFO -   Batch 1650/2527: Loss = 0.2603
2026-01-06 07:40:42,184 - INFO -   Batch 1700/2527: Loss = 0.1844
2026-01-06 07:41:06,912 - INFO -   Batch 1750/2527: Loss = 0.1730
2026-01-06 07:41:31,751 - INFO -   Batch 1800/2527: Loss = 0.1976
2026-01-06 07:41:56,880 - INFO -   Batch 1850/2527: Loss = 0.1795
2026-01-06 07:42:21,859 - INFO -   Batch 1900/2527: Loss = 0.2679
2026-01-06 07:42:47,039 - INFO -   Batch 1950/2527: Loss = 0.2766
2026-01-06 07:43:11,896 - INFO -   Batch 2000/2527: Loss = 0.2316
2026-01-06 07:43:36,914 - INFO -   Batch 2050/2527: Loss = 0.1118
2026-01-06 07:44:01,761 - INFO -   Batch 2100/2527: Loss = 0.2843
2026-01-06 07:44:26,609 - INFO -   Batch 2150/2527: Loss = 0.2223
2026-01-06 07:44:51,765 - INFO -   Batch 2200/2527: Loss = 0.3398
2026-01-06 07:45:17,114 - INFO -   Batch 2250/2527: Loss = 0.2081
2026-01-06 07:45:41,950 - INFO -   Batch 2300/2527: Loss = 0.1966
2026-01-06 07:46:06,791 - INFO -   Batch 2350/2527: Loss = 0.1904
2026-01-06 07:46:31,526 - INFO -   Batch 2400/2527: Loss = 0.3418
2026-01-06 07:46:56,362 - INFO -   Batch 2450/2527: Loss = 0.2759
2026-01-06 07:47:21,524 - INFO -   Batch 2500/2527: Loss = 0.2170
2026-01-06 07:52:14,331 - INFO - Epoch 18/160: Train Loss: 0.2443, Val Loss: 0.9569, Val mIoU: 39.37%, Val PixelAcc: 77.90%
2026-01-06 07:52:14,333 - INFO -   → New best mIoU: 39.37%
2026-01-06 07:52:16,591 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 07:52:16,594 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 07:52:41,707 - INFO -   Batch 50/2527: Loss = 0.1716
2026-01-06 07:53:04,611 - INFO -   Batch 100/2527: Loss = 0.2156
2026-01-06 07:53:27,538 - INFO -   Batch 150/2527: Loss = 0.2619
2026-01-06 07:53:50,499 - INFO -   Batch 200/2527: Loss = 0.1849
2026-01-06 07:54:13,457 - INFO -   Batch 250/2527: Loss = 0.1630
2026-01-06 07:54:36,406 - INFO -   Batch 300/2527: Loss = 0.1599
2026-01-06 07:54:59,381 - INFO -   Batch 350/2527: Loss = 0.3144
2026-01-06 07:55:22,331 - INFO -   Batch 400/2527: Loss = 0.2280
2026-01-06 07:55:45,693 - INFO -   Batch 450/2527: Loss = 0.3140
2026-01-06 07:56:08,646 - INFO -   Batch 500/2527: Loss = 0.2569
2026-01-06 07:56:31,642 - INFO -   Batch 550/2527: Loss = 0.1579
2026-01-06 07:56:54,596 - INFO -   Batch 600/2527: Loss = 0.1355
2026-01-06 07:57:17,557 - INFO -   Batch 650/2527: Loss = 0.3204
2026-01-06 07:57:40,513 - INFO -   Batch 700/2527: Loss = 0.1730
2026-01-06 07:58:03,510 - INFO -   Batch 750/2527: Loss = 0.2403
2026-01-06 07:58:26,470 - INFO -   Batch 800/2527: Loss = 0.3591
2026-01-06 07:58:49,792 - INFO -   Batch 850/2527: Loss = 0.2166
2026-01-06 07:59:12,765 - INFO -   Batch 900/2527: Loss = 0.1843
2026-01-06 07:59:35,720 - INFO -   Batch 950/2527: Loss = 0.1517
2026-01-06 07:59:58,687 - INFO -   Batch 1000/2527: Loss = 0.2249
2026-01-06 08:00:21,641 - INFO -   Batch 1050/2527: Loss = 0.3489
2026-01-06 08:00:44,598 - INFO -   Batch 1100/2527: Loss = 0.2074
2026-01-06 08:01:07,927 - INFO -   Batch 1150/2527: Loss = 0.2317
2026-01-06 08:01:30,877 - INFO -   Batch 1200/2527: Loss = 0.1792
2026-01-06 08:01:53,830 - INFO -   Batch 1250/2527: Loss = 0.2433
2026-01-06 08:02:16,781 - INFO -   Batch 1300/2527: Loss = 0.2060
2026-01-06 08:02:39,743 - INFO -   Batch 1350/2527: Loss = 0.2436
2026-01-06 08:03:02,695 - INFO -   Batch 1400/2527: Loss = 0.2292
2026-01-06 08:03:25,644 - INFO -   Batch 1450/2527: Loss = 0.2149
2026-01-06 08:03:48,601 - INFO -   Batch 1500/2527: Loss = 0.2228
2026-01-06 08:04:11,553 - INFO -   Batch 1550/2527: Loss = 0.3379
2026-01-06 08:04:34,513 - INFO -   Batch 1600/2527: Loss = 0.2173
2026-01-06 08:04:57,465 - INFO -   Batch 1650/2527: Loss = 0.2901
2026-01-06 08:05:20,417 - INFO -   Batch 1700/2527: Loss = 0.2930
2026-01-06 08:05:43,370 - INFO -   Batch 1750/2527: Loss = 0.3205
2026-01-06 08:06:06,348 - INFO -   Batch 1800/2527: Loss = 0.2726
2026-01-06 08:06:29,307 - INFO -   Batch 1850/2527: Loss = 0.3105
2026-01-06 08:06:52,618 - INFO -   Batch 1900/2527: Loss = 0.2192
2026-01-06 08:07:15,578 - INFO -   Batch 1950/2527: Loss = 0.1484
2026-01-06 08:07:38,542 - INFO -   Batch 2000/2527: Loss = 0.2332
2026-01-06 08:08:01,498 - INFO -   Batch 2050/2527: Loss = 0.2887
2026-01-06 08:08:24,454 - INFO -   Batch 2100/2527: Loss = 0.2304
2026-01-06 08:08:47,405 - INFO -   Batch 2150/2527: Loss = 0.2458
2026-01-06 08:09:10,400 - INFO -   Batch 2200/2527: Loss = 0.1879
2026-01-06 08:09:33,378 - INFO -   Batch 2250/2527: Loss = 0.2439
2026-01-06 08:09:56,686 - INFO -   Batch 2300/2527: Loss = 0.2771
2026-01-06 08:10:19,643 - INFO -   Batch 2350/2527: Loss = 0.1948
2026-01-06 08:10:42,632 - INFO -   Batch 2400/2527: Loss = 0.3479
2026-01-06 08:11:05,583 - INFO -   Batch 2450/2527: Loss = 0.2371
2026-01-06 08:11:28,541 - INFO -   Batch 2500/2527: Loss = 0.2096
2026-01-06 08:16:08,147 - INFO - Epoch 19/160: Train Loss: 0.2366, Val Loss: 0.9357, Val mIoU: 39.03%, Val PixelAcc: 78.54%
2026-01-06 08:16:33,650 - INFO -   Batch 50/2527: Loss = 0.2107
2026-01-06 08:16:57,587 - INFO -   Batch 100/2527: Loss = 0.1504
2026-01-06 08:17:21,516 - INFO -   Batch 150/2527: Loss = 0.2051
2026-01-06 08:17:45,075 - INFO -   Batch 200/2527: Loss = 0.1251
2026-01-06 08:18:08,851 - INFO -   Batch 250/2527: Loss = 0.2830
2026-01-06 08:18:32,456 - INFO -   Batch 300/2527: Loss = 0.2630
2026-01-06 08:18:56,267 - INFO -   Batch 350/2527: Loss = 0.2346
2026-01-06 08:19:20,126 - INFO -   Batch 400/2527: Loss = 0.2300
2026-01-06 08:19:43,911 - INFO -   Batch 450/2527: Loss = 0.1298
2026-01-06 08:20:07,513 - INFO -   Batch 500/2527: Loss = 0.2237
2026-01-06 08:20:31,142 - INFO -   Batch 550/2527: Loss = 0.2003
2026-01-06 08:20:55,098 - INFO -   Batch 600/2527: Loss = 0.2610
2026-01-06 08:21:18,670 - INFO -   Batch 650/2527: Loss = 0.1787
2026-01-06 08:21:42,420 - INFO -   Batch 700/2527: Loss = 0.2416
2026-01-06 08:22:06,039 - INFO -   Batch 750/2527: Loss = 0.1847
2026-01-06 08:22:30,237 - INFO -   Batch 800/2527: Loss = 0.1740
2026-01-06 08:22:54,100 - INFO -   Batch 850/2527: Loss = 0.3191
2026-01-06 08:23:17,898 - INFO -   Batch 900/2527: Loss = 0.1657
2026-01-06 08:23:41,493 - INFO -   Batch 950/2527: Loss = 0.1953
2026-01-06 08:24:05,141 - INFO -   Batch 1000/2527: Loss = 0.1939
2026-01-06 08:24:29,107 - INFO -   Batch 1050/2527: Loss = 0.1610
2026-01-06 08:24:52,689 - INFO -   Batch 1100/2527: Loss = 0.1104
2026-01-06 08:25:16,471 - INFO -   Batch 1150/2527: Loss = 0.2120
2026-01-06 08:25:40,085 - INFO -   Batch 1200/2527: Loss = 0.1672
2026-01-06 08:26:04,268 - INFO -   Batch 1250/2527: Loss = 0.1408
2026-01-06 08:26:28,137 - INFO -   Batch 1300/2527: Loss = 0.2308
2026-01-06 08:26:51,926 - INFO -   Batch 1350/2527: Loss = 0.1772
2026-01-06 08:27:15,560 - INFO -   Batch 1400/2527: Loss = 0.1324
2026-01-06 08:27:39,226 - INFO -   Batch 1450/2527: Loss = 0.1705
2026-01-06 08:28:03,555 - INFO -   Batch 1500/2527: Loss = 0.3253
2026-01-06 08:28:27,128 - INFO -   Batch 1550/2527: Loss = 0.1729
2026-01-06 08:28:50,887 - INFO -   Batch 1600/2527: Loss = 0.2410
2026-01-06 08:29:14,521 - INFO -   Batch 1650/2527: Loss = 0.2247
2026-01-06 08:29:38,392 - INFO -   Batch 1700/2527: Loss = 0.2107
2026-01-06 08:30:02,271 - INFO -   Batch 1750/2527: Loss = 0.3894
2026-01-06 08:30:26,060 - INFO -   Batch 1800/2527: Loss = 0.1843
2026-01-06 08:30:49,657 - INFO -   Batch 1850/2527: Loss = 0.2963
2026-01-06 08:31:13,281 - INFO -   Batch 1900/2527: Loss = 0.2188
2026-01-06 08:31:37,256 - INFO -   Batch 1950/2527: Loss = 0.1953
2026-01-06 08:32:00,849 - INFO -   Batch 2000/2527: Loss = 0.2687
2026-01-06 08:32:24,599 - INFO -   Batch 2050/2527: Loss = 0.1292
2026-01-06 08:32:48,206 - INFO -   Batch 2100/2527: Loss = 0.1830
2026-01-06 08:33:12,044 - INFO -   Batch 2150/2527: Loss = 0.2579
2026-01-06 08:33:35,902 - INFO -   Batch 2200/2527: Loss = 0.1928
2026-01-06 08:34:00,059 - INFO -   Batch 2250/2527: Loss = 0.1511
2026-01-06 08:34:23,661 - INFO -   Batch 2300/2527: Loss = 0.1798
2026-01-06 08:34:47,293 - INFO -   Batch 2350/2527: Loss = 0.3270
2026-01-06 08:35:11,280 - INFO -   Batch 2400/2527: Loss = 0.2819
2026-01-06 08:35:34,851 - INFO -   Batch 2450/2527: Loss = 0.2169
2026-01-06 08:35:58,614 - INFO -   Batch 2500/2527: Loss = 0.2539
2026-01-06 08:40:50,543 - INFO - Epoch 20/160: Train Loss: 0.2239, Val Loss: 0.9660, Val mIoU: 38.20%, Val PixelAcc: 77.62%
2026-01-06 08:40:52,819 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_20.pth
2026-01-06 08:40:52,821 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_20.pth
2026-01-06 08:41:18,786 - INFO -   Batch 50/2527: Loss = 0.2664
2026-01-06 08:41:43,840 - INFO -   Batch 100/2527: Loss = 0.1649
2026-01-06 08:42:09,177 - INFO -   Batch 150/2527: Loss = 0.1842
2026-01-06 08:42:33,988 - INFO -   Batch 200/2527: Loss = 0.1785
2026-01-06 08:42:58,825 - INFO -   Batch 250/2527: Loss = 0.1897
2026-01-06 08:43:23,847 - INFO -   Batch 300/2527: Loss = 0.2197
2026-01-06 08:43:48,671 - INFO -   Batch 350/2527: Loss = 0.1788
2026-01-06 08:44:13,545 - INFO -   Batch 400/2527: Loss = 0.2256
2026-01-06 08:44:39,066 - INFO -   Batch 450/2527: Loss = 0.2174
2026-01-06 08:45:04,050 - INFO -   Batch 500/2527: Loss = 0.2012
2026-01-06 08:45:28,900 - INFO -   Batch 550/2527: Loss = 0.2196
2026-01-06 08:45:53,721 - INFO -   Batch 600/2527: Loss = 0.3089
2026-01-06 08:46:18,453 - INFO -   Batch 650/2527: Loss = 0.2094
2026-01-06 08:46:43,307 - INFO -   Batch 700/2527: Loss = 0.1845
2026-01-06 08:47:08,436 - INFO -   Batch 750/2527: Loss = 0.2573
2026-01-06 08:47:33,401 - INFO -   Batch 800/2527: Loss = 0.2387
2026-01-06 08:47:58,234 - INFO -   Batch 850/2527: Loss = 0.1131
2026-01-06 08:48:23,080 - INFO -   Batch 900/2527: Loss = 0.2725
2026-01-06 08:48:48,098 - INFO -   Batch 950/2527: Loss = 0.1968
2026-01-06 08:49:12,916 - INFO -   Batch 1000/2527: Loss = 0.2725
2026-01-06 08:49:37,770 - INFO -   Batch 1050/2527: Loss = 0.2161
2026-01-06 08:50:02,894 - INFO -   Batch 1100/2527: Loss = 0.2274
2026-01-06 08:50:27,870 - INFO -   Batch 1150/2527: Loss = 0.2128
2026-01-06 08:50:53,059 - INFO -   Batch 1200/2527: Loss = 0.1866
2026-01-06 08:51:17,909 - INFO -   Batch 1250/2527: Loss = 0.1727
2026-01-06 08:51:42,625 - INFO -   Batch 1300/2527: Loss = 0.1706
2026-01-06 08:52:07,485 - INFO -   Batch 1350/2527: Loss = 0.1902
2026-01-06 08:52:32,596 - INFO -   Batch 1400/2527: Loss = 0.2845
2026-01-06 08:52:57,572 - INFO -   Batch 1450/2527: Loss = 0.2025
2026-01-06 08:53:22,391 - INFO -   Batch 1500/2527: Loss = 0.2266
2026-01-06 08:53:47,233 - INFO -   Batch 1550/2527: Loss = 0.3339
2026-01-06 08:54:12,632 - INFO -   Batch 1600/2527: Loss = 0.1682
2026-01-06 08:54:37,472 - INFO -   Batch 1650/2527: Loss = 0.2265
2026-01-06 08:55:02,307 - INFO -   Batch 1700/2527: Loss = 0.2598
2026-01-06 08:55:27,435 - INFO -   Batch 1750/2527: Loss = 0.2234
2026-01-06 08:55:52,421 - INFO -   Batch 1800/2527: Loss = 0.2711
2026-01-06 08:56:17,271 - INFO -   Batch 1850/2527: Loss = 0.0820
2026-01-06 08:56:42,536 - INFO -   Batch 1900/2527: Loss = 0.3088
2026-01-06 08:57:07,250 - INFO -   Batch 1950/2527: Loss = 0.3004
2026-01-06 08:57:32,119 - INFO -   Batch 2000/2527: Loss = 0.1857
2026-01-06 08:57:57,248 - INFO -   Batch 2050/2527: Loss = 0.2807
2026-01-06 08:58:22,228 - INFO -   Batch 2100/2527: Loss = 0.2326
2026-01-06 08:58:47,061 - INFO -   Batch 2150/2527: Loss = 0.2843
2026-01-06 08:59:11,942 - INFO -   Batch 2200/2527: Loss = 0.3488
2026-01-06 08:59:36,961 - INFO -   Batch 2250/2527: Loss = 0.2727
2026-01-06 09:00:01,846 - INFO -   Batch 2300/2527: Loss = 0.2353
2026-01-06 09:00:26,685 - INFO -   Batch 2350/2527: Loss = 0.1798
2026-01-06 09:00:51,833 - INFO -   Batch 2400/2527: Loss = 0.2806
2026-01-06 09:01:16,801 - INFO -   Batch 2450/2527: Loss = 0.1774
2026-01-06 09:01:41,632 - INFO -   Batch 2500/2527: Loss = 0.1665
2026-01-06 09:06:22,004 - INFO - Epoch 21/160: Train Loss: 0.2173, Val Loss: 0.9159, Val mIoU: 39.75%, Val PixelAcc: 78.89%
2026-01-06 09:06:22,006 - INFO -   → New best mIoU: 39.75%
2026-01-06 09:06:24,213 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 09:06:24,215 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 09:06:50,373 - INFO -   Batch 50/2527: Loss = 0.1835
2026-01-06 09:07:15,707 - INFO -   Batch 100/2527: Loss = 0.1585
2026-01-06 09:07:40,502 - INFO -   Batch 150/2527: Loss = 0.2318
2026-01-06 09:08:05,338 - INFO -   Batch 200/2527: Loss = 0.1649
2026-01-06 09:08:30,446 - INFO -   Batch 250/2527: Loss = 0.1885
2026-01-06 09:08:55,410 - INFO -   Batch 300/2527: Loss = 0.2652
2026-01-06 09:09:20,230 - INFO -   Batch 350/2527: Loss = 0.2166
2026-01-06 09:09:45,080 - INFO -   Batch 400/2527: Loss = 0.1896
2026-01-06 09:10:09,831 - INFO -   Batch 450/2527: Loss = 0.1869
2026-01-06 09:10:35,017 - INFO -   Batch 500/2527: Loss = 0.1684
2026-01-06 09:11:00,150 - INFO -   Batch 550/2527: Loss = 0.1981
2026-01-06 09:11:25,114 - INFO -   Batch 600/2527: Loss = 0.2327
2026-01-06 09:11:49,934 - INFO -   Batch 650/2527: Loss = 0.1783
2026-01-06 09:12:14,770 - INFO -   Batch 700/2527: Loss = 0.2554
2026-01-06 09:12:39,792 - INFO -   Batch 750/2527: Loss = 0.1477
2026-01-06 09:13:05,008 - INFO -   Batch 800/2527: Loss = 0.1616
2026-01-06 09:13:29,854 - INFO -   Batch 850/2527: Loss = 0.2108
2026-01-06 09:13:54,981 - INFO -   Batch 900/2527: Loss = 0.2092
2026-01-06 09:14:19,959 - INFO -   Batch 950/2527: Loss = 0.1434
2026-01-06 09:14:44,782 - INFO -   Batch 1000/2527: Loss = 0.1381
2026-01-06 09:15:09,663 - INFO -   Batch 1050/2527: Loss = 0.2032
2026-01-06 09:15:34,411 - INFO -   Batch 1100/2527: Loss = 0.1825
2026-01-06 09:15:59,258 - INFO -   Batch 1150/2527: Loss = 0.1896
2026-01-06 09:16:24,384 - INFO -   Batch 1200/2527: Loss = 0.2062
2026-01-06 09:16:49,356 - INFO -   Batch 1250/2527: Loss = 0.1650
2026-01-06 09:17:14,180 - INFO -   Batch 1300/2527: Loss = 0.1486
2026-01-06 09:17:39,037 - INFO -   Batch 1350/2527: Loss = 0.1711
2026-01-06 09:18:04,051 - INFO -   Batch 1400/2527: Loss = 0.2296
2026-01-06 09:18:28,885 - INFO -   Batch 1450/2527: Loss = 0.2174
2026-01-06 09:18:53,714 - INFO -   Batch 1500/2527: Loss = 0.2196
2026-01-06 09:19:19,186 - INFO -   Batch 1550/2527: Loss = 0.2052
2026-01-06 09:19:44,155 - INFO -   Batch 1600/2527: Loss = 0.2253
2026-01-06 09:20:08,994 - INFO -   Batch 1650/2527: Loss = 0.1735
2026-01-06 09:20:33,831 - INFO -   Batch 1700/2527: Loss = 0.2379
2026-01-06 09:20:58,566 - INFO -   Batch 1750/2527: Loss = 0.1927
2026-01-06 09:21:23,408 - INFO -   Batch 1800/2527: Loss = 0.2077
2026-01-06 09:21:48,641 - INFO -   Batch 1850/2527: Loss = 0.1933
2026-01-06 09:22:13,660 - INFO -   Batch 1900/2527: Loss = 0.2369
2026-01-06 09:22:38,854 - INFO -   Batch 1950/2527: Loss = 0.1320
2026-01-06 09:23:03,705 - INFO -   Batch 2000/2527: Loss = 0.2438
2026-01-06 09:23:28,749 - INFO -   Batch 2050/2527: Loss = 0.1685
2026-01-06 09:23:53,575 - INFO -   Batch 2100/2527: Loss = 0.2549
2026-01-06 09:24:18,409 - INFO -   Batch 2150/2527: Loss = 0.2131
2026-01-06 09:24:43,528 - INFO -   Batch 2200/2527: Loss = 0.1762
2026-01-06 09:25:08,867 - INFO -   Batch 2250/2527: Loss = 0.2956
2026-01-06 09:25:33,692 - INFO -   Batch 2300/2527: Loss = 0.1655
2026-01-06 09:25:58,573 - INFO -   Batch 2350/2527: Loss = 0.1488
2026-01-06 09:26:23,308 - INFO -   Batch 2400/2527: Loss = 0.1643
2026-01-06 09:26:48,172 - INFO -   Batch 2450/2527: Loss = 0.2403
2026-01-06 09:27:13,287 - INFO -   Batch 2500/2527: Loss = 0.1803
2026-01-06 09:32:06,371 - INFO - Epoch 22/160: Train Loss: 0.2045, Val Loss: 0.9718, Val mIoU: 39.04%, Val PixelAcc: 78.82%
2026-01-06 09:32:31,339 - INFO -   Batch 50/2527: Loss = 0.1759
2026-01-06 09:32:54,242 - INFO -   Batch 100/2527: Loss = 0.2545
2026-01-06 09:33:17,190 - INFO -   Batch 150/2527: Loss = 0.1453
2026-01-06 09:33:40,122 - INFO -   Batch 200/2527: Loss = 0.2377
2026-01-06 09:34:03,071 - INFO -   Batch 250/2527: Loss = 0.1476
2026-01-06 09:34:26,021 - INFO -   Batch 300/2527: Loss = 0.1554
2026-01-06 09:34:48,988 - INFO -   Batch 350/2527: Loss = 0.2109
2026-01-06 09:35:11,938 - INFO -   Batch 400/2527: Loss = 0.1665
2026-01-06 09:35:35,264 - INFO -   Batch 450/2527: Loss = 0.1773
2026-01-06 09:35:58,224 - INFO -   Batch 500/2527: Loss = 0.1420
2026-01-06 09:36:21,179 - INFO -   Batch 550/2527: Loss = 0.1955
2026-01-06 09:36:44,132 - INFO -   Batch 600/2527: Loss = 0.1543
2026-01-06 09:37:07,114 - INFO -   Batch 650/2527: Loss = 0.1757
2026-01-06 09:37:30,076 - INFO -   Batch 700/2527: Loss = 0.1642
2026-01-06 09:37:53,034 - INFO -   Batch 750/2527: Loss = 0.1641
2026-01-06 09:38:15,982 - INFO -   Batch 800/2527: Loss = 0.1901
2026-01-06 09:38:38,970 - INFO -   Batch 850/2527: Loss = 0.1159
2026-01-06 09:39:02,274 - INFO -   Batch 900/2527: Loss = 0.1616
2026-01-06 09:39:25,229 - INFO -   Batch 950/2527: Loss = 0.1840
2026-01-06 09:39:48,217 - INFO -   Batch 1000/2527: Loss = 0.1150
2026-01-06 09:40:11,176 - INFO -   Batch 1050/2527: Loss = 0.2080
2026-01-06 09:40:34,129 - INFO -   Batch 1100/2527: Loss = 0.2230
2026-01-06 09:40:57,076 - INFO -   Batch 1150/2527: Loss = 0.2310
2026-01-06 09:41:20,417 - INFO -   Batch 1200/2527: Loss = 0.1957
2026-01-06 09:41:43,372 - INFO -   Batch 1250/2527: Loss = 0.1884
2026-01-06 09:42:06,348 - INFO -   Batch 1300/2527: Loss = 0.1777
2026-01-06 09:42:29,297 - INFO -   Batch 1350/2527: Loss = 0.1225
2026-01-06 09:42:52,246 - INFO -   Batch 1400/2527: Loss = 0.2104
2026-01-06 09:43:15,196 - INFO -   Batch 1450/2527: Loss = 0.1490
2026-01-06 09:43:38,151 - INFO -   Batch 1500/2527: Loss = 0.1625
2026-01-06 09:44:01,107 - INFO -   Batch 1550/2527: Loss = 0.1424
2026-01-06 09:44:24,058 - INFO -   Batch 1600/2527: Loss = 0.2638
2026-01-06 09:44:47,014 - INFO -   Batch 1650/2527: Loss = 0.1807
2026-01-06 09:45:09,973 - INFO -   Batch 1700/2527: Loss = 0.2072
2026-01-06 09:45:32,922 - INFO -   Batch 1750/2527: Loss = 0.1629
2026-01-06 09:45:55,884 - INFO -   Batch 1800/2527: Loss = 0.1749
2026-01-06 09:46:18,824 - INFO -   Batch 1850/2527: Loss = 0.2337
2026-01-06 09:46:42,178 - INFO -   Batch 1900/2527: Loss = 0.2592
2026-01-06 09:47:05,127 - INFO -   Batch 1950/2527: Loss = 0.1987
2026-01-06 09:47:28,077 - INFO -   Batch 2000/2527: Loss = 0.1983
2026-01-06 09:47:51,028 - INFO -   Batch 2050/2527: Loss = 0.2155
2026-01-06 09:48:13,980 - INFO -   Batch 2100/2527: Loss = 0.3082
2026-01-06 09:48:36,938 - INFO -   Batch 2150/2527: Loss = 0.2391
2026-01-06 09:48:59,891 - INFO -   Batch 2200/2527: Loss = 0.3355
2026-01-06 09:49:22,836 - INFO -   Batch 2250/2527: Loss = 0.1625
2026-01-06 09:49:45,789 - INFO -   Batch 2300/2527: Loss = 0.1725
2026-01-06 09:50:09,109 - INFO -   Batch 2350/2527: Loss = 0.2012
2026-01-06 09:50:32,059 - INFO -   Batch 2400/2527: Loss = 0.2129
2026-01-06 09:50:55,012 - INFO -   Batch 2450/2527: Loss = 0.2375
2026-01-06 09:51:17,966 - INFO -   Batch 2500/2527: Loss = 0.1644
2026-01-06 09:55:57,464 - INFO - Epoch 23/160: Train Loss: 0.1969, Val Loss: 0.9464, Val mIoU: 40.45%, Val PixelAcc: 79.08%
2026-01-06 09:55:57,466 - INFO -   → New best mIoU: 40.45%
2026-01-06 09:55:59,670 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 09:55:59,673 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 09:56:25,172 - INFO -   Batch 50/2527: Loss = 0.2976
2026-01-06 09:56:48,738 - INFO -   Batch 100/2527: Loss = 0.0931
2026-01-06 09:57:13,028 - INFO -   Batch 150/2527: Loss = 0.1761
2026-01-06 09:57:36,581 - INFO -   Batch 200/2527: Loss = 0.1789
2026-01-06 09:58:00,356 - INFO -   Batch 250/2527: Loss = 0.1566
2026-01-06 09:58:23,962 - INFO -   Batch 300/2527: Loss = 0.2415
2026-01-06 09:58:47,781 - INFO -   Batch 350/2527: Loss = 0.1803
2026-01-06 09:59:12,011 - INFO -   Batch 400/2527: Loss = 0.2445
2026-01-06 09:59:35,793 - INFO -   Batch 450/2527: Loss = 0.2060
2026-01-06 09:59:59,430 - INFO -   Batch 500/2527: Loss = 0.1663
2026-01-06 10:00:23,058 - INFO -   Batch 550/2527: Loss = 0.1582
2026-01-06 10:00:47,010 - INFO -   Batch 600/2527: Loss = 0.1894
2026-01-06 10:01:10,587 - INFO -   Batch 650/2527: Loss = 0.2650
2026-01-06 10:01:34,342 - INFO -   Batch 700/2527: Loss = 0.1464
2026-01-06 10:01:57,960 - INFO -   Batch 750/2527: Loss = 0.1956
2026-01-06 10:02:21,774 - INFO -   Batch 800/2527: Loss = 0.2157
2026-01-06 10:02:45,634 - INFO -   Batch 850/2527: Loss = 0.1157
2026-01-06 10:03:09,423 - INFO -   Batch 900/2527: Loss = 0.1151
2026-01-06 10:03:33,023 - INFO -   Batch 950/2527: Loss = 0.1978
2026-01-06 10:03:56,646 - INFO -   Batch 1000/2527: Loss = 0.1783
2026-01-06 10:04:20,611 - INFO -   Batch 1050/2527: Loss = 0.1852
2026-01-06 10:04:44,189 - INFO -   Batch 1100/2527: Loss = 0.3113
2026-01-06 10:05:08,353 - INFO -   Batch 1150/2527: Loss = 0.1636
2026-01-06 10:05:31,949 - INFO -   Batch 1200/2527: Loss = 0.1415
2026-01-06 10:05:55,761 - INFO -   Batch 1250/2527: Loss = 0.1469
2026-01-06 10:06:19,622 - INFO -   Batch 1300/2527: Loss = 0.2289
2026-01-06 10:06:43,444 - INFO -   Batch 1350/2527: Loss = 0.2108
2026-01-06 10:07:07,072 - INFO -   Batch 1400/2527: Loss = 0.1679
2026-01-06 10:07:30,738 - INFO -   Batch 1450/2527: Loss = 0.2526
2026-01-06 10:07:54,699 - INFO -   Batch 1500/2527: Loss = 0.1329
2026-01-06 10:08:18,293 - INFO -   Batch 1550/2527: Loss = 0.1474
2026-01-06 10:08:42,418 - INFO -   Batch 1600/2527: Loss = 0.1798
2026-01-06 10:09:06,034 - INFO -   Batch 1650/2527: Loss = 0.1523
2026-01-06 10:09:29,882 - INFO -   Batch 1700/2527: Loss = 0.1165
2026-01-06 10:09:53,738 - INFO -   Batch 1750/2527: Loss = 0.1766
2026-01-06 10:10:17,558 - INFO -   Batch 1800/2527: Loss = 0.1964
2026-01-06 10:10:41,542 - INFO -   Batch 1850/2527: Loss = 0.1714
2026-01-06 10:11:05,173 - INFO -   Batch 1900/2527: Loss = 0.1573
2026-01-06 10:11:29,134 - INFO -   Batch 1950/2527: Loss = 0.1556
2026-01-06 10:11:52,699 - INFO -   Batch 2000/2527: Loss = 0.1510
2026-01-06 10:12:16,486 - INFO -   Batch 2050/2527: Loss = 0.1609
2026-01-06 10:12:40,111 - INFO -   Batch 2100/2527: Loss = 0.2409
2026-01-06 10:13:03,930 - INFO -   Batch 2150/2527: Loss = 0.2574
2026-01-06 10:13:27,810 - INFO -   Batch 2200/2527: Loss = 0.1417
2026-01-06 10:13:51,603 - INFO -   Batch 2250/2527: Loss = 0.1939
2026-01-06 10:14:15,199 - INFO -   Batch 2300/2527: Loss = 0.1622
2026-01-06 10:14:38,822 - INFO -   Batch 2350/2527: Loss = 0.3685
2026-01-06 10:15:02,827 - INFO -   Batch 2400/2527: Loss = 0.1541
2026-01-06 10:15:26,407 - INFO -   Batch 2450/2527: Loss = 0.2376
2026-01-06 10:15:50,176 - INFO -   Batch 2500/2527: Loss = 0.1690
2026-01-06 10:20:42,337 - INFO - Epoch 24/160: Train Loss: 0.1923, Val Loss: 0.9713, Val mIoU: 39.93%, Val PixelAcc: 79.04%
2026-01-06 10:21:08,364 - INFO -   Batch 50/2527: Loss = 0.1354
2026-01-06 10:21:33,795 - INFO -   Batch 100/2527: Loss = 0.1941
2026-01-06 10:21:58,725 - INFO -   Batch 150/2527: Loss = 0.1432
2026-01-06 10:22:23,521 - INFO -   Batch 200/2527: Loss = 0.2015
2026-01-06 10:22:48,382 - INFO -   Batch 250/2527: Loss = 0.2647
2026-01-06 10:23:13,425 - INFO -   Batch 300/2527: Loss = 0.1623
2026-01-06 10:23:38,254 - INFO -   Batch 350/2527: Loss = 0.1327
2026-01-06 10:24:03,125 - INFO -   Batch 400/2527: Loss = 0.1421
2026-01-06 10:24:28,244 - INFO -   Batch 450/2527: Loss = 0.1305
2026-01-06 10:24:53,562 - INFO -   Batch 500/2527: Loss = 0.2072
2026-01-06 10:25:18,395 - INFO -   Batch 550/2527: Loss = 0.2618
2026-01-06 10:25:43,221 - INFO -   Batch 600/2527: Loss = 0.1534
2026-01-06 10:26:07,940 - INFO -   Batch 650/2527: Loss = 0.1797
2026-01-06 10:26:32,820 - INFO -   Batch 700/2527: Loss = 0.2073
2026-01-06 10:26:57,948 - INFO -   Batch 750/2527: Loss = 0.1356
2026-01-06 10:27:23,285 - INFO -   Batch 800/2527: Loss = 0.1953
2026-01-06 10:27:48,109 - INFO -   Batch 850/2527: Loss = 0.1849
2026-01-06 10:28:12,976 - INFO -   Batch 900/2527: Loss = 0.1242
2026-01-06 10:28:38,015 - INFO -   Batch 950/2527: Loss = 0.2019
2026-01-06 10:29:02,855 - INFO -   Batch 1000/2527: Loss = 0.1437
2026-01-06 10:29:27,699 - INFO -   Batch 1050/2527: Loss = 0.1573
2026-01-06 10:29:52,825 - INFO -   Batch 1100/2527: Loss = 0.1779
2026-01-06 10:30:17,801 - INFO -   Batch 1150/2527: Loss = 0.1852
2026-01-06 10:30:42,655 - INFO -   Batch 1200/2527: Loss = 0.1121
2026-01-06 10:31:07,498 - INFO -   Batch 1250/2527: Loss = 0.1486
2026-01-06 10:31:32,251 - INFO -   Batch 1300/2527: Loss = 0.1441
2026-01-06 10:31:57,083 - INFO -   Batch 1350/2527: Loss = 0.1598
2026-01-06 10:32:22,220 - INFO -   Batch 1400/2527: Loss = 0.1280
2026-01-06 10:32:47,204 - INFO -   Batch 1450/2527: Loss = 0.2416
2026-01-06 10:33:12,022 - INFO -   Batch 1500/2527: Loss = 0.1250
2026-01-06 10:33:37,234 - INFO -   Batch 1550/2527: Loss = 0.1377
2026-01-06 10:34:02,249 - INFO -   Batch 1600/2527: Loss = 0.1554
2026-01-06 10:34:27,084 - INFO -   Batch 1650/2527: Loss = 0.1281
2026-01-06 10:34:51,934 - INFO -   Batch 1700/2527: Loss = 0.1629
2026-01-06 10:35:17,053 - INFO -   Batch 1750/2527: Loss = 0.1541
2026-01-06 10:35:42,042 - INFO -   Batch 1800/2527: Loss = 0.2223
2026-01-06 10:36:06,889 - INFO -   Batch 1850/2527: Loss = 0.1251
2026-01-06 10:36:31,723 - INFO -   Batch 1900/2527: Loss = 0.1545
2026-01-06 10:36:56,821 - INFO -   Batch 1950/2527: Loss = 0.1861
2026-01-06 10:37:21,696 - INFO -   Batch 2000/2527: Loss = 0.1638
2026-01-06 10:37:46,814 - INFO -   Batch 2050/2527: Loss = 0.2147
2026-01-06 10:38:11,796 - INFO -   Batch 2100/2527: Loss = 0.1420
2026-01-06 10:38:36,630 - INFO -   Batch 2150/2527: Loss = 0.1779
2026-01-06 10:39:01,459 - INFO -   Batch 2200/2527: Loss = 0.2243
2026-01-06 10:39:26,823 - INFO -   Batch 2250/2527: Loss = 0.1282
2026-01-06 10:39:51,654 - INFO -   Batch 2300/2527: Loss = 0.2030
2026-01-06 10:40:16,487 - INFO -   Batch 2350/2527: Loss = 0.1860
2026-01-06 10:40:41,629 - INFO -   Batch 2400/2527: Loss = 0.1737
2026-01-06 10:41:06,610 - INFO -   Batch 2450/2527: Loss = 0.1875
2026-01-06 10:41:31,437 - INFO -   Batch 2500/2527: Loss = 0.2935
2026-01-06 10:46:12,238 - INFO - Epoch 25/160: Train Loss: 0.1869, Val Loss: 1.0004, Val mIoU: 39.57%, Val PixelAcc: 78.57%
2026-01-06 10:46:38,622 - INFO -   Batch 50/2527: Loss = 0.3183
2026-01-06 10:47:03,576 - INFO -   Batch 100/2527: Loss = 0.1353
2026-01-06 10:47:28,370 - INFO -   Batch 150/2527: Loss = 0.2220
2026-01-06 10:47:53,187 - INFO -   Batch 200/2527: Loss = 0.2404
2026-01-06 10:48:18,281 - INFO -   Batch 250/2527: Loss = 0.2016
2026-01-06 10:48:43,260 - INFO -   Batch 300/2527: Loss = 0.1843
2026-01-06 10:49:08,091 - INFO -   Batch 350/2527: Loss = 0.1319
2026-01-06 10:49:32,918 - INFO -   Batch 400/2527: Loss = 0.1501
2026-01-06 10:49:58,010 - INFO -   Batch 450/2527: Loss = 0.1838
2026-01-06 10:50:22,828 - INFO -   Batch 500/2527: Loss = 0.1722
2026-01-06 10:50:47,934 - INFO -   Batch 550/2527: Loss = 0.1147
2026-01-06 10:51:12,893 - INFO -   Batch 600/2527: Loss = 0.1657
2026-01-06 10:51:37,722 - INFO -   Batch 650/2527: Loss = 0.1878
2026-01-06 10:52:02,568 - INFO -   Batch 700/2527: Loss = 0.2762
2026-01-06 10:52:27,574 - INFO -   Batch 750/2527: Loss = 0.1693
2026-01-06 10:52:52,400 - INFO -   Batch 800/2527: Loss = 0.1780
2026-01-06 10:53:17,609 - INFO -   Batch 850/2527: Loss = 0.2518
2026-01-06 10:53:42,760 - INFO -   Batch 900/2527: Loss = 0.1787
2026-01-06 10:54:07,740 - INFO -   Batch 950/2527: Loss = 0.1307
2026-01-06 10:54:32,578 - INFO -   Batch 1000/2527: Loss = 0.1603
2026-01-06 10:54:57,406 - INFO -   Batch 1050/2527: Loss = 0.1879
2026-01-06 10:55:22,121 - INFO -   Batch 1100/2527: Loss = 0.1954
2026-01-06 10:55:47,332 - INFO -   Batch 1150/2527: Loss = 0.1271
2026-01-06 10:56:12,471 - INFO -   Batch 1200/2527: Loss = 0.3109
2026-01-06 10:56:37,446 - INFO -   Batch 1250/2527: Loss = 0.2488
2026-01-06 10:57:02,262 - INFO -   Batch 1300/2527: Loss = 0.2312
2026-01-06 10:57:27,099 - INFO -   Batch 1350/2527: Loss = 0.2491
2026-01-06 10:57:52,127 - INFO -   Batch 1400/2527: Loss = 0.2381
2026-01-06 10:58:16,976 - INFO -   Batch 1450/2527: Loss = 0.1858
2026-01-06 10:58:41,824 - INFO -   Batch 1500/2527: Loss = 0.1635
2026-01-06 10:59:06,948 - INFO -   Batch 1550/2527: Loss = 0.1402
2026-01-06 10:59:31,925 - INFO -   Batch 1600/2527: Loss = 0.1892
2026-01-06 10:59:56,758 - INFO -   Batch 1650/2527: Loss = 0.1877
2026-01-06 11:00:21,589 - INFO -   Batch 1700/2527: Loss = 0.1969
2026-01-06 11:00:46,301 - INFO -   Batch 1750/2527: Loss = 0.1824
2026-01-06 11:01:11,141 - INFO -   Batch 1800/2527: Loss = 0.2999
2026-01-06 11:01:36,252 - INFO -   Batch 1850/2527: Loss = 0.1729
2026-01-06 11:02:01,592 - INFO -   Batch 1900/2527: Loss = 0.1557
2026-01-06 11:02:26,409 - INFO -   Batch 1950/2527: Loss = 0.1879
2026-01-06 11:02:51,259 - INFO -   Batch 2000/2527: Loss = 0.1669
2026-01-06 11:03:16,274 - INFO -   Batch 2050/2527: Loss = 0.1903
2026-01-06 11:03:41,100 - INFO -   Batch 2100/2527: Loss = 0.1852
2026-01-06 11:04:05,955 - INFO -   Batch 2150/2527: Loss = 0.2247
2026-01-06 11:04:31,105 - INFO -   Batch 2200/2527: Loss = 0.2591
2026-01-06 11:04:56,068 - INFO -   Batch 2250/2527: Loss = 0.1272
2026-01-06 11:05:21,245 - INFO -   Batch 2300/2527: Loss = 0.2127
2026-01-06 11:05:46,102 - INFO -   Batch 2350/2527: Loss = 0.1222
2026-01-06 11:06:10,834 - INFO -   Batch 2400/2527: Loss = 0.1876
2026-01-06 11:06:35,667 - INFO -   Batch 2450/2527: Loss = 0.1864
2026-01-06 11:07:00,783 - INFO -   Batch 2500/2527: Loss = 0.2299
2026-01-06 11:11:53,562 - INFO - Epoch 26/160: Train Loss: 0.1925, Val Loss: 0.9948, Val mIoU: 39.79%, Val PixelAcc: 78.80%
2026-01-06 11:12:18,602 - INFO -   Batch 50/2527: Loss = 0.1837
2026-01-06 11:12:41,878 - INFO -   Batch 100/2527: Loss = 0.1231
2026-01-06 11:13:04,800 - INFO -   Batch 150/2527: Loss = 0.1543
2026-01-06 11:13:27,738 - INFO -   Batch 200/2527: Loss = 0.1736
2026-01-06 11:13:50,676 - INFO -   Batch 250/2527: Loss = 0.1702
2026-01-06 11:14:13,639 - INFO -   Batch 300/2527: Loss = 0.1424
2026-01-06 11:14:36,586 - INFO -   Batch 350/2527: Loss = 0.1381
2026-01-06 11:14:59,539 - INFO -   Batch 400/2527: Loss = 0.1595
2026-01-06 11:15:22,488 - INFO -   Batch 450/2527: Loss = 0.1787
2026-01-06 11:15:45,461 - INFO -   Batch 500/2527: Loss = 0.0796
2026-01-06 11:16:08,417 - INFO -   Batch 550/2527: Loss = 0.1931
2026-01-06 11:16:31,367 - INFO -   Batch 600/2527: Loss = 0.2211
2026-01-06 11:16:54,322 - INFO -   Batch 650/2527: Loss = 0.1924
2026-01-06 11:17:17,291 - INFO -   Batch 700/2527: Loss = 0.2600
2026-01-06 11:17:40,243 - INFO -   Batch 750/2527: Loss = 0.1351
2026-01-06 11:18:03,565 - INFO -   Batch 800/2527: Loss = 0.1415
2026-01-06 11:18:26,517 - INFO -   Batch 850/2527: Loss = 0.2537
2026-01-06 11:18:49,478 - INFO -   Batch 900/2527: Loss = 0.1970
2026-01-06 11:19:12,434 - INFO -   Batch 950/2527: Loss = 0.1598
2026-01-06 11:19:35,394 - INFO -   Batch 1000/2527: Loss = 0.2209
2026-01-06 11:19:58,354 - INFO -   Batch 1050/2527: Loss = 0.1902
2026-01-06 11:20:21,313 - INFO -   Batch 1100/2527: Loss = 0.1793
2026-01-06 11:20:44,278 - INFO -   Batch 1150/2527: Loss = 0.1162
2026-01-06 11:21:07,234 - INFO -   Batch 1200/2527: Loss = 0.2068
2026-01-06 11:21:30,559 - INFO -   Batch 1250/2527: Loss = 0.1692
2026-01-06 11:21:53,504 - INFO -   Batch 1300/2527: Loss = 0.1708
2026-01-06 11:22:16,460 - INFO -   Batch 1350/2527: Loss = 0.2211
2026-01-06 11:22:39,425 - INFO -   Batch 1400/2527: Loss = 0.2588
2026-01-06 11:23:02,383 - INFO -   Batch 1450/2527: Loss = 0.1275
2026-01-06 11:23:25,710 - INFO -   Batch 1500/2527: Loss = 0.1805
2026-01-06 11:23:48,675 - INFO -   Batch 1550/2527: Loss = 0.1257
2026-01-06 11:24:11,664 - INFO -   Batch 1600/2527: Loss = 0.1575
2026-01-06 11:24:34,607 - INFO -   Batch 1650/2527: Loss = 0.1913
2026-01-06 11:24:57,567 - INFO -   Batch 1700/2527: Loss = 0.1870
2026-01-06 11:25:20,519 - INFO -   Batch 1750/2527: Loss = 0.1354
2026-01-06 11:25:43,496 - INFO -   Batch 1800/2527: Loss = 0.1388
2026-01-06 11:26:06,445 - INFO -   Batch 1850/2527: Loss = 0.2415
2026-01-06 11:26:29,406 - INFO -   Batch 1900/2527: Loss = 0.1630
2026-01-06 11:26:52,368 - INFO -   Batch 1950/2527: Loss = 0.1684
2026-01-06 11:27:15,351 - INFO -   Batch 2000/2527: Loss = 0.1881
2026-01-06 11:27:38,304 - INFO -   Batch 2050/2527: Loss = 0.1157
2026-01-06 11:28:01,259 - INFO -   Batch 2100/2527: Loss = 0.1976
2026-01-06 11:28:24,206 - INFO -   Batch 2150/2527: Loss = 0.2626
2026-01-06 11:28:47,194 - INFO -   Batch 2200/2527: Loss = 0.1397
2026-01-06 11:29:10,499 - INFO -   Batch 2250/2527: Loss = 0.2333
2026-01-06 11:29:33,486 - INFO -   Batch 2300/2527: Loss = 0.1708
2026-01-06 11:29:56,441 - INFO -   Batch 2350/2527: Loss = 0.1049
2026-01-06 11:30:19,394 - INFO -   Batch 2400/2527: Loss = 0.1511
2026-01-06 11:30:42,346 - INFO -   Batch 2450/2527: Loss = 0.1199
2026-01-06 11:31:05,342 - INFO -   Batch 2500/2527: Loss = 0.1699
2026-01-06 11:35:45,201 - INFO - Epoch 27/160: Train Loss: 0.1759, Val Loss: 1.0160, Val mIoU: 40.32%, Val PixelAcc: 79.04%
2026-01-06 11:36:10,577 - INFO -   Batch 50/2527: Loss = 0.1506
2026-01-06 11:36:34,151 - INFO -   Batch 100/2527: Loss = 0.1722
2026-01-06 11:36:58,440 - INFO -   Batch 150/2527: Loss = 0.1171
2026-01-06 11:37:21,996 - INFO -   Batch 200/2527: Loss = 0.1570
2026-01-06 11:37:45,754 - INFO -   Batch 250/2527: Loss = 0.1047
2026-01-06 11:38:09,354 - INFO -   Batch 300/2527: Loss = 0.1604
2026-01-06 11:38:33,303 - INFO -   Batch 350/2527: Loss = 0.1199
2026-01-06 11:38:57,160 - INFO -   Batch 400/2527: Loss = 0.1348
2026-01-06 11:39:21,340 - INFO -   Batch 450/2527: Loss = 0.1463
2026-01-06 11:39:44,970 - INFO -   Batch 500/2527: Loss = 0.1241
2026-01-06 11:40:08,583 - INFO -   Batch 550/2527: Loss = 0.2550
2026-01-06 11:40:32,541 - INFO -   Batch 600/2527: Loss = 0.2048
2026-01-06 11:40:56,112 - INFO -   Batch 650/2527: Loss = 0.1823
2026-01-06 11:41:19,876 - INFO -   Batch 700/2527: Loss = 0.1354
2026-01-06 11:41:43,544 - INFO -   Batch 750/2527: Loss = 0.1891
2026-01-06 11:42:07,364 - INFO -   Batch 800/2527: Loss = 0.1839
2026-01-06 11:42:31,238 - INFO -   Batch 850/2527: Loss = 0.1865
2026-01-06 11:42:55,129 - INFO -   Batch 900/2527: Loss = 0.1249
2026-01-06 11:43:18,734 - INFO -   Batch 950/2527: Loss = 0.2166
2026-01-06 11:43:42,427 - INFO -   Batch 1000/2527: Loss = 0.0923
2026-01-06 11:44:06,387 - INFO -   Batch 1050/2527: Loss = 0.1628
2026-01-06 11:44:29,964 - INFO -   Batch 1100/2527: Loss = 0.1726
2026-01-06 11:44:53,740 - INFO -   Batch 1150/2527: Loss = 0.1356
2026-01-06 11:45:17,716 - INFO -   Batch 1200/2527: Loss = 0.1383
2026-01-06 11:45:41,555 - INFO -   Batch 1250/2527: Loss = 0.1940
2026-01-06 11:46:05,412 - INFO -   Batch 1300/2527: Loss = 0.1705
2026-01-06 11:46:29,212 - INFO -   Batch 1350/2527: Loss = 0.2232
2026-01-06 11:46:52,809 - INFO -   Batch 1400/2527: Loss = 0.1553
2026-01-06 11:47:16,459 - INFO -   Batch 1450/2527: Loss = 0.1653
2026-01-06 11:47:40,439 - INFO -   Batch 1500/2527: Loss = 0.1627
2026-01-06 11:48:04,038 - INFO -   Batch 1550/2527: Loss = 0.1128
2026-01-06 11:48:28,184 - INFO -   Batch 1600/2527: Loss = 0.1321
2026-01-06 11:48:51,785 - INFO -   Batch 1650/2527: Loss = 0.1821
2026-01-06 11:49:15,623 - INFO -   Batch 1700/2527: Loss = 0.1733
2026-01-06 11:49:39,482 - INFO -   Batch 1750/2527: Loss = 0.1177
2026-01-06 11:50:03,284 - INFO -   Batch 1800/2527: Loss = 0.1897
2026-01-06 11:50:26,885 - INFO -   Batch 1850/2527: Loss = 0.1059
2026-01-06 11:50:50,877 - INFO -   Batch 1900/2527: Loss = 0.1546
2026-01-06 11:51:14,868 - INFO -   Batch 1950/2527: Loss = 0.1771
2026-01-06 11:51:38,456 - INFO -   Batch 2000/2527: Loss = 0.1656
2026-01-06 11:52:02,237 - INFO -   Batch 2050/2527: Loss = 0.1627
2026-01-06 11:52:25,852 - INFO -   Batch 2100/2527: Loss = 0.1649
2026-01-06 11:52:49,677 - INFO -   Batch 2150/2527: Loss = 0.1670
2026-01-06 11:53:13,538 - INFO -   Batch 2200/2527: Loss = 0.1340
2026-01-06 11:53:37,344 - INFO -   Batch 2250/2527: Loss = 0.1934
2026-01-06 11:54:01,002 - INFO -   Batch 2300/2527: Loss = 0.1664
2026-01-06 11:54:24,621 - INFO -   Batch 2350/2527: Loss = 0.2150
2026-01-06 11:54:48,577 - INFO -   Batch 2400/2527: Loss = 0.1151
2026-01-06 11:55:12,187 - INFO -   Batch 2450/2527: Loss = 0.1874
2026-01-06 11:55:35,973 - INFO -   Batch 2500/2527: Loss = 0.1747
2026-01-06 12:00:28,279 - INFO - Epoch 28/160: Train Loss: 0.1678, Val Loss: 1.0228, Val mIoU: 39.92%, Val PixelAcc: 79.03%
2026-01-06 12:00:54,393 - INFO -   Batch 50/2527: Loss = 0.1758
2026-01-06 12:01:19,842 - INFO -   Batch 100/2527: Loss = 0.1768
2026-01-06 12:01:44,799 - INFO -   Batch 150/2527: Loss = 0.1670
2026-01-06 12:02:09,610 - INFO -   Batch 200/2527: Loss = 0.1827
2026-01-06 12:02:34,443 - INFO -   Batch 250/2527: Loss = 0.1292
2026-01-06 12:02:59,438 - INFO -   Batch 300/2527: Loss = 0.1196
2026-01-06 12:03:24,293 - INFO -   Batch 350/2527: Loss = 0.1308
2026-01-06 12:03:49,174 - INFO -   Batch 400/2527: Loss = 0.1515
2026-01-06 12:04:14,325 - INFO -   Batch 450/2527: Loss = 0.2211
2026-01-06 12:04:39,666 - INFO -   Batch 500/2527: Loss = 0.1372
2026-01-06 12:05:04,506 - INFO -   Batch 550/2527: Loss = 0.2505
2026-01-06 12:05:29,362 - INFO -   Batch 600/2527: Loss = 0.2357
2026-01-06 12:05:54,091 - INFO -   Batch 650/2527: Loss = 0.1082
2026-01-06 12:06:18,937 - INFO -   Batch 700/2527: Loss = 0.1725
2026-01-06 12:06:44,052 - INFO -   Batch 750/2527: Loss = 0.1864
2026-01-06 12:07:09,397 - INFO -   Batch 800/2527: Loss = 0.1451
2026-01-06 12:07:34,226 - INFO -   Batch 850/2527: Loss = 0.1439
2026-01-06 12:07:59,070 - INFO -   Batch 900/2527: Loss = 0.1450
2026-01-06 12:08:24,149 - INFO -   Batch 950/2527: Loss = 0.1395
2026-01-06 12:08:48,983 - INFO -   Batch 1000/2527: Loss = 0.2016
2026-01-06 12:09:13,847 - INFO -   Batch 1050/2527: Loss = 0.9293
2026-01-06 12:09:38,962 - INFO -   Batch 1100/2527: Loss = 0.1970
2026-01-06 12:10:03,933 - INFO -   Batch 1150/2527: Loss = 0.2653
2026-01-06 12:10:28,750 - INFO -   Batch 1200/2527: Loss = 0.1548
2026-01-06 12:10:53,586 - INFO -   Batch 1250/2527: Loss = 0.2015
2026-01-06 12:11:18,309 - INFO -   Batch 1300/2527: Loss = 0.2178
2026-01-06 12:11:43,170 - INFO -   Batch 1350/2527: Loss = 0.1390
2026-01-06 12:12:08,280 - INFO -   Batch 1400/2527: Loss = 0.1510
2026-01-06 12:12:33,251 - INFO -   Batch 1450/2527: Loss = 0.1857
2026-01-06 12:12:58,123 - INFO -   Batch 1500/2527: Loss = 0.2298
2026-01-06 12:13:23,494 - INFO -   Batch 1550/2527: Loss = 0.1451
2026-01-06 12:13:48,548 - INFO -   Batch 1600/2527: Loss = 0.2413
2026-01-06 12:14:13,400 - INFO -   Batch 1650/2527: Loss = 0.1869
2026-01-06 12:14:38,243 - INFO -   Batch 1700/2527: Loss = 0.1201
2026-01-06 12:15:03,370 - INFO -   Batch 1750/2527: Loss = 0.1944
2026-01-06 12:15:28,341 - INFO -   Batch 1800/2527: Loss = 0.1216
2026-01-06 12:15:53,151 - INFO -   Batch 1850/2527: Loss = 0.2059
2026-01-06 12:16:18,029 - INFO -   Batch 1900/2527: Loss = 0.1278
2026-01-06 12:16:43,104 - INFO -   Batch 1950/2527: Loss = 0.1730
2026-01-06 12:17:07,956 - INFO -   Batch 2000/2527: Loss = 0.1191
2026-01-06 12:17:33,111 - INFO -   Batch 2050/2527: Loss = 0.1446
2026-01-06 12:17:58,117 - INFO -   Batch 2100/2527: Loss = 0.1648
2026-01-06 12:18:22,933 - INFO -   Batch 2150/2527: Loss = 0.1944
2026-01-06 12:18:47,761 - INFO -   Batch 2200/2527: Loss = 0.1616
2026-01-06 12:19:13,163 - INFO -   Batch 2250/2527: Loss = 0.1033
2026-01-06 12:19:38,025 - INFO -   Batch 2300/2527: Loss = 0.1520
2026-01-06 12:20:02,847 - INFO -   Batch 2350/2527: Loss = 0.1625
2026-01-06 12:20:27,980 - INFO -   Batch 2400/2527: Loss = 0.1721
2026-01-06 12:20:52,949 - INFO -   Batch 2450/2527: Loss = 0.1654
2026-01-06 12:21:17,765 - INFO -   Batch 2500/2527: Loss = 0.1570
2026-01-06 12:25:58,390 - INFO - Epoch 29/160: Train Loss: 0.1768, Val Loss: 1.0199, Val mIoU: 40.80%, Val PixelAcc: 79.31%
2026-01-06 12:25:58,392 - INFO -   → New best mIoU: 40.80%
2026-01-06 12:26:00,731 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 12:26:00,733 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 12:26:26,937 - INFO -   Batch 50/2527: Loss = 0.2175
2026-01-06 12:26:51,911 - INFO -   Batch 100/2527: Loss = 0.1405
2026-01-06 12:27:16,706 - INFO -   Batch 150/2527: Loss = 0.2286
2026-01-06 12:27:41,552 - INFO -   Batch 200/2527: Loss = 0.1223
2026-01-06 12:28:06,663 - INFO -   Batch 250/2527: Loss = 0.1402
2026-01-06 12:28:31,633 - INFO -   Batch 300/2527: Loss = 0.1377
2026-01-06 12:28:56,440 - INFO -   Batch 350/2527: Loss = 0.1759
2026-01-06 12:29:21,272 - INFO -   Batch 400/2527: Loss = 0.2379
2026-01-06 12:29:46,352 - INFO -   Batch 450/2527: Loss = 0.1384
2026-01-06 12:30:11,189 - INFO -   Batch 500/2527: Loss = 0.1578
2026-01-06 12:30:36,315 - INFO -   Batch 550/2527: Loss = 0.2002
2026-01-06 12:31:01,285 - INFO -   Batch 600/2527: Loss = 0.2804
2026-01-06 12:31:26,113 - INFO -   Batch 650/2527: Loss = 0.1076
2026-01-06 12:31:50,949 - INFO -   Batch 700/2527: Loss = 0.1527
2026-01-06 12:32:15,987 - INFO -   Batch 750/2527: Loss = 0.1256
2026-01-06 12:32:40,808 - INFO -   Batch 800/2527: Loss = 0.3622
2026-01-06 12:33:05,642 - INFO -   Batch 850/2527: Loss = 0.1772
2026-01-06 12:33:31,122 - INFO -   Batch 900/2527: Loss = 0.1989
2026-01-06 12:33:56,092 - INFO -   Batch 950/2527: Loss = 0.1152
2026-01-06 12:34:20,914 - INFO -   Batch 1000/2527: Loss = 0.1178
2026-01-06 12:34:45,756 - INFO -   Batch 1050/2527: Loss = 0.1889
2026-01-06 12:35:10,494 - INFO -   Batch 1100/2527: Loss = 0.1447
2026-01-06 12:35:35,718 - INFO -   Batch 1150/2527: Loss = 0.1909
2026-01-06 12:36:00,832 - INFO -   Batch 1200/2527: Loss = 0.1582
2026-01-06 12:36:25,803 - INFO -   Batch 1250/2527: Loss = 0.1828
2026-01-06 12:36:50,629 - INFO -   Batch 1300/2527: Loss = 0.1583
2026-01-06 12:37:15,482 - INFO -   Batch 1350/2527: Loss = 0.0958
2026-01-06 12:37:40,511 - INFO -   Batch 1400/2527: Loss = 0.1466
2026-01-06 12:38:05,340 - INFO -   Batch 1450/2527: Loss = 0.1780
2026-01-06 12:38:30,169 - INFO -   Batch 1500/2527: Loss = 0.1405
2026-01-06 12:38:55,285 - INFO -   Batch 1550/2527: Loss = 0.1474
2026-01-06 12:39:20,255 - INFO -   Batch 1600/2527: Loss = 0.1322
2026-01-06 12:39:45,068 - INFO -   Batch 1650/2527: Loss = 0.1489
2026-01-06 12:40:09,935 - INFO -   Batch 1700/2527: Loss = 0.1852
2026-01-06 12:40:34,683 - INFO -   Batch 1750/2527: Loss = 0.1206
2026-01-06 12:40:59,513 - INFO -   Batch 1800/2527: Loss = 0.1373
2026-01-06 12:41:24,621 - INFO -   Batch 1850/2527: Loss = 0.1241
2026-01-06 12:41:49,953 - INFO -   Batch 1900/2527: Loss = 0.2159
2026-01-06 12:42:14,764 - INFO -   Batch 1950/2527: Loss = 0.1421
2026-01-06 12:42:39,600 - INFO -   Batch 2000/2527: Loss = 0.5578
2026-01-06 12:43:04,646 - INFO -   Batch 2050/2527: Loss = 0.1593
2026-01-06 12:43:29,485 - INFO -   Batch 2100/2527: Loss = 0.2141
2026-01-06 12:43:54,324 - INFO -   Batch 2150/2527: Loss = 0.2255
2026-01-06 12:44:19,478 - INFO -   Batch 2200/2527: Loss = 0.2677
2026-01-06 12:44:44,476 - INFO -   Batch 2250/2527: Loss = 0.1341
2026-01-06 12:45:09,318 - INFO -   Batch 2300/2527: Loss = 0.2527
2026-01-06 12:45:34,520 - INFO -   Batch 2350/2527: Loss = 0.1812
2026-01-06 12:45:59,238 - INFO -   Batch 2400/2527: Loss = 0.1518
2026-01-06 12:46:24,067 - INFO -   Batch 2450/2527: Loss = 0.2754
2026-01-06 12:46:49,198 - INFO -   Batch 2500/2527: Loss = 0.1923
2026-01-06 12:51:41,964 - INFO - Epoch 30/160: Train Loss: 0.1641, Val Loss: 1.0454, Val mIoU: 39.95%, Val PixelAcc: 78.68%
2026-01-06 12:51:44,108 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_30.pth
2026-01-06 12:51:44,110 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_30.pth
2026-01-06 12:52:09,162 - INFO -   Batch 50/2527: Loss = 0.1408
2026-01-06 12:52:32,473 - INFO -   Batch 100/2527: Loss = 0.2003
2026-01-06 12:52:55,402 - INFO -   Batch 150/2527: Loss = 0.1560
2026-01-06 12:53:18,344 - INFO -   Batch 200/2527: Loss = 0.1362
2026-01-06 12:53:41,292 - INFO -   Batch 250/2527: Loss = 0.1633
2026-01-06 12:54:04,245 - INFO -   Batch 300/2527: Loss = 0.1760
2026-01-06 12:54:27,189 - INFO -   Batch 350/2527: Loss = 0.2505
2026-01-06 12:54:50,136 - INFO -   Batch 400/2527: Loss = 0.1656
2026-01-06 12:55:13,091 - INFO -   Batch 450/2527: Loss = 0.1538
2026-01-06 12:55:36,054 - INFO -   Batch 500/2527: Loss = 0.1394
2026-01-06 12:55:59,010 - INFO -   Batch 550/2527: Loss = 0.1801
2026-01-06 12:56:21,968 - INFO -   Batch 600/2527: Loss = 0.2707
2026-01-06 12:56:44,924 - INFO -   Batch 650/2527: Loss = 0.1455
2026-01-06 12:57:07,885 - INFO -   Batch 700/2527: Loss = 0.1811
2026-01-06 12:57:30,841 - INFO -   Batch 750/2527: Loss = 0.0805
2026-01-06 12:57:53,797 - INFO -   Batch 800/2527: Loss = 0.1582
2026-01-06 12:58:17,112 - INFO -   Batch 850/2527: Loss = 0.1469
2026-01-06 12:58:40,068 - INFO -   Batch 900/2527: Loss = 0.1017
2026-01-06 12:59:03,032 - INFO -   Batch 950/2527: Loss = 0.1764
2026-01-06 12:59:25,999 - INFO -   Batch 1000/2527: Loss = 0.1472
2026-01-06 12:59:48,981 - INFO -   Batch 1050/2527: Loss = 0.1677
2026-01-06 13:00:11,945 - INFO -   Batch 1100/2527: Loss = 0.1965
2026-01-06 13:00:34,918 - INFO -   Batch 1150/2527: Loss = 0.1363
2026-01-06 13:00:57,915 - INFO -   Batch 1200/2527: Loss = 0.2270
2026-01-06 13:01:21,230 - INFO -   Batch 1250/2527: Loss = 0.1427
2026-01-06 13:01:44,194 - INFO -   Batch 1300/2527: Loss = 0.1758
2026-01-06 13:02:07,196 - INFO -   Batch 1350/2527: Loss = 0.1666
2026-01-06 13:02:30,159 - INFO -   Batch 1400/2527: Loss = 0.1491
2026-01-06 13:02:53,119 - INFO -   Batch 1450/2527: Loss = 0.1036
2026-01-06 13:03:16,083 - INFO -   Batch 1500/2527: Loss = 0.1044
2026-01-06 13:03:39,414 - INFO -   Batch 1550/2527: Loss = 0.1158
2026-01-06 13:04:02,371 - INFO -   Batch 1600/2527: Loss = 0.2300
2026-01-06 13:04:25,326 - INFO -   Batch 1650/2527: Loss = 0.1787
2026-01-06 13:04:48,283 - INFO -   Batch 1700/2527: Loss = 0.1475
2026-01-06 13:05:11,245 - INFO -   Batch 1750/2527: Loss = 0.1520
2026-01-06 13:05:34,205 - INFO -   Batch 1800/2527: Loss = 0.1245
2026-01-06 13:05:57,160 - INFO -   Batch 1850/2527: Loss = 0.1557
2026-01-06 13:06:20,129 - INFO -   Batch 1900/2527: Loss = 0.1163
2026-01-06 13:06:43,085 - INFO -   Batch 1950/2527: Loss = 0.1949
2026-01-06 13:07:06,044 - INFO -   Batch 2000/2527: Loss = 0.1821
2026-01-06 13:07:29,017 - INFO -   Batch 2050/2527: Loss = 0.1221
2026-01-06 13:07:51,969 - INFO -   Batch 2100/2527: Loss = 0.1272
2026-01-06 13:08:14,923 - INFO -   Batch 2150/2527: Loss = 0.1823
2026-01-06 13:08:37,879 - INFO -   Batch 2200/2527: Loss = 0.1451
2026-01-06 13:09:00,847 - INFO -   Batch 2250/2527: Loss = 0.1701
2026-01-06 13:09:24,176 - INFO -   Batch 2300/2527: Loss = 0.1283
2026-01-06 13:09:47,155 - INFO -   Batch 2350/2527: Loss = 0.1873
2026-01-06 13:10:10,108 - INFO -   Batch 2400/2527: Loss = 0.1509
2026-01-06 13:10:33,069 - INFO -   Batch 2450/2527: Loss = 0.3056
2026-01-06 13:10:56,024 - INFO -   Batch 2500/2527: Loss = 0.2460
2026-01-06 13:15:36,056 - INFO - Epoch 31/160: Train Loss: 0.1543, Val Loss: 1.0443, Val mIoU: 39.74%, Val PixelAcc: 79.04%
2026-01-06 13:16:01,261 - INFO -   Batch 50/2527: Loss = 0.1293
2026-01-06 13:16:24,838 - INFO -   Batch 100/2527: Loss = 0.1014
2026-01-06 13:16:49,124 - INFO -   Batch 150/2527: Loss = 0.1890
2026-01-06 13:17:12,707 - INFO -   Batch 200/2527: Loss = 0.1929
2026-01-06 13:17:36,484 - INFO -   Batch 250/2527: Loss = 0.1552
2026-01-06 13:18:00,108 - INFO -   Batch 300/2527: Loss = 0.0859
2026-01-06 13:18:23,921 - INFO -   Batch 350/2527: Loss = 0.1518
2026-01-06 13:18:47,775 - INFO -   Batch 400/2527: Loss = 0.1541
2026-01-06 13:19:11,939 - INFO -   Batch 450/2527: Loss = 0.1605
2026-01-06 13:19:35,556 - INFO -   Batch 500/2527: Loss = 0.0849
2026-01-06 13:19:59,211 - INFO -   Batch 550/2527: Loss = 0.1712
2026-01-06 13:20:23,162 - INFO -   Batch 600/2527: Loss = 0.1372
2026-01-06 13:20:46,818 - INFO -   Batch 650/2527: Loss = 0.1576
2026-01-06 13:21:10,574 - INFO -   Batch 700/2527: Loss = 0.1244
2026-01-06 13:21:34,175 - INFO -   Batch 750/2527: Loss = 0.2079
2026-01-06 13:21:57,990 - INFO -   Batch 800/2527: Loss = 0.2031
2026-01-06 13:22:21,914 - INFO -   Batch 850/2527: Loss = 0.1064
2026-01-06 13:22:45,705 - INFO -   Batch 900/2527: Loss = 0.2170
2026-01-06 13:23:09,308 - INFO -   Batch 950/2527: Loss = 0.1846
2026-01-06 13:23:32,965 - INFO -   Batch 1000/2527: Loss = 0.1585
2026-01-06 13:23:56,925 - INFO -   Batch 1050/2527: Loss = 0.1206
2026-01-06 13:24:20,528 - INFO -   Batch 1100/2527: Loss = 0.0945
2026-01-06 13:24:44,295 - INFO -   Batch 1150/2527: Loss = 0.2309
2026-01-06 13:25:08,275 - INFO -   Batch 1200/2527: Loss = 0.1729
2026-01-06 13:25:32,123 - INFO -   Batch 1250/2527: Loss = 0.1260
2026-01-06 13:25:55,979 - INFO -   Batch 1300/2527: Loss = 0.1535
2026-01-06 13:26:19,755 - INFO -   Batch 1350/2527: Loss = 0.1415
2026-01-06 13:26:43,363 - INFO -   Batch 1400/2527: Loss = 0.0935
2026-01-06 13:27:07,070 - INFO -   Batch 1450/2527: Loss = 0.1734
2026-01-06 13:27:31,022 - INFO -   Batch 1500/2527: Loss = 0.1221
2026-01-06 13:27:54,594 - INFO -   Batch 1550/2527: Loss = 0.1306
2026-01-06 13:28:18,736 - INFO -   Batch 1600/2527: Loss = 0.2011
2026-01-06 13:28:42,373 - INFO -   Batch 1650/2527: Loss = 0.0973
2026-01-06 13:29:06,258 - INFO -   Batch 1700/2527: Loss = 0.1187
2026-01-06 13:29:30,149 - INFO -   Batch 1750/2527: Loss = 0.1606
2026-01-06 13:29:53,945 - INFO -   Batch 1800/2527: Loss = 0.1366
2026-01-06 13:30:17,572 - INFO -   Batch 1850/2527: Loss = 0.2082
2026-01-06 13:30:41,573 - INFO -   Batch 1900/2527: Loss = 0.1651
2026-01-06 13:31:05,529 - INFO -   Batch 1950/2527: Loss = 0.1375
2026-01-06 13:31:29,227 - INFO -   Batch 2000/2527: Loss = 0.1438
2026-01-06 13:31:52,986 - INFO -   Batch 2050/2527: Loss = 0.2021
2026-01-06 13:32:16,667 - INFO -   Batch 2100/2527: Loss = 0.2562
2026-01-06 13:32:40,490 - INFO -   Batch 2150/2527: Loss = 0.1358
2026-01-06 13:33:04,360 - INFO -   Batch 2200/2527: Loss = 0.1702
2026-01-06 13:33:28,155 - INFO -   Batch 2250/2527: Loss = 0.0932
2026-01-06 13:33:51,776 - INFO -   Batch 2300/2527: Loss = 0.1393
2026-01-06 13:34:15,419 - INFO -   Batch 2350/2527: Loss = 0.1858
2026-01-06 13:34:39,469 - INFO -   Batch 2400/2527: Loss = 0.1414
2026-01-06 13:35:03,081 - INFO -   Batch 2450/2527: Loss = 0.1713
2026-01-06 13:35:26,840 - INFO -   Batch 2500/2527: Loss = 0.1882
2026-01-06 13:40:19,136 - INFO - Epoch 32/160: Train Loss: 0.1578, Val Loss: 1.0480, Val mIoU: 39.95%, Val PixelAcc: 79.16%
2026-01-06 13:40:45,135 - INFO -   Batch 50/2527: Loss = 0.1509
2026-01-06 13:41:10,574 - INFO -   Batch 100/2527: Loss = 0.1464
2026-01-06 13:41:35,510 - INFO -   Batch 150/2527: Loss = 0.1486
2026-01-06 13:42:00,321 - INFO -   Batch 200/2527: Loss = 0.2491
2026-01-06 13:42:25,138 - INFO -   Batch 250/2527: Loss = 0.1358
2026-01-06 13:42:50,176 - INFO -   Batch 300/2527: Loss = 0.1139
2026-01-06 13:43:15,028 - INFO -   Batch 350/2527: Loss = 0.1245
2026-01-06 13:43:39,889 - INFO -   Batch 400/2527: Loss = 0.1436
2026-01-06 13:44:05,002 - INFO -   Batch 450/2527: Loss = 0.1263
2026-01-06 13:44:29,992 - INFO -   Batch 500/2527: Loss = 0.1384
2026-01-06 13:44:55,173 - INFO -   Batch 550/2527: Loss = 0.1522
2026-01-06 13:45:20,010 - INFO -   Batch 600/2527: Loss = 0.1457
2026-01-06 13:45:44,751 - INFO -   Batch 650/2527: Loss = 0.1532
2026-01-06 13:46:09,584 - INFO -   Batch 700/2527: Loss = 0.1453
2026-01-06 13:46:34,725 - INFO -   Batch 750/2527: Loss = 0.1623
2026-01-06 13:47:00,077 - INFO -   Batch 800/2527: Loss = 0.2045
2026-01-06 13:47:24,905 - INFO -   Batch 850/2527: Loss = 0.2433
2026-01-06 13:47:49,737 - INFO -   Batch 900/2527: Loss = 0.1461
2026-01-06 13:48:14,763 - INFO -   Batch 950/2527: Loss = 0.1183
2026-01-06 13:48:39,617 - INFO -   Batch 1000/2527: Loss = 0.1226
2026-01-06 13:49:04,444 - INFO -   Batch 1050/2527: Loss = 0.1912
2026-01-06 13:49:29,575 - INFO -   Batch 1100/2527: Loss = 0.3729
2026-01-06 13:49:54,541 - INFO -   Batch 1150/2527: Loss = 0.1441
2026-01-06 13:50:19,365 - INFO -   Batch 1200/2527: Loss = 0.1275
2026-01-06 13:50:44,235 - INFO -   Batch 1250/2527: Loss = 0.1700
2026-01-06 13:51:08,961 - INFO -   Batch 1300/2527: Loss = 0.1251
2026-01-06 13:51:33,820 - INFO -   Batch 1350/2527: Loss = 0.3615
2026-01-06 13:51:58,930 - INFO -   Batch 1400/2527: Loss = 0.2034
2026-01-06 13:52:23,901 - INFO -   Batch 1450/2527: Loss = 0.1659
2026-01-06 13:52:48,762 - INFO -   Batch 1500/2527: Loss = 0.1604
2026-01-06 13:53:13,965 - INFO -   Batch 1550/2527: Loss = 0.1939
2026-01-06 13:53:38,976 - INFO -   Batch 1600/2527: Loss = 0.1615
2026-01-06 13:54:03,796 - INFO -   Batch 1650/2527: Loss = 0.1184
2026-01-06 13:54:28,646 - INFO -   Batch 1700/2527: Loss = 0.1484
2026-01-06 13:54:53,773 - INFO -   Batch 1750/2527: Loss = 0.0817
2026-01-06 13:55:18,732 - INFO -   Batch 1800/2527: Loss = 0.1593
2026-01-06 13:55:43,578 - INFO -   Batch 1850/2527: Loss = 0.1780
2026-01-06 13:56:08,418 - INFO -   Batch 1900/2527: Loss = 0.1491
2026-01-06 13:56:33,168 - INFO -   Batch 1950/2527: Loss = 0.1110
2026-01-06 13:56:58,352 - INFO -   Batch 2000/2527: Loss = 0.1997
2026-01-06 13:57:23,462 - INFO -   Batch 2050/2527: Loss = 0.1981
2026-01-06 13:57:48,444 - INFO -   Batch 2100/2527: Loss = 0.1402
2026-01-06 13:58:13,275 - INFO -   Batch 2150/2527: Loss = 0.1566
2026-01-06 13:58:38,141 - INFO -   Batch 2200/2527: Loss = 0.1173
2026-01-06 13:59:03,537 - INFO -   Batch 2250/2527: Loss = 0.1016
2026-01-06 13:59:28,358 - INFO -   Batch 2300/2527: Loss = 0.1753
2026-01-06 13:59:53,193 - INFO -   Batch 2350/2527: Loss = 0.1467
2026-01-06 14:00:18,329 - INFO -   Batch 2400/2527: Loss = 0.1912
2026-01-06 14:00:43,310 - INFO -   Batch 2450/2527: Loss = 0.0985
2026-01-06 14:01:08,141 - INFO -   Batch 2500/2527: Loss = 0.2000
2026-01-06 14:05:48,665 - INFO - Epoch 33/160: Train Loss: 0.1524, Val Loss: 1.0624, Val mIoU: 40.00%, Val PixelAcc: 78.96%
2026-01-06 14:06:15,192 - INFO -   Batch 50/2527: Loss = 0.1529
2026-01-06 14:06:40,166 - INFO -   Batch 100/2527: Loss = 0.1445
2026-01-06 14:07:04,966 - INFO -   Batch 150/2527: Loss = 0.1305
2026-01-06 14:07:29,811 - INFO -   Batch 200/2527: Loss = 0.1170
2026-01-06 14:07:54,919 - INFO -   Batch 250/2527: Loss = 0.1724
2026-01-06 14:08:19,891 - INFO -   Batch 300/2527: Loss = 0.1083
2026-01-06 14:08:44,701 - INFO -   Batch 350/2527: Loss = 0.1249
2026-01-06 14:09:09,560 - INFO -   Batch 400/2527: Loss = 0.1412
2026-01-06 14:09:34,336 - INFO -   Batch 450/2527: Loss = 0.1242
2026-01-06 14:09:59,528 - INFO -   Batch 500/2527: Loss = 0.0940
2026-01-06 14:10:24,635 - INFO -   Batch 550/2527: Loss = 0.1900
2026-01-06 14:10:49,609 - INFO -   Batch 600/2527: Loss = 0.1554
2026-01-06 14:11:14,515 - INFO -   Batch 650/2527: Loss = 0.1297
2026-01-06 14:11:39,359 - INFO -   Batch 700/2527: Loss = 0.1989
2026-01-06 14:12:04,385 - INFO -   Batch 750/2527: Loss = 0.1716
2026-01-06 14:12:29,216 - INFO -   Batch 800/2527: Loss = 0.1770
2026-01-06 14:12:54,061 - INFO -   Batch 850/2527: Loss = 0.1468
2026-01-06 14:13:19,555 - INFO -   Batch 900/2527: Loss = 0.1511
2026-01-06 14:13:44,525 - INFO -   Batch 950/2527: Loss = 0.1226
2026-01-06 14:14:09,482 - INFO -   Batch 1000/2527: Loss = 0.1233
2026-01-06 14:14:34,363 - INFO -   Batch 1050/2527: Loss = 0.2209
2026-01-06 14:14:59,090 - INFO -   Batch 1100/2527: Loss = 0.1367
2026-01-06 14:15:23,966 - INFO -   Batch 1150/2527: Loss = 0.1647
2026-01-06 14:15:49,443 - INFO -   Batch 1200/2527: Loss = 0.1959
2026-01-06 14:16:14,427 - INFO -   Batch 1250/2527: Loss = 0.1828
2026-01-06 14:16:39,275 - INFO -   Batch 1300/2527: Loss = 0.1399
2026-01-06 14:17:04,111 - INFO -   Batch 1350/2527: Loss = 0.1815
2026-01-06 14:17:29,151 - INFO -   Batch 1400/2527: Loss = 0.1282
2026-01-06 14:17:54,051 - INFO -   Batch 1450/2527: Loss = 0.1676
2026-01-06 14:18:18,925 - INFO -   Batch 1500/2527: Loss = 0.1015
2026-01-06 14:18:44,057 - INFO -   Batch 1550/2527: Loss = 0.1667
2026-01-06 14:19:09,027 - INFO -   Batch 1600/2527: Loss = 0.1607
2026-01-06 14:19:33,835 - INFO -   Batch 1650/2527: Loss = 0.1656
2026-01-06 14:19:58,686 - INFO -   Batch 1700/2527: Loss = 0.1427
2026-01-06 14:20:23,404 - INFO -   Batch 1750/2527: Loss = 0.1866
2026-01-06 14:20:48,232 - INFO -   Batch 1800/2527: Loss = 0.1085
2026-01-06 14:21:13,375 - INFO -   Batch 1850/2527: Loss = 0.1140
2026-01-06 14:21:38,380 - INFO -   Batch 1900/2527: Loss = 0.1439
2026-01-06 14:22:03,599 - INFO -   Batch 1950/2527: Loss = 0.1978
2026-01-06 14:22:28,451 - INFO -   Batch 2000/2527: Loss = 0.1685
2026-01-06 14:22:53,469 - INFO -   Batch 2050/2527: Loss = 0.1177
2026-01-06 14:23:18,293 - INFO -   Batch 2100/2527: Loss = 0.1035
2026-01-06 14:23:43,126 - INFO -   Batch 2150/2527: Loss = 0.1348
2026-01-06 14:24:08,269 - INFO -   Batch 2200/2527: Loss = 0.0932
2026-01-06 14:24:33,240 - INFO -   Batch 2250/2527: Loss = 0.1855
2026-01-06 14:24:58,116 - INFO -   Batch 2300/2527: Loss = 0.1230
2026-01-06 14:25:23,336 - INFO -   Batch 2350/2527: Loss = 0.2102
2026-01-06 14:25:48,059 - INFO -   Batch 2400/2527: Loss = 0.1070
2026-01-06 14:26:12,891 - INFO -   Batch 2450/2527: Loss = 0.1472
2026-01-06 14:26:38,009 - INFO -   Batch 2500/2527: Loss = 0.1356
2026-01-06 14:31:30,971 - INFO - Epoch 34/160: Train Loss: 0.1545, Val Loss: 1.0642, Val mIoU: 40.77%, Val PixelAcc: 79.39%
2026-01-06 14:31:55,801 - INFO -   Batch 50/2527: Loss = 0.1075
2026-01-06 14:32:19,041 - INFO -   Batch 100/2527: Loss = 0.1748
2026-01-06 14:32:42,067 - INFO -   Batch 150/2527: Loss = 0.0885
2026-01-06 14:33:05,020 - INFO -   Batch 200/2527: Loss = 0.1027
2026-01-06 14:33:28,006 - INFO -   Batch 250/2527: Loss = 0.1415
2026-01-06 14:33:50,951 - INFO -   Batch 300/2527: Loss = 0.1385
2026-01-06 14:34:13,901 - INFO -   Batch 350/2527: Loss = 0.1391
2026-01-06 14:34:36,860 - INFO -   Batch 400/2527: Loss = 0.0917
2026-01-06 14:34:59,849 - INFO -   Batch 450/2527: Loss = 0.1092
2026-01-06 14:35:22,811 - INFO -   Batch 500/2527: Loss = 0.1119
2026-01-06 14:35:45,771 - INFO -   Batch 550/2527: Loss = 0.1042
2026-01-06 14:36:08,726 - INFO -   Batch 600/2527: Loss = 0.0714
2026-01-06 14:36:31,709 - INFO -   Batch 650/2527: Loss = 0.1048
2026-01-06 14:36:54,709 - INFO -   Batch 700/2527: Loss = 0.1661
2026-01-06 14:37:17,677 - INFO -   Batch 750/2527: Loss = 0.1373
2026-01-06 14:37:40,644 - INFO -   Batch 800/2527: Loss = 0.1602
2026-01-06 14:38:04,001 - INFO -   Batch 850/2527: Loss = 0.1880
2026-01-06 14:38:26,965 - INFO -   Batch 900/2527: Loss = 0.1565
2026-01-06 14:38:49,950 - INFO -   Batch 950/2527: Loss = 0.1360
2026-01-06 14:39:12,915 - INFO -   Batch 1000/2527: Loss = 0.1462
2026-01-06 14:39:35,875 - INFO -   Batch 1050/2527: Loss = 0.0950
2026-01-06 14:39:58,874 - INFO -   Batch 1100/2527: Loss = 0.1001
2026-01-06 14:40:21,834 - INFO -   Batch 1150/2527: Loss = 0.1509
2026-01-06 14:40:44,793 - INFO -   Batch 1200/2527: Loss = 0.1431
2026-01-06 14:41:08,126 - INFO -   Batch 1250/2527: Loss = 0.1049
2026-01-06 14:41:31,136 - INFO -   Batch 1300/2527: Loss = 0.2513
2026-01-06 14:41:54,097 - INFO -   Batch 1350/2527: Loss = 0.1008
2026-01-06 14:42:17,064 - INFO -   Batch 1400/2527: Loss = 0.0899
2026-01-06 14:42:40,025 - INFO -   Batch 1450/2527: Loss = 0.1916
2026-01-06 14:43:02,988 - INFO -   Batch 1500/2527: Loss = 0.1780
2026-01-06 14:43:26,314 - INFO -   Batch 1550/2527: Loss = 0.1401
2026-01-06 14:43:49,264 - INFO -   Batch 1600/2527: Loss = 0.2104
2026-01-06 14:44:12,243 - INFO -   Batch 1650/2527: Loss = 0.1901
2026-01-06 14:44:35,236 - INFO -   Batch 1700/2527: Loss = 0.1195
2026-01-06 14:44:58,202 - INFO -   Batch 1750/2527: Loss = 0.2007
2026-01-06 14:45:21,172 - INFO -   Batch 1800/2527: Loss = 0.2404
2026-01-06 14:45:44,124 - INFO -   Batch 1850/2527: Loss = 0.1519
2026-01-06 14:46:07,104 - INFO -   Batch 1900/2527: Loss = 0.1477
2026-01-06 14:46:30,055 - INFO -   Batch 1950/2527: Loss = 0.1177
2026-01-06 14:46:53,002 - INFO -   Batch 2000/2527: Loss = 0.2600
2026-01-06 14:47:16,013 - INFO -   Batch 2050/2527: Loss = 0.1658
2026-01-06 14:47:39,004 - INFO -   Batch 2100/2527: Loss = 0.1187
2026-01-06 14:48:02,137 - INFO -   Batch 2150/2527: Loss = 0.2603
2026-01-06 14:48:25,065 - INFO -   Batch 2200/2527: Loss = 0.1893
2026-01-06 14:48:47,979 - INFO -   Batch 2250/2527: Loss = 0.1563
2026-01-06 14:49:11,269 - INFO -   Batch 2300/2527: Loss = 0.1934
2026-01-06 14:49:34,202 - INFO -   Batch 2350/2527: Loss = 0.1797
2026-01-06 14:49:57,127 - INFO -   Batch 2400/2527: Loss = 0.1382
2026-01-06 14:50:20,070 - INFO -   Batch 2450/2527: Loss = 0.2157
2026-01-06 14:50:43,015 - INFO -   Batch 2500/2527: Loss = 0.1985
2026-01-06 14:55:22,866 - INFO - Epoch 35/160: Train Loss: 0.1517, Val Loss: 1.0530, Val mIoU: 40.18%, Val PixelAcc: 78.93%
2026-01-06 14:55:48,373 - INFO -   Batch 50/2527: Loss = 0.1413
2026-01-06 14:56:11,954 - INFO -   Batch 100/2527: Loss = 0.1515
2026-01-06 14:56:35,874 - INFO -   Batch 150/2527: Loss = 0.1214
2026-01-06 14:56:59,814 - INFO -   Batch 200/2527: Loss = 0.1276
2026-01-06 14:57:23,559 - INFO -   Batch 250/2527: Loss = 0.1455
2026-01-06 14:57:47,191 - INFO -   Batch 300/2527: Loss = 0.1386
2026-01-06 14:58:11,012 - INFO -   Batch 350/2527: Loss = 0.1525
2026-01-06 14:58:34,854 - INFO -   Batch 400/2527: Loss = 0.1426
2026-01-06 14:58:58,654 - INFO -   Batch 450/2527: Loss = 0.1676
2026-01-06 14:59:22,628 - INFO -   Batch 500/2527: Loss = 0.1514
2026-01-06 14:59:46,281 - INFO -   Batch 550/2527: Loss = 0.0922
2026-01-06 15:00:10,249 - INFO -   Batch 600/2527: Loss = 0.1402
2026-01-06 15:00:33,915 - INFO -   Batch 650/2527: Loss = 0.1682
2026-01-06 15:00:57,671 - INFO -   Batch 700/2527: Loss = 0.1126
2026-01-06 15:01:21,373 - INFO -   Batch 750/2527: Loss = 0.1938
2026-01-06 15:01:45,189 - INFO -   Batch 800/2527: Loss = 0.1859
2026-01-06 15:02:09,042 - INFO -   Batch 850/2527: Loss = 0.1645
2026-01-06 15:02:32,825 - INFO -   Batch 900/2527: Loss = 0.1393
2026-01-06 15:02:56,467 - INFO -   Batch 950/2527: Loss = 0.1235
2026-01-06 15:03:20,168 - INFO -   Batch 1000/2527: Loss = 0.1295
2026-01-06 15:03:44,164 - INFO -   Batch 1050/2527: Loss = 0.1460
2026-01-06 15:04:08,170 - INFO -   Batch 1100/2527: Loss = 0.1588
2026-01-06 15:04:31,932 - INFO -   Batch 1150/2527: Loss = 0.1161
2026-01-06 15:04:55,536 - INFO -   Batch 1200/2527: Loss = 0.1701
2026-01-06 15:05:19,367 - INFO -   Batch 1250/2527: Loss = 0.1440
2026-01-06 15:05:43,236 - INFO -   Batch 1300/2527: Loss = 0.1453
2026-01-06 15:06:07,055 - INFO -   Batch 1350/2527: Loss = 0.1227
2026-01-06 15:06:31,007 - INFO -   Batch 1400/2527: Loss = 0.1417
2026-01-06 15:06:54,622 - INFO -   Batch 1450/2527: Loss = 0.1520
2026-01-06 15:07:18,564 - INFO -   Batch 1500/2527: Loss = 0.1147
2026-01-06 15:07:42,139 - INFO -   Batch 1550/2527: Loss = 0.1256
2026-01-06 15:08:05,926 - INFO -   Batch 1600/2527: Loss = 0.1204
2026-01-06 15:08:29,528 - INFO -   Batch 1650/2527: Loss = 0.3625
2026-01-06 15:08:53,345 - INFO -   Batch 1700/2527: Loss = 0.1467
2026-01-06 15:09:17,210 - INFO -   Batch 1750/2527: Loss = 0.1222
2026-01-06 15:09:41,004 - INFO -   Batch 1800/2527: Loss = 0.1095
2026-01-06 15:10:04,611 - INFO -   Batch 1850/2527: Loss = 0.1046
2026-01-06 15:10:28,269 - INFO -   Batch 1900/2527: Loss = 0.0869
2026-01-06 15:10:52,221 - INFO -   Batch 1950/2527: Loss = 0.1208
2026-01-06 15:11:15,837 - INFO -   Batch 2000/2527: Loss = 0.1053
2026-01-06 15:11:39,585 - INFO -   Batch 2050/2527: Loss = 0.0994
2026-01-06 15:12:03,200 - INFO -   Batch 2100/2527: Loss = 0.1180
2026-01-06 15:12:27,380 - INFO -   Batch 2150/2527: Loss = 0.1026
2026-01-06 15:12:51,315 - INFO -   Batch 2200/2527: Loss = 0.1878
2026-01-06 15:13:15,175 - INFO -   Batch 2250/2527: Loss = 0.1410
2026-01-06 15:13:38,806 - INFO -   Batch 2300/2527: Loss = 0.1416
2026-01-06 15:14:02,466 - INFO -   Batch 2350/2527: Loss = 0.1549
2026-01-06 15:14:26,425 - INFO -   Batch 2400/2527: Loss = 0.1835
2026-01-06 15:14:50,003 - INFO -   Batch 2450/2527: Loss = 0.1337
2026-01-06 15:15:13,773 - INFO -   Batch 2500/2527: Loss = 0.1538
2026-01-06 15:20:05,875 - INFO - Epoch 36/160: Train Loss: 0.1441, Val Loss: 1.0646, Val mIoU: 40.79%, Val PixelAcc: 79.02%
2026-01-06 15:20:32,531 - INFO -   Batch 50/2527: Loss = 0.1816
2026-01-06 15:20:57,600 - INFO -   Batch 100/2527: Loss = 0.1633
2026-01-06 15:21:22,530 - INFO -   Batch 150/2527: Loss = 0.1101
2026-01-06 15:21:47,361 - INFO -   Batch 200/2527: Loss = 0.1623
2026-01-06 15:22:12,187 - INFO -   Batch 250/2527: Loss = 0.2199
2026-01-06 15:22:37,231 - INFO -   Batch 300/2527: Loss = 0.1503
2026-01-06 15:23:02,453 - INFO -   Batch 350/2527: Loss = 0.1376
2026-01-06 15:23:27,277 - INFO -   Batch 400/2527: Loss = 0.1869
2026-01-06 15:23:52,383 - INFO -   Batch 450/2527: Loss = 0.1370
2026-01-06 15:24:17,341 - INFO -   Batch 500/2527: Loss = 0.1189
2026-01-06 15:24:42,189 - INFO -   Batch 550/2527: Loss = 0.0905
2026-01-06 15:25:07,023 - INFO -   Batch 600/2527: Loss = 0.1772
2026-01-06 15:25:31,749 - INFO -   Batch 650/2527: Loss = 0.1263
2026-01-06 15:25:56,592 - INFO -   Batch 700/2527: Loss = 0.1296
2026-01-06 15:26:21,707 - INFO -   Batch 750/2527: Loss = 0.1601
2026-01-06 15:26:46,707 - INFO -   Batch 800/2527: Loss = 0.1694
2026-01-06 15:27:11,553 - INFO -   Batch 850/2527: Loss = 0.1816
2026-01-06 15:27:36,426 - INFO -   Batch 900/2527: Loss = 0.1387
2026-01-06 15:28:01,430 - INFO -   Batch 950/2527: Loss = 0.1541
2026-01-06 15:28:26,235 - INFO -   Batch 1000/2527: Loss = 0.1039
2026-01-06 15:28:51,460 - INFO -   Batch 1050/2527: Loss = 0.1520
2026-01-06 15:29:16,590 - INFO -   Batch 1100/2527: Loss = 0.1117
2026-01-06 15:29:41,580 - INFO -   Batch 1150/2527: Loss = 0.1264
2026-01-06 15:30:06,404 - INFO -   Batch 1200/2527: Loss = 0.1158
2026-01-06 15:30:31,257 - INFO -   Batch 1250/2527: Loss = 0.0794
2026-01-06 15:30:55,977 - INFO -   Batch 1300/2527: Loss = 0.1320
2026-01-06 15:31:20,820 - INFO -   Batch 1350/2527: Loss = 0.2145
2026-01-06 15:31:45,978 - INFO -   Batch 1400/2527: Loss = 0.1536
2026-01-06 15:32:10,957 - INFO -   Batch 1450/2527: Loss = 0.1203
2026-01-06 15:32:36,161 - INFO -   Batch 1500/2527: Loss = 0.1304
2026-01-06 15:33:01,000 - INFO -   Batch 1550/2527: Loss = 0.1333
2026-01-06 15:33:26,011 - INFO -   Batch 1600/2527: Loss = 0.1678
2026-01-06 15:33:50,845 - INFO -   Batch 1650/2527: Loss = 0.1513
2026-01-06 15:34:15,681 - INFO -   Batch 1700/2527: Loss = 0.1378
2026-01-06 15:34:40,846 - INFO -   Batch 1750/2527: Loss = 0.1276
2026-01-06 15:35:06,182 - INFO -   Batch 1800/2527: Loss = 0.1052
2026-01-06 15:35:30,995 - INFO -   Batch 1850/2527: Loss = 0.0854
2026-01-06 15:35:55,834 - INFO -   Batch 1900/2527: Loss = 0.1008
2026-01-06 15:36:20,560 - INFO -   Batch 1950/2527: Loss = 0.1039
2026-01-06 15:36:45,408 - INFO -   Batch 2000/2527: Loss = 0.0772
2026-01-06 15:37:10,517 - INFO -   Batch 2050/2527: Loss = 0.1830
2026-01-06 15:37:35,504 - INFO -   Batch 2100/2527: Loss = 0.0880
2026-01-06 15:38:00,320 - INFO -   Batch 2150/2527: Loss = 0.0846
2026-01-06 15:38:25,157 - INFO -   Batch 2200/2527: Loss = 0.1111
2026-01-06 15:38:50,189 - INFO -   Batch 2250/2527: Loss = 0.1304
2026-01-06 15:39:15,015 - INFO -   Batch 2300/2527: Loss = 0.1230
2026-01-06 15:39:39,868 - INFO -   Batch 2350/2527: Loss = 0.1405
2026-01-06 15:40:04,982 - INFO -   Batch 2400/2527: Loss = 0.1960
2026-01-06 15:40:29,977 - INFO -   Batch 2450/2527: Loss = 0.1309
2026-01-06 15:40:55,176 - INFO -   Batch 2500/2527: Loss = 0.1328
2026-01-06 15:45:35,215 - INFO - Epoch 37/160: Train Loss: 0.1359, Val Loss: 1.0917, Val mIoU: 40.48%, Val PixelAcc: 79.39%
2026-01-06 15:46:01,529 - INFO -   Batch 50/2527: Loss = 0.1158
2026-01-06 15:46:26,498 - INFO -   Batch 100/2527: Loss = 0.1662
2026-01-06 15:46:51,322 - INFO -   Batch 150/2527: Loss = 0.1347
2026-01-06 15:47:16,157 - INFO -   Batch 200/2527: Loss = 0.0948
2026-01-06 15:47:41,259 - INFO -   Batch 250/2527: Loss = 0.1284
2026-01-06 15:48:06,225 - INFO -   Batch 300/2527: Loss = 0.0892
2026-01-06 15:48:31,034 - INFO -   Batch 350/2527: Loss = 0.1445
2026-01-06 15:48:56,236 - INFO -   Batch 400/2527: Loss = 0.1418
2026-01-06 15:49:20,953 - INFO -   Batch 450/2527: Loss = 0.1134
2026-01-06 15:49:45,885 - INFO -   Batch 500/2527: Loss = 0.1187
2026-01-06 15:50:11,086 - INFO -   Batch 550/2527: Loss = 0.1359
2026-01-06 15:50:36,064 - INFO -   Batch 600/2527: Loss = 0.1348
2026-01-06 15:51:00,984 - INFO -   Batch 650/2527: Loss = 0.1532
2026-01-06 15:51:26,319 - INFO -   Batch 700/2527: Loss = 0.1097
2026-01-06 15:51:51,621 - INFO -   Batch 750/2527: Loss = 0.1919
2026-01-06 15:52:16,608 - INFO -   Batch 800/2527: Loss = 0.2415
2026-01-06 15:52:42,063 - INFO -   Batch 850/2527: Loss = 0.2338
2026-01-06 15:53:07,189 - INFO -   Batch 900/2527: Loss = 0.1856
2026-01-06 15:53:32,174 - INFO -   Batch 950/2527: Loss = 0.3408
2026-01-06 15:53:56,990 - INFO -   Batch 1000/2527: Loss = 0.1965
2026-01-06 15:54:21,865 - INFO -   Batch 1050/2527: Loss = 0.2185
2026-01-06 15:54:46,597 - INFO -   Batch 1100/2527: Loss = 0.1709
2026-01-06 15:55:11,438 - INFO -   Batch 1150/2527: Loss = 0.0994
2026-01-06 15:55:36,579 - INFO -   Batch 1200/2527: Loss = 0.1753
2026-01-06 15:56:01,542 - INFO -   Batch 1250/2527: Loss = 0.1500
2026-01-06 15:56:26,386 - INFO -   Batch 1300/2527: Loss = 0.1370
2026-01-06 15:56:51,222 - INFO -   Batch 1350/2527: Loss = 0.2237
2026-01-06 15:57:16,320 - INFO -   Batch 1400/2527: Loss = 0.1669
2026-01-06 15:57:41,528 - INFO -   Batch 1450/2527: Loss = 0.1958
2026-01-06 15:58:06,366 - INFO -   Batch 1500/2527: Loss = 0.1558
2026-01-06 15:58:31,501 - INFO -   Batch 1550/2527: Loss = 0.1063
2026-01-06 15:58:56,476 - INFO -   Batch 1600/2527: Loss = 0.1714
2026-01-06 15:59:21,313 - INFO -   Batch 1650/2527: Loss = 0.1457
2026-01-06 15:59:46,186 - INFO -   Batch 1700/2527: Loss = 0.1167
2026-01-06 16:00:10,949 - INFO -   Batch 1750/2527: Loss = 0.1940
2026-01-06 16:00:36,098 - INFO -   Batch 1800/2527: Loss = 0.1842
2026-01-06 16:01:01,586 - INFO -   Batch 1850/2527: Loss = 0.2366
2026-01-06 16:01:26,540 - INFO -   Batch 1900/2527: Loss = 0.2062
2026-01-06 16:01:51,437 - INFO -   Batch 1950/2527: Loss = 0.1692
2026-01-06 16:02:16,318 - INFO -   Batch 2000/2527: Loss = 0.1443
2026-01-06 16:02:41,320 - INFO -   Batch 2050/2527: Loss = 0.1161
2026-01-06 16:03:06,198 - INFO -   Batch 2100/2527: Loss = 0.1241
2026-01-06 16:03:31,537 - INFO -   Batch 2150/2527: Loss = 0.1075
2026-01-06 16:03:56,675 - INFO -   Batch 2200/2527: Loss = 0.1699
2026-01-06 16:04:21,650 - INFO -   Batch 2250/2527: Loss = 0.1296
2026-01-06 16:04:46,479 - INFO -   Batch 2300/2527: Loss = 0.1494
2026-01-06 16:05:11,295 - INFO -   Batch 2350/2527: Loss = 0.1817
2026-01-06 16:05:35,997 - INFO -   Batch 2400/2527: Loss = 0.1074
2026-01-06 16:06:00,859 - INFO -   Batch 2450/2527: Loss = 0.1088
2026-01-06 16:06:25,993 - INFO -   Batch 2500/2527: Loss = 0.1111
2026-01-06 16:11:18,922 - INFO - Epoch 38/160: Train Loss: 0.1529, Val Loss: 1.0822, Val mIoU: 40.40%, Val PixelAcc: 79.31%
2026-01-06 16:11:43,990 - INFO -   Batch 50/2527: Loss = 0.1299
2026-01-06 16:12:06,876 - INFO -   Batch 100/2527: Loss = 0.0886
2026-01-06 16:12:29,833 - INFO -   Batch 150/2527: Loss = 0.1382
2026-01-06 16:12:52,816 - INFO -   Batch 200/2527: Loss = 0.1437
2026-01-06 16:13:15,756 - INFO -   Batch 250/2527: Loss = 0.1579
2026-01-06 16:13:38,655 - INFO -   Batch 300/2527: Loss = 0.1090
2026-01-06 16:14:01,934 - INFO -   Batch 350/2527: Loss = 0.1458
2026-01-06 16:14:24,866 - INFO -   Batch 400/2527: Loss = 0.1217
2026-01-06 16:14:47,794 - INFO -   Batch 450/2527: Loss = 0.1656
2026-01-06 16:15:10,774 - INFO -   Batch 500/2527: Loss = 0.1122
2026-01-06 16:15:33,714 - INFO -   Batch 550/2527: Loss = 0.1626
2026-01-06 16:15:56,664 - INFO -   Batch 600/2527: Loss = 0.1530
2026-01-06 16:16:19,623 - INFO -   Batch 650/2527: Loss = 0.1773
2026-01-06 16:16:42,663 - INFO -   Batch 700/2527: Loss = 0.2109
2026-01-06 16:17:05,617 - INFO -   Batch 750/2527: Loss = 0.0893
2026-01-06 16:17:28,947 - INFO -   Batch 800/2527: Loss = 0.1941
2026-01-06 16:17:51,897 - INFO -   Batch 850/2527: Loss = 0.1793
2026-01-06 16:18:14,851 - INFO -   Batch 900/2527: Loss = 0.1740
2026-01-06 16:18:37,807 - INFO -   Batch 950/2527: Loss = 0.1546
2026-01-06 16:19:00,780 - INFO -   Batch 1000/2527: Loss = 0.1007
2026-01-06 16:19:24,108 - INFO -   Batch 1050/2527: Loss = 0.1303
2026-01-06 16:19:47,069 - INFO -   Batch 1100/2527: Loss = 0.1016
2026-01-06 16:20:10,014 - INFO -   Batch 1150/2527: Loss = 0.1850
2026-01-06 16:20:32,967 - INFO -   Batch 1200/2527: Loss = 0.1040
2026-01-06 16:20:55,919 - INFO -   Batch 1250/2527: Loss = 0.1225
2026-01-06 16:21:18,870 - INFO -   Batch 1300/2527: Loss = 0.1142
2026-01-06 16:21:41,936 - INFO -   Batch 1350/2527: Loss = 0.1263
2026-01-06 16:22:04,879 - INFO -   Batch 1400/2527: Loss = 0.1447
2026-01-06 16:22:27,826 - INFO -   Batch 1450/2527: Loss = 0.1383
2026-01-06 16:22:50,762 - INFO -   Batch 1500/2527: Loss = 0.1626
2026-01-06 16:23:13,714 - INFO -   Batch 1550/2527: Loss = 0.1735
2026-01-06 16:23:36,698 - INFO -   Batch 1600/2527: Loss = 0.1285
2026-01-06 16:23:59,640 - INFO -   Batch 1650/2527: Loss = 0.1546
2026-01-06 16:24:22,579 - INFO -   Batch 1700/2527: Loss = 0.1134
2026-01-06 16:24:45,517 - INFO -   Batch 1750/2527: Loss = 0.1063
2026-01-06 16:25:08,846 - INFO -   Batch 1800/2527: Loss = 0.1418
2026-01-06 16:25:31,773 - INFO -   Batch 1850/2527: Loss = 0.1073
2026-01-06 16:25:54,755 - INFO -   Batch 1900/2527: Loss = 0.1411
2026-01-06 16:26:17,743 - INFO -   Batch 1950/2527: Loss = 0.0910
2026-01-06 16:26:40,723 - INFO -   Batch 2000/2527: Loss = 0.1471
2026-01-06 16:27:03,676 - INFO -   Batch 2050/2527: Loss = 0.1217
2026-01-06 16:27:26,595 - INFO -   Batch 2100/2527: Loss = 0.1543
2026-01-06 16:27:49,675 - INFO -   Batch 2150/2527: Loss = 0.1064
2026-01-06 16:28:13,001 - INFO -   Batch 2200/2527: Loss = 0.1293
2026-01-06 16:28:36,041 - INFO -   Batch 2250/2527: Loss = 0.1004
2026-01-06 16:28:58,998 - INFO -   Batch 2300/2527: Loss = 0.1299
2026-01-06 16:29:21,996 - INFO -   Batch 2350/2527: Loss = 0.1311
2026-01-06 16:29:44,985 - INFO -   Batch 2400/2527: Loss = 0.1178
2026-01-06 16:30:07,945 - INFO -   Batch 2450/2527: Loss = 0.1530
2026-01-06 16:30:31,265 - INFO -   Batch 2500/2527: Loss = 0.1013
2026-01-06 16:35:10,452 - INFO - Epoch 39/160: Train Loss: 0.1313, Val Loss: 1.1031, Val mIoU: 41.22%, Val PixelAcc: 79.53%
2026-01-06 16:35:10,454 - INFO -   → New best mIoU: 41.22%
2026-01-06 16:35:12,669 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 16:35:12,671 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 16:35:37,824 - INFO -   Batch 50/2527: Loss = 0.1502
2026-01-06 16:36:01,403 - INFO -   Batch 100/2527: Loss = 0.1013
2026-01-06 16:36:25,331 - INFO -   Batch 150/2527: Loss = 0.0986
2026-01-06 16:36:48,890 - INFO -   Batch 200/2527: Loss = 0.1588
2026-01-06 16:37:12,630 - INFO -   Batch 250/2527: Loss = 0.1062
2026-01-06 16:37:36,225 - INFO -   Batch 300/2527: Loss = 0.0865
2026-01-06 16:38:00,041 - INFO -   Batch 350/2527: Loss = 0.0924
2026-01-06 16:38:23,898 - INFO -   Batch 400/2527: Loss = 0.1232
2026-01-06 16:38:47,707 - INFO -   Batch 450/2527: Loss = 0.0939
2026-01-06 16:39:11,331 - INFO -   Batch 500/2527: Loss = 0.1477
2026-01-06 16:39:35,001 - INFO -   Batch 550/2527: Loss = 0.1555
2026-01-06 16:39:58,999 - INFO -   Batch 600/2527: Loss = 0.1130
2026-01-06 16:40:22,583 - INFO -   Batch 650/2527: Loss = 0.1160
2026-01-06 16:40:46,343 - INFO -   Batch 700/2527: Loss = 0.1348
2026-01-06 16:41:10,302 - INFO -   Batch 750/2527: Loss = 0.1003
2026-01-06 16:41:34,160 - INFO -   Batch 800/2527: Loss = 0.1356
2026-01-06 16:41:58,079 - INFO -   Batch 850/2527: Loss = 0.1457
2026-01-06 16:42:21,994 - INFO -   Batch 900/2527: Loss = 0.1550
2026-01-06 16:42:45,638 - INFO -   Batch 950/2527: Loss = 0.1204
2026-01-06 16:43:09,350 - INFO -   Batch 1000/2527: Loss = 0.1132
2026-01-06 16:43:33,359 - INFO -   Batch 1050/2527: Loss = 0.1608
2026-01-06 16:43:57,018 - INFO -   Batch 1100/2527: Loss = 0.1817
2026-01-06 16:44:21,166 - INFO -   Batch 1150/2527: Loss = 0.1131
2026-01-06 16:44:44,909 - INFO -   Batch 1200/2527: Loss = 0.1579
2026-01-06 16:45:08,726 - INFO -   Batch 1250/2527: Loss = 0.0972
2026-01-06 16:45:32,566 - INFO -   Batch 1300/2527: Loss = 0.1742
2026-01-06 16:45:56,354 - INFO -   Batch 1350/2527: Loss = 0.1232
2026-01-06 16:46:19,993 - INFO -   Batch 1400/2527: Loss = 0.1513
2026-01-06 16:46:43,979 - INFO -   Batch 1450/2527: Loss = 0.1247
2026-01-06 16:47:07,907 - INFO -   Batch 1500/2527: Loss = 0.1731
2026-01-06 16:47:31,462 - INFO -   Batch 1550/2527: Loss = 0.1217
2026-01-06 16:47:55,226 - INFO -   Batch 1600/2527: Loss = 0.1046
2026-01-06 16:48:18,824 - INFO -   Batch 1650/2527: Loss = 0.1194
2026-01-06 16:48:42,688 - INFO -   Batch 1700/2527: Loss = 0.0868
2026-01-06 16:49:06,557 - INFO -   Batch 1750/2527: Loss = 0.1631
2026-01-06 16:49:30,334 - INFO -   Batch 1800/2527: Loss = 0.1280
2026-01-06 16:49:53,897 - INFO -   Batch 1850/2527: Loss = 0.1081
2026-01-06 16:50:17,556 - INFO -   Batch 1900/2527: Loss = 0.1183
2026-01-06 16:50:41,499 - INFO -   Batch 1950/2527: Loss = 0.1107
2026-01-06 16:51:05,082 - INFO -   Batch 2000/2527: Loss = 0.1331
2026-01-06 16:51:28,858 - INFO -   Batch 2050/2527: Loss = 0.1291
2026-01-06 16:51:52,459 - INFO -   Batch 2100/2527: Loss = 0.1722
2026-01-06 16:52:16,645 - INFO -   Batch 2150/2527: Loss = 0.1358
2026-01-06 16:52:40,546 - INFO -   Batch 2200/2527: Loss = 0.0912
2026-01-06 16:53:04,300 - INFO -   Batch 2250/2527: Loss = 0.1699
2026-01-06 16:53:27,910 - INFO -   Batch 2300/2527: Loss = 0.0657
2026-01-06 16:53:51,516 - INFO -   Batch 2350/2527: Loss = 0.1572
2026-01-06 16:54:15,464 - INFO -   Batch 2400/2527: Loss = 0.1195
2026-01-06 16:54:39,022 - INFO -   Batch 2450/2527: Loss = 0.1102
2026-01-06 16:55:02,776 - INFO -   Batch 2500/2527: Loss = 0.1301
2026-01-06 16:59:55,716 - INFO - Epoch 40/160: Train Loss: 0.1259, Val Loss: 1.1901, Val mIoU: 39.39%, Val PixelAcc: 78.30%
2026-01-06 16:59:57,934 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_40.pth
2026-01-06 16:59:57,937 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_40.pth
2026-01-06 17:00:24,525 - INFO -   Batch 50/2527: Loss = 0.1989
2026-01-06 17:00:49,619 - INFO -   Batch 100/2527: Loss = 0.1415
2026-01-06 17:01:14,581 - INFO -   Batch 150/2527: Loss = 0.2386
2026-01-06 17:01:39,421 - INFO -   Batch 200/2527: Loss = 0.1231
2026-01-06 17:02:04,252 - INFO -   Batch 250/2527: Loss = 0.2516
2026-01-06 17:02:29,271 - INFO -   Batch 300/2527: Loss = 0.1380
2026-01-06 17:02:54,523 - INFO -   Batch 350/2527: Loss = 0.0892
2026-01-06 17:03:19,382 - INFO -   Batch 400/2527: Loss = 0.2760
2026-01-06 17:03:44,514 - INFO -   Batch 450/2527: Loss = 0.1485
2026-01-06 17:04:09,491 - INFO -   Batch 500/2527: Loss = 0.1605
2026-01-06 17:04:34,312 - INFO -   Batch 550/2527: Loss = 0.1500
2026-01-06 17:04:59,155 - INFO -   Batch 600/2527: Loss = 0.0708
2026-01-06 17:05:23,888 - INFO -   Batch 650/2527: Loss = 0.1316
2026-01-06 17:05:48,730 - INFO -   Batch 700/2527: Loss = 0.1257
2026-01-06 17:06:13,848 - INFO -   Batch 750/2527: Loss = 0.0922
2026-01-06 17:06:38,832 - INFO -   Batch 800/2527: Loss = 0.1255
2026-01-06 17:07:03,652 - INFO -   Batch 850/2527: Loss = 0.1470
2026-01-06 17:07:28,540 - INFO -   Batch 900/2527: Loss = 0.1577
2026-01-06 17:07:53,567 - INFO -   Batch 950/2527: Loss = 0.1593
2026-01-06 17:08:18,428 - INFO -   Batch 1000/2527: Loss = 0.1951
2026-01-06 17:08:43,274 - INFO -   Batch 1050/2527: Loss = 0.1552
2026-01-06 17:09:08,795 - INFO -   Batch 1100/2527: Loss = 0.1014
2026-01-06 17:09:33,804 - INFO -   Batch 1150/2527: Loss = 0.1398
2026-01-06 17:09:58,628 - INFO -   Batch 1200/2527: Loss = 0.1037
2026-01-06 17:10:23,478 - INFO -   Batch 1250/2527: Loss = 0.1258
2026-01-06 17:10:48,208 - INFO -   Batch 1300/2527: Loss = 0.2034
2026-01-06 17:11:13,084 - INFO -   Batch 1350/2527: Loss = 0.1145
2026-01-06 17:11:38,235 - INFO -   Batch 1400/2527: Loss = 0.1760
2026-01-06 17:12:03,216 - INFO -   Batch 1450/2527: Loss = 0.1455
2026-01-06 17:12:28,446 - INFO -   Batch 1500/2527: Loss = 0.1154
2026-01-06 17:12:53,296 - INFO -   Batch 1550/2527: Loss = 0.1290
2026-01-06 17:13:18,442 - INFO -   Batch 1600/2527: Loss = 0.1558
2026-01-06 17:13:43,297 - INFO -   Batch 1650/2527: Loss = 0.1077
2026-01-06 17:14:08,158 - INFO -   Batch 1700/2527: Loss = 0.1282
2026-01-06 17:14:33,332 - INFO -   Batch 1750/2527: Loss = 0.1554
2026-01-06 17:14:58,693 - INFO -   Batch 1800/2527: Loss = 0.1585
2026-01-06 17:15:23,532 - INFO -   Batch 1850/2527: Loss = 0.1433
2026-01-06 17:15:48,387 - INFO -   Batch 1900/2527: Loss = 0.1900
2026-01-06 17:16:13,147 - INFO -   Batch 1950/2527: Loss = 0.1325
2026-01-06 17:16:37,989 - INFO -   Batch 2000/2527: Loss = 0.1153
2026-01-06 17:17:03,140 - INFO -   Batch 2050/2527: Loss = 0.0920
2026-01-06 17:17:28,137 - INFO -   Batch 2100/2527: Loss = 0.1619
2026-01-06 17:17:52,970 - INFO -   Batch 2150/2527: Loss = 0.0840
2026-01-06 17:18:17,824 - INFO -   Batch 2200/2527: Loss = 0.2102
2026-01-06 17:18:42,834 - INFO -   Batch 2250/2527: Loss = 0.1175
2026-01-06 17:19:07,681 - INFO -   Batch 2300/2527: Loss = 0.1395
2026-01-06 17:19:32,535 - INFO -   Batch 2350/2527: Loss = 0.1729
2026-01-06 17:19:57,645 - INFO -   Batch 2400/2527: Loss = 0.1291
2026-01-06 17:20:22,676 - INFO -   Batch 2450/2527: Loss = 0.1275
2026-01-06 17:20:47,531 - INFO -   Batch 2500/2527: Loss = 0.1436
2026-01-06 17:25:28,486 - INFO - Epoch 41/160: Train Loss: 0.1348, Val Loss: 1.1080, Val mIoU: 40.65%, Val PixelAcc: 79.30%
2026-01-06 17:25:54,869 - INFO -   Batch 50/2527: Loss = 0.1264
2026-01-06 17:26:19,854 - INFO -   Batch 100/2527: Loss = 0.0905
2026-01-06 17:26:44,647 - INFO -   Batch 150/2527: Loss = 0.0870
2026-01-06 17:27:09,466 - INFO -   Batch 200/2527: Loss = 0.1190
2026-01-06 17:27:34,561 - INFO -   Batch 250/2527: Loss = 0.1427
2026-01-06 17:27:59,524 - INFO -   Batch 300/2527: Loss = 0.1513
2026-01-06 17:28:24,354 - INFO -   Batch 350/2527: Loss = 0.1056
2026-01-06 17:28:49,197 - INFO -   Batch 400/2527: Loss = 0.1354
2026-01-06 17:29:13,935 - INFO -   Batch 450/2527: Loss = 0.1089
2026-01-06 17:29:38,782 - INFO -   Batch 500/2527: Loss = 0.1273
2026-01-06 17:30:03,898 - INFO -   Batch 550/2527: Loss = 0.0923
2026-01-06 17:30:28,881 - INFO -   Batch 600/2527: Loss = 0.0848
2026-01-06 17:30:53,709 - INFO -   Batch 650/2527: Loss = 0.0847
2026-01-06 17:31:18,550 - INFO -   Batch 700/2527: Loss = 0.0944
2026-01-06 17:31:43,950 - INFO -   Batch 750/2527: Loss = 0.1391
2026-01-06 17:32:08,773 - INFO -   Batch 800/2527: Loss = 0.0920
2026-01-06 17:32:33,635 - INFO -   Batch 850/2527: Loss = 0.1232
2026-01-06 17:32:58,768 - INFO -   Batch 900/2527: Loss = 0.1298
2026-01-06 17:33:23,744 - INFO -   Batch 950/2527: Loss = 0.1567
2026-01-06 17:33:48,575 - INFO -   Batch 1000/2527: Loss = 0.0733
2026-01-06 17:34:13,419 - INFO -   Batch 1050/2527: Loss = 0.1189
2026-01-06 17:34:38,140 - INFO -   Batch 1100/2527: Loss = 0.1415
2026-01-06 17:35:03,354 - INFO -   Batch 1150/2527: Loss = 0.0978
2026-01-06 17:35:28,487 - INFO -   Batch 1200/2527: Loss = 0.1107
2026-01-06 17:35:53,468 - INFO -   Batch 1250/2527: Loss = 0.1110
2026-01-06 17:36:18,290 - INFO -   Batch 1300/2527: Loss = 0.1996
2026-01-06 17:36:43,131 - INFO -   Batch 1350/2527: Loss = 0.1574
2026-01-06 17:37:08,153 - INFO -   Batch 1400/2527: Loss = 0.1099
2026-01-06 17:37:33,356 - INFO -   Batch 1450/2527: Loss = 0.1899
2026-01-06 17:37:58,192 - INFO -   Batch 1500/2527: Loss = 0.2363
2026-01-06 17:38:23,315 - INFO -   Batch 1550/2527: Loss = 0.1424
2026-01-06 17:38:48,303 - INFO -   Batch 1600/2527: Loss = 0.1729
2026-01-06 17:39:13,142 - INFO -   Batch 1650/2527: Loss = 0.1325
2026-01-06 17:39:37,993 - INFO -   Batch 1700/2527: Loss = 0.4362
2026-01-06 17:40:02,710 - INFO -   Batch 1750/2527: Loss = 0.1497
2026-01-06 17:40:27,541 - INFO -   Batch 1800/2527: Loss = 0.1544
2026-01-06 17:40:52,675 - INFO -   Batch 1850/2527: Loss = 0.2028
2026-01-06 17:41:17,650 - INFO -   Batch 1900/2527: Loss = 0.1117
2026-01-06 17:41:42,479 - INFO -   Batch 1950/2527: Loss = 0.1497
2026-01-06 17:42:07,330 - INFO -   Batch 2000/2527: Loss = 0.1364
2026-01-06 17:42:32,343 - INFO -   Batch 2050/2527: Loss = 0.1020
2026-01-06 17:42:57,183 - INFO -   Batch 2100/2527: Loss = 0.3068
2026-01-06 17:43:22,010 - INFO -   Batch 2150/2527: Loss = 0.1439
2026-01-06 17:43:47,482 - INFO -   Batch 2200/2527: Loss = 0.1626
2026-01-06 17:44:12,441 - INFO -   Batch 2250/2527: Loss = 0.1317
2026-01-06 17:44:37,264 - INFO -   Batch 2300/2527: Loss = 0.1498
2026-01-06 17:45:02,124 - INFO -   Batch 2350/2527: Loss = 0.2326
2026-01-06 17:45:26,881 - INFO -   Batch 2400/2527: Loss = 0.1930
2026-01-06 17:45:51,719 - INFO -   Batch 2450/2527: Loss = 0.1632
2026-01-06 17:46:16,838 - INFO -   Batch 2500/2527: Loss = 0.1528
2026-01-06 17:51:09,987 - INFO - Epoch 42/160: Train Loss: 0.1371, Val Loss: 1.1295, Val mIoU: 39.27%, Val PixelAcc: 78.49%
2026-01-06 17:51:34,902 - INFO -   Batch 50/2527: Loss = 0.1215
2026-01-06 17:51:58,163 - INFO -   Batch 100/2527: Loss = 0.1221
2026-01-06 17:52:21,095 - INFO -   Batch 150/2527: Loss = 0.0921
2026-01-06 17:52:44,040 - INFO -   Batch 200/2527: Loss = 0.1158
2026-01-06 17:53:06,987 - INFO -   Batch 250/2527: Loss = 0.0987
2026-01-06 17:53:29,941 - INFO -   Batch 300/2527: Loss = 0.1287
2026-01-06 17:53:53,276 - INFO -   Batch 350/2527: Loss = 0.1023
2026-01-06 17:54:16,269 - INFO -   Batch 400/2527: Loss = 0.1156
2026-01-06 17:54:39,220 - INFO -   Batch 450/2527: Loss = 0.1682
2026-01-06 17:55:02,187 - INFO -   Batch 500/2527: Loss = 0.1473
2026-01-06 17:55:25,149 - INFO -   Batch 550/2527: Loss = 0.1541
2026-01-06 17:55:48,138 - INFO -   Batch 600/2527: Loss = 0.1181
2026-01-06 17:56:11,103 - INFO -   Batch 650/2527: Loss = 0.1001
2026-01-06 17:56:34,069 - INFO -   Batch 700/2527: Loss = 0.1475
2026-01-06 17:56:57,033 - INFO -   Batch 750/2527: Loss = 0.1700
2026-01-06 17:57:20,024 - INFO -   Batch 800/2527: Loss = 0.0901
2026-01-06 17:57:42,983 - INFO -   Batch 850/2527: Loss = 0.1289
2026-01-06 17:58:05,946 - INFO -   Batch 900/2527: Loss = 0.0876
2026-01-06 17:58:28,942 - INFO -   Batch 950/2527: Loss = 0.1267
2026-01-06 17:58:51,900 - INFO -   Batch 1000/2527: Loss = 0.0908
2026-01-06 17:59:14,853 - INFO -   Batch 1050/2527: Loss = 0.1180
2026-01-06 17:59:38,199 - INFO -   Batch 1100/2527: Loss = 0.1019
2026-01-06 18:00:01,177 - INFO -   Batch 1150/2527: Loss = 0.1686
2026-01-06 18:00:24,163 - INFO -   Batch 1200/2527: Loss = 0.1257
2026-01-06 18:00:47,134 - INFO -   Batch 1250/2527: Loss = 0.1087
2026-01-06 18:01:10,096 - INFO -   Batch 1300/2527: Loss = 0.1455
2026-01-06 18:01:33,074 - INFO -   Batch 1350/2527: Loss = 0.1096
2026-01-06 18:01:56,029 - INFO -   Batch 1400/2527: Loss = 0.0967
2026-01-06 18:02:19,042 - INFO -   Batch 1450/2527: Loss = 0.1337
2026-01-06 18:02:42,364 - INFO -   Batch 1500/2527: Loss = 0.1225
2026-01-06 18:03:05,349 - INFO -   Batch 1550/2527: Loss = 0.1393
2026-01-06 18:03:28,310 - INFO -   Batch 1600/2527: Loss = 0.1446
2026-01-06 18:03:51,271 - INFO -   Batch 1650/2527: Loss = 0.1150
2026-01-06 18:04:14,237 - INFO -   Batch 1700/2527: Loss = 0.1068
2026-01-06 18:04:37,212 - INFO -   Batch 1750/2527: Loss = 0.1359
2026-01-06 18:05:00,546 - INFO -   Batch 1800/2527: Loss = 0.1322
2026-01-06 18:05:23,497 - INFO -   Batch 1850/2527: Loss = 0.1372
2026-01-06 18:05:46,464 - INFO -   Batch 1900/2527: Loss = 0.1217
2026-01-06 18:06:09,418 - INFO -   Batch 1950/2527: Loss = 0.1321
2026-01-06 18:06:32,377 - INFO -   Batch 2000/2527: Loss = 0.1145
2026-01-06 18:06:55,340 - INFO -   Batch 2050/2527: Loss = 0.1567
2026-01-06 18:07:18,295 - INFO -   Batch 2100/2527: Loss = 0.1310
2026-01-06 18:07:41,255 - INFO -   Batch 2150/2527: Loss = 0.1238
2026-01-06 18:08:04,210 - INFO -   Batch 2200/2527: Loss = 0.1232
2026-01-06 18:08:27,162 - INFO -   Batch 2250/2527: Loss = 0.0975
2026-01-06 18:08:50,134 - INFO -   Batch 2300/2527: Loss = 0.1208
2026-01-06 18:09:13,091 - INFO -   Batch 2350/2527: Loss = 0.0898
2026-01-06 18:09:36,057 - INFO -   Batch 2400/2527: Loss = 0.1341
2026-01-06 18:09:59,019 - INFO -   Batch 2450/2527: Loss = 0.1307
2026-01-06 18:10:21,976 - INFO -   Batch 2500/2527: Loss = 0.1189
2026-01-06 18:15:01,498 - INFO - Epoch 43/160: Train Loss: 0.1236, Val Loss: 1.1102, Val mIoU: 41.34%, Val PixelAcc: 79.66%
2026-01-06 18:15:01,500 - INFO -   → New best mIoU: 41.34%
2026-01-06 18:15:03,678 - INFO - ✅ Checkpoint saved: runs/run_261/best_model.pth
2026-01-06 18:15:03,680 - INFO -   → Best model saved to runs/run_261/best_model.pth
2026-01-06 18:15:29,455 - INFO -   Batch 50/2527: Loss = 0.0846
2026-01-06 18:15:53,030 - INFO -   Batch 100/2527: Loss = 0.1669
2026-01-06 18:16:16,964 - INFO -   Batch 150/2527: Loss = 0.1170
2026-01-06 18:16:40,520 - INFO -   Batch 200/2527: Loss = 0.0645
2026-01-06 18:17:04,266 - INFO -   Batch 250/2527: Loss = 0.0984
2026-01-06 18:17:27,859 - INFO -   Batch 300/2527: Loss = 0.0913
2026-01-06 18:17:51,682 - INFO -   Batch 350/2527: Loss = 0.1058
2026-01-06 18:18:15,551 - INFO -   Batch 400/2527: Loss = 0.0797
2026-01-06 18:18:39,694 - INFO -   Batch 450/2527: Loss = 0.2442
2026-01-06 18:19:03,299 - INFO -   Batch 500/2527: Loss = 0.1800
2026-01-06 18:19:26,927 - INFO -   Batch 550/2527: Loss = 0.1591
2026-01-06 18:19:50,887 - INFO -   Batch 600/2527: Loss = 0.1326
2026-01-06 18:20:14,472 - INFO -   Batch 650/2527: Loss = 0.1352
2026-01-06 18:20:38,236 - INFO -   Batch 700/2527: Loss = 0.1227
2026-01-06 18:21:02,213 - INFO -   Batch 750/2527: Loss = 0.1303
2026-01-06 18:21:26,035 - INFO -   Batch 800/2527: Loss = 0.1266
2026-01-06 18:21:49,887 - INFO -   Batch 850/2527: Loss = 0.0980
2026-01-06 18:22:13,672 - INFO -   Batch 900/2527: Loss = 0.1133
2026-01-06 18:22:37,270 - INFO -   Batch 950/2527: Loss = 0.1157
2026-01-06 18:23:00,905 - INFO -   Batch 1000/2527: Loss = 0.1368
2026-01-06 18:23:24,869 - INFO -   Batch 1050/2527: Loss = 0.1588
2026-01-06 18:23:48,478 - INFO -   Batch 1100/2527: Loss = 0.0836
2026-01-06 18:24:12,270 - INFO -   Batch 1150/2527: Loss = 0.1284
2026-01-06 18:24:35,894 - INFO -   Batch 1200/2527: Loss = 0.1179
2026-01-06 18:24:59,720 - INFO -   Batch 1250/2527: Loss = 0.1301
2026-01-06 18:25:23,590 - INFO -   Batch 1300/2527: Loss = 0.0944
2026-01-06 18:25:47,378 - INFO -   Batch 1350/2527: Loss = 0.1571
2026-01-06 18:26:11,006 - INFO -   Batch 1400/2527: Loss = 0.1238
2026-01-06 18:26:35,008 - INFO -   Batch 1450/2527: Loss = 0.1215
2026-01-06 18:26:59,000 - INFO -   Batch 1500/2527: Loss = 0.1721
2026-01-06 18:27:22,569 - INFO -   Batch 1550/2527: Loss = 0.1528
2026-01-06 18:27:46,331 - INFO -   Batch 1600/2527: Loss = 0.1125
2026-01-06 18:28:09,951 - INFO -   Batch 1650/2527: Loss = 0.1337
2026-01-06 18:28:33,761 - INFO -   Batch 1700/2527: Loss = 0.1594
2026-01-06 18:28:57,611 - INFO -   Batch 1750/2527: Loss = 0.0830
2026-01-06 18:29:21,395 - INFO -   Batch 1800/2527: Loss = 0.0954
2026-01-06 18:29:45,026 - INFO -   Batch 1850/2527: Loss = 0.1416
2026-01-06 18:30:09,047 - INFO -   Batch 1900/2527: Loss = 0.1354
2026-01-06 18:30:32,995 - INFO -   Batch 1950/2527: Loss = 0.0845
2026-01-06 18:30:56,600 - INFO -   Batch 2000/2527: Loss = 0.1062
2026-01-06 18:31:20,475 - INFO -   Batch 2050/2527: Loss = 0.1496
2026-01-06 18:31:44,133 - INFO -   Batch 2100/2527: Loss = 0.0921
2026-01-06 18:32:07,953 - INFO -   Batch 2150/2527: Loss = 0.0959
2026-01-06 18:32:32,188 - INFO -   Batch 2200/2527: Loss = 0.0988
2026-01-06 18:32:55,975 - INFO -   Batch 2250/2527: Loss = 0.1600
2026-01-06 18:33:19,581 - INFO -   Batch 2300/2527: Loss = 0.1444
2026-01-06 18:33:43,206 - INFO -   Batch 2350/2527: Loss = 0.0763
2026-01-06 18:34:07,181 - INFO -   Batch 2400/2527: Loss = 0.1266
2026-01-06 18:34:30,785 - INFO -   Batch 2450/2527: Loss = 0.1010
2026-01-06 18:34:54,544 - INFO -   Batch 2500/2527: Loss = 0.1538
2026-01-06 18:39:46,660 - INFO - Epoch 44/160: Train Loss: 0.1274, Val Loss: 1.1377, Val mIoU: 41.05%, Val PixelAcc: 79.47%
2026-01-06 18:40:12,750 - INFO -   Batch 50/2527: Loss = 0.0897
2026-01-06 18:40:37,827 - INFO -   Batch 100/2527: Loss = 0.1387
2026-01-06 18:41:02,758 - INFO -   Batch 150/2527: Loss = 0.1076
2026-01-06 18:41:27,551 - INFO -   Batch 200/2527: Loss = 0.1266
2026-01-06 18:41:52,402 - INFO -   Batch 250/2527: Loss = 0.0677
2026-01-06 18:42:17,420 - INFO -   Batch 300/2527: Loss = 0.1639
2026-01-06 18:42:42,273 - INFO -   Batch 350/2527: Loss = 0.1385
2026-01-06 18:43:07,477 - INFO -   Batch 400/2527: Loss = 0.0634
2026-01-06 18:43:32,608 - INFO -   Batch 450/2527: Loss = 0.1283
2026-01-06 18:43:57,575 - INFO -   Batch 500/2527: Loss = 0.0950
2026-01-06 18:44:22,389 - INFO -   Batch 550/2527: Loss = 0.1224
2026-01-06 18:44:47,259 - INFO -   Batch 600/2527: Loss = 0.0919
2026-01-06 18:45:11,986 - INFO -   Batch 650/2527: Loss = 0.1202
2026-01-06 18:45:36,823 - INFO -   Batch 700/2527: Loss = 0.1162
2026-01-06 18:46:02,180 - INFO -   Batch 750/2527: Loss = 0.1175
2026-01-06 18:46:27,569 - INFO -   Batch 800/2527: Loss = 0.0888
2026-01-06 18:46:52,439 - INFO -   Batch 850/2527: Loss = 0.1335
2026-01-06 18:47:17,275 - INFO -   Batch 900/2527: Loss = 0.1275
2026-01-06 18:47:42,296 - INFO -   Batch 950/2527: Loss = 0.0881
2026-01-06 18:48:07,135 - INFO -   Batch 1000/2527: Loss = 0.0816
2026-01-06 18:48:31,968 - INFO -   Batch 1050/2527: Loss = 0.1257
2026-01-06 18:48:57,446 - INFO -   Batch 1100/2527: Loss = 0.1037
2026-01-06 18:49:22,417 - INFO -   Batch 1150/2527: Loss = 0.1028
2026-01-06 18:49:47,261 - INFO -   Batch 1200/2527: Loss = 0.1454
2026-01-06 18:50:12,106 - INFO -   Batch 1250/2527: Loss = 0.1441
2026-01-06 18:50:36,829 - INFO -   Batch 1300/2527: Loss = 0.1320
2026-01-06 18:51:01,704 - INFO -   Batch 1350/2527: Loss = 0.0943
2026-01-06 18:51:26,969 - INFO -   Batch 1400/2527: Loss = 0.1458
2026-01-06 18:51:52,656 - INFO -   Batch 1450/2527: Loss = 0.0909
2026-01-06 18:52:17,696 - INFO -   Batch 1500/2527: Loss = 0.0941
2026-01-06 18:52:42,629 - INFO -   Batch 1550/2527: Loss = 0.1338
2026-01-06 18:53:07,650 - INFO -   Batch 1600/2527: Loss = 0.1229
2026-01-06 18:53:32,607 - INFO -   Batch 1650/2527: Loss = 0.1009
2026-01-06 18:53:57,507 - INFO -   Batch 1700/2527: Loss = 0.1106
2026-01-06 18:54:22,690 - INFO -   Batch 1750/2527: Loss = 0.0876
2026-01-06 18:54:47,700 - INFO -   Batch 1800/2527: Loss = 0.1751
2026-01-06 18:55:12,872 - INFO -   Batch 1850/2527: Loss = 0.1102
2026-01-06 18:55:37,759 - INFO -   Batch 1900/2527: Loss = 0.1045
2026-01-06 18:56:02,487 - INFO -   Batch 1950/2527: Loss = 0.1000
2026-01-06 18:56:27,392 - INFO -   Batch 2000/2527: Loss = 0.1061
2026-01-06 18:56:52,562 - INFO -   Batch 2050/2527: Loss = 0.1122
2026-01-06 18:57:17,587 - INFO -   Batch 2100/2527: Loss = 0.1108
2026-01-06 18:57:42,478 - INFO -   Batch 2150/2527: Loss = 0.1166
2026-01-06 18:58:07,356 - INFO -   Batch 2200/2527: Loss = 0.1156
2026-01-06 18:58:32,810 - INFO -   Batch 2250/2527: Loss = 0.1260
2026-01-06 18:58:57,657 - INFO -   Batch 2300/2527: Loss = 0.1341
2026-01-06 18:59:22,499 - INFO -   Batch 2350/2527: Loss = 0.1050
2026-01-06 18:59:47,694 - INFO -   Batch 2400/2527: Loss = 0.0733
2026-01-06 19:00:12,665 - INFO -   Batch 2450/2527: Loss = 0.1349
2026-01-06 19:00:37,482 - INFO -   Batch 2500/2527: Loss = 0.1489
2026-01-06 19:05:17,869 - INFO - Epoch 45/160: Train Loss: 0.1171, Val Loss: 1.1829, Val mIoU: 40.44%, Val PixelAcc: 79.07%
2026-01-06 19:05:44,740 - INFO -   Batch 50/2527: Loss = 0.1099
2026-01-06 19:06:09,698 - INFO -   Batch 100/2527: Loss = 0.0995
2026-01-06 19:06:34,517 - INFO -   Batch 150/2527: Loss = 0.1544
2026-01-06 19:06:59,332 - INFO -   Batch 200/2527: Loss = 0.0766
2026-01-06 19:07:24,437 - INFO -   Batch 250/2527: Loss = 0.0747
2026-01-06 19:07:49,441 - INFO -   Batch 300/2527: Loss = 0.2238
2026-01-06 19:08:14,265 - INFO -   Batch 350/2527: Loss = 0.1577
2026-01-06 19:08:39,107 - INFO -   Batch 400/2527: Loss = 0.1292
2026-01-06 19:09:03,881 - INFO -   Batch 450/2527: Loss = 0.1183
2026-01-06 19:09:28,723 - INFO -   Batch 500/2527: Loss = 0.0930
2026-01-06 19:09:53,855 - INFO -   Batch 550/2527: Loss = 0.1240
2026-01-06 19:10:18,855 - INFO -   Batch 600/2527: Loss = 0.0740
2026-01-06 19:10:43,693 - INFO -   Batch 650/2527: Loss = 0.0784
2026-01-06 19:11:08,589 - INFO -   Batch 700/2527: Loss = 0.1982
2026-01-06 19:11:33,982 - INFO -   Batch 750/2527: Loss = 0.1207
2026-01-06 19:11:58,835 - INFO -   Batch 800/2527: Loss = 0.1412
2026-01-06 19:12:23,676 - INFO -   Batch 850/2527: Loss = 0.1029
2026-01-06 19:12:48,806 - INFO -   Batch 900/2527: Loss = 0.1520
2026-01-06 19:13:13,793 - INFO -   Batch 950/2527: Loss = 0.1483
2026-01-06 19:13:38,615 - INFO -   Batch 1000/2527: Loss = 0.0798
2026-01-06 19:14:03,486 - INFO -   Batch 1050/2527: Loss = 0.1516
2026-01-06 19:14:28,223 - INFO -   Batch 1100/2527: Loss = 0.1293
2026-01-06 19:14:53,061 - INFO -   Batch 1150/2527: Loss = 0.1055
2026-01-06 19:15:18,569 - INFO -   Batch 1200/2527: Loss = 0.1087
2026-01-06 19:15:43,554 - INFO -   Batch 1250/2527: Loss = 0.1182
2026-01-06 19:16:08,381 - INFO -   Batch 1300/2527: Loss = 0.1338
2026-01-06 19:16:33,219 - INFO -   Batch 1350/2527: Loss = 0.0909
2026-01-06 19:16:58,250 - INFO -   Batch 1400/2527: Loss = 0.1062
2026-01-06 19:17:23,450 - INFO -   Batch 1450/2527: Loss = 0.0743
2026-01-06 19:17:48,290 - INFO -   Batch 1500/2527: Loss = 0.1450
2026-01-06 19:18:13,413 - INFO -   Batch 1550/2527: Loss = 0.1247
2026-01-06 19:18:38,404 - INFO -   Batch 1600/2527: Loss = 0.1191
2026-01-06 19:19:03,237 - INFO -   Batch 1650/2527: Loss = 0.1364
2026-01-06 19:19:28,074 - INFO -   Batch 1700/2527: Loss = 0.1077
2026-01-06 19:19:52,794 - INFO -   Batch 1750/2527: Loss = 0.1086
2026-01-06 19:20:17,629 - INFO -   Batch 1800/2527: Loss = 0.1501
2026-01-06 19:20:42,760 - INFO -   Batch 1850/2527: Loss = 0.1181
2026-01-06 19:21:07,744 - INFO -   Batch 1900/2527: Loss = 0.1447
2026-01-06 19:21:32,567 - INFO -   Batch 1950/2527: Loss = 0.0931
2026-01-06 19:21:57,406 - INFO -   Batch 2000/2527: Loss = 0.1125
2026-01-06 19:22:22,450 - INFO -   Batch 2050/2527: Loss = 0.1554
2026-01-06 19:22:47,292 - INFO -   Batch 2100/2527: Loss = 0.1077
2026-01-06 19:23:12,139 - INFO -   Batch 2150/2527: Loss = 0.0723
2026-01-06 19:23:37,633 - INFO -   Batch 2200/2527: Loss = 0.1053
2026-01-06 19:24:02,743 - INFO -   Batch 2250/2527: Loss = 0.1060
2026-01-06 19:24:27,564 - INFO -   Batch 2300/2527: Loss = 0.1171
2026-01-06 19:24:52,386 - INFO -   Batch 2350/2527: Loss = 0.0749
2026-01-06 19:25:17,112 - INFO -   Batch 2400/2527: Loss = 0.1193
2026-01-06 19:25:41,969 - INFO -   Batch 2450/2527: Loss = 0.1106
2026-01-06 19:26:07,097 - INFO -   Batch 2500/2527: Loss = 0.1286
2026-01-06 19:31:00,455 - INFO - Epoch 46/160: Train Loss: 0.1248, Val Loss: 1.1669, Val mIoU: 41.12%, Val PixelAcc: 79.58%
2026-01-06 19:31:25,365 - INFO -   Batch 50/2527: Loss = 0.1508
2026-01-06 19:31:48,636 - INFO -   Batch 100/2527: Loss = 0.1069
2026-01-06 19:32:11,564 - INFO -   Batch 150/2527: Loss = 0.1432
2026-01-06 19:32:34,507 - INFO -   Batch 200/2527: Loss = 0.0908
2026-01-06 19:32:57,455 - INFO -   Batch 250/2527: Loss = 0.0729
2026-01-06 19:33:20,410 - INFO -   Batch 300/2527: Loss = 0.0698
2026-01-06 19:33:43,364 - INFO -   Batch 350/2527: Loss = 0.0761
2026-01-06 19:34:06,701 - INFO -   Batch 400/2527: Loss = 0.1292
2026-01-06 19:34:29,657 - INFO -   Batch 450/2527: Loss = 0.0869
2026-01-06 19:34:52,611 - INFO -   Batch 500/2527: Loss = 0.0811
2026-01-06 19:35:15,571 - INFO -   Batch 550/2527: Loss = 0.1192
2026-01-06 19:35:38,535 - INFO -   Batch 600/2527: Loss = 0.0904
2026-01-06 19:36:01,494 - INFO -   Batch 650/2527: Loss = 0.0921
2026-01-06 19:36:24,455 - INFO -   Batch 700/2527: Loss = 0.1101
2026-01-06 19:36:47,414 - INFO -   Batch 750/2527: Loss = 0.1105
2026-01-06 19:37:10,382 - INFO -   Batch 800/2527: Loss = 0.1030
2026-01-06 19:37:33,346 - INFO -   Batch 850/2527: Loss = 0.0881
2026-01-06 19:37:56,307 - INFO -   Batch 900/2527: Loss = 0.1294
2026-01-06 19:38:19,263 - INFO -   Batch 950/2527: Loss = 0.0936
2026-01-06 19:38:42,239 - INFO -   Batch 1000/2527: Loss = 0.0735
2026-01-06 19:39:05,229 - INFO -   Batch 1050/2527: Loss = 0.1562
2026-01-06 19:39:28,194 - INFO -   Batch 1100/2527: Loss = 0.1125
2026-01-06 19:39:51,507 - INFO -   Batch 1150/2527: Loss = 0.1072
2026-01-06 19:40:14,499 - INFO -   Batch 1200/2527: Loss = 0.0946
2026-01-06 19:40:37,453 - INFO -   Batch 1250/2527: Loss = 0.1110
2026-01-06 19:41:00,411 - INFO -   Batch 1300/2527: Loss = 0.1201
2026-01-06 19:41:23,368 - INFO -   Batch 1350/2527: Loss = 0.0762
2026-01-06 19:41:46,363 - INFO -   Batch 1400/2527: Loss = 0.0975
2026-01-06 19:42:09,312 - INFO -   Batch 1450/2527: Loss = 0.1290
2026-01-06 19:42:32,271 - INFO -   Batch 1500/2527: Loss = 0.1306
2026-01-06 19:42:55,583 - INFO -   Batch 1550/2527: Loss = 0.0788
2026-01-06 19:43:18,542 - INFO -   Batch 1600/2527: Loss = 0.1331
2026-01-06 19:43:41,499 - INFO -   Batch 1650/2527: Loss = 0.0560
2026-01-06 19:44:04,462 - INFO -   Batch 1700/2527: Loss = 0.1156
2026-01-06 19:44:27,415 - INFO -   Batch 1750/2527: Loss = 0.1276
2026-01-06 19:44:50,374 - INFO -   Batch 1800/2527: Loss = 0.1326
2026-01-06 19:45:13,704 - INFO -   Batch 1850/2527: Loss = 0.0980
2026-01-06 19:45:36,651 - INFO -   Batch 1900/2527: Loss = 0.0965
2026-01-06 19:45:59,610 - INFO -   Batch 1950/2527: Loss = 0.1088
2026-01-06 19:46:22,570 - INFO -   Batch 2000/2527: Loss = 0.0974
2026-01-06 19:46:45,548 - INFO -   Batch 2050/2527: Loss = 0.0872
2026-01-06 19:47:08,502 - INFO -   Batch 2100/2527: Loss = 0.0759
2026-01-06 19:47:31,458 - INFO -   Batch 2150/2527: Loss = 0.0917
2026-01-06 19:47:54,411 - INFO -   Batch 2200/2527: Loss = 0.1388
2026-01-06 19:48:17,366 - INFO -   Batch 2250/2527: Loss = 0.1547
2026-01-06 19:48:40,320 - INFO -   Batch 2300/2527: Loss = 0.0837
2026-01-06 19:49:03,275 - INFO -   Batch 2350/2527: Loss = 0.0874
2026-01-06 19:49:26,221 - INFO -   Batch 2400/2527: Loss = 0.0769
2026-01-06 19:49:49,193 - INFO -   Batch 2450/2527: Loss = 0.1199
2026-01-06 19:50:12,139 - INFO -   Batch 2500/2527: Loss = 0.1230
2026-01-06 19:54:52,131 - INFO - Epoch 47/160: Train Loss: 0.1105, Val Loss: 1.1735, Val mIoU: 41.11%, Val PixelAcc: 79.68%
2026-01-06 19:55:17,988 - INFO -   Batch 50/2527: Loss = 0.1392
2026-01-06 19:55:41,583 - INFO -   Batch 100/2527: Loss = 0.1023
2026-01-06 19:56:05,503 - INFO -   Batch 150/2527: Loss = 0.1355
2026-01-06 19:56:29,067 - INFO -   Batch 200/2527: Loss = 0.0695
2026-01-06 19:56:52,803 - INFO -   Batch 250/2527: Loss = 0.0741
2026-01-06 19:57:16,406 - INFO -   Batch 300/2527: Loss = 0.1464
2026-01-06 19:57:40,245 - INFO -   Batch 350/2527: Loss = 0.1538
2026-01-06 19:58:04,136 - INFO -   Batch 400/2527: Loss = 0.1007
2026-01-06 19:58:28,272 - INFO -   Batch 450/2527: Loss = 0.0902
2026-01-06 19:58:51,868 - INFO -   Batch 500/2527: Loss = 0.1068
2026-01-06 19:59:15,517 - INFO -   Batch 550/2527: Loss = 0.1041
2026-01-06 19:59:39,483 - INFO -   Batch 600/2527: Loss = 0.0796
2026-01-06 20:00:03,091 - INFO -   Batch 650/2527: Loss = 0.1272
2026-01-06 20:00:26,859 - INFO -   Batch 700/2527: Loss = 0.1038
2026-01-06 20:00:50,870 - INFO -   Batch 750/2527: Loss = 0.0833
2026-01-06 20:01:14,655 - INFO -   Batch 800/2527: Loss = 0.0909
2026-01-06 20:01:38,505 - INFO -   Batch 850/2527: Loss = 0.1511
2026-01-06 20:02:02,318 - INFO -   Batch 900/2527: Loss = 0.1591
2026-01-06 20:02:25,912 - INFO -   Batch 950/2527: Loss = 0.0646
2026-01-06 20:02:49,554 - INFO -   Batch 1000/2527: Loss = 0.0588
2026-01-06 20:03:13,515 - INFO -   Batch 1050/2527: Loss = 0.1596
2026-01-06 20:03:37,091 - INFO -   Batch 1100/2527: Loss = 0.0865
2026-01-06 20:04:00,844 - INFO -   Batch 1150/2527: Loss = 0.1037
2026-01-06 20:04:24,449 - INFO -   Batch 1200/2527: Loss = 0.0842
2026-01-06 20:04:48,271 - INFO -   Batch 1250/2527: Loss = 0.1769
2026-01-06 20:05:12,139 - INFO -   Batch 1300/2527: Loss = 0.1006
2026-01-06 20:05:35,901 - INFO -   Batch 1350/2527: Loss = 0.1020
2026-01-06 20:05:59,511 - INFO -   Batch 1400/2527: Loss = 0.1001
2026-01-06 20:06:23,148 - INFO -   Batch 1450/2527: Loss = 0.0929
2026-01-06 20:06:47,472 - INFO -   Batch 1500/2527: Loss = 0.2319
2026-01-06 20:07:11,057 - INFO -   Batch 1550/2527: Loss = 0.1770
2026-01-06 20:07:34,810 - INFO -   Batch 1600/2527: Loss = 0.1548
2026-01-06 20:07:58,449 - INFO -   Batch 1650/2527: Loss = 0.1557
2026-01-06 20:08:22,275 - INFO -   Batch 1700/2527: Loss = 0.1541
2026-01-06 20:08:46,125 - INFO -   Batch 1750/2527: Loss = 0.1368
2026-01-06 20:09:09,968 - INFO -   Batch 1800/2527: Loss = 0.1372
2026-01-06 20:09:33,566 - INFO -   Batch 1850/2527: Loss = 0.1105
2026-01-06 20:09:57,555 - INFO -   Batch 1900/2527: Loss = 0.1268
2026-01-06 20:10:21,503 - INFO -   Batch 1950/2527: Loss = 0.1024
2026-01-06 20:10:45,082 - INFO -   Batch 2000/2527: Loss = 0.1040
2026-01-06 20:11:08,876 - INFO -   Batch 2050/2527: Loss = 0.0716
2026-01-06 20:11:32,507 - INFO -   Batch 2100/2527: Loss = 0.1040
2026-01-06 20:11:56,329 - INFO -   Batch 2150/2527: Loss = 0.0864
2026-01-06 20:12:20,577 - INFO -   Batch 2200/2527: Loss = 0.0946
2026-01-06 20:12:44,351 - INFO -   Batch 2250/2527: Loss = 0.1028
2026-01-06 20:13:07,977 - INFO -   Batch 2300/2527: Loss = 0.1025
2026-01-06 20:13:31,603 - INFO -   Batch 2350/2527: Loss = 0.1221
2026-01-06 20:13:55,584 - INFO -   Batch 2400/2527: Loss = 0.1126
2026-01-06 20:14:19,156 - INFO -   Batch 2450/2527: Loss = 0.1148
2026-01-06 20:14:42,894 - INFO -   Batch 2500/2527: Loss = 0.0914
2026-01-06 20:19:35,413 - INFO - Epoch 48/160: Train Loss: 0.1192, Val Loss: 1.1753, Val mIoU: 40.92%, Val PixelAcc: 79.59%
2026-01-06 20:20:01,647 - INFO -   Batch 50/2527: Loss = 0.1320
2026-01-06 20:20:26,708 - INFO -   Batch 100/2527: Loss = 0.1065
2026-01-06 20:20:51,651 - INFO -   Batch 150/2527: Loss = 0.0800
2026-01-06 20:21:16,477 - INFO -   Batch 200/2527: Loss = 0.1124
2026-01-06 20:21:41,297 - INFO -   Batch 250/2527: Loss = 0.1110
2026-01-06 20:22:06,307 - INFO -   Batch 300/2527: Loss = 0.1526
2026-01-06 20:22:31,117 - INFO -   Batch 350/2527: Loss = 0.1433
2026-01-06 20:22:56,315 - INFO -   Batch 400/2527: Loss = 0.1213
2026-01-06 20:23:21,425 - INFO -   Batch 450/2527: Loss = 0.1325
2026-01-06 20:23:46,423 - INFO -   Batch 500/2527: Loss = 0.1352
2026-01-06 20:24:11,231 - INFO -   Batch 550/2527: Loss = 0.1568
2026-01-06 20:24:36,097 - INFO -   Batch 600/2527: Loss = 0.1077
2026-01-06 20:25:00,833 - INFO -   Batch 650/2527: Loss = 0.1411
2026-01-06 20:25:25,672 - INFO -   Batch 700/2527: Loss = 0.0652
2026-01-06 20:25:50,788 - INFO -   Batch 750/2527: Loss = 0.1334
2026-01-06 20:26:15,771 - INFO -   Batch 800/2527: Loss = 0.1015
2026-01-06 20:26:40,944 - INFO -   Batch 850/2527: Loss = 0.1102
2026-01-06 20:27:05,782 - INFO -   Batch 900/2527: Loss = 0.1254
2026-01-06 20:27:30,830 - INFO -   Batch 950/2527: Loss = 0.0759
2026-01-06 20:27:55,667 - INFO -   Batch 1000/2527: Loss = 0.1116
2026-01-06 20:28:20,512 - INFO -   Batch 1050/2527: Loss = 0.2092
2026-01-06 20:28:45,655 - INFO -   Batch 1100/2527: Loss = 0.1452
2026-01-06 20:29:11,017 - INFO -   Batch 1150/2527: Loss = 0.1426
2026-01-06 20:29:35,869 - INFO -   Batch 1200/2527: Loss = 0.1573
2026-01-06 20:30:00,728 - INFO -   Batch 1250/2527: Loss = 0.1267
2026-01-06 20:30:25,448 - INFO -   Batch 1300/2527: Loss = 0.1057
2026-01-06 20:30:50,326 - INFO -   Batch 1350/2527: Loss = 0.1359
2026-01-06 20:31:15,486 - INFO -   Batch 1400/2527: Loss = 0.1117
2026-01-06 20:31:40,480 - INFO -   Batch 1450/2527: Loss = 0.0907
2026-01-06 20:32:05,303 - INFO -   Batch 1500/2527: Loss = 0.1182
2026-01-06 20:32:30,173 - INFO -   Batch 1550/2527: Loss = 0.1013
2026-01-06 20:32:55,182 - INFO -   Batch 1600/2527: Loss = 0.1234
2026-01-06 20:33:20,033 - INFO -   Batch 1650/2527: Loss = 0.1338
2026-01-06 20:33:44,880 - INFO -   Batch 1700/2527: Loss = 0.1104
2026-01-06 20:34:10,003 - INFO -   Batch 1750/2527: Loss = 0.1079
2026-01-06 20:34:34,982 - INFO -   Batch 1800/2527: Loss = 0.0973
2026-01-06 20:35:00,176 - INFO -   Batch 1850/2527: Loss = 0.0751
2026-01-06 20:35:25,021 - INFO -   Batch 1900/2527: Loss = 0.1222
2026-01-06 20:35:49,759 - INFO -   Batch 1950/2527: Loss = 0.1314
2026-01-06 20:36:14,619 - INFO -   Batch 2000/2527: Loss = 0.1018
2026-01-06 20:36:39,743 - INFO -   Batch 2050/2527: Loss = 0.1577
2026-01-06 20:37:04,719 - INFO -   Batch 2100/2527: Loss = 0.1018
2026-01-06 20:37:29,538 - INFO -   Batch 2150/2527: Loss = 0.0980
2026-01-06 20:37:54,383 - INFO -   Batch 2200/2527: Loss = 0.0736
2026-01-06 20:38:19,385 - INFO -   Batch 2250/2527: Loss = 0.1464
2026-01-06 20:38:44,599 - INFO -   Batch 2300/2527: Loss = 0.0703
2026-01-06 20:39:09,451 - INFO -   Batch 2350/2527: Loss = 0.1089
2026-01-06 20:39:34,575 - INFO -   Batch 2400/2527: Loss = 0.1111
2026-01-06 20:39:59,553 - INFO -   Batch 2450/2527: Loss = 0.1558
2026-01-06 20:40:24,370 - INFO -   Batch 2500/2527: Loss = 0.1175
2026-01-06 20:45:04,515 - INFO - Epoch 49/160: Train Loss: 0.1146, Val Loss: 1.1775, Val mIoU: 40.49%, Val PixelAcc: 79.50%
2026-01-06 20:45:31,331 - INFO -   Batch 50/2527: Loss = 0.1512
2026-01-06 20:45:56,295 - INFO -   Batch 100/2527: Loss = 0.1560
2026-01-06 20:46:21,093 - INFO -   Batch 150/2527: Loss = 0.1043
2026-01-06 20:46:45,922 - INFO -   Batch 200/2527: Loss = 0.1193
2026-01-06 20:47:11,047 - INFO -   Batch 250/2527: Loss = 0.1103
2026-01-06 20:47:36,011 - INFO -   Batch 300/2527: Loss = 0.1531
2026-01-06 20:48:00,841 - INFO -   Batch 350/2527: Loss = 0.1150
2026-01-06 20:48:25,681 - INFO -   Batch 400/2527: Loss = 0.1079
2026-01-06 20:48:50,418 - INFO -   Batch 450/2527: Loss = 0.1076
2026-01-06 20:49:15,261 - INFO -   Batch 500/2527: Loss = 0.1254
2026-01-06 20:49:40,374 - INFO -   Batch 550/2527: Loss = 0.0937
2026-01-06 20:50:05,343 - INFO -   Batch 600/2527: Loss = 0.1153
2026-01-06 20:50:30,172 - INFO -   Batch 650/2527: Loss = 0.0559
2026-01-06 20:50:55,011 - INFO -   Batch 700/2527: Loss = 0.0917
2026-01-06 20:51:20,028 - INFO -   Batch 750/2527: Loss = 0.0881
2026-01-06 20:51:45,217 - INFO -   Batch 800/2527: Loss = 0.0969
2026-01-06 20:52:10,050 - INFO -   Batch 850/2527: Loss = 0.0945
2026-01-06 20:52:35,200 - INFO -   Batch 900/2527: Loss = 0.1383
2026-01-06 20:53:00,190 - INFO -   Batch 950/2527: Loss = 0.0692
2026-01-06 20:53:25,004 - INFO -   Batch 1000/2527: Loss = 0.1118
2026-01-06 20:53:49,851 - INFO -   Batch 1050/2527: Loss = 0.0767
2026-01-06 20:54:14,573 - INFO -   Batch 1100/2527: Loss = 0.0687
2026-01-06 20:54:39,429 - INFO -   Batch 1150/2527: Loss = 0.1359
2026-01-06 20:55:04,902 - INFO -   Batch 1200/2527: Loss = 0.0921
2026-01-06 20:55:29,875 - INFO -   Batch 1250/2527: Loss = 0.1236
2026-01-06 20:55:54,695 - INFO -   Batch 1300/2527: Loss = 0.1247
2026-01-06 20:56:19,534 - INFO -   Batch 1350/2527: Loss = 0.1379
2026-01-06 20:56:44,570 - INFO -   Batch 1400/2527: Loss = 0.0606
2026-01-06 20:57:09,404 - INFO -   Batch 1450/2527: Loss = 0.1078
2026-01-06 20:57:34,614 - INFO -   Batch 1500/2527: Loss = 0.1041
2026-01-06 20:57:59,764 - INFO -   Batch 1550/2527: Loss = 0.2226
2026-01-06 20:58:24,729 - INFO -   Batch 1600/2527: Loss = 0.1422
2026-01-06 20:58:49,558 - INFO -   Batch 1650/2527: Loss = 0.1132
2026-01-06 20:59:14,402 - INFO -   Batch 1700/2527: Loss = 0.1447
2026-01-06 20:59:39,129 - INFO -   Batch 1750/2527: Loss = 0.1627
2026-01-06 21:00:03,971 - INFO -   Batch 1800/2527: Loss = 0.1338
2026-01-06 21:00:29,108 - INFO -   Batch 1850/2527: Loss = 0.1046
2026-01-06 21:00:54,079 - INFO -   Batch 1900/2527: Loss = 0.0926
2026-01-06 21:01:18,930 - INFO -   Batch 1950/2527: Loss = 0.0974
2026-01-06 21:01:43,774 - INFO -   Batch 2000/2527: Loss = 0.1107
2026-01-06 21:02:08,784 - INFO -   Batch 2050/2527: Loss = 0.1153
2026-01-06 21:02:33,629 - INFO -   Batch 2100/2527: Loss = 0.0892
2026-01-06 21:02:58,462 - INFO -   Batch 2150/2527: Loss = 0.1279
2026-01-06 21:03:23,599 - INFO -   Batch 2200/2527: Loss = 0.1093
2026-01-06 21:03:48,966 - INFO -   Batch 2250/2527: Loss = 0.0690
2026-01-06 21:04:13,797 - INFO -   Batch 2300/2527: Loss = 0.0988
2026-01-06 21:04:38,702 - INFO -   Batch 2350/2527: Loss = 0.1320
2026-01-06 21:05:03,468 - INFO -   Batch 2400/2527: Loss = 0.1229
2026-01-06 21:05:28,307 - INFO -   Batch 2450/2527: Loss = 0.0634
2026-01-06 21:05:53,431 - INFO -   Batch 2500/2527: Loss = 0.0888
2026-01-06 21:10:46,771 - INFO - Epoch 50/160: Train Loss: 0.1089, Val Loss: 1.2218, Val mIoU: 40.59%, Val PixelAcc: 79.53%
2026-01-06 21:10:48,995 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_50.pth
2026-01-06 21:10:48,998 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_50.pth
2026-01-06 21:11:13,889 - INFO -   Batch 50/2527: Loss = 0.2070
2026-01-06 21:11:37,143 - INFO -   Batch 100/2527: Loss = 0.1321
2026-01-06 21:12:00,023 - INFO -   Batch 150/2527: Loss = 0.2339
2026-01-06 21:12:22,949 - INFO -   Batch 200/2527: Loss = 0.1046
2026-01-06 21:12:45,907 - INFO -   Batch 250/2527: Loss = 0.0804
2026-01-06 21:13:08,881 - INFO -   Batch 300/2527: Loss = 0.1278
2026-01-06 21:13:31,823 - INFO -   Batch 350/2527: Loss = 0.1063
2026-01-06 21:13:55,140 - INFO -   Batch 400/2527: Loss = 0.1901
2026-01-06 21:14:18,136 - INFO -   Batch 450/2527: Loss = 0.1021
2026-01-06 21:14:41,107 - INFO -   Batch 500/2527: Loss = 0.0853
2026-01-06 21:15:04,053 - INFO -   Batch 550/2527: Loss = 0.1383
2026-01-06 21:15:27,022 - INFO -   Batch 600/2527: Loss = 0.1546
2026-01-06 21:15:50,026 - INFO -   Batch 650/2527: Loss = 0.1430
2026-01-06 21:16:12,988 - INFO -   Batch 700/2527: Loss = 0.1052
2026-01-06 21:16:35,947 - INFO -   Batch 750/2527: Loss = 0.0924
2026-01-06 21:16:58,936 - INFO -   Batch 800/2527: Loss = 0.0887
2026-01-06 21:17:21,891 - INFO -   Batch 850/2527: Loss = 0.0965
2026-01-06 21:17:44,847 - INFO -   Batch 900/2527: Loss = 0.1211
2026-01-06 21:18:07,812 - INFO -   Batch 950/2527: Loss = 0.0859
2026-01-06 21:18:30,778 - INFO -   Batch 1000/2527: Loss = 0.1303
2026-01-06 21:18:53,733 - INFO -   Batch 1050/2527: Loss = 0.0749
2026-01-06 21:19:16,693 - INFO -   Batch 1100/2527: Loss = 0.1587
2026-01-06 21:19:40,022 - INFO -   Batch 1150/2527: Loss = 0.0908
2026-01-06 21:20:02,991 - INFO -   Batch 1200/2527: Loss = 0.1349
2026-01-06 21:20:25,954 - INFO -   Batch 1250/2527: Loss = 0.1024
2026-01-06 21:20:48,920 - INFO -   Batch 1300/2527: Loss = 0.1034
2026-01-06 21:21:11,869 - INFO -   Batch 1350/2527: Loss = 0.1255
2026-01-06 21:21:34,822 - INFO -   Batch 1400/2527: Loss = 0.1150
2026-01-06 21:21:57,786 - INFO -   Batch 1450/2527: Loss = 0.1404
2026-01-06 21:22:20,738 - INFO -   Batch 1500/2527: Loss = 0.1066
2026-01-06 21:22:44,070 - INFO -   Batch 1550/2527: Loss = 0.1340
2026-01-06 21:23:07,065 - INFO -   Batch 1600/2527: Loss = 0.0931
2026-01-06 21:23:30,025 - INFO -   Batch 1650/2527: Loss = 0.1271
2026-01-06 21:23:52,983 - INFO -   Batch 1700/2527: Loss = 0.1085
2026-01-06 21:24:15,948 - INFO -   Batch 1750/2527: Loss = 0.1105
2026-01-06 21:24:38,945 - INFO -   Batch 1800/2527: Loss = 0.0792
2026-01-06 21:25:02,270 - INFO -   Batch 1850/2527: Loss = 0.1176
2026-01-06 21:25:25,229 - INFO -   Batch 1900/2527: Loss = 0.0887
2026-01-06 21:25:48,220 - INFO -   Batch 1950/2527: Loss = 0.1156
2026-01-06 21:26:11,184 - INFO -   Batch 2000/2527: Loss = 0.1023
2026-01-06 21:26:34,139 - INFO -   Batch 2050/2527: Loss = 0.0973
2026-01-06 21:26:57,096 - INFO -   Batch 2100/2527: Loss = 0.1006
2026-01-06 21:27:20,066 - INFO -   Batch 2150/2527: Loss = 0.0703
2026-01-06 21:27:43,026 - INFO -   Batch 2200/2527: Loss = 0.1321
2026-01-06 21:28:05,986 - INFO -   Batch 2250/2527: Loss = 0.1010
2026-01-06 21:28:28,956 - INFO -   Batch 2300/2527: Loss = 0.1477
2026-01-06 21:28:51,910 - INFO -   Batch 2350/2527: Loss = 0.0913
2026-01-06 21:29:14,868 - INFO -   Batch 2400/2527: Loss = 0.0755
2026-01-06 21:29:37,825 - INFO -   Batch 2450/2527: Loss = 0.1382
2026-01-06 21:30:00,803 - INFO -   Batch 2500/2527: Loss = 0.0866
2026-01-06 21:34:40,289 - INFO - Epoch 51/160: Train Loss: 0.1148, Val Loss: 1.2122, Val mIoU: 40.95%, Val PixelAcc: 79.48%
2026-01-06 21:35:05,903 - INFO -   Batch 50/2527: Loss = 0.1292
2026-01-06 21:35:29,489 - INFO -   Batch 100/2527: Loss = 0.0822
2026-01-06 21:35:53,437 - INFO -   Batch 150/2527: Loss = 0.0963
2026-01-06 21:36:16,995 - INFO -   Batch 200/2527: Loss = 0.1060
2026-01-06 21:36:40,777 - INFO -   Batch 250/2527: Loss = 0.1012
2026-01-06 21:37:04,366 - INFO -   Batch 300/2527: Loss = 0.0953
2026-01-06 21:37:28,187 - INFO -   Batch 350/2527: Loss = 0.0994
2026-01-06 21:37:52,044 - INFO -   Batch 400/2527: Loss = 0.0896
2026-01-06 21:38:15,828 - INFO -   Batch 450/2527: Loss = 0.0947
2026-01-06 21:38:39,788 - INFO -   Batch 500/2527: Loss = 0.0882
2026-01-06 21:39:03,417 - INFO -   Batch 550/2527: Loss = 0.1022
2026-01-06 21:39:27,379 - INFO -   Batch 600/2527: Loss = 0.1067
2026-01-06 21:39:50,949 - INFO -   Batch 650/2527: Loss = 0.0842
2026-01-06 21:40:14,698 - INFO -   Batch 700/2527: Loss = 0.1037
2026-01-06 21:40:38,315 - INFO -   Batch 750/2527: Loss = 0.0783
2026-01-06 21:41:02,561 - INFO -   Batch 800/2527: Loss = 0.1821
2026-01-06 21:41:26,413 - INFO -   Batch 850/2527: Loss = 0.1059
2026-01-06 21:41:50,192 - INFO -   Batch 900/2527: Loss = 0.1045
2026-01-06 21:42:13,797 - INFO -   Batch 950/2527: Loss = 0.1105
2026-01-06 21:42:37,424 - INFO -   Batch 1000/2527: Loss = 0.0913
2026-01-06 21:43:01,408 - INFO -   Batch 1050/2527: Loss = 0.0690
2026-01-06 21:43:24,988 - INFO -   Batch 1100/2527: Loss = 0.0692
2026-01-06 21:43:48,750 - INFO -   Batch 1150/2527: Loss = 0.1059
2026-01-06 21:44:12,396 - INFO -   Batch 1200/2527: Loss = 0.0917
2026-01-06 21:44:36,208 - INFO -   Batch 1250/2527: Loss = 0.1101
2026-01-06 21:45:00,066 - INFO -   Batch 1300/2527: Loss = 0.1813
2026-01-06 21:45:23,847 - INFO -   Batch 1350/2527: Loss = 0.1034
2026-01-06 21:45:47,454 - INFO -   Batch 1400/2527: Loss = 0.1297
2026-01-06 21:46:11,102 - INFO -   Batch 1450/2527: Loss = 0.1659
2026-01-06 21:46:35,445 - INFO -   Batch 1500/2527: Loss = 0.1750
2026-01-06 21:46:59,025 - INFO -   Batch 1550/2527: Loss = 0.1124
2026-01-06 21:47:22,784 - INFO -   Batch 1600/2527: Loss = 0.1167
2026-01-06 21:47:46,397 - INFO -   Batch 1650/2527: Loss = 0.1350
2026-01-06 21:48:10,250 - INFO -   Batch 1700/2527: Loss = 0.1101
2026-01-06 21:48:34,144 - INFO -   Batch 1750/2527: Loss = 0.0928
2026-01-06 21:48:57,967 - INFO -   Batch 1800/2527: Loss = 0.0710
2026-01-06 21:49:21,569 - INFO -   Batch 1850/2527: Loss = 0.0715
2026-01-06 21:49:45,206 - INFO -   Batch 1900/2527: Loss = 0.1188
2026-01-06 21:50:09,519 - INFO -   Batch 1950/2527: Loss = 0.1047
2026-01-06 21:50:33,127 - INFO -   Batch 2000/2527: Loss = 0.0792
2026-01-06 21:50:56,881 - INFO -   Batch 2050/2527: Loss = 0.0766
2026-01-06 21:51:20,501 - INFO -   Batch 2100/2527: Loss = 0.0987
2026-01-06 21:51:44,326 - INFO -   Batch 2150/2527: Loss = 0.0848
2026-01-06 21:52:08,223 - INFO -   Batch 2200/2527: Loss = 0.0878
2026-01-06 21:52:32,372 - INFO -   Batch 2250/2527: Loss = 0.0871
2026-01-06 21:52:55,970 - INFO -   Batch 2300/2527: Loss = 0.1283
2026-01-06 21:53:19,609 - INFO -   Batch 2350/2527: Loss = 0.1166
2026-01-06 21:53:43,563 - INFO -   Batch 2400/2527: Loss = 0.0939
2026-01-06 21:54:07,170 - INFO -   Batch 2450/2527: Loss = 0.0932
2026-01-06 21:54:30,929 - INFO -   Batch 2500/2527: Loss = 0.1353
2026-01-06 21:59:22,982 - INFO - Epoch 52/160: Train Loss: 0.1074, Val Loss: 1.2214, Val mIoU: 40.83%, Val PixelAcc: 79.71%
2026-01-06 21:59:49,018 - INFO -   Batch 50/2527: Loss = 0.0719
2026-01-06 22:00:14,079 - INFO -   Batch 100/2527: Loss = 0.1094
2026-01-06 22:00:39,013 - INFO -   Batch 150/2527: Loss = 0.1011
2026-01-06 22:01:03,837 - INFO -   Batch 200/2527: Loss = 0.0884
2026-01-06 22:01:28,698 - INFO -   Batch 250/2527: Loss = 0.1223
2026-01-06 22:01:53,701 - INFO -   Batch 300/2527: Loss = 0.0916
2026-01-06 22:02:18,517 - INFO -   Batch 350/2527: Loss = 0.0745
2026-01-06 22:02:43,349 - INFO -   Batch 400/2527: Loss = 0.1082
2026-01-06 22:03:08,860 - INFO -   Batch 450/2527: Loss = 0.1123
2026-01-06 22:03:33,823 - INFO -   Batch 500/2527: Loss = 0.1082
2026-01-06 22:03:58,648 - INFO -   Batch 550/2527: Loss = 0.0825
2026-01-06 22:04:23,471 - INFO -   Batch 600/2527: Loss = 0.0813
2026-01-06 22:04:48,190 - INFO -   Batch 650/2527: Loss = 0.0870
2026-01-06 22:05:13,066 - INFO -   Batch 700/2527: Loss = 0.1059
2026-01-06 22:05:38,208 - INFO -   Batch 750/2527: Loss = 0.0515
2026-01-06 22:06:03,164 - INFO -   Batch 800/2527: Loss = 0.0740
2026-01-06 22:06:28,374 - INFO -   Batch 850/2527: Loss = 0.1045
2026-01-06 22:06:53,219 - INFO -   Batch 900/2527: Loss = 0.1241
2026-01-06 22:07:18,266 - INFO -   Batch 950/2527: Loss = 0.1193
2026-01-06 22:07:43,081 - INFO -   Batch 1000/2527: Loss = 0.0739
2026-01-06 22:08:07,926 - INFO -   Batch 1050/2527: Loss = 0.1258
2026-01-06 22:08:33,052 - INFO -   Batch 1100/2527: Loss = 0.1545
2026-01-06 22:08:58,396 - INFO -   Batch 1150/2527: Loss = 0.0644
2026-01-06 22:09:23,228 - INFO -   Batch 1200/2527: Loss = 0.0668
2026-01-06 22:09:48,083 - INFO -   Batch 1250/2527: Loss = 0.0657
2026-01-06 22:10:12,825 - INFO -   Batch 1300/2527: Loss = 0.1087
2026-01-06 22:10:37,706 - INFO -   Batch 1350/2527: Loss = 0.0846
2026-01-06 22:11:02,845 - INFO -   Batch 1400/2527: Loss = 0.1333
2026-01-06 22:11:27,818 - INFO -   Batch 1450/2527: Loss = 0.1206
2026-01-06 22:11:52,645 - INFO -   Batch 1500/2527: Loss = 0.0998
2026-01-06 22:12:17,484 - INFO -   Batch 1550/2527: Loss = 0.1121
2026-01-06 22:12:42,527 - INFO -   Batch 1600/2527: Loss = 0.0720
2026-01-06 22:13:07,388 - INFO -   Batch 1650/2527: Loss = 0.1380
2026-01-06 22:13:32,251 - INFO -   Batch 1700/2527: Loss = 0.0906
2026-01-06 22:13:57,379 - INFO -   Batch 1750/2527: Loss = 0.1172
2026-01-06 22:14:22,349 - INFO -   Batch 1800/2527: Loss = 0.1128
2026-01-06 22:14:47,206 - INFO -   Batch 1850/2527: Loss = 0.0892
2026-01-06 22:15:12,400 - INFO -   Batch 1900/2527: Loss = 0.1188
2026-01-06 22:15:37,160 - INFO -   Batch 1950/2527: Loss = 0.0902
2026-01-06 22:16:02,023 - INFO -   Batch 2000/2527: Loss = 0.1053
2026-01-06 22:16:27,142 - INFO -   Batch 2050/2527: Loss = 0.1195
2026-01-06 22:16:52,110 - INFO -   Batch 2100/2527: Loss = 0.0767
2026-01-06 22:17:16,941 - INFO -   Batch 2150/2527: Loss = 0.1296
2026-01-06 22:17:41,777 - INFO -   Batch 2200/2527: Loss = 0.1198
2026-01-06 22:18:06,787 - INFO -   Batch 2250/2527: Loss = 0.1223
2026-01-06 22:18:31,980 - INFO -   Batch 2300/2527: Loss = 0.1158
2026-01-06 22:18:56,823 - INFO -   Batch 2350/2527: Loss = 0.0986
2026-01-06 22:19:21,945 - INFO -   Batch 2400/2527: Loss = 0.1183
2026-01-06 22:19:46,922 - INFO -   Batch 2450/2527: Loss = 0.1061
2026-01-06 22:20:11,756 - INFO -   Batch 2500/2527: Loss = 0.1106
2026-01-06 22:24:51,847 - INFO - Epoch 53/160: Train Loss: 0.1044, Val Loss: 1.2420, Val mIoU: 40.86%, Val PixelAcc: 79.62%
2026-01-06 22:25:18,472 - INFO -   Batch 50/2527: Loss = 0.1165
2026-01-06 22:25:43,435 - INFO -   Batch 100/2527: Loss = 0.0706
2026-01-06 22:26:08,228 - INFO -   Batch 150/2527: Loss = 0.0967
2026-01-06 22:26:33,055 - INFO -   Batch 200/2527: Loss = 0.1180
2026-01-06 22:26:58,153 - INFO -   Batch 250/2527: Loss = 0.0567
2026-01-06 22:27:23,113 - INFO -   Batch 300/2527: Loss = 0.0898
2026-01-06 22:27:47,932 - INFO -   Batch 350/2527: Loss = 0.1173
2026-01-06 22:28:12,759 - INFO -   Batch 400/2527: Loss = 0.1157
2026-01-06 22:28:37,494 - INFO -   Batch 450/2527: Loss = 0.0800
2026-01-06 22:29:02,345 - INFO -   Batch 500/2527: Loss = 0.1173
2026-01-06 22:29:27,464 - INFO -   Batch 550/2527: Loss = 0.0769
2026-01-06 22:29:52,440 - INFO -   Batch 600/2527: Loss = 0.1263
2026-01-06 22:30:17,268 - INFO -   Batch 650/2527: Loss = 0.1075
2026-01-06 22:30:42,108 - INFO -   Batch 700/2527: Loss = 0.1651
2026-01-06 22:31:07,126 - INFO -   Batch 750/2527: Loss = 0.0918
2026-01-06 22:31:32,315 - INFO -   Batch 800/2527: Loss = 0.0771
2026-01-06 22:31:57,142 - INFO -   Batch 850/2527: Loss = 0.0814
2026-01-06 22:32:22,284 - INFO -   Batch 900/2527: Loss = 0.0871
2026-01-06 22:32:47,266 - INFO -   Batch 950/2527: Loss = 0.0600
2026-01-06 22:33:12,095 - INFO -   Batch 1000/2527: Loss = 0.1013
2026-01-06 22:33:36,939 - INFO -   Batch 1050/2527: Loss = 0.0940
2026-01-06 22:34:01,653 - INFO -   Batch 1100/2527: Loss = 0.1216
2026-01-06 22:34:26,491 - INFO -   Batch 1150/2527: Loss = 0.1037
2026-01-06 22:34:51,969 - INFO -   Batch 1200/2527: Loss = 0.1299
2026-01-06 22:35:16,949 - INFO -   Batch 1250/2527: Loss = 0.1009
2026-01-06 22:35:41,769 - INFO -   Batch 1300/2527: Loss = 0.0879
2026-01-06 22:36:06,617 - INFO -   Batch 1350/2527: Loss = 0.1606
2026-01-06 22:36:31,661 - INFO -   Batch 1400/2527: Loss = 0.0865
2026-01-06 22:36:56,488 - INFO -   Batch 1450/2527: Loss = 0.0755
2026-01-06 22:37:21,700 - INFO -   Batch 1500/2527: Loss = 0.1122
2026-01-06 22:37:46,813 - INFO -   Batch 1550/2527: Loss = 0.0590
2026-01-06 22:38:11,798 - INFO -   Batch 1600/2527: Loss = 0.0826
2026-01-06 22:38:36,644 - INFO -   Batch 1650/2527: Loss = 0.0884
2026-01-06 22:39:01,497 - INFO -   Batch 1700/2527: Loss = 0.0923
2026-01-06 22:39:26,217 - INFO -   Batch 1750/2527: Loss = 0.0638
2026-01-06 22:39:51,053 - INFO -   Batch 1800/2527: Loss = 0.1223
2026-01-06 22:40:16,186 - INFO -   Batch 1850/2527: Loss = 0.0753
2026-01-06 22:40:41,167 - INFO -   Batch 1900/2527: Loss = 0.1202
2026-01-06 22:41:05,985 - INFO -   Batch 1950/2527: Loss = 0.0601
2026-01-06 22:41:30,811 - INFO -   Batch 2000/2527: Loss = 0.0932
2026-01-06 22:41:55,815 - INFO -   Batch 2050/2527: Loss = 0.0910
2026-01-06 22:42:20,637 - INFO -   Batch 2100/2527: Loss = 0.0970
2026-01-06 22:42:45,460 - INFO -   Batch 2150/2527: Loss = 0.1192
2026-01-06 22:43:10,574 - INFO -   Batch 2200/2527: Loss = 0.1427
2026-01-06 22:43:35,895 - INFO -   Batch 2250/2527: Loss = 0.0923
2026-01-06 22:44:00,738 - INFO -   Batch 2300/2527: Loss = 0.0814
2026-01-06 22:44:25,569 - INFO -   Batch 2350/2527: Loss = 0.1372
2026-01-06 22:44:50,327 - INFO -   Batch 2400/2527: Loss = 0.1159
2026-01-06 22:45:15,183 - INFO -   Batch 2450/2527: Loss = 0.0950
2026-01-06 22:45:40,319 - INFO -   Batch 2500/2527: Loss = 0.1097
2026-01-06 22:50:33,683 - INFO - Epoch 54/160: Train Loss: 0.1017, Val Loss: 1.2500, Val mIoU: 41.10%, Val PixelAcc: 79.71%
2026-01-06 22:50:58,902 - INFO -   Batch 50/2527: Loss = 0.0972
2026-01-06 22:51:21,806 - INFO -   Batch 100/2527: Loss = 0.1158
2026-01-06 22:51:45,098 - INFO -   Batch 150/2527: Loss = 0.1030
2026-01-06 22:52:08,065 - INFO -   Batch 200/2527: Loss = 0.0851
2026-01-06 22:52:31,011 - INFO -   Batch 250/2527: Loss = 0.0510
2026-01-06 22:52:53,965 - INFO -   Batch 300/2527: Loss = 0.1247
2026-01-06 22:53:16,918 - INFO -   Batch 350/2527: Loss = 0.1340
2026-01-06 22:53:39,908 - INFO -   Batch 400/2527: Loss = 0.0791
2026-01-06 22:54:03,225 - INFO -   Batch 450/2527: Loss = 0.0770
2026-01-06 22:54:26,173 - INFO -   Batch 500/2527: Loss = 0.0898
2026-01-06 22:54:49,139 - INFO -   Batch 550/2527: Loss = 0.1404
2026-01-06 22:55:12,100 - INFO -   Batch 600/2527: Loss = 0.0622
2026-01-06 22:55:35,050 - INFO -   Batch 650/2527: Loss = 0.0902
2026-01-06 22:55:58,026 - INFO -   Batch 700/2527: Loss = 0.0677
2026-01-06 22:56:20,982 - INFO -   Batch 750/2527: Loss = 0.1452
2026-01-06 22:56:43,936 - INFO -   Batch 800/2527: Loss = 0.0820
2026-01-06 22:57:06,893 - INFO -   Batch 850/2527: Loss = 0.0757
2026-01-06 22:57:29,870 - INFO -   Batch 900/2527: Loss = 0.1211
2026-01-06 22:57:52,823 - INFO -   Batch 950/2527: Loss = 0.0978
2026-01-06 22:58:15,784 - INFO -   Batch 1000/2527: Loss = 0.1059
2026-01-06 22:58:38,741 - INFO -   Batch 1050/2527: Loss = 0.0675
2026-01-06 22:59:01,713 - INFO -   Batch 1100/2527: Loss = 0.0895
2026-01-06 22:59:24,670 - INFO -   Batch 1150/2527: Loss = 0.1226
2026-01-06 22:59:47,985 - INFO -   Batch 1200/2527: Loss = 0.0856
2026-01-06 23:00:10,943 - INFO -   Batch 1250/2527: Loss = 0.0766
2026-01-06 23:00:33,904 - INFO -   Batch 1300/2527: Loss = 0.0716
2026-01-06 23:00:56,864 - INFO -   Batch 1350/2527: Loss = 0.0712
2026-01-06 23:01:19,823 - INFO -   Batch 1400/2527: Loss = 0.0664
2026-01-06 23:01:42,793 - INFO -   Batch 1450/2527: Loss = 0.1273
2026-01-06 23:02:05,751 - INFO -   Batch 1500/2527: Loss = 0.0904
2026-01-06 23:02:28,711 - INFO -   Batch 1550/2527: Loss = 0.1085
2026-01-06 23:02:52,024 - INFO -   Batch 1600/2527: Loss = 0.1103
2026-01-06 23:03:14,984 - INFO -   Batch 1650/2527: Loss = 0.1010
2026-01-06 23:03:37,969 - INFO -   Batch 1700/2527: Loss = 0.0922
2026-01-06 23:04:00,940 - INFO -   Batch 1750/2527: Loss = 0.0985
2026-01-06 23:04:23,904 - INFO -   Batch 1800/2527: Loss = 0.1074
2026-01-06 23:04:46,861 - INFO -   Batch 1850/2527: Loss = 0.0766
2026-01-06 23:05:10,183 - INFO -   Batch 1900/2527: Loss = 0.1483
2026-01-06 23:05:33,139 - INFO -   Batch 1950/2527: Loss = 0.1459
2026-01-06 23:05:56,095 - INFO -   Batch 2000/2527: Loss = 0.1389
2026-01-06 23:06:19,081 - INFO -   Batch 2050/2527: Loss = 0.1242
2026-01-06 23:06:42,036 - INFO -   Batch 2100/2527: Loss = 0.1245
2026-01-06 23:07:04,988 - INFO -   Batch 2150/2527: Loss = 0.0623
2026-01-06 23:07:27,985 - INFO -   Batch 2200/2527: Loss = 0.1414
2026-01-06 23:07:50,945 - INFO -   Batch 2250/2527: Loss = 0.0824
2026-01-06 23:08:13,905 - INFO -   Batch 2300/2527: Loss = 0.1435
2026-01-06 23:08:36,860 - INFO -   Batch 2350/2527: Loss = 0.1065
2026-01-06 23:08:59,856 - INFO -   Batch 2400/2527: Loss = 0.0891
2026-01-06 23:09:22,815 - INFO -   Batch 2450/2527: Loss = 0.0770
2026-01-06 23:09:45,768 - INFO -   Batch 2500/2527: Loss = 0.1090
2026-01-06 23:14:25,146 - INFO - Epoch 55/160: Train Loss: 0.1016, Val Loss: 1.2782, Val mIoU: 40.83%, Val PixelAcc: 79.43%
2026-01-06 23:14:50,698 - INFO -   Batch 50/2527: Loss = 0.1045
2026-01-06 23:15:14,653 - INFO -   Batch 100/2527: Loss = 0.1008
2026-01-06 23:15:38,577 - INFO -   Batch 150/2527: Loss = 0.0597
2026-01-06 23:16:02,136 - INFO -   Batch 200/2527: Loss = 0.0981
2026-01-06 23:16:25,884 - INFO -   Batch 250/2527: Loss = 0.1063
2026-01-06 23:16:49,474 - INFO -   Batch 300/2527: Loss = 0.0906
2026-01-06 23:17:13,291 - INFO -   Batch 350/2527: Loss = 0.1202
2026-01-06 23:17:37,165 - INFO -   Batch 400/2527: Loss = 0.0420
2026-01-06 23:18:00,944 - INFO -   Batch 450/2527: Loss = 0.0483
2026-01-06 23:18:24,909 - INFO -   Batch 500/2527: Loss = 0.1267
2026-01-06 23:18:48,538 - INFO -   Batch 550/2527: Loss = 0.1289
2026-01-06 23:19:12,490 - INFO -   Batch 600/2527: Loss = 0.1023
2026-01-06 23:19:36,065 - INFO -   Batch 650/2527: Loss = 0.0879
2026-01-06 23:19:59,818 - INFO -   Batch 700/2527: Loss = 0.0797
2026-01-06 23:20:23,424 - INFO -   Batch 750/2527: Loss = 0.1317
2026-01-06 23:20:47,619 - INFO -   Batch 800/2527: Loss = 0.1150
2026-01-06 23:21:11,477 - INFO -   Batch 850/2527: Loss = 0.0839
2026-01-06 23:21:35,257 - INFO -   Batch 900/2527: Loss = 0.0850
2026-01-06 23:21:58,857 - INFO -   Batch 950/2527: Loss = 0.1397
2026-01-06 23:22:22,474 - INFO -   Batch 1000/2527: Loss = 0.0589
2026-01-06 23:22:46,426 - INFO -   Batch 1050/2527: Loss = 0.0701
2026-01-06 23:23:09,999 - INFO -   Batch 1100/2527: Loss = 0.1240
2026-01-06 23:23:33,755 - INFO -   Batch 1150/2527: Loss = 0.1032
2026-01-06 23:23:57,360 - INFO -   Batch 1200/2527: Loss = 0.1189
2026-01-06 23:24:21,203 - INFO -   Batch 1250/2527: Loss = 0.1048
2026-01-06 23:24:45,086 - INFO -   Batch 1300/2527: Loss = 0.0842
2026-01-06 23:25:08,861 - INFO -   Batch 1350/2527: Loss = 0.0757
2026-01-06 23:25:32,484 - INFO -   Batch 1400/2527: Loss = 0.1278
2026-01-06 23:25:56,112 - INFO -   Batch 1450/2527: Loss = 0.1291
2026-01-06 23:26:20,065 - INFO -   Batch 1500/2527: Loss = 0.0972
2026-01-06 23:26:44,004 - INFO -   Batch 1550/2527: Loss = 0.0772
2026-01-06 23:27:07,761 - INFO -   Batch 1600/2527: Loss = 0.1056
2026-01-06 23:27:31,387 - INFO -   Batch 1650/2527: Loss = 0.0997
2026-01-06 23:27:55,210 - INFO -   Batch 1700/2527: Loss = 0.1295
2026-01-06 23:28:19,072 - INFO -   Batch 1750/2527: Loss = 0.1107
2026-01-06 23:28:42,857 - INFO -   Batch 1800/2527: Loss = 0.0590
2026-01-06 23:29:06,476 - INFO -   Batch 1850/2527: Loss = 0.1099
2026-01-06 23:29:30,099 - INFO -   Batch 1900/2527: Loss = 0.0964
2026-01-06 23:29:54,412 - INFO -   Batch 1950/2527: Loss = 0.1277
2026-01-06 23:30:17,995 - INFO -   Batch 2000/2527: Loss = 0.0596
2026-01-06 23:30:41,767 - INFO -   Batch 2050/2527: Loss = 0.1245
2026-01-06 23:31:05,411 - INFO -   Batch 2100/2527: Loss = 0.1192
2026-01-06 23:31:29,220 - INFO -   Batch 2150/2527: Loss = 0.1170
2026-01-06 23:31:53,079 - INFO -   Batch 2200/2527: Loss = 0.1343
2026-01-06 23:32:17,234 - INFO -   Batch 2250/2527: Loss = 0.0727
2026-01-06 23:32:40,831 - INFO -   Batch 2300/2527: Loss = 0.1407
2026-01-06 23:33:04,496 - INFO -   Batch 2350/2527: Loss = 0.0916
2026-01-06 23:33:28,449 - INFO -   Batch 2400/2527: Loss = 0.0930
2026-01-06 23:33:52,026 - INFO -   Batch 2450/2527: Loss = 0.1113
2026-01-06 23:34:15,778 - INFO -   Batch 2500/2527: Loss = 0.1548
2026-01-06 23:39:08,146 - INFO - Epoch 56/160: Train Loss: 0.0985, Val Loss: 1.2945, Val mIoU: 40.08%, Val PixelAcc: 78.97%
2026-01-06 23:39:34,399 - INFO -   Batch 50/2527: Loss = 0.0926
2026-01-06 23:39:59,467 - INFO -   Batch 100/2527: Loss = 0.1026
2026-01-06 23:40:24,404 - INFO -   Batch 150/2527: Loss = 0.1057
2026-01-06 23:40:49,212 - INFO -   Batch 200/2527: Loss = 0.0722
2026-01-06 23:41:14,041 - INFO -   Batch 250/2527: Loss = 0.1046
2026-01-06 23:41:39,067 - INFO -   Batch 300/2527: Loss = 0.0965
2026-01-06 23:42:03,884 - INFO -   Batch 350/2527: Loss = 0.1403
2026-01-06 23:42:28,745 - INFO -   Batch 400/2527: Loss = 0.0833
2026-01-06 23:42:54,215 - INFO -   Batch 450/2527: Loss = 0.0828
2026-01-06 23:43:19,182 - INFO -   Batch 500/2527: Loss = 0.0926
2026-01-06 23:43:44,007 - INFO -   Batch 550/2527: Loss = 0.0809
2026-01-06 23:44:08,872 - INFO -   Batch 600/2527: Loss = 0.1099
2026-01-06 23:44:33,620 - INFO -   Batch 650/2527: Loss = 0.1362
2026-01-06 23:44:58,453 - INFO -   Batch 700/2527: Loss = 0.0930
2026-01-06 23:45:23,566 - INFO -   Batch 750/2527: Loss = 0.0894
2026-01-06 23:45:48,569 - INFO -   Batch 800/2527: Loss = 0.0658
2026-01-06 23:46:13,384 - INFO -   Batch 850/2527: Loss = 0.0783
2026-01-06 23:46:38,587 - INFO -   Batch 900/2527: Loss = 0.0779
2026-01-06 23:47:03,594 - INFO -   Batch 950/2527: Loss = 0.1121
2026-01-06 23:47:28,432 - INFO -   Batch 1000/2527: Loss = 0.0859
2026-01-06 23:47:53,267 - INFO -   Batch 1050/2527: Loss = 0.0975
2026-01-06 23:48:18,416 - INFO -   Batch 1100/2527: Loss = 0.0771
2026-01-06 23:48:43,770 - INFO -   Batch 1150/2527: Loss = 0.0781
2026-01-06 23:49:08,585 - INFO -   Batch 1200/2527: Loss = 0.0874
2026-01-06 23:49:33,435 - INFO -   Batch 1250/2527: Loss = 0.0991
2026-01-06 23:49:58,170 - INFO -   Batch 1300/2527: Loss = 0.1026
2026-01-06 23:50:22,995 - INFO -   Batch 1350/2527: Loss = 0.0814
2026-01-06 23:50:48,125 - INFO -   Batch 1400/2527: Loss = 0.0850
2026-01-06 23:51:13,097 - INFO -   Batch 1450/2527: Loss = 0.0848
2026-01-06 23:51:37,909 - INFO -   Batch 1500/2527: Loss = 0.1194
2026-01-06 23:52:02,778 - INFO -   Batch 1550/2527: Loss = 0.0735
2026-01-06 23:52:27,819 - INFO -   Batch 1600/2527: Loss = 0.1208
2026-01-06 23:52:52,646 - INFO -   Batch 1650/2527: Loss = 0.1019
2026-01-06 23:53:17,471 - INFO -   Batch 1700/2527: Loss = 0.0845
2026-01-06 23:53:42,606 - INFO -   Batch 1750/2527: Loss = 0.1047
2026-01-06 23:54:07,577 - INFO -   Batch 1800/2527: Loss = 0.0766
2026-01-06 23:54:32,421 - INFO -   Batch 1850/2527: Loss = 0.1263
2026-01-06 23:54:57,612 - INFO -   Batch 1900/2527: Loss = 0.2062
2026-01-06 23:55:22,338 - INFO -   Batch 1950/2527: Loss = 0.1038
2026-01-06 23:55:47,179 - INFO -   Batch 2000/2527: Loss = 0.0555
2026-01-06 23:56:12,330 - INFO -   Batch 2050/2527: Loss = 0.1561
2026-01-06 23:56:37,326 - INFO -   Batch 2100/2527: Loss = 0.0584
2026-01-06 23:57:02,147 - INFO -   Batch 2150/2527: Loss = 0.1614
2026-01-06 23:57:26,991 - INFO -   Batch 2200/2527: Loss = 0.0932
2026-01-06 23:57:52,023 - INFO -   Batch 2250/2527: Loss = 0.1243
2026-01-06 23:58:16,846 - INFO -   Batch 2300/2527: Loss = 0.0896
2026-01-06 23:58:42,056 - INFO -   Batch 2350/2527: Loss = 0.0712
2026-01-06 23:59:07,185 - INFO -   Batch 2400/2527: Loss = 0.1085
2026-01-06 23:59:32,163 - INFO -   Batch 2450/2527: Loss = 0.0748
2026-01-06 23:59:56,986 - INFO -   Batch 2500/2527: Loss = 0.0774
2026-01-07 00:04:37,953 - INFO - Epoch 57/160: Train Loss: 0.0975, Val Loss: 1.2771, Val mIoU: 41.11%, Val PixelAcc: 79.67%
2026-01-07 00:05:04,220 - INFO -   Batch 50/2527: Loss = 0.1024
2026-01-07 00:05:29,546 - INFO -   Batch 100/2527: Loss = 0.0843
2026-01-07 00:05:54,349 - INFO -   Batch 150/2527: Loss = 0.0824
2026-01-07 00:06:19,191 - INFO -   Batch 200/2527: Loss = 0.1332
2026-01-07 00:06:44,321 - INFO -   Batch 250/2527: Loss = 0.0691
2026-01-07 00:07:09,280 - INFO -   Batch 300/2527: Loss = 0.0843
2026-01-07 00:07:34,106 - INFO -   Batch 350/2527: Loss = 0.0865
2026-01-07 00:07:58,934 - INFO -   Batch 400/2527: Loss = 0.0748
2026-01-07 00:08:23,665 - INFO -   Batch 450/2527: Loss = 0.0501
2026-01-07 00:08:48,524 - INFO -   Batch 500/2527: Loss = 0.0829
2026-01-07 00:09:13,638 - INFO -   Batch 550/2527: Loss = 0.1070
2026-01-07 00:09:38,645 - INFO -   Batch 600/2527: Loss = 0.1001
2026-01-07 00:10:03,479 - INFO -   Batch 650/2527: Loss = 0.1223
2026-01-07 00:10:28,314 - INFO -   Batch 700/2527: Loss = 0.1061
2026-01-07 00:10:53,332 - INFO -   Batch 750/2527: Loss = 0.0984
2026-01-07 00:11:18,158 - INFO -   Batch 800/2527: Loss = 0.0957
2026-01-07 00:11:43,360 - INFO -   Batch 850/2527: Loss = 0.0963
2026-01-07 00:12:08,475 - INFO -   Batch 900/2527: Loss = 0.1286
2026-01-07 00:12:33,473 - INFO -   Batch 950/2527: Loss = 0.1442
2026-01-07 00:12:58,304 - INFO -   Batch 1000/2527: Loss = 0.0779
2026-01-07 00:13:23,137 - INFO -   Batch 1050/2527: Loss = 0.1037
2026-01-07 00:13:47,891 - INFO -   Batch 1100/2527: Loss = 0.0856
2026-01-07 00:14:12,816 - INFO -   Batch 1150/2527: Loss = 0.0649
2026-01-07 00:14:37,979 - INFO -   Batch 1200/2527: Loss = 0.0657
2026-01-07 00:15:03,327 - INFO -   Batch 1250/2527: Loss = 0.0556
2026-01-07 00:15:28,169 - INFO -   Batch 1300/2527: Loss = 0.0912
2026-01-07 00:15:53,017 - INFO -   Batch 1350/2527: Loss = 0.0788
2026-01-07 00:16:18,047 - INFO -   Batch 1400/2527: Loss = 0.0788
2026-01-07 00:16:42,880 - INFO -   Batch 1450/2527: Loss = 0.1379
2026-01-07 00:17:07,729 - INFO -   Batch 1500/2527: Loss = 0.0714
2026-01-07 00:17:33,208 - INFO -   Batch 1550/2527: Loss = 0.1009
2026-01-07 00:17:58,172 - INFO -   Batch 1600/2527: Loss = 0.1079
2026-01-07 00:18:23,128 - INFO -   Batch 1650/2527: Loss = 0.1121
2026-01-07 00:18:48,008 - INFO -   Batch 1700/2527: Loss = 0.0813
2026-01-07 00:19:12,743 - INFO -   Batch 1750/2527: Loss = 0.1050
2026-01-07 00:19:37,599 - INFO -   Batch 1800/2527: Loss = 0.0963
2026-01-07 00:20:02,746 - INFO -   Batch 1850/2527: Loss = 0.1169
2026-01-07 00:20:27,704 - INFO -   Batch 1900/2527: Loss = 0.1010
2026-01-07 00:20:52,509 - INFO -   Batch 1950/2527: Loss = 0.1128
2026-01-07 00:21:17,361 - INFO -   Batch 2000/2527: Loss = 0.0941
2026-01-07 00:21:42,367 - INFO -   Batch 2050/2527: Loss = 0.1252
2026-01-07 00:22:07,194 - INFO -   Batch 2100/2527: Loss = 0.0974
2026-01-07 00:22:32,102 - INFO -   Batch 2150/2527: Loss = 0.0769
2026-01-07 00:22:57,202 - INFO -   Batch 2200/2527: Loss = 0.0828
2026-01-07 00:23:22,170 - INFO -   Batch 2250/2527: Loss = 0.0799
2026-01-07 00:23:47,363 - INFO -   Batch 2300/2527: Loss = 0.0981
2026-01-07 00:24:12,224 - INFO -   Batch 2350/2527: Loss = 0.1441
2026-01-07 00:24:36,949 - INFO -   Batch 2400/2527: Loss = 0.0853
2026-01-07 00:25:01,806 - INFO -   Batch 2450/2527: Loss = 0.0868
2026-01-07 00:25:26,977 - INFO -   Batch 2500/2527: Loss = 0.1098
2026-01-07 00:30:20,062 - INFO - Epoch 58/160: Train Loss: 0.0981, Val Loss: 1.2744, Val mIoU: 40.71%, Val PixelAcc: 79.44%
2026-01-07 00:30:44,952 - INFO -   Batch 50/2527: Loss = 0.0544
2026-01-07 00:31:07,858 - INFO -   Batch 100/2527: Loss = 0.0888
2026-01-07 00:31:31,131 - INFO -   Batch 150/2527: Loss = 0.0848
2026-01-07 00:31:54,061 - INFO -   Batch 200/2527: Loss = 0.0894
2026-01-07 00:32:16,996 - INFO -   Batch 250/2527: Loss = 0.1140
2026-01-07 00:32:39,957 - INFO -   Batch 300/2527: Loss = 0.0803
2026-01-07 00:33:02,924 - INFO -   Batch 350/2527: Loss = 0.1080
2026-01-07 00:33:25,861 - INFO -   Batch 400/2527: Loss = 0.0773
2026-01-07 00:33:49,188 - INFO -   Batch 450/2527: Loss = 0.1094
2026-01-07 00:34:12,139 - INFO -   Batch 500/2527: Loss = 0.0879
2026-01-07 00:34:35,088 - INFO -   Batch 550/2527: Loss = 0.1008
2026-01-07 00:34:58,077 - INFO -   Batch 600/2527: Loss = 0.0802
2026-01-07 00:35:21,030 - INFO -   Batch 650/2527: Loss = 0.0529
2026-01-07 00:35:43,984 - INFO -   Batch 700/2527: Loss = 0.0947
2026-01-07 00:36:06,931 - INFO -   Batch 750/2527: Loss = 0.0845
2026-01-07 00:36:29,890 - INFO -   Batch 800/2527: Loss = 0.1016
2026-01-07 00:36:52,848 - INFO -   Batch 850/2527: Loss = 0.0588
2026-01-07 00:37:15,798 - INFO -   Batch 900/2527: Loss = 0.0593
2026-01-07 00:37:38,758 - INFO -   Batch 950/2527: Loss = 0.0919
2026-01-07 00:38:01,709 - INFO -   Batch 1000/2527: Loss = 0.1135
2026-01-07 00:38:24,666 - INFO -   Batch 1050/2527: Loss = 0.1001
2026-01-07 00:38:47,613 - INFO -   Batch 1100/2527: Loss = 0.0686
2026-01-07 00:39:10,570 - INFO -   Batch 1150/2527: Loss = 0.0862
2026-01-07 00:39:33,891 - INFO -   Batch 1200/2527: Loss = 0.0793
2026-01-07 00:39:56,843 - INFO -   Batch 1250/2527: Loss = 0.1077
2026-01-07 00:40:19,865 - INFO -   Batch 1300/2527: Loss = 0.0934
2026-01-07 00:40:42,815 - INFO -   Batch 1350/2527: Loss = 0.0878
2026-01-07 00:41:05,779 - INFO -   Batch 1400/2527: Loss = 0.1282
2026-01-07 00:41:28,731 - INFO -   Batch 1450/2527: Loss = 0.0943
2026-01-07 00:41:51,693 - INFO -   Batch 1500/2527: Loss = 0.0688
2026-01-07 00:42:14,650 - INFO -   Batch 1550/2527: Loss = 0.1099
2026-01-07 00:42:37,961 - INFO -   Batch 1600/2527: Loss = 0.0720
2026-01-07 00:43:00,914 - INFO -   Batch 1650/2527: Loss = 0.1148
2026-01-07 00:43:23,869 - INFO -   Batch 1700/2527: Loss = 0.0708
2026-01-07 00:43:46,830 - INFO -   Batch 1750/2527: Loss = 0.0444
2026-01-07 00:44:09,820 - INFO -   Batch 1800/2527: Loss = 0.0790
2026-01-07 00:44:32,776 - INFO -   Batch 1850/2527: Loss = 0.0815
2026-01-07 00:44:56,093 - INFO -   Batch 1900/2527: Loss = 0.1161
2026-01-07 00:45:19,075 - INFO -   Batch 1950/2527: Loss = 0.0838
2026-01-07 00:45:42,040 - INFO -   Batch 2000/2527: Loss = 0.0733
2026-01-07 00:46:04,979 - INFO -   Batch 2050/2527: Loss = 0.0594
2026-01-07 00:46:27,960 - INFO -   Batch 2100/2527: Loss = 0.0964
2026-01-07 00:46:50,917 - INFO -   Batch 2150/2527: Loss = 0.0903
2026-01-07 00:47:13,865 - INFO -   Batch 2200/2527: Loss = 0.0845
2026-01-07 00:47:36,816 - INFO -   Batch 2250/2527: Loss = 0.0688
2026-01-07 00:47:59,799 - INFO -   Batch 2300/2527: Loss = 0.1143
2026-01-07 00:48:22,745 - INFO -   Batch 2350/2527: Loss = 0.1024
2026-01-07 00:48:45,686 - INFO -   Batch 2400/2527: Loss = 0.1418
2026-01-07 00:49:08,638 - INFO -   Batch 2450/2527: Loss = 0.0769
2026-01-07 00:49:31,626 - INFO -   Batch 2500/2527: Loss = 0.1343
2026-01-07 00:54:10,805 - INFO - Epoch 59/160: Train Loss: 0.0961, Val Loss: 1.2797, Val mIoU: 40.56%, Val PixelAcc: 79.66%
2026-01-07 00:54:36,155 - INFO -   Batch 50/2527: Loss = 0.1093
2026-01-07 00:55:00,105 - INFO -   Batch 100/2527: Loss = 0.0976
2026-01-07 00:55:24,028 - INFO -   Batch 150/2527: Loss = 0.1007
2026-01-07 00:55:47,582 - INFO -   Batch 200/2527: Loss = 0.0830
2026-01-07 00:56:11,327 - INFO -   Batch 250/2527: Loss = 0.0799
2026-01-07 00:56:34,916 - INFO -   Batch 300/2527: Loss = 0.1131
2026-01-07 00:56:58,722 - INFO -   Batch 350/2527: Loss = 0.0930
2026-01-07 00:57:22,582 - INFO -   Batch 400/2527: Loss = 0.1167
2026-01-07 00:57:46,393 - INFO -   Batch 450/2527: Loss = 0.0734
2026-01-07 00:58:10,013 - INFO -   Batch 500/2527: Loss = 0.1061
2026-01-07 00:58:34,019 - INFO -   Batch 550/2527: Loss = 0.0610
2026-01-07 00:58:57,980 - INFO -   Batch 600/2527: Loss = 0.0805
2026-01-07 00:59:21,560 - INFO -   Batch 650/2527: Loss = 0.0599
2026-01-07 00:59:45,340 - INFO -   Batch 700/2527: Loss = 0.0685
2026-01-07 01:00:08,965 - INFO -   Batch 750/2527: Loss = 0.0817
2026-01-07 01:00:32,788 - INFO -   Batch 800/2527: Loss = 0.1029
2026-01-07 01:00:57,034 - INFO -   Batch 850/2527: Loss = 0.0792
2026-01-07 01:01:20,814 - INFO -   Batch 900/2527: Loss = 0.0799
2026-01-07 01:01:44,448 - INFO -   Batch 950/2527: Loss = 0.0752
2026-01-07 01:02:08,078 - INFO -   Batch 1000/2527: Loss = 0.0695
2026-01-07 01:02:32,042 - INFO -   Batch 1050/2527: Loss = 0.0932
2026-01-07 01:02:55,618 - INFO -   Batch 1100/2527: Loss = 0.1012
2026-01-07 01:03:19,397 - INFO -   Batch 1150/2527: Loss = 0.0715
2026-01-07 01:03:43,011 - INFO -   Batch 1200/2527: Loss = 0.0665
2026-01-07 01:04:06,861 - INFO -   Batch 1250/2527: Loss = 0.0764
2026-01-07 01:04:30,712 - INFO -   Batch 1300/2527: Loss = 0.0724
2026-01-07 01:04:54,494 - INFO -   Batch 1350/2527: Loss = 0.1100
2026-01-07 01:05:18,092 - INFO -   Batch 1400/2527: Loss = 0.0620
2026-01-07 01:05:41,713 - INFO -   Batch 1450/2527: Loss = 0.0863
2026-01-07 01:06:05,696 - INFO -   Batch 1500/2527: Loss = 0.1050
2026-01-07 01:06:29,642 - INFO -   Batch 1550/2527: Loss = 0.1213
2026-01-07 01:06:53,398 - INFO -   Batch 1600/2527: Loss = 0.0850
2026-01-07 01:07:17,013 - INFO -   Batch 1650/2527: Loss = 0.0874
2026-01-07 01:07:40,830 - INFO -   Batch 1700/2527: Loss = 0.1335
2026-01-07 01:08:04,722 - INFO -   Batch 1750/2527: Loss = 0.1010
2026-01-07 01:08:28,535 - INFO -   Batch 1800/2527: Loss = 0.0801
2026-01-07 01:08:52,128 - INFO -   Batch 1850/2527: Loss = 0.1116
2026-01-07 01:09:15,761 - INFO -   Batch 1900/2527: Loss = 0.0842
2026-01-07 01:09:39,712 - INFO -   Batch 1950/2527: Loss = 0.1150
2026-01-07 01:10:03,654 - INFO -   Batch 2000/2527: Loss = 0.1514
2026-01-07 01:10:27,419 - INFO -   Batch 2050/2527: Loss = 0.1082
2026-01-07 01:10:51,052 - INFO -   Batch 2100/2527: Loss = 0.1298
2026-01-07 01:11:14,877 - INFO -   Batch 2150/2527: Loss = 0.1164
2026-01-07 01:11:38,742 - INFO -   Batch 2200/2527: Loss = 0.1206
2026-01-07 01:12:02,892 - INFO -   Batch 2250/2527: Loss = 0.1147
2026-01-07 01:12:26,495 - INFO -   Batch 2300/2527: Loss = 0.1151
2026-01-07 01:12:50,151 - INFO -   Batch 2350/2527: Loss = 0.0863
2026-01-07 01:13:14,116 - INFO -   Batch 2400/2527: Loss = 0.0802
2026-01-07 01:13:37,690 - INFO -   Batch 2450/2527: Loss = 0.0971
2026-01-07 01:14:01,448 - INFO -   Batch 2500/2527: Loss = 0.1090
2026-01-07 01:18:53,748 - INFO - Epoch 60/160: Train Loss: 0.0952, Val Loss: 1.2969, Val mIoU: 39.81%, Val PixelAcc: 79.37%
2026-01-07 01:18:55,923 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_60.pth
2026-01-07 01:18:55,925 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_60.pth
2026-01-07 01:19:21,938 - INFO -   Batch 50/2527: Loss = 0.1167
2026-01-07 01:19:46,992 - INFO -   Batch 100/2527: Loss = 0.0847
2026-01-07 01:20:11,947 - INFO -   Batch 150/2527: Loss = 0.0832
2026-01-07 01:20:36,748 - INFO -   Batch 200/2527: Loss = 0.0966
2026-01-07 01:21:01,572 - INFO -   Batch 250/2527: Loss = 0.0993
2026-01-07 01:21:26,577 - INFO -   Batch 300/2527: Loss = 0.0531
2026-01-07 01:21:51,387 - INFO -   Batch 350/2527: Loss = 0.0927
2026-01-07 01:22:16,229 - INFO -   Batch 400/2527: Loss = 0.0635
2026-01-07 01:22:41,346 - INFO -   Batch 450/2527: Loss = 0.0955
2026-01-07 01:23:06,678 - INFO -   Batch 500/2527: Loss = 0.0988
2026-01-07 01:23:31,497 - INFO -   Batch 550/2527: Loss = 0.0974
2026-01-07 01:23:56,329 - INFO -   Batch 600/2527: Loss = 0.0770
2026-01-07 01:24:21,044 - INFO -   Batch 650/2527: Loss = 0.1154
2026-01-07 01:24:45,904 - INFO -   Batch 700/2527: Loss = 0.0733
2026-01-07 01:25:11,022 - INFO -   Batch 750/2527: Loss = 0.1113
2026-01-07 01:25:36,026 - INFO -   Batch 800/2527: Loss = 0.0924
2026-01-07 01:26:00,849 - INFO -   Batch 850/2527: Loss = 0.1138
2026-01-07 01:26:26,036 - INFO -   Batch 900/2527: Loss = 0.0686
2026-01-07 01:26:51,082 - INFO -   Batch 950/2527: Loss = 0.0701
2026-01-07 01:27:15,927 - INFO -   Batch 1000/2527: Loss = 0.0625
2026-01-07 01:27:40,768 - INFO -   Batch 1050/2527: Loss = 0.1256
2026-01-07 01:28:05,887 - INFO -   Batch 1100/2527: Loss = 0.0824
2026-01-07 01:28:30,859 - INFO -   Batch 1150/2527: Loss = 0.0586
2026-01-07 01:28:56,032 - INFO -   Batch 1200/2527: Loss = 0.0979
2026-01-07 01:29:20,875 - INFO -   Batch 1250/2527: Loss = 0.1176
2026-01-07 01:29:45,597 - INFO -   Batch 1300/2527: Loss = 0.0776
2026-01-07 01:30:10,428 - INFO -   Batch 1350/2527: Loss = 0.0623
2026-01-07 01:30:35,570 - INFO -   Batch 1400/2527: Loss = 0.0989
2026-01-07 01:31:00,547 - INFO -   Batch 1450/2527: Loss = 0.1197
2026-01-07 01:31:25,371 - INFO -   Batch 1500/2527: Loss = 0.1150
2026-01-07 01:31:50,191 - INFO -   Batch 1550/2527: Loss = 0.0845
2026-01-07 01:32:15,190 - INFO -   Batch 1600/2527: Loss = 0.0863
2026-01-07 01:32:40,041 - INFO -   Batch 1650/2527: Loss = 0.0746
2026-01-07 01:33:04,869 - INFO -   Batch 1700/2527: Loss = 0.0819
2026-01-07 01:33:30,009 - INFO -   Batch 1750/2527: Loss = 0.0763
2026-01-07 01:33:54,974 - INFO -   Batch 1800/2527: Loss = 0.0685
2026-01-07 01:34:19,818 - INFO -   Batch 1850/2527: Loss = 0.1047
2026-01-07 01:34:44,657 - INFO -   Batch 1900/2527: Loss = 0.0796
2026-01-07 01:35:09,756 - INFO -   Batch 1950/2527: Loss = 0.0845
2026-01-07 01:35:34,596 - INFO -   Batch 2000/2527: Loss = 0.0749
2026-01-07 01:35:59,715 - INFO -   Batch 2050/2527: Loss = 0.0799
2026-01-07 01:36:24,676 - INFO -   Batch 2100/2527: Loss = 0.0945
2026-01-07 01:36:49,534 - INFO -   Batch 2150/2527: Loss = 0.0817
2026-01-07 01:37:14,375 - INFO -   Batch 2200/2527: Loss = 0.0758
2026-01-07 01:37:39,416 - INFO -   Batch 2250/2527: Loss = 0.0812
2026-01-07 01:38:04,271 - INFO -   Batch 2300/2527: Loss = 0.0753
2026-01-07 01:38:29,490 - INFO -   Batch 2350/2527: Loss = 0.0770
2026-01-07 01:38:54,606 - INFO -   Batch 2400/2527: Loss = 0.0868
2026-01-07 01:39:19,599 - INFO -   Batch 2450/2527: Loss = 0.0850
2026-01-07 01:39:44,422 - INFO -   Batch 2500/2527: Loss = 0.0872
2026-01-07 01:44:23,773 - INFO - Epoch 61/160: Train Loss: 0.0893, Val Loss: 1.2880, Val mIoU: 41.06%, Val PixelAcc: 79.80%
2026-01-07 01:44:50,133 - INFO -   Batch 50/2527: Loss = 0.0591
2026-01-07 01:45:15,460 - INFO -   Batch 100/2527: Loss = 0.0663
2026-01-07 01:45:40,253 - INFO -   Batch 150/2527: Loss = 0.0644
2026-01-07 01:46:05,108 - INFO -   Batch 200/2527: Loss = 0.0743
2026-01-07 01:46:30,228 - INFO -   Batch 250/2527: Loss = 0.0809
2026-01-07 01:46:55,191 - INFO -   Batch 300/2527: Loss = 0.0934
2026-01-07 01:47:20,026 - INFO -   Batch 350/2527: Loss = 0.1193
2026-01-07 01:47:44,854 - INFO -   Batch 400/2527: Loss = 0.0833
2026-01-07 01:48:09,599 - INFO -   Batch 450/2527: Loss = 0.0569
2026-01-07 01:48:34,430 - INFO -   Batch 500/2527: Loss = 0.0889
2026-01-07 01:48:59,562 - INFO -   Batch 550/2527: Loss = 0.0669
2026-01-07 01:49:24,538 - INFO -   Batch 600/2527: Loss = 0.1552
2026-01-07 01:49:49,379 - INFO -   Batch 650/2527: Loss = 0.0775
2026-01-07 01:50:14,206 - INFO -   Batch 700/2527: Loss = 0.1050
2026-01-07 01:50:39,228 - INFO -   Batch 750/2527: Loss = 0.0620
2026-01-07 01:51:04,059 - INFO -   Batch 800/2527: Loss = 0.0809
2026-01-07 01:51:29,277 - INFO -   Batch 850/2527: Loss = 0.0983
2026-01-07 01:51:54,386 - INFO -   Batch 900/2527: Loss = 0.0978
2026-01-07 01:52:19,391 - INFO -   Batch 950/2527: Loss = 0.0755
2026-01-07 01:52:44,217 - INFO -   Batch 1000/2527: Loss = 0.0511
2026-01-07 01:53:09,063 - INFO -   Batch 1050/2527: Loss = 0.0685
2026-01-07 01:53:33,782 - INFO -   Batch 1100/2527: Loss = 0.0964
2026-01-07 01:53:58,625 - INFO -   Batch 1150/2527: Loss = 0.0869
2026-01-07 01:54:23,745 - INFO -   Batch 1200/2527: Loss = 0.0599
2026-01-07 01:54:49,080 - INFO -   Batch 1250/2527: Loss = 0.0977
2026-01-07 01:55:13,901 - INFO -   Batch 1300/2527: Loss = 0.0647
2026-01-07 01:55:38,738 - INFO -   Batch 1350/2527: Loss = 0.0595
2026-01-07 01:56:03,777 - INFO -   Batch 1400/2527: Loss = 0.0803
2026-01-07 01:56:28,587 - INFO -   Batch 1450/2527: Loss = 0.0828
2026-01-07 01:56:53,436 - INFO -   Batch 1500/2527: Loss = 0.0862
2026-01-07 01:57:18,946 - INFO -   Batch 1550/2527: Loss = 0.0650
2026-01-07 01:57:43,914 - INFO -   Batch 1600/2527: Loss = 0.1000
2026-01-07 01:58:08,735 - INFO -   Batch 1650/2527: Loss = 0.0843
2026-01-07 01:58:33,576 - INFO -   Batch 1700/2527: Loss = 0.0661
2026-01-07 01:58:58,306 - INFO -   Batch 1750/2527: Loss = 0.0828
2026-01-07 01:59:23,155 - INFO -   Batch 1800/2527: Loss = 0.0658
2026-01-07 01:59:48,285 - INFO -   Batch 1850/2527: Loss = 0.0926
2026-01-07 02:00:13,273 - INFO -   Batch 1900/2527: Loss = 0.0503
2026-01-07 02:00:38,097 - INFO -   Batch 1950/2527: Loss = 0.0685
2026-01-07 02:01:02,943 - INFO -   Batch 2000/2527: Loss = 0.0971
2026-01-07 02:01:27,966 - INFO -   Batch 2050/2527: Loss = 0.0596
2026-01-07 02:01:52,789 - INFO -   Batch 2100/2527: Loss = 0.0797
2026-01-07 02:02:17,624 - INFO -   Batch 2150/2527: Loss = 0.1059
2026-01-07 02:02:42,753 - INFO -   Batch 2200/2527: Loss = 0.0701
2026-01-07 02:03:07,746 - INFO -   Batch 2250/2527: Loss = 0.0911
2026-01-07 02:03:32,945 - INFO -   Batch 2300/2527: Loss = 0.0927
2026-01-07 02:03:57,809 - INFO -   Batch 2350/2527: Loss = 0.0946
2026-01-07 02:04:22,530 - INFO -   Batch 2400/2527: Loss = 0.0659
2026-01-07 02:04:47,371 - INFO -   Batch 2450/2527: Loss = 0.0974
2026-01-07 02:05:12,495 - INFO -   Batch 2500/2527: Loss = 0.1384
2026-01-07 02:10:05,406 - INFO - Epoch 62/160: Train Loss: 0.0854, Val Loss: 1.3027, Val mIoU: 40.77%, Val PixelAcc: 79.62%
2026-01-07 02:10:30,511 - INFO -   Batch 50/2527: Loss = 0.0721
2026-01-07 02:10:53,416 - INFO -   Batch 100/2527: Loss = 0.0700
2026-01-07 02:11:16,371 - INFO -   Batch 150/2527: Loss = 0.1037
2026-01-07 02:11:39,666 - INFO -   Batch 200/2527: Loss = 0.1065
2026-01-07 02:12:02,636 - INFO -   Batch 250/2527: Loss = 0.0909
2026-01-07 02:12:25,587 - INFO -   Batch 300/2527: Loss = 0.0610
2026-01-07 02:12:48,543 - INFO -   Batch 350/2527: Loss = 0.1257
2026-01-07 02:13:11,493 - INFO -   Batch 400/2527: Loss = 0.1741
2026-01-07 02:13:34,462 - INFO -   Batch 450/2527: Loss = 0.1348
2026-01-07 02:13:57,779 - INFO -   Batch 500/2527: Loss = 0.0727
2026-01-07 02:14:20,737 - INFO -   Batch 550/2527: Loss = 0.1004
2026-01-07 02:14:43,705 - INFO -   Batch 600/2527: Loss = 0.1216
2026-01-07 02:15:06,662 - INFO -   Batch 650/2527: Loss = 0.0991
2026-01-07 02:15:29,623 - INFO -   Batch 700/2527: Loss = 0.1130
2026-01-07 02:15:52,582 - INFO -   Batch 750/2527: Loss = 0.1039
2026-01-07 02:16:15,544 - INFO -   Batch 800/2527: Loss = 0.0886
2026-01-07 02:16:38,501 - INFO -   Batch 850/2527: Loss = 0.0965
2026-01-07 02:17:01,457 - INFO -   Batch 900/2527: Loss = 0.0829
2026-01-07 02:17:24,412 - INFO -   Batch 950/2527: Loss = 0.0852
2026-01-07 02:17:47,375 - INFO -   Batch 1000/2527: Loss = 0.1465
2026-01-07 02:18:10,329 - INFO -   Batch 1050/2527: Loss = 0.0713
2026-01-07 02:18:33,288 - INFO -   Batch 1100/2527: Loss = 0.0992
2026-01-07 02:18:56,246 - INFO -   Batch 1150/2527: Loss = 0.0875
2026-01-07 02:19:19,568 - INFO -   Batch 1200/2527: Loss = 0.1290
2026-01-07 02:19:42,529 - INFO -   Batch 1250/2527: Loss = 0.1742
2026-01-07 02:20:05,492 - INFO -   Batch 1300/2527: Loss = 0.0836
2026-01-07 02:20:28,472 - INFO -   Batch 1350/2527: Loss = 0.0622
2026-01-07 02:20:51,427 - INFO -   Batch 1400/2527: Loss = 0.1088
2026-01-07 02:21:14,408 - INFO -   Batch 1450/2527: Loss = 0.1095
2026-01-07 02:21:37,364 - INFO -   Batch 1500/2527: Loss = 0.1105
2026-01-07 02:22:00,328 - INFO -   Batch 1550/2527: Loss = 0.0780
2026-01-07 02:22:23,285 - INFO -   Batch 1600/2527: Loss = 0.1358
2026-01-07 02:22:46,588 - INFO -   Batch 1650/2527: Loss = 0.1129
2026-01-07 02:23:09,553 - INFO -   Batch 1700/2527: Loss = 0.0772
2026-01-07 02:23:32,549 - INFO -   Batch 1750/2527: Loss = 0.0846
2026-01-07 02:23:55,509 - INFO -   Batch 1800/2527: Loss = 0.0701
2026-01-07 02:24:18,464 - INFO -   Batch 1850/2527: Loss = 0.1232
2026-01-07 02:24:41,426 - INFO -   Batch 1900/2527: Loss = 0.0751
2026-01-07 02:25:04,750 - INFO -   Batch 1950/2527: Loss = 0.0774
2026-01-07 02:25:27,702 - INFO -   Batch 2000/2527: Loss = 0.0696
2026-01-07 02:25:50,654 - INFO -   Batch 2050/2527: Loss = 0.0944
2026-01-07 02:26:13,630 - INFO -   Batch 2100/2527: Loss = 0.0692
2026-01-07 02:26:36,577 - INFO -   Batch 2150/2527: Loss = 0.0719
2026-01-07 02:26:59,524 - INFO -   Batch 2200/2527: Loss = 0.1402
2026-01-07 02:27:22,475 - INFO -   Batch 2250/2527: Loss = 0.0645
2026-01-07 02:27:45,457 - INFO -   Batch 2300/2527: Loss = 0.0886
2026-01-07 02:28:08,412 - INFO -   Batch 2350/2527: Loss = 0.1039
2026-01-07 02:28:31,363 - INFO -   Batch 2400/2527: Loss = 0.0809
2026-01-07 02:28:54,312 - INFO -   Batch 2450/2527: Loss = 0.1121
2026-01-07 02:29:17,296 - INFO -   Batch 2500/2527: Loss = 0.0938
2026-01-07 02:33:56,714 - INFO - Epoch 63/160: Train Loss: 0.0982, Val Loss: 1.2854, Val mIoU: 40.86%, Val PixelAcc: 79.69%
2026-01-07 02:34:22,011 - INFO -   Batch 50/2527: Loss = 0.0770
2026-01-07 02:34:45,589 - INFO -   Batch 100/2527: Loss = 0.0631
2026-01-07 02:35:09,911 - INFO -   Batch 150/2527: Loss = 0.0961
2026-01-07 02:35:33,462 - INFO -   Batch 200/2527: Loss = 0.0840
2026-01-07 02:35:57,205 - INFO -   Batch 250/2527: Loss = 0.1225
2026-01-07 02:36:20,810 - INFO -   Batch 300/2527: Loss = 0.0686
2026-01-07 02:36:44,636 - INFO -   Batch 350/2527: Loss = 0.1205
2026-01-07 02:37:08,501 - INFO -   Batch 400/2527: Loss = 0.0888
2026-01-07 02:37:32,313 - INFO -   Batch 450/2527: Loss = 0.1518
2026-01-07 02:37:55,912 - INFO -   Batch 500/2527: Loss = 0.1360
2026-01-07 02:38:19,923 - INFO -   Batch 550/2527: Loss = 0.0645
2026-01-07 02:38:43,893 - INFO -   Batch 600/2527: Loss = 0.0699
2026-01-07 02:39:07,470 - INFO -   Batch 650/2527: Loss = 0.0807
2026-01-07 02:39:31,251 - INFO -   Batch 700/2527: Loss = 0.1198
2026-01-07 02:39:54,852 - INFO -   Batch 750/2527: Loss = 0.0687
2026-01-07 02:40:18,689 - INFO -   Batch 800/2527: Loss = 0.0753
2026-01-07 02:40:42,914 - INFO -   Batch 850/2527: Loss = 0.0769
2026-01-07 02:41:06,684 - INFO -   Batch 900/2527: Loss = 0.0840
2026-01-07 02:41:30,314 - INFO -   Batch 950/2527: Loss = 0.0770
2026-01-07 02:41:53,943 - INFO -   Batch 1000/2527: Loss = 0.0820
2026-01-07 02:42:17,897 - INFO -   Batch 1050/2527: Loss = 0.0963
2026-01-07 02:42:41,468 - INFO -   Batch 1100/2527: Loss = 0.0696
2026-01-07 02:43:05,220 - INFO -   Batch 1150/2527: Loss = 0.1036
2026-01-07 02:43:28,824 - INFO -   Batch 1200/2527: Loss = 0.0547
2026-01-07 02:43:52,637 - INFO -   Batch 1250/2527: Loss = 0.1377
2026-01-07 02:44:16,487 - INFO -   Batch 1300/2527: Loss = 0.1313
2026-01-07 02:44:40,258 - INFO -   Batch 1350/2527: Loss = 0.0581
2026-01-07 02:45:03,853 - INFO -   Batch 1400/2527: Loss = 0.0668
2026-01-07 02:45:27,472 - INFO -   Batch 1450/2527: Loss = 0.0461
2026-01-07 02:45:51,470 - INFO -   Batch 1500/2527: Loss = 0.1122
2026-01-07 02:46:15,041 - INFO -   Batch 1550/2527: Loss = 0.0613
2026-01-07 02:46:39,157 - INFO -   Batch 1600/2527: Loss = 0.1042
2026-01-07 02:47:02,770 - INFO -   Batch 1650/2527: Loss = 0.0596
2026-01-07 02:47:26,579 - INFO -   Batch 1700/2527: Loss = 0.0907
2026-01-07 02:47:50,474 - INFO -   Batch 1750/2527: Loss = 0.0910
2026-01-07 02:48:14,265 - INFO -   Batch 1800/2527: Loss = 0.0595
2026-01-07 02:48:37,863 - INFO -   Batch 1850/2527: Loss = 0.0764
2026-01-07 02:49:01,494 - INFO -   Batch 1900/2527: Loss = 0.0685
2026-01-07 02:49:25,452 - INFO -   Batch 1950/2527: Loss = 0.0884
2026-01-07 02:49:49,372 - INFO -   Batch 2000/2527: Loss = 0.0911
2026-01-07 02:50:13,152 - INFO -   Batch 2050/2527: Loss = 0.0595
2026-01-07 02:50:36,755 - INFO -   Batch 2100/2527: Loss = 0.0791
2026-01-07 02:51:00,575 - INFO -   Batch 2150/2527: Loss = 0.1386
2026-01-07 02:51:24,428 - INFO -   Batch 2200/2527: Loss = 0.1362
2026-01-07 02:51:48,215 - INFO -   Batch 2250/2527: Loss = 0.0907
2026-01-07 02:52:12,183 - INFO -   Batch 2300/2527: Loss = 0.1335
2026-01-07 02:52:35,807 - INFO -   Batch 2350/2527: Loss = 0.1179
2026-01-07 02:52:59,754 - INFO -   Batch 2400/2527: Loss = 0.1152
2026-01-07 02:53:23,333 - INFO -   Batch 2450/2527: Loss = 0.1833
2026-01-07 02:53:47,087 - INFO -   Batch 2500/2527: Loss = 0.1114
2026-01-07 02:58:38,884 - INFO - Epoch 64/160: Train Loss: 0.0897, Val Loss: 1.3186, Val mIoU: 40.20%, Val PixelAcc: 78.97%
2026-01-07 02:59:04,996 - INFO -   Batch 50/2527: Loss = 0.1073
2026-01-07 02:59:30,060 - INFO -   Batch 100/2527: Loss = 0.0943
2026-01-07 02:59:55,017 - INFO -   Batch 150/2527: Loss = 0.0755
2026-01-07 03:00:19,819 - INFO -   Batch 200/2527: Loss = 0.5179
2026-01-07 03:00:44,669 - INFO -   Batch 250/2527: Loss = 0.1018
2026-01-07 03:01:09,676 - INFO -   Batch 300/2527: Loss = 0.1070
2026-01-07 03:01:34,532 - INFO -   Batch 350/2527: Loss = 0.1456
2026-01-07 03:01:59,378 - INFO -   Batch 400/2527: Loss = 0.0700
2026-01-07 03:02:24,487 - INFO -   Batch 450/2527: Loss = 0.1198
2026-01-07 03:02:49,833 - INFO -   Batch 500/2527: Loss = 0.1268
2026-01-07 03:03:14,651 - INFO -   Batch 550/2527: Loss = 0.0672
2026-01-07 03:03:39,477 - INFO -   Batch 600/2527: Loss = 0.0897
2026-01-07 03:04:04,197 - INFO -   Batch 650/2527: Loss = 0.0778
2026-01-07 03:04:29,060 - INFO -   Batch 700/2527: Loss = 0.1213
2026-01-07 03:04:54,179 - INFO -   Batch 750/2527: Loss = 0.0664
2026-01-07 03:05:19,156 - INFO -   Batch 800/2527: Loss = 0.0687
2026-01-07 03:05:44,010 - INFO -   Batch 850/2527: Loss = 0.0780
2026-01-07 03:06:09,255 - INFO -   Batch 900/2527: Loss = 0.0753
2026-01-07 03:06:34,300 - INFO -   Batch 950/2527: Loss = 0.0724
2026-01-07 03:06:59,133 - INFO -   Batch 1000/2527: Loss = 0.0799
2026-01-07 03:07:23,972 - INFO -   Batch 1050/2527: Loss = 0.1059
2026-01-07 03:07:49,114 - INFO -   Batch 1100/2527: Loss = 0.1030
2026-01-07 03:08:14,094 - INFO -   Batch 1150/2527: Loss = 0.1161
2026-01-07 03:08:39,302 - INFO -   Batch 1200/2527: Loss = 0.1155
2026-01-07 03:09:04,140 - INFO -   Batch 1250/2527: Loss = 0.0756
2026-01-07 03:09:28,858 - INFO -   Batch 1300/2527: Loss = 0.0897
2026-01-07 03:09:53,680 - INFO -   Batch 1350/2527: Loss = 0.0413
2026-01-07 03:10:18,816 - INFO -   Batch 1400/2527: Loss = 0.0644
2026-01-07 03:10:43,803 - INFO -   Batch 1450/2527: Loss = 0.0932
2026-01-07 03:11:08,614 - INFO -   Batch 1500/2527: Loss = 0.0993
2026-01-07 03:11:33,473 - INFO -   Batch 1550/2527: Loss = 0.0633
2026-01-07 03:11:58,488 - INFO -   Batch 1600/2527: Loss = 0.1216
2026-01-07 03:12:23,313 - INFO -   Batch 1650/2527: Loss = 0.0930
2026-01-07 03:12:48,176 - INFO -   Batch 1700/2527: Loss = 0.0731
2026-01-07 03:13:13,293 - INFO -   Batch 1750/2527: Loss = 0.0944
2026-01-07 03:13:38,285 - INFO -   Batch 1800/2527: Loss = 0.0570
2026-01-07 03:14:03,132 - INFO -   Batch 1850/2527: Loss = 0.0985
2026-01-07 03:14:27,960 - INFO -   Batch 1900/2527: Loss = 0.1000
2026-01-07 03:14:53,037 - INFO -   Batch 1950/2527: Loss = 0.1246
2026-01-07 03:15:17,881 - INFO -   Batch 2000/2527: Loss = 0.0893
2026-01-07 03:15:43,024 - INFO -   Batch 2050/2527: Loss = 0.1044
2026-01-07 03:16:07,993 - INFO -   Batch 2100/2527: Loss = 0.0692
2026-01-07 03:16:32,836 - INFO -   Batch 2150/2527: Loss = 0.0805
2026-01-07 03:16:57,677 - INFO -   Batch 2200/2527: Loss = 0.0799
2026-01-07 03:17:22,685 - INFO -   Batch 2250/2527: Loss = 0.0880
2026-01-07 03:17:47,533 - INFO -   Batch 2300/2527: Loss = 0.0722
2026-01-07 03:18:12,764 - INFO -   Batch 2350/2527: Loss = 0.0955
2026-01-07 03:18:37,874 - INFO -   Batch 2400/2527: Loss = 0.1012
2026-01-07 03:19:02,842 - INFO -   Batch 2450/2527: Loss = 0.0801
2026-01-07 03:19:27,689 - INFO -   Batch 2500/2527: Loss = 0.0708
2026-01-07 03:24:08,361 - INFO - Epoch 65/160: Train Loss: 0.0897, Val Loss: 1.3256, Val mIoU: 40.68%, Val PixelAcc: 79.57%
2026-01-07 03:24:34,903 - INFO -   Batch 50/2527: Loss = 0.0607
2026-01-07 03:24:59,867 - INFO -   Batch 100/2527: Loss = 0.0882
2026-01-07 03:25:25,036 - INFO -   Batch 150/2527: Loss = 0.0950
2026-01-07 03:25:49,878 - INFO -   Batch 200/2527: Loss = 0.0638
2026-01-07 03:26:14,984 - INFO -   Batch 250/2527: Loss = 0.0761
2026-01-07 03:26:39,972 - INFO -   Batch 300/2527: Loss = 0.0501
2026-01-07 03:27:04,786 - INFO -   Batch 350/2527: Loss = 0.0694
2026-01-07 03:27:29,663 - INFO -   Batch 400/2527: Loss = 0.0889
2026-01-07 03:27:54,372 - INFO -   Batch 450/2527: Loss = 0.0701
2026-01-07 03:28:19,226 - INFO -   Batch 500/2527: Loss = 0.0659
2026-01-07 03:28:44,346 - INFO -   Batch 550/2527: Loss = 0.0984
2026-01-07 03:29:09,320 - INFO -   Batch 600/2527: Loss = 0.0777
2026-01-07 03:29:34,146 - INFO -   Batch 650/2527: Loss = 0.0741
2026-01-07 03:29:59,021 - INFO -   Batch 700/2527: Loss = 0.0729
2026-01-07 03:30:24,027 - INFO -   Batch 750/2527: Loss = 0.0900
2026-01-07 03:30:48,857 - INFO -   Batch 800/2527: Loss = 0.0770
2026-01-07 03:31:14,071 - INFO -   Batch 850/2527: Loss = 0.0910
2026-01-07 03:31:39,212 - INFO -   Batch 900/2527: Loss = 0.0602
2026-01-07 03:32:04,202 - INFO -   Batch 950/2527: Loss = 0.0921
2026-01-07 03:32:29,018 - INFO -   Batch 1000/2527: Loss = 0.0508
2026-01-07 03:32:53,848 - INFO -   Batch 1050/2527: Loss = 0.0811
2026-01-07 03:33:18,573 - INFO -   Batch 1100/2527: Loss = 0.0980
2026-01-07 03:33:43,401 - INFO -   Batch 1150/2527: Loss = 0.0673
2026-01-07 03:34:08,548 - INFO -   Batch 1200/2527: Loss = 0.0656
2026-01-07 03:34:33,525 - INFO -   Batch 1250/2527: Loss = 0.1014
2026-01-07 03:34:58,696 - INFO -   Batch 1300/2527: Loss = 0.0727
2026-01-07 03:35:23,530 - INFO -   Batch 1350/2527: Loss = 0.1093
2026-01-07 03:35:48,582 - INFO -   Batch 1400/2527: Loss = 0.0504
2026-01-07 03:36:13,425 - INFO -   Batch 1450/2527: Loss = 0.0960
2026-01-07 03:36:38,257 - INFO -   Batch 1500/2527: Loss = 0.0715
2026-01-07 03:37:03,370 - INFO -   Batch 1550/2527: Loss = 0.1050
2026-01-07 03:37:28,707 - INFO -   Batch 1600/2527: Loss = 0.0607
2026-01-07 03:37:53,529 - INFO -   Batch 1650/2527: Loss = 0.0735
2026-01-07 03:38:18,371 - INFO -   Batch 1700/2527: Loss = 0.0722
2026-01-07 03:38:43,138 - INFO -   Batch 1750/2527: Loss = 0.0817
2026-01-07 03:39:07,967 - INFO -   Batch 1800/2527: Loss = 0.1067
2026-01-07 03:39:33,111 - INFO -   Batch 1850/2527: Loss = 0.0975
2026-01-07 03:39:58,099 - INFO -   Batch 1900/2527: Loss = 0.0431
2026-01-07 03:40:22,916 - INFO -   Batch 1950/2527: Loss = 0.0553
2026-01-07 03:40:47,757 - INFO -   Batch 2000/2527: Loss = 0.0780
2026-01-07 03:41:12,782 - INFO -   Batch 2050/2527: Loss = 0.0952
2026-01-07 03:41:37,606 - INFO -   Batch 2100/2527: Loss = 0.0801
2026-01-07 03:42:02,471 - INFO -   Batch 2150/2527: Loss = 0.1130
2026-01-07 03:42:27,586 - INFO -   Batch 2200/2527: Loss = 0.0778
2026-01-07 03:42:52,558 - INFO -   Batch 2250/2527: Loss = 0.0748
2026-01-07 03:43:17,759 - INFO -   Batch 2300/2527: Loss = 0.0639
2026-01-07 03:43:42,627 - INFO -   Batch 2350/2527: Loss = 0.0774
2026-01-07 03:44:07,370 - INFO -   Batch 2400/2527: Loss = 0.0625
2026-01-07 03:44:32,200 - INFO -   Batch 2450/2527: Loss = 0.0775
2026-01-07 03:44:57,324 - INFO -   Batch 2500/2527: Loss = 0.0629
2026-01-07 03:49:49,935 - INFO - Epoch 66/160: Train Loss: 0.0822, Val Loss: 1.3319, Val mIoU: 39.63%, Val PixelAcc: 79.05%
2026-01-07 03:50:15,043 - INFO -   Batch 50/2527: Loss = 0.0885
2026-01-07 03:50:37,986 - INFO -   Batch 100/2527: Loss = 0.0787
2026-01-07 03:51:00,915 - INFO -   Batch 150/2527: Loss = 0.0651
2026-01-07 03:51:24,209 - INFO -   Batch 200/2527: Loss = 0.0731
2026-01-07 03:51:47,187 - INFO -   Batch 250/2527: Loss = 0.1205
2026-01-07 03:52:10,136 - INFO -   Batch 300/2527: Loss = 0.1280
2026-01-07 03:52:33,090 - INFO -   Batch 350/2527: Loss = 0.0777
2026-01-07 03:52:56,045 - INFO -   Batch 400/2527: Loss = 0.0794
2026-01-07 03:53:19,029 - INFO -   Batch 450/2527: Loss = 0.0754
2026-01-07 03:53:42,359 - INFO -   Batch 500/2527: Loss = 0.0657
2026-01-07 03:54:05,343 - INFO -   Batch 550/2527: Loss = 0.0512
2026-01-07 03:54:28,314 - INFO -   Batch 600/2527: Loss = 0.0827
2026-01-07 03:54:51,271 - INFO -   Batch 650/2527: Loss = 0.1191
2026-01-07 03:55:14,231 - INFO -   Batch 700/2527: Loss = 0.1028
2026-01-07 03:55:37,207 - INFO -   Batch 750/2527: Loss = 0.0745
2026-01-07 03:56:00,163 - INFO -   Batch 800/2527: Loss = 0.0469
2026-01-07 03:56:23,121 - INFO -   Batch 850/2527: Loss = 0.1018
2026-01-07 03:56:46,078 - INFO -   Batch 900/2527: Loss = 0.0952
2026-01-07 03:57:09,048 - INFO -   Batch 950/2527: Loss = 0.0977
2026-01-07 03:57:32,003 - INFO -   Batch 1000/2527: Loss = 0.0740
2026-01-07 03:57:54,957 - INFO -   Batch 1050/2527: Loss = 0.1181
2026-01-07 03:58:17,911 - INFO -   Batch 1100/2527: Loss = 0.0872
2026-01-07 03:58:40,882 - INFO -   Batch 1150/2527: Loss = 0.0926
2026-01-07 03:59:03,836 - INFO -   Batch 1200/2527: Loss = 0.0826
2026-01-07 03:59:27,157 - INFO -   Batch 1250/2527: Loss = 0.0676
2026-01-07 03:59:50,139 - INFO -   Batch 1300/2527: Loss = 0.0689
2026-01-07 04:00:13,099 - INFO -   Batch 1350/2527: Loss = 0.0520
2026-01-07 04:00:36,060 - INFO -   Batch 1400/2527: Loss = 0.0652
2026-01-07 04:00:59,025 - INFO -   Batch 1450/2527: Loss = 0.0956
2026-01-07 04:01:21,981 - INFO -   Batch 1500/2527: Loss = 0.0585
2026-01-07 04:01:44,942 - INFO -   Batch 1550/2527: Loss = 0.0565
2026-01-07 04:02:07,906 - INFO -   Batch 1600/2527: Loss = 0.0738
2026-01-07 04:02:31,241 - INFO -   Batch 1650/2527: Loss = 0.0809
2026-01-07 04:02:54,198 - INFO -   Batch 1700/2527: Loss = 0.0653
2026-01-07 04:03:17,193 - INFO -   Batch 1750/2527: Loss = 0.0636
2026-01-07 04:03:40,152 - INFO -   Batch 1800/2527: Loss = 0.0678
2026-01-07 04:04:03,110 - INFO -   Batch 1850/2527: Loss = 0.0993
2026-01-07 04:04:26,073 - INFO -   Batch 1900/2527: Loss = 0.0503
2026-01-07 04:04:49,422 - INFO -   Batch 1950/2527: Loss = 0.0718
2026-01-07 04:05:12,369 - INFO -   Batch 2000/2527: Loss = 0.0658
2026-01-07 04:05:35,355 - INFO -   Batch 2050/2527: Loss = 0.1050
2026-01-07 04:05:58,308 - INFO -   Batch 2100/2527: Loss = 0.0931
2026-01-07 04:06:21,262 - INFO -   Batch 2150/2527: Loss = 0.0566
2026-01-07 04:06:44,214 - INFO -   Batch 2200/2527: Loss = 0.0534
2026-01-07 04:07:07,201 - INFO -   Batch 2250/2527: Loss = 0.0936
2026-01-07 04:07:30,155 - INFO -   Batch 2300/2527: Loss = 0.0599
2026-01-07 04:07:53,105 - INFO -   Batch 2350/2527: Loss = 0.0943
2026-01-07 04:08:16,055 - INFO -   Batch 2400/2527: Loss = 0.1123
2026-01-07 04:08:39,035 - INFO -   Batch 2450/2527: Loss = 0.0717
2026-01-07 04:09:01,986 - INFO -   Batch 2500/2527: Loss = 0.0673
2026-01-07 04:13:41,230 - INFO - Epoch 67/160: Train Loss: 0.0812, Val Loss: 1.3685, Val mIoU: 40.65%, Val PixelAcc: 79.50%
2026-01-07 04:14:06,563 - INFO -   Batch 50/2527: Loss = 0.0484
2026-01-07 04:14:30,132 - INFO -   Batch 100/2527: Loss = 0.0644
2026-01-07 04:14:54,431 - INFO -   Batch 150/2527: Loss = 0.0641
2026-01-07 04:15:17,981 - INFO -   Batch 200/2527: Loss = 0.0519
2026-01-07 04:15:41,729 - INFO -   Batch 250/2527: Loss = 0.0939
2026-01-07 04:16:05,390 - INFO -   Batch 300/2527: Loss = 0.0633
2026-01-07 04:16:29,212 - INFO -   Batch 350/2527: Loss = 0.0846
2026-01-07 04:16:53,067 - INFO -   Batch 400/2527: Loss = 0.0781
2026-01-07 04:17:16,846 - INFO -   Batch 450/2527: Loss = 0.0960
2026-01-07 04:17:40,434 - INFO -   Batch 500/2527: Loss = 0.0644
2026-01-07 04:18:04,442 - INFO -   Batch 550/2527: Loss = 0.0990
2026-01-07 04:18:28,391 - INFO -   Batch 600/2527: Loss = 0.0976
2026-01-07 04:18:51,958 - INFO -   Batch 650/2527: Loss = 0.0828
2026-01-07 04:19:15,720 - INFO -   Batch 700/2527: Loss = 0.0736
2026-01-07 04:19:39,330 - INFO -   Batch 750/2527: Loss = 0.0740
2026-01-07 04:20:03,185 - INFO -   Batch 800/2527: Loss = 0.1097
2026-01-07 04:20:27,409 - INFO -   Batch 850/2527: Loss = 0.0764
2026-01-07 04:20:51,186 - INFO -   Batch 900/2527: Loss = 0.0727
2026-01-07 04:21:14,778 - INFO -   Batch 950/2527: Loss = 0.0765
2026-01-07 04:21:38,406 - INFO -   Batch 1000/2527: Loss = 0.0746
2026-01-07 04:22:02,368 - INFO -   Batch 1050/2527: Loss = 0.0665
2026-01-07 04:22:25,942 - INFO -   Batch 1100/2527: Loss = 0.0684
2026-01-07 04:22:49,694 - INFO -   Batch 1150/2527: Loss = 0.1187
2026-01-07 04:23:13,324 - INFO -   Batch 1200/2527: Loss = 0.0848
2026-01-07 04:23:37,148 - INFO -   Batch 1250/2527: Loss = 0.0919
2026-01-07 04:24:00,999 - INFO -   Batch 1300/2527: Loss = 0.0697
2026-01-07 04:24:24,790 - INFO -   Batch 1350/2527: Loss = 0.0531
2026-01-07 04:24:48,409 - INFO -   Batch 1400/2527: Loss = 0.1034
2026-01-07 04:25:12,039 - INFO -   Batch 1450/2527: Loss = 0.0704
2026-01-07 04:25:36,000 - INFO -   Batch 1500/2527: Loss = 0.0838
2026-01-07 04:25:59,568 - INFO -   Batch 1550/2527: Loss = 0.0747
2026-01-07 04:26:23,684 - INFO -   Batch 1600/2527: Loss = 0.0825
2026-01-07 04:26:47,318 - INFO -   Batch 1650/2527: Loss = 0.0735
2026-01-07 04:27:11,148 - INFO -   Batch 1700/2527: Loss = 0.0587
2026-01-07 04:27:35,020 - INFO -   Batch 1750/2527: Loss = 0.0734
2026-01-07 04:27:58,797 - INFO -   Batch 1800/2527: Loss = 0.0692
2026-01-07 04:28:22,386 - INFO -   Batch 1850/2527: Loss = 0.0905
2026-01-07 04:28:45,998 - INFO -   Batch 1900/2527: Loss = 0.1126
2026-01-07 04:29:09,972 - INFO -   Batch 1950/2527: Loss = 0.0552
2026-01-07 04:29:33,909 - INFO -   Batch 2000/2527: Loss = 0.0561
2026-01-07 04:29:57,658 - INFO -   Batch 2050/2527: Loss = 0.0736
2026-01-07 04:30:21,253 - INFO -   Batch 2100/2527: Loss = 0.1040
2026-01-07 04:30:45,064 - INFO -   Batch 2150/2527: Loss = 0.0960
2026-01-07 04:31:08,959 - INFO -   Batch 2200/2527: Loss = 0.0731
2026-01-07 04:31:32,742 - INFO -   Batch 2250/2527: Loss = 0.0661
2026-01-07 04:31:56,709 - INFO -   Batch 2300/2527: Loss = 0.0633
2026-01-07 04:32:20,326 - INFO -   Batch 2350/2527: Loss = 0.0869
2026-01-07 04:32:44,274 - INFO -   Batch 2400/2527: Loss = 0.0750
2026-01-07 04:33:07,880 - INFO -   Batch 2450/2527: Loss = 0.0855
2026-01-07 04:33:31,641 - INFO -   Batch 2500/2527: Loss = 0.0971
2026-01-07 04:38:23,574 - INFO - Epoch 68/160: Train Loss: 0.0831, Val Loss: 1.3466, Val mIoU: 40.71%, Val PixelAcc: 79.59%
2026-01-07 04:38:49,536 - INFO -   Batch 50/2527: Loss = 0.0787
2026-01-07 04:39:14,606 - INFO -   Batch 100/2527: Loss = 0.0636
2026-01-07 04:39:39,550 - INFO -   Batch 150/2527: Loss = 0.0778
2026-01-07 04:40:04,372 - INFO -   Batch 200/2527: Loss = 0.0797
2026-01-07 04:40:29,187 - INFO -   Batch 250/2527: Loss = 0.1098
2026-01-07 04:40:54,192 - INFO -   Batch 300/2527: Loss = 0.1066
2026-01-07 04:41:19,016 - INFO -   Batch 350/2527: Loss = 0.1285
2026-01-07 04:41:43,855 - INFO -   Batch 400/2527: Loss = 0.0826
2026-01-07 04:42:08,979 - INFO -   Batch 450/2527: Loss = 0.0788
2026-01-07 04:42:34,307 - INFO -   Batch 500/2527: Loss = 0.0867
2026-01-07 04:42:59,127 - INFO -   Batch 550/2527: Loss = 0.0966
2026-01-07 04:43:23,980 - INFO -   Batch 600/2527: Loss = 0.0734
2026-01-07 04:43:48,697 - INFO -   Batch 650/2527: Loss = 0.1220
2026-01-07 04:44:13,538 - INFO -   Batch 700/2527: Loss = 0.0749
2026-01-07 04:44:38,665 - INFO -   Batch 750/2527: Loss = 0.1232
2026-01-07 04:45:03,637 - INFO -   Batch 800/2527: Loss = 0.0764
2026-01-07 04:45:28,491 - INFO -   Batch 850/2527: Loss = 0.0800
2026-01-07 04:45:53,330 - INFO -   Batch 900/2527: Loss = 0.1076
2026-01-07 04:46:18,709 - INFO -   Batch 950/2527: Loss = 0.0725
2026-01-07 04:46:43,539 - INFO -   Batch 1000/2527: Loss = 0.0846
2026-01-07 04:47:08,385 - INFO -   Batch 1050/2527: Loss = 0.0955
2026-01-07 04:47:33,491 - INFO -   Batch 1100/2527: Loss = 0.0680
2026-01-07 04:47:58,467 - INFO -   Batch 1150/2527: Loss = 0.0639
2026-01-07 04:48:23,291 - INFO -   Batch 1200/2527: Loss = 0.0751
2026-01-07 04:48:48,504 - INFO -   Batch 1250/2527: Loss = 0.0623
2026-01-07 04:49:13,221 - INFO -   Batch 1300/2527: Loss = 0.0913
2026-01-07 04:49:38,045 - INFO -   Batch 1350/2527: Loss = 0.0819
2026-01-07 04:50:03,161 - INFO -   Batch 1400/2527: Loss = 0.0724
2026-01-07 04:50:28,151 - INFO -   Batch 1450/2527: Loss = 0.0825
2026-01-07 04:50:52,971 - INFO -   Batch 1500/2527: Loss = 0.0854
2026-01-07 04:51:17,838 - INFO -   Batch 1550/2527: Loss = 0.0640
2026-01-07 04:51:42,848 - INFO -   Batch 1600/2527: Loss = 0.0651
2026-01-07 04:52:07,664 - INFO -   Batch 1650/2527: Loss = 0.0761
2026-01-07 04:52:32,514 - INFO -   Batch 1700/2527: Loss = 0.0611
2026-01-07 04:52:57,623 - INFO -   Batch 1750/2527: Loss = 0.0669
2026-01-07 04:53:22,582 - INFO -   Batch 1800/2527: Loss = 0.0855
2026-01-07 04:53:47,424 - INFO -   Batch 1850/2527: Loss = 0.0772
2026-01-07 04:54:12,285 - INFO -   Batch 1900/2527: Loss = 0.0753
2026-01-07 04:54:37,365 - INFO -   Batch 1950/2527: Loss = 0.0598
2026-01-07 04:55:02,186 - INFO -   Batch 2000/2527: Loss = 0.0852
2026-01-07 04:55:27,299 - INFO -   Batch 2050/2527: Loss = 0.0953
2026-01-07 04:55:52,273 - INFO -   Batch 2100/2527: Loss = 0.0624
2026-01-07 04:56:17,098 - INFO -   Batch 2150/2527: Loss = 0.1143
2026-01-07 04:56:41,956 - INFO -   Batch 2200/2527: Loss = 0.1021
2026-01-07 04:57:06,966 - INFO -   Batch 2250/2527: Loss = 0.1094
2026-01-07 04:57:31,785 - INFO -   Batch 2300/2527: Loss = 0.0640
2026-01-07 04:57:56,609 - INFO -   Batch 2350/2527: Loss = 0.0487
2026-01-07 04:58:22,094 - INFO -   Batch 2400/2527: Loss = 0.0795
2026-01-07 04:58:47,097 - INFO -   Batch 2450/2527: Loss = 0.0968
2026-01-07 04:59:11,928 - INFO -   Batch 2500/2527: Loss = 0.0889
2026-01-07 05:03:51,801 - INFO - Epoch 69/160: Train Loss: 0.0876, Val Loss: 1.3850, Val mIoU: 40.52%, Val PixelAcc: 79.42%
2026-01-07 05:04:18,169 - INFO -   Batch 50/2527: Loss = 0.0874
2026-01-07 05:04:43,127 - INFO -   Batch 100/2527: Loss = 0.0834
2026-01-07 05:05:08,308 - INFO -   Batch 150/2527: Loss = 0.0706
2026-01-07 05:05:33,135 - INFO -   Batch 200/2527: Loss = 0.0560
2026-01-07 05:05:58,237 - INFO -   Batch 250/2527: Loss = 0.0687
2026-01-07 05:06:23,195 - INFO -   Batch 300/2527: Loss = 0.1063
2026-01-07 05:06:48,032 - INFO -   Batch 350/2527: Loss = 0.0616
2026-01-07 05:07:12,865 - INFO -   Batch 400/2527: Loss = 0.0661
2026-01-07 05:07:37,588 - INFO -   Batch 450/2527: Loss = 0.0660
2026-01-07 05:08:02,442 - INFO -   Batch 500/2527: Loss = 0.0820
2026-01-07 05:08:27,563 - INFO -   Batch 550/2527: Loss = 0.0814
2026-01-07 05:08:52,522 - INFO -   Batch 600/2527: Loss = 0.0940
2026-01-07 05:09:17,360 - INFO -   Batch 650/2527: Loss = 0.0459
2026-01-07 05:09:42,187 - INFO -   Batch 700/2527: Loss = 0.0729
2026-01-07 05:10:07,228 - INFO -   Batch 750/2527: Loss = 0.0564
2026-01-07 05:10:32,053 - INFO -   Batch 800/2527: Loss = 0.0639
2026-01-07 05:10:56,878 - INFO -   Batch 850/2527: Loss = 0.0920
2026-01-07 05:11:22,369 - INFO -   Batch 900/2527: Loss = 0.0549
2026-01-07 05:11:47,347 - INFO -   Batch 950/2527: Loss = 0.0461
2026-01-07 05:12:12,191 - INFO -   Batch 1000/2527: Loss = 0.0738
2026-01-07 05:12:37,041 - INFO -   Batch 1050/2527: Loss = 0.1040
2026-01-07 05:13:01,788 - INFO -   Batch 1100/2527: Loss = 0.0592
2026-01-07 05:13:26,632 - INFO -   Batch 1150/2527: Loss = 0.0709
2026-01-07 05:13:51,744 - INFO -   Batch 1200/2527: Loss = 0.0817
2026-01-07 05:14:16,745 - INFO -   Batch 1250/2527: Loss = 0.0947
2026-01-07 05:14:41,929 - INFO -   Batch 1300/2527: Loss = 0.2399
2026-01-07 05:15:06,778 - INFO -   Batch 1350/2527: Loss = 0.0976
2026-01-07 05:15:31,796 - INFO -   Batch 1400/2527: Loss = 0.0511
2026-01-07 05:15:56,622 - INFO -   Batch 1450/2527: Loss = 0.0753
2026-01-07 05:16:21,491 - INFO -   Batch 1500/2527: Loss = 0.0879
2026-01-07 05:16:46,601 - INFO -   Batch 1550/2527: Loss = 0.1033
2026-01-07 05:17:11,952 - INFO -   Batch 1600/2527: Loss = 0.0502
2026-01-07 05:17:36,802 - INFO -   Batch 1650/2527: Loss = 0.0562
2026-01-07 05:18:01,629 - INFO -   Batch 1700/2527: Loss = 0.1063
2026-01-07 05:18:26,351 - INFO -   Batch 1750/2527: Loss = 0.0804
2026-01-07 05:18:51,204 - INFO -   Batch 1800/2527: Loss = 0.0946
2026-01-07 05:19:16,316 - INFO -   Batch 1850/2527: Loss = 0.0812
2026-01-07 05:19:41,288 - INFO -   Batch 1900/2527: Loss = 0.0696
2026-01-07 05:20:06,126 - INFO -   Batch 1950/2527: Loss = 0.0602
2026-01-07 05:20:30,968 - INFO -   Batch 2000/2527: Loss = 0.0741
2026-01-07 05:20:55,970 - INFO -   Batch 2050/2527: Loss = 0.0795
2026-01-07 05:21:20,820 - INFO -   Batch 2100/2527: Loss = 0.0672
2026-01-07 05:21:45,657 - INFO -   Batch 2150/2527: Loss = 0.0995
2026-01-07 05:22:10,790 - INFO -   Batch 2200/2527: Loss = 0.0642
2026-01-07 05:22:35,769 - INFO -   Batch 2250/2527: Loss = 0.0741
2026-01-07 05:23:00,600 - INFO -   Batch 2300/2527: Loss = 0.0564
2026-01-07 05:23:25,790 - INFO -   Batch 2350/2527: Loss = 0.0552
2026-01-07 05:23:50,509 - INFO -   Batch 2400/2527: Loss = 0.0823
2026-01-07 05:24:15,366 - INFO -   Batch 2450/2527: Loss = 0.0679
2026-01-07 05:24:40,486 - INFO -   Batch 2500/2527: Loss = 0.1170
2026-01-07 05:29:33,388 - INFO - Epoch 70/160: Train Loss: 0.0753, Val Loss: 1.4183, Val mIoU: 39.49%, Val PixelAcc: 78.96%
2026-01-07 05:29:35,623 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_70.pth
2026-01-07 05:29:35,625 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_70.pth
2026-01-07 05:30:00,393 - INFO -   Batch 50/2527: Loss = 0.0530
2026-01-07 05:30:23,285 - INFO -   Batch 100/2527: Loss = 0.0538
2026-01-07 05:30:46,291 - INFO -   Batch 150/2527: Loss = 0.1055
2026-01-07 05:31:09,630 - INFO -   Batch 200/2527: Loss = 0.0656
2026-01-07 05:31:32,607 - INFO -   Batch 250/2527: Loss = 0.1039
2026-01-07 05:31:55,560 - INFO -   Batch 300/2527: Loss = 0.0673
2026-01-07 05:32:18,517 - INFO -   Batch 350/2527: Loss = 0.0537
2026-01-07 05:32:41,475 - INFO -   Batch 400/2527: Loss = 0.0610
2026-01-07 05:33:04,455 - INFO -   Batch 450/2527: Loss = 0.0681
2026-01-07 05:33:27,775 - INFO -   Batch 500/2527: Loss = 0.1902
2026-01-07 05:33:50,733 - INFO -   Batch 550/2527: Loss = 0.0979
2026-01-07 05:34:13,710 - INFO -   Batch 600/2527: Loss = 0.0802
2026-01-07 05:34:36,661 - INFO -   Batch 650/2527: Loss = 0.0681
2026-01-07 05:34:59,629 - INFO -   Batch 700/2527: Loss = 0.0822
2026-01-07 05:35:22,585 - INFO -   Batch 750/2527: Loss = 0.0717
2026-01-07 05:35:45,538 - INFO -   Batch 800/2527: Loss = 0.0912
2026-01-07 05:36:08,491 - INFO -   Batch 850/2527: Loss = 0.1117
2026-01-07 05:36:31,446 - INFO -   Batch 900/2527: Loss = 0.0716
2026-01-07 05:36:54,398 - INFO -   Batch 950/2527: Loss = 0.0893
2026-01-07 05:37:17,369 - INFO -   Batch 1000/2527: Loss = 0.0888
2026-01-07 05:37:40,323 - INFO -   Batch 1050/2527: Loss = 0.0876
2026-01-07 05:38:03,284 - INFO -   Batch 1100/2527: Loss = 0.1031
2026-01-07 05:38:26,239 - INFO -   Batch 1150/2527: Loss = 0.0880
2026-01-07 05:38:49,212 - INFO -   Batch 1200/2527: Loss = 0.0697
2026-01-07 05:39:12,532 - INFO -   Batch 1250/2527: Loss = 0.0608
2026-01-07 05:39:35,493 - INFO -   Batch 1300/2527: Loss = 0.0679
2026-01-07 05:39:58,478 - INFO -   Batch 1350/2527: Loss = 0.0765
2026-01-07 05:40:21,442 - INFO -   Batch 1400/2527: Loss = 0.0520
2026-01-07 05:40:44,436 - INFO -   Batch 1450/2527: Loss = 0.0821
2026-01-07 05:41:07,395 - INFO -   Batch 1500/2527: Loss = 0.0834
2026-01-07 05:41:30,357 - INFO -   Batch 1550/2527: Loss = 0.0717
2026-01-07 05:41:53,314 - INFO -   Batch 1600/2527: Loss = 0.0654
2026-01-07 05:42:16,667 - INFO -   Batch 1650/2527: Loss = 0.0943
2026-01-07 05:42:39,625 - INFO -   Batch 1700/2527: Loss = 0.0533
2026-01-07 05:43:02,618 - INFO -   Batch 1750/2527: Loss = 0.0593
2026-01-07 05:43:25,581 - INFO -   Batch 1800/2527: Loss = 0.0866
2026-01-07 05:43:48,541 - INFO -   Batch 1850/2527: Loss = 0.0742
2026-01-07 05:44:11,500 - INFO -   Batch 1900/2527: Loss = 0.1065
2026-01-07 05:44:34,826 - INFO -   Batch 1950/2527: Loss = 0.0859
2026-01-07 05:44:57,785 - INFO -   Batch 2000/2527: Loss = 0.0758
2026-01-07 05:45:20,738 - INFO -   Batch 2050/2527: Loss = 0.1024
2026-01-07 05:45:43,705 - INFO -   Batch 2100/2527: Loss = 0.1153
2026-01-07 05:46:06,661 - INFO -   Batch 2150/2527: Loss = 0.0679
2026-01-07 05:46:29,622 - INFO -   Batch 2200/2527: Loss = 0.0468
2026-01-07 05:46:52,578 - INFO -   Batch 2250/2527: Loss = 0.0470
2026-01-07 05:47:15,537 - INFO -   Batch 2300/2527: Loss = 0.0906
2026-01-07 05:47:38,492 - INFO -   Batch 2350/2527: Loss = 0.0695
2026-01-07 05:48:01,452 - INFO -   Batch 2400/2527: Loss = 0.0660
2026-01-07 05:48:24,401 - INFO -   Batch 2450/2527: Loss = 0.0759
2026-01-07 05:48:47,355 - INFO -   Batch 2500/2527: Loss = 0.0628
2026-01-07 05:53:26,829 - INFO - Epoch 71/160: Train Loss: 0.0801, Val Loss: 1.3752, Val mIoU: 40.25%, Val PixelAcc: 79.55%
2026-01-07 05:53:52,212 - INFO -   Batch 50/2527: Loss = 0.1032
2026-01-07 05:54:15,783 - INFO -   Batch 100/2527: Loss = 0.0794
2026-01-07 05:54:40,085 - INFO -   Batch 150/2527: Loss = 0.0893
2026-01-07 05:55:03,659 - INFO -   Batch 200/2527: Loss = 0.0578
2026-01-07 05:55:27,408 - INFO -   Batch 250/2527: Loss = 0.0705
2026-01-07 05:55:51,024 - INFO -   Batch 300/2527: Loss = 0.0795
2026-01-07 05:56:14,830 - INFO -   Batch 350/2527: Loss = 0.0886
2026-01-07 05:56:38,686 - INFO -   Batch 400/2527: Loss = 0.0639
2026-01-07 05:57:02,490 - INFO -   Batch 450/2527: Loss = 0.1313
2026-01-07 05:57:26,079 - INFO -   Batch 500/2527: Loss = 0.0919
2026-01-07 05:57:49,703 - INFO -   Batch 550/2527: Loss = 0.0746
2026-01-07 05:58:14,016 - INFO -   Batch 600/2527: Loss = 0.0750
2026-01-07 05:58:37,604 - INFO -   Batch 650/2527: Loss = 0.0771
2026-01-07 05:59:01,395 - INFO -   Batch 700/2527: Loss = 0.0595
2026-01-07 05:59:25,004 - INFO -   Batch 750/2527: Loss = 0.0641
2026-01-07 05:59:48,821 - INFO -   Batch 800/2527: Loss = 0.0687
2026-01-07 06:00:12,710 - INFO -   Batch 850/2527: Loss = 0.0894
2026-01-07 06:00:36,859 - INFO -   Batch 900/2527: Loss = 0.0633
2026-01-07 06:01:00,466 - INFO -   Batch 950/2527: Loss = 0.0569
2026-01-07 06:01:24,088 - INFO -   Batch 1000/2527: Loss = 0.0423
2026-01-07 06:01:48,035 - INFO -   Batch 1050/2527: Loss = 0.0608
2026-01-07 06:02:11,645 - INFO -   Batch 1100/2527: Loss = 0.1098
2026-01-07 06:02:35,421 - INFO -   Batch 1150/2527: Loss = 0.0798
2026-01-07 06:02:59,024 - INFO -   Batch 1200/2527: Loss = 0.0719
2026-01-07 06:03:22,839 - INFO -   Batch 1250/2527: Loss = 0.0560
2026-01-07 06:03:46,699 - INFO -   Batch 1300/2527: Loss = 0.1810
2026-01-07 06:04:10,503 - INFO -   Batch 1350/2527: Loss = 0.0801
2026-01-07 06:04:34,098 - INFO -   Batch 1400/2527: Loss = 0.0596
2026-01-07 06:04:57,725 - INFO -   Batch 1450/2527: Loss = 0.0801
2026-01-07 06:05:21,677 - INFO -   Batch 1500/2527: Loss = 0.0630
2026-01-07 06:05:45,248 - INFO -   Batch 1550/2527: Loss = 0.0702
2026-01-07 06:06:09,378 - INFO -   Batch 1600/2527: Loss = 0.0781
2026-01-07 06:06:32,992 - INFO -   Batch 1650/2527: Loss = 0.0757
2026-01-07 06:06:56,807 - INFO -   Batch 1700/2527: Loss = 0.0942
2026-01-07 06:07:20,662 - INFO -   Batch 1750/2527: Loss = 0.0681
2026-01-07 06:07:44,451 - INFO -   Batch 1800/2527: Loss = 0.1019
2026-01-07 06:08:08,086 - INFO -   Batch 1850/2527: Loss = 0.0739
2026-01-07 06:08:31,711 - INFO -   Batch 1900/2527: Loss = 0.1066
2026-01-07 06:08:55,665 - INFO -   Batch 1950/2527: Loss = 0.0775
2026-01-07 06:09:19,288 - INFO -   Batch 2000/2527: Loss = 0.0771
2026-01-07 06:09:43,402 - INFO -   Batch 2050/2527: Loss = 0.0674
2026-01-07 06:10:07,016 - INFO -   Batch 2100/2527: Loss = 0.0774
2026-01-07 06:10:30,837 - INFO -   Batch 2150/2527: Loss = 0.1213
2026-01-07 06:10:54,694 - INFO -   Batch 2200/2527: Loss = 0.0768
2026-01-07 06:11:18,504 - INFO -   Batch 2250/2527: Loss = 0.0669
2026-01-07 06:11:42,474 - INFO -   Batch 2300/2527: Loss = 0.0799
2026-01-07 06:12:06,107 - INFO -   Batch 2350/2527: Loss = 0.0756
2026-01-07 06:12:30,059 - INFO -   Batch 2400/2527: Loss = 0.0592
2026-01-07 06:12:53,627 - INFO -   Batch 2450/2527: Loss = 0.0440
2026-01-07 06:13:17,423 - INFO -   Batch 2500/2527: Loss = 0.0607
2026-01-07 06:18:08,991 - INFO - Epoch 72/160: Train Loss: 0.0761, Val Loss: 1.3955, Val mIoU: 40.65%, Val PixelAcc: 79.51%
2026-01-07 06:18:34,945 - INFO -   Batch 50/2527: Loss = 0.0886
2026-01-07 06:19:00,005 - INFO -   Batch 100/2527: Loss = 0.1315
2026-01-07 06:19:24,952 - INFO -   Batch 150/2527: Loss = 0.0434
2026-01-07 06:19:49,757 - INFO -   Batch 200/2527: Loss = 0.0479
2026-01-07 06:20:14,612 - INFO -   Batch 250/2527: Loss = 0.0923
2026-01-07 06:20:39,626 - INFO -   Batch 300/2527: Loss = 0.0747
2026-01-07 06:21:04,480 - INFO -   Batch 350/2527: Loss = 0.0664
2026-01-07 06:21:29,321 - INFO -   Batch 400/2527: Loss = 0.0784
2026-01-07 06:21:54,435 - INFO -   Batch 450/2527: Loss = 0.0783
2026-01-07 06:22:19,434 - INFO -   Batch 500/2527: Loss = 0.0823
2026-01-07 06:22:44,634 - INFO -   Batch 550/2527: Loss = 0.0918
2026-01-07 06:23:09,468 - INFO -   Batch 600/2527: Loss = 0.0634
2026-01-07 06:23:34,201 - INFO -   Batch 650/2527: Loss = 0.0564
2026-01-07 06:23:59,076 - INFO -   Batch 700/2527: Loss = 0.0963
2026-01-07 06:24:24,189 - INFO -   Batch 750/2527: Loss = 0.0827
2026-01-07 06:24:49,163 - INFO -   Batch 800/2527: Loss = 0.0842
2026-01-07 06:25:14,154 - INFO -   Batch 850/2527: Loss = 0.0640
2026-01-07 06:25:39,014 - INFO -   Batch 900/2527: Loss = 0.0475
2026-01-07 06:26:04,385 - INFO -   Batch 950/2527: Loss = 0.0608
2026-01-07 06:26:29,212 - INFO -   Batch 1000/2527: Loss = 0.0905
2026-01-07 06:26:54,045 - INFO -   Batch 1050/2527: Loss = 0.0795
2026-01-07 06:27:19,178 - INFO -   Batch 1100/2527: Loss = 0.0785
2026-01-07 06:27:44,154 - INFO -   Batch 1150/2527: Loss = 0.0645
2026-01-07 06:28:08,971 - INFO -   Batch 1200/2527: Loss = 0.0554
2026-01-07 06:28:34,175 - INFO -   Batch 1250/2527: Loss = 0.0571
2026-01-07 06:28:58,892 - INFO -   Batch 1300/2527: Loss = 0.0997
2026-01-07 06:29:23,734 - INFO -   Batch 1350/2527: Loss = 0.0666
2026-01-07 06:29:48,869 - INFO -   Batch 1400/2527: Loss = 0.0824
2026-01-07 06:30:13,834 - INFO -   Batch 1450/2527: Loss = 0.0662
2026-01-07 06:30:38,655 - INFO -   Batch 1500/2527: Loss = 0.0537
2026-01-07 06:31:03,532 - INFO -   Batch 1550/2527: Loss = 0.0867
2026-01-07 06:31:28,539 - INFO -   Batch 1600/2527: Loss = 0.0812
2026-01-07 06:31:53,357 - INFO -   Batch 1650/2527: Loss = 0.0453
2026-01-07 06:32:18,187 - INFO -   Batch 1700/2527: Loss = 0.0521
2026-01-07 06:32:43,303 - INFO -   Batch 1750/2527: Loss = 0.0641
2026-01-07 06:33:08,286 - INFO -   Batch 1800/2527: Loss = 0.0539
2026-01-07 06:33:33,135 - INFO -   Batch 1850/2527: Loss = 0.0646
2026-01-07 06:33:58,004 - INFO -   Batch 1900/2527: Loss = 0.0662
2026-01-07 06:34:22,729 - INFO -   Batch 1950/2527: Loss = 0.0767
2026-01-07 06:34:47,915 - INFO -   Batch 2000/2527: Loss = 0.0704
2026-01-07 06:35:13,047 - INFO -   Batch 2050/2527: Loss = 0.0694
2026-01-07 06:35:38,024 - INFO -   Batch 2100/2527: Loss = 0.0984
2026-01-07 06:36:02,850 - INFO -   Batch 2150/2527: Loss = 0.0711
2026-01-07 06:36:27,677 - INFO -   Batch 2200/2527: Loss = 0.0976
2026-01-07 06:36:52,688 - INFO -   Batch 2250/2527: Loss = 0.0518
2026-01-07 06:37:17,533 - INFO -   Batch 2300/2527: Loss = 0.0788
2026-01-07 06:37:42,392 - INFO -   Batch 2350/2527: Loss = 0.0755
2026-01-07 06:38:07,876 - INFO -   Batch 2400/2527: Loss = 0.0480
2026-01-07 06:38:32,849 - INFO -   Batch 2450/2527: Loss = 0.0697
2026-01-07 06:38:57,696 - INFO -   Batch 2500/2527: Loss = 0.0800
2026-01-07 06:43:37,861 - INFO - Epoch 73/160: Train Loss: 0.0753, Val Loss: 1.3970, Val mIoU: 40.67%, Val PixelAcc: 79.55%
2026-01-07 06:44:04,261 - INFO -   Batch 50/2527: Loss = 0.1055
2026-01-07 06:44:29,219 - INFO -   Batch 100/2527: Loss = 0.0839
2026-01-07 06:44:54,392 - INFO -   Batch 150/2527: Loss = 0.0460
2026-01-07 06:45:19,240 - INFO -   Batch 200/2527: Loss = 0.0552
2026-01-07 06:45:44,341 - INFO -   Batch 250/2527: Loss = 0.0493
2026-01-07 06:46:09,317 - INFO -   Batch 300/2527: Loss = 0.0800
2026-01-07 06:46:34,147 - INFO -   Batch 350/2527: Loss = 0.0848
2026-01-07 06:46:58,977 - INFO -   Batch 400/2527: Loss = 0.0510
2026-01-07 06:47:23,690 - INFO -   Batch 450/2527: Loss = 0.0645
2026-01-07 06:47:48,560 - INFO -   Batch 500/2527: Loss = 0.0861
2026-01-07 06:48:13,671 - INFO -   Batch 550/2527: Loss = 0.0773
2026-01-07 06:48:38,668 - INFO -   Batch 600/2527: Loss = 0.0886
2026-01-07 06:49:03,504 - INFO -   Batch 650/2527: Loss = 0.0821
2026-01-07 06:49:28,338 - INFO -   Batch 700/2527: Loss = 0.0576
2026-01-07 06:49:53,344 - INFO -   Batch 750/2527: Loss = 0.0646
2026-01-07 06:50:18,167 - INFO -   Batch 800/2527: Loss = 0.0918
2026-01-07 06:50:42,988 - INFO -   Batch 850/2527: Loss = 0.0489
2026-01-07 06:51:08,471 - INFO -   Batch 900/2527: Loss = 0.0715
2026-01-07 06:51:33,468 - INFO -   Batch 950/2527: Loss = 0.0696
2026-01-07 06:51:58,296 - INFO -   Batch 1000/2527: Loss = 0.0754
2026-01-07 06:52:23,133 - INFO -   Batch 1050/2527: Loss = 0.0480
2026-01-07 06:52:47,875 - INFO -   Batch 1100/2527: Loss = 0.0625
2026-01-07 06:53:12,730 - INFO -   Batch 1150/2527: Loss = 0.0447
2026-01-07 06:53:37,843 - INFO -   Batch 1200/2527: Loss = 0.0603
2026-01-07 06:54:02,815 - INFO -   Batch 1250/2527: Loss = 0.0750
2026-01-07 06:54:28,000 - INFO -   Batch 1300/2527: Loss = 0.0949
2026-01-07 06:54:52,832 - INFO -   Batch 1350/2527: Loss = 0.0700
2026-01-07 06:55:17,842 - INFO -   Batch 1400/2527: Loss = 0.0759
2026-01-07 06:55:42,705 - INFO -   Batch 1450/2527: Loss = 0.1092
2026-01-07 06:56:07,537 - INFO -   Batch 1500/2527: Loss = 0.0772
2026-01-07 06:56:32,678 - INFO -   Batch 1550/2527: Loss = 0.0906
2026-01-07 06:56:58,031 - INFO -   Batch 1600/2527: Loss = 0.0733
2026-01-07 06:57:22,847 - INFO -   Batch 1650/2527: Loss = 0.1042
2026-01-07 06:57:47,706 - INFO -   Batch 1700/2527: Loss = 0.0650
2026-01-07 06:58:12,433 - INFO -   Batch 1750/2527: Loss = 0.0607
2026-01-07 06:58:37,273 - INFO -   Batch 1800/2527: Loss = 0.0729
2026-01-07 06:59:02,387 - INFO -   Batch 1850/2527: Loss = 0.0862
2026-01-07 06:59:27,357 - INFO -   Batch 1900/2527: Loss = 0.0746
2026-01-07 06:59:52,207 - INFO -   Batch 1950/2527: Loss = 0.1053
2026-01-07 07:00:17,032 - INFO -   Batch 2000/2527: Loss = 0.0903
2026-01-07 07:00:42,081 - INFO -   Batch 2050/2527: Loss = 0.0566
2026-01-07 07:01:06,948 - INFO -   Batch 2100/2527: Loss = 0.0933
2026-01-07 07:01:31,778 - INFO -   Batch 2150/2527: Loss = 0.0698
2026-01-07 07:01:56,892 - INFO -   Batch 2200/2527: Loss = 0.0478
2026-01-07 07:02:21,857 - INFO -   Batch 2250/2527: Loss = 0.0662
2026-01-07 07:02:46,678 - INFO -   Batch 2300/2527: Loss = 0.0971
2026-01-07 07:03:11,917 - INFO -   Batch 2350/2527: Loss = 0.0473
2026-01-07 07:03:36,676 - INFO -   Batch 2400/2527: Loss = 0.0934
2026-01-07 07:04:01,497 - INFO -   Batch 2450/2527: Loss = 0.0996
2026-01-07 07:04:26,618 - INFO -   Batch 2500/2527: Loss = 0.0590
2026-01-07 07:09:19,519 - INFO - Epoch 74/160: Train Loss: 0.0770, Val Loss: 1.3763, Val mIoU: 40.59%, Val PixelAcc: 79.38%
2026-01-07 07:09:44,589 - INFO -   Batch 50/2527: Loss = 0.0695
2026-01-07 07:10:07,490 - INFO -   Batch 100/2527: Loss = 0.0524
2026-01-07 07:10:30,413 - INFO -   Batch 150/2527: Loss = 0.0898
2026-01-07 07:10:53,350 - INFO -   Batch 200/2527: Loss = 0.0703
2026-01-07 07:11:16,687 - INFO -   Batch 250/2527: Loss = 0.1084
2026-01-07 07:11:39,632 - INFO -   Batch 300/2527: Loss = 0.0943
2026-01-07 07:12:02,606 - INFO -   Batch 350/2527: Loss = 0.1037
2026-01-07 07:12:25,558 - INFO -   Batch 400/2527: Loss = 0.0789
2026-01-07 07:12:48,511 - INFO -   Batch 450/2527: Loss = 0.0726
2026-01-07 07:13:11,470 - INFO -   Batch 500/2527: Loss = 0.0828
2026-01-07 07:13:34,791 - INFO -   Batch 550/2527: Loss = 0.0599
2026-01-07 07:13:57,743 - INFO -   Batch 600/2527: Loss = 0.0654
2026-01-07 07:14:20,693 - INFO -   Batch 650/2527: Loss = 0.0881
2026-01-07 07:14:43,649 - INFO -   Batch 700/2527: Loss = 0.0646
2026-01-07 07:15:06,599 - INFO -   Batch 750/2527: Loss = 0.0651
2026-01-07 07:15:29,547 - INFO -   Batch 800/2527: Loss = 0.0554
2026-01-07 07:15:52,508 - INFO -   Batch 850/2527: Loss = 0.0527
2026-01-07 07:16:15,472 - INFO -   Batch 900/2527: Loss = 0.0362
2026-01-07 07:16:38,424 - INFO -   Batch 950/2527: Loss = 0.0714
2026-01-07 07:17:01,381 - INFO -   Batch 1000/2527: Loss = 0.0599
2026-01-07 07:17:24,335 - INFO -   Batch 1050/2527: Loss = 0.0712
2026-01-07 07:17:47,303 - INFO -   Batch 1100/2527: Loss = 0.0630
2026-01-07 07:18:10,261 - INFO -   Batch 1150/2527: Loss = 0.1034
2026-01-07 07:18:33,213 - INFO -   Batch 1200/2527: Loss = 0.0755
2026-01-07 07:18:56,531 - INFO -   Batch 1250/2527: Loss = 0.0923
2026-01-07 07:19:19,483 - INFO -   Batch 1300/2527: Loss = 0.0645
2026-01-07 07:19:42,438 - INFO -   Batch 1350/2527: Loss = 0.0601
2026-01-07 07:20:05,395 - INFO -   Batch 1400/2527: Loss = 0.0800
2026-01-07 07:20:28,346 - INFO -   Batch 1450/2527: Loss = 0.0662
2026-01-07 07:20:51,306 - INFO -   Batch 1500/2527: Loss = 0.0672
2026-01-07 07:21:14,266 - INFO -   Batch 1550/2527: Loss = 0.0852
2026-01-07 07:21:37,219 - INFO -   Batch 1600/2527: Loss = 0.0509
2026-01-07 07:22:00,166 - INFO -   Batch 1650/2527: Loss = 0.0706
2026-01-07 07:22:23,480 - INFO -   Batch 1700/2527: Loss = 0.0986
2026-01-07 07:22:46,444 - INFO -   Batch 1750/2527: Loss = 0.0788
2026-01-07 07:23:09,391 - INFO -   Batch 1800/2527: Loss = 0.0716
2026-01-07 07:23:32,342 - INFO -   Batch 1850/2527: Loss = 0.0683
2026-01-07 07:23:55,295 - INFO -   Batch 1900/2527: Loss = 0.0857
2026-01-07 07:24:18,254 - INFO -   Batch 1950/2527: Loss = 0.0856
2026-01-07 07:24:41,570 - INFO -   Batch 2000/2527: Loss = 0.0630
2026-01-07 07:25:04,522 - INFO -   Batch 2050/2527: Loss = 0.0520
2026-01-07 07:25:27,475 - INFO -   Batch 2100/2527: Loss = 0.0721
2026-01-07 07:25:50,423 - INFO -   Batch 2150/2527: Loss = 0.0741
2026-01-07 07:26:13,377 - INFO -   Batch 2200/2527: Loss = 0.0751
2026-01-07 07:26:36,349 - INFO -   Batch 2250/2527: Loss = 0.0884
2026-01-07 07:26:59,299 - INFO -   Batch 2300/2527: Loss = 0.0888
2026-01-07 07:27:22,249 - INFO -   Batch 2350/2527: Loss = 0.0536
2026-01-07 07:27:45,199 - INFO -   Batch 2400/2527: Loss = 0.0843
2026-01-07 07:28:08,177 - INFO -   Batch 2450/2527: Loss = 0.0946
2026-01-07 07:28:31,124 - INFO -   Batch 2500/2527: Loss = 0.0844
2026-01-07 07:33:10,580 - INFO - Epoch 75/160: Train Loss: 0.0786, Val Loss: 1.4193, Val mIoU: 40.54%, Val PixelAcc: 79.34%
2026-01-07 07:33:36,086 - INFO -   Batch 50/2527: Loss = 0.1074
2026-01-07 07:33:59,660 - INFO -   Batch 100/2527: Loss = 0.0553
2026-01-07 07:34:23,583 - INFO -   Batch 150/2527: Loss = 0.1735
2026-01-07 07:34:47,529 - INFO -   Batch 200/2527: Loss = 0.0770
2026-01-07 07:35:11,270 - INFO -   Batch 250/2527: Loss = 0.0525
2026-01-07 07:35:34,863 - INFO -   Batch 300/2527: Loss = 0.0408
2026-01-07 07:35:58,671 - INFO -   Batch 350/2527: Loss = 0.0752
2026-01-07 07:36:22,518 - INFO -   Batch 400/2527: Loss = 0.0616
2026-01-07 07:36:46,323 - INFO -   Batch 450/2527: Loss = 0.0778
2026-01-07 07:37:09,935 - INFO -   Batch 500/2527: Loss = 0.0788
2026-01-07 07:37:33,555 - INFO -   Batch 550/2527: Loss = 0.0742
2026-01-07 07:37:57,877 - INFO -   Batch 600/2527: Loss = 0.0786
2026-01-07 07:38:21,445 - INFO -   Batch 650/2527: Loss = 0.1049
2026-01-07 07:38:45,221 - INFO -   Batch 700/2527: Loss = 0.0610
2026-01-07 07:39:08,852 - INFO -   Batch 750/2527: Loss = 0.0538
2026-01-07 07:39:32,664 - INFO -   Batch 800/2527: Loss = 0.0487
2026-01-07 07:39:56,522 - INFO -   Batch 850/2527: Loss = 0.0569
2026-01-07 07:40:20,693 - INFO -   Batch 900/2527: Loss = 0.0445
2026-01-07 07:40:44,314 - INFO -   Batch 950/2527: Loss = 0.1065
2026-01-07 07:41:07,959 - INFO -   Batch 1000/2527: Loss = 0.0490
2026-01-07 07:41:31,899 - INFO -   Batch 1050/2527: Loss = 0.1221
2026-01-07 07:41:55,478 - INFO -   Batch 1100/2527: Loss = 0.0623
2026-01-07 07:42:19,252 - INFO -   Batch 1150/2527: Loss = 0.1095
2026-01-07 07:42:42,854 - INFO -   Batch 1200/2527: Loss = 0.0488
2026-01-07 07:43:06,664 - INFO -   Batch 1250/2527: Loss = 0.0763
2026-01-07 07:43:30,501 - INFO -   Batch 1300/2527: Loss = 0.0892
2026-01-07 07:43:54,276 - INFO -   Batch 1350/2527: Loss = 0.0824
2026-01-07 07:44:17,897 - INFO -   Batch 1400/2527: Loss = 0.0714
2026-01-07 07:44:41,556 - INFO -   Batch 1450/2527: Loss = 0.0839
2026-01-07 07:45:05,521 - INFO -   Batch 1500/2527: Loss = 0.0600
2026-01-07 07:45:29,093 - INFO -   Batch 1550/2527: Loss = 0.0860
2026-01-07 07:45:52,843 - INFO -   Batch 1600/2527: Loss = 0.0750
2026-01-07 07:46:16,827 - INFO -   Batch 1650/2527: Loss = 0.0645
2026-01-07 07:46:40,670 - INFO -   Batch 1700/2527: Loss = 0.0758
2026-01-07 07:47:04,525 - INFO -   Batch 1750/2527: Loss = 0.0317
2026-01-07 07:47:28,316 - INFO -   Batch 1800/2527: Loss = 0.0673
2026-01-07 07:47:51,911 - INFO -   Batch 1850/2527: Loss = 0.0843
2026-01-07 07:48:15,541 - INFO -   Batch 1900/2527: Loss = 0.0764
2026-01-07 07:48:39,508 - INFO -   Batch 1950/2527: Loss = 0.0721
2026-01-07 07:49:03,110 - INFO -   Batch 2000/2527: Loss = 0.0555
2026-01-07 07:49:27,234 - INFO -   Batch 2050/2527: Loss = 0.1815
2026-01-07 07:49:50,835 - INFO -   Batch 2100/2527: Loss = 0.0493
2026-01-07 07:50:14,664 - INFO -   Batch 2150/2527: Loss = 0.0675
2026-01-07 07:50:38,519 - INFO -   Batch 2200/2527: Loss = 0.1163
2026-01-07 07:51:02,337 - INFO -   Batch 2250/2527: Loss = 0.0455
2026-01-07 07:51:25,932 - INFO -   Batch 2300/2527: Loss = 0.0489
2026-01-07 07:51:49,927 - INFO -   Batch 2350/2527: Loss = 0.0739
2026-01-07 07:52:13,878 - INFO -   Batch 2400/2527: Loss = 0.0759
2026-01-07 07:52:37,453 - INFO -   Batch 2450/2527: Loss = 0.0729
2026-01-07 07:53:01,237 - INFO -   Batch 2500/2527: Loss = 0.0774
2026-01-07 07:57:52,974 - INFO - Epoch 76/160: Train Loss: 0.0717, Val Loss: 1.5615, Val mIoU: 38.37%, Val PixelAcc: 77.68%
2026-01-07 07:58:19,071 - INFO -   Batch 50/2527: Loss = 0.4981
2026-01-07 07:58:44,140 - INFO -   Batch 100/2527: Loss = 0.1070
2026-01-07 07:59:09,077 - INFO -   Batch 150/2527: Loss = 0.0648
2026-01-07 07:59:33,913 - INFO -   Batch 200/2527: Loss = 0.0757
2026-01-07 07:59:58,732 - INFO -   Batch 250/2527: Loss = 0.0799
2026-01-07 08:00:23,743 - INFO -   Batch 300/2527: Loss = 0.0768
2026-01-07 08:00:48,567 - INFO -   Batch 350/2527: Loss = 0.1120
2026-01-07 08:01:13,432 - INFO -   Batch 400/2527: Loss = 0.0782
2026-01-07 08:01:38,555 - INFO -   Batch 450/2527: Loss = 0.1313
2026-01-07 08:02:03,537 - INFO -   Batch 500/2527: Loss = 0.1179
2026-01-07 08:02:28,714 - INFO -   Batch 550/2527: Loss = 0.0799
2026-01-07 08:02:53,543 - INFO -   Batch 600/2527: Loss = 0.0850
2026-01-07 08:03:18,262 - INFO -   Batch 650/2527: Loss = 0.0964
2026-01-07 08:03:43,132 - INFO -   Batch 700/2527: Loss = 0.1115
2026-01-07 08:04:08,258 - INFO -   Batch 750/2527: Loss = 0.1229
2026-01-07 08:04:33,233 - INFO -   Batch 800/2527: Loss = 0.0997
2026-01-07 08:04:58,069 - INFO -   Batch 850/2527: Loss = 0.0612
2026-01-07 08:05:22,905 - INFO -   Batch 900/2527: Loss = 0.0786
2026-01-07 08:05:48,287 - INFO -   Batch 950/2527: Loss = 0.1083
2026-01-07 08:06:13,111 - INFO -   Batch 1000/2527: Loss = 0.1091
2026-01-07 08:06:37,939 - INFO -   Batch 1050/2527: Loss = 0.1617
2026-01-07 08:07:03,053 - INFO -   Batch 1100/2527: Loss = 0.0959
2026-01-07 08:07:28,063 - INFO -   Batch 1150/2527: Loss = 0.0832
2026-01-07 08:07:52,885 - INFO -   Batch 1200/2527: Loss = 0.0937
2026-01-07 08:08:18,101 - INFO -   Batch 1250/2527: Loss = 0.0637
2026-01-07 08:08:42,819 - INFO -   Batch 1300/2527: Loss = 0.0810
2026-01-07 08:09:07,682 - INFO -   Batch 1350/2527: Loss = 0.0772
2026-01-07 08:09:32,818 - INFO -   Batch 1400/2527: Loss = 0.0724
2026-01-07 08:09:57,787 - INFO -   Batch 1450/2527: Loss = 0.0836
2026-01-07 08:10:22,607 - INFO -   Batch 1500/2527: Loss = 0.0948
2026-01-07 08:10:47,439 - INFO -   Batch 1550/2527: Loss = 0.0565
2026-01-07 08:11:12,451 - INFO -   Batch 1600/2527: Loss = 0.1019
2026-01-07 08:11:37,318 - INFO -   Batch 1650/2527: Loss = 0.0846
2026-01-07 08:12:02,166 - INFO -   Batch 1700/2527: Loss = 0.0752
2026-01-07 08:12:27,274 - INFO -   Batch 1750/2527: Loss = 0.0675
2026-01-07 08:12:52,239 - INFO -   Batch 1800/2527: Loss = 0.0786
2026-01-07 08:13:17,088 - INFO -   Batch 1850/2527: Loss = 0.0762
2026-01-07 08:13:41,921 - INFO -   Batch 1900/2527: Loss = 0.0940
2026-01-07 08:14:06,676 - INFO -   Batch 1950/2527: Loss = 0.1121
2026-01-07 08:14:31,867 - INFO -   Batch 2000/2527: Loss = 0.0738
2026-01-07 08:14:56,973 - INFO -   Batch 2050/2527: Loss = 0.0551
2026-01-07 08:15:21,944 - INFO -   Batch 2100/2527: Loss = 0.0601
2026-01-07 08:15:46,804 - INFO -   Batch 2150/2527: Loss = 0.0755
2026-01-07 08:16:11,669 - INFO -   Batch 2200/2527: Loss = 0.0888
2026-01-07 08:16:36,692 - INFO -   Batch 2250/2527: Loss = 0.0924
2026-01-07 08:17:01,545 - INFO -   Batch 2300/2527: Loss = 0.0811
2026-01-07 08:17:26,374 - INFO -   Batch 2350/2527: Loss = 0.0869
2026-01-07 08:17:51,855 - INFO -   Batch 2400/2527: Loss = 0.1109
2026-01-07 08:18:16,835 - INFO -   Batch 2450/2527: Loss = 0.1102
2026-01-07 08:18:41,654 - INFO -   Batch 2500/2527: Loss = 0.0678
2026-01-07 08:23:21,646 - INFO - Epoch 77/160: Train Loss: 0.0866, Val Loss: 1.3877, Val mIoU: 40.78%, Val PixelAcc: 79.47%
2026-01-07 08:23:48,002 - INFO -   Batch 50/2527: Loss = 0.0639
2026-01-07 08:24:12,978 - INFO -   Batch 100/2527: Loss = 0.0695
2026-01-07 08:24:37,781 - INFO -   Batch 150/2527: Loss = 0.0994
2026-01-07 08:25:02,974 - INFO -   Batch 200/2527: Loss = 0.0666
2026-01-07 08:25:28,098 - INFO -   Batch 250/2527: Loss = 0.0742
2026-01-07 08:25:53,051 - INFO -   Batch 300/2527: Loss = 0.0736
2026-01-07 08:26:17,873 - INFO -   Batch 350/2527: Loss = 0.0595
2026-01-07 08:26:42,718 - INFO -   Batch 400/2527: Loss = 0.0688
2026-01-07 08:27:07,454 - INFO -   Batch 450/2527: Loss = 0.0574
2026-01-07 08:27:32,297 - INFO -   Batch 500/2527: Loss = 0.0619
2026-01-07 08:27:57,408 - INFO -   Batch 550/2527: Loss = 0.0659
2026-01-07 08:28:22,359 - INFO -   Batch 600/2527: Loss = 0.0688
2026-01-07 08:28:47,172 - INFO -   Batch 650/2527: Loss = 0.0668
2026-01-07 08:29:12,016 - INFO -   Batch 700/2527: Loss = 0.0557
2026-01-07 08:29:37,021 - INFO -   Batch 750/2527: Loss = 0.0666
2026-01-07 08:30:01,872 - INFO -   Batch 800/2527: Loss = 0.0437
2026-01-07 08:30:26,690 - INFO -   Batch 850/2527: Loss = 0.0719
2026-01-07 08:30:52,195 - INFO -   Batch 900/2527: Loss = 0.0604
2026-01-07 08:31:17,194 - INFO -   Batch 950/2527: Loss = 0.0761
2026-01-07 08:31:42,037 - INFO -   Batch 1000/2527: Loss = 0.0828
2026-01-07 08:32:06,868 - INFO -   Batch 1050/2527: Loss = 0.0717
2026-01-07 08:32:31,577 - INFO -   Batch 1100/2527: Loss = 0.0643
2026-01-07 08:32:56,403 - INFO -   Batch 1150/2527: Loss = 0.0613
2026-01-07 08:33:21,553 - INFO -   Batch 1200/2527: Loss = 0.0633
2026-01-07 08:33:46,527 - INFO -   Batch 1250/2527: Loss = 0.0580
2026-01-07 08:34:11,381 - INFO -   Batch 1300/2527: Loss = 0.0737
2026-01-07 08:34:36,590 - INFO -   Batch 1350/2527: Loss = 0.0735
2026-01-07 08:35:01,618 - INFO -   Batch 1400/2527: Loss = 0.0654
2026-01-07 08:35:26,434 - INFO -   Batch 1450/2527: Loss = 0.0625
2026-01-07 08:35:51,287 - INFO -   Batch 1500/2527: Loss = 0.0639
2026-01-07 08:36:16,393 - INFO -   Batch 1550/2527: Loss = 0.0459
2026-01-07 08:36:41,369 - INFO -   Batch 1600/2527: Loss = 0.0599
2026-01-07 08:37:06,563 - INFO -   Batch 1650/2527: Loss = 0.0550
2026-01-07 08:37:31,405 - INFO -   Batch 1700/2527: Loss = 0.0756
2026-01-07 08:37:56,115 - INFO -   Batch 1750/2527: Loss = 0.0541
2026-01-07 08:38:20,951 - INFO -   Batch 1800/2527: Loss = 0.0590
2026-01-07 08:38:46,075 - INFO -   Batch 1850/2527: Loss = 0.0631
2026-01-07 08:39:11,067 - INFO -   Batch 1900/2527: Loss = 0.0750
2026-01-07 08:39:35,880 - INFO -   Batch 1950/2527: Loss = 0.0720
2026-01-07 08:40:00,714 - INFO -   Batch 2000/2527: Loss = 0.0443
2026-01-07 08:40:25,723 - INFO -   Batch 2050/2527: Loss = 0.0715
2026-01-07 08:40:50,549 - INFO -   Batch 2100/2527: Loss = 0.0735
2026-01-07 08:41:15,418 - INFO -   Batch 2150/2527: Loss = 0.0760
2026-01-07 08:41:40,545 - INFO -   Batch 2200/2527: Loss = 0.0484
2026-01-07 08:42:05,542 - INFO -   Batch 2250/2527: Loss = 0.0622
2026-01-07 08:42:30,382 - INFO -   Batch 2300/2527: Loss = 0.0699
2026-01-07 08:42:55,582 - INFO -   Batch 2350/2527: Loss = 0.0517
2026-01-07 08:43:20,325 - INFO -   Batch 2400/2527: Loss = 0.0471
2026-01-07 08:43:45,144 - INFO -   Batch 2450/2527: Loss = 0.0472
2026-01-07 08:44:10,256 - INFO -   Batch 2500/2527: Loss = 0.0645
2026-01-07 08:49:02,795 - INFO - Epoch 78/160: Train Loss: 0.0685, Val Loss: 1.4236, Val mIoU: 40.78%, Val PixelAcc: 79.57%
2026-01-07 08:49:27,716 - INFO -   Batch 50/2527: Loss = 0.1035
2026-01-07 08:49:50,617 - INFO -   Batch 100/2527: Loss = 0.0832
2026-01-07 08:50:13,589 - INFO -   Batch 150/2527: Loss = 0.0774
2026-01-07 08:50:36,525 - INFO -   Batch 200/2527: Loss = 0.0626
2026-01-07 08:50:59,853 - INFO -   Batch 250/2527: Loss = 0.0665
2026-01-07 08:51:22,794 - INFO -   Batch 300/2527: Loss = 0.0410
2026-01-07 08:51:45,738 - INFO -   Batch 350/2527: Loss = 0.0617
2026-01-07 08:52:08,690 - INFO -   Batch 400/2527: Loss = 0.0436
2026-01-07 08:52:31,675 - INFO -   Batch 450/2527: Loss = 0.0436
2026-01-07 08:52:54,629 - INFO -   Batch 500/2527: Loss = 0.0597
2026-01-07 08:53:17,925 - INFO -   Batch 550/2527: Loss = 0.0572
2026-01-07 08:53:40,865 - INFO -   Batch 600/2527: Loss = 0.0616
2026-01-07 08:54:03,806 - INFO -   Batch 650/2527: Loss = 0.0615
2026-01-07 08:54:26,839 - INFO -   Batch 700/2527: Loss = 0.0747
2026-01-07 08:54:49,799 - INFO -   Batch 750/2527: Loss = 0.0830
2026-01-07 08:55:12,741 - INFO -   Batch 800/2527: Loss = 0.0582
2026-01-07 08:55:35,689 - INFO -   Batch 850/2527: Loss = 0.0708
2026-01-07 08:55:58,636 - INFO -   Batch 900/2527: Loss = 0.0405
2026-01-07 08:56:21,574 - INFO -   Batch 950/2527: Loss = 0.0522
2026-01-07 08:56:44,535 - INFO -   Batch 1000/2527: Loss = 0.0562
2026-01-07 08:57:07,478 - INFO -   Batch 1050/2527: Loss = 0.0537
2026-01-07 08:57:30,418 - INFO -   Batch 1100/2527: Loss = 0.0677
2026-01-07 08:57:53,368 - INFO -   Batch 1150/2527: Loss = 0.0817
2026-01-07 08:58:16,339 - INFO -   Batch 1200/2527: Loss = 0.0607
2026-01-07 08:58:39,291 - INFO -   Batch 1250/2527: Loss = 0.0591
2026-01-07 08:59:02,594 - INFO -   Batch 1300/2527: Loss = 0.0721
2026-01-07 08:59:25,544 - INFO -   Batch 1350/2527: Loss = 0.0427
2026-01-07 08:59:48,482 - INFO -   Batch 1400/2527: Loss = 0.0699
2026-01-07 09:00:11,429 - INFO -   Batch 1450/2527: Loss = 0.0546
2026-01-07 09:00:34,399 - INFO -   Batch 1500/2527: Loss = 0.0518
2026-01-07 09:00:57,346 - INFO -   Batch 1550/2527: Loss = 0.0656
2026-01-07 09:01:20,288 - INFO -   Batch 1600/2527: Loss = 0.0444
2026-01-07 09:01:43,217 - INFO -   Batch 1650/2527: Loss = 0.0804
2026-01-07 09:02:06,501 - INFO -   Batch 1700/2527: Loss = 0.0632
2026-01-07 09:02:29,437 - INFO -   Batch 1750/2527: Loss = 0.0749
2026-01-07 09:02:52,384 - INFO -   Batch 1800/2527: Loss = 0.0462
2026-01-07 09:03:15,362 - INFO -   Batch 1850/2527: Loss = 0.0763
2026-01-07 09:03:38,304 - INFO -   Batch 1900/2527: Loss = 0.0596
2026-01-07 09:04:01,243 - INFO -   Batch 1950/2527: Loss = 0.0798
2026-01-07 09:04:24,552 - INFO -   Batch 2000/2527: Loss = 0.0601
2026-01-07 09:04:47,501 - INFO -   Batch 2050/2527: Loss = 0.0753
2026-01-07 09:05:10,452 - INFO -   Batch 2100/2527: Loss = 0.0694
2026-01-07 09:05:33,430 - INFO -   Batch 2150/2527: Loss = 0.0646
2026-01-07 09:05:56,380 - INFO -   Batch 2200/2527: Loss = 0.0463
2026-01-07 09:06:19,316 - INFO -   Batch 2250/2527: Loss = 0.0502
2026-01-07 09:06:42,259 - INFO -   Batch 2300/2527: Loss = 0.0617
2026-01-07 09:07:05,243 - INFO -   Batch 2350/2527: Loss = 0.0553
2026-01-07 09:07:28,192 - INFO -   Batch 2400/2527: Loss = 0.0838
2026-01-07 09:07:51,122 - INFO -   Batch 2450/2527: Loss = 0.0490
2026-01-07 09:08:14,092 - INFO -   Batch 2500/2527: Loss = 0.0788
2026-01-07 09:12:54,196 - INFO - Epoch 79/160: Train Loss: 0.0670, Val Loss: 1.4476, Val mIoU: 40.63%, Val PixelAcc: 79.62%
2026-01-07 09:13:19,537 - INFO -   Batch 50/2527: Loss = 0.0846
2026-01-07 09:13:43,108 - INFO -   Batch 100/2527: Loss = 0.0809
2026-01-07 09:14:07,024 - INFO -   Batch 150/2527: Loss = 0.0681
2026-01-07 09:14:30,937 - INFO -   Batch 200/2527: Loss = 0.0559
2026-01-07 09:14:54,672 - INFO -   Batch 250/2527: Loss = 0.0584
2026-01-07 09:15:18,268 - INFO -   Batch 300/2527: Loss = 0.0434
2026-01-07 09:15:42,116 - INFO -   Batch 350/2527: Loss = 0.0764
2026-01-07 09:16:05,964 - INFO -   Batch 400/2527: Loss = 0.1180
2026-01-07 09:16:29,750 - INFO -   Batch 450/2527: Loss = 0.0577
2026-01-07 09:16:53,350 - INFO -   Batch 500/2527: Loss = 0.0482
2026-01-07 09:17:16,973 - INFO -   Batch 550/2527: Loss = 0.0539
2026-01-07 09:17:41,307 - INFO -   Batch 600/2527: Loss = 0.0520
2026-01-07 09:18:04,870 - INFO -   Batch 650/2527: Loss = 0.0901
2026-01-07 09:18:28,627 - INFO -   Batch 700/2527: Loss = 0.0889
2026-01-07 09:18:52,227 - INFO -   Batch 750/2527: Loss = 0.0565
2026-01-07 09:19:16,038 - INFO -   Batch 800/2527: Loss = 0.0606
2026-01-07 09:19:39,894 - INFO -   Batch 850/2527: Loss = 0.0588
2026-01-07 09:20:04,050 - INFO -   Batch 900/2527: Loss = 0.0471
2026-01-07 09:20:27,646 - INFO -   Batch 950/2527: Loss = 0.0686
2026-01-07 09:20:51,268 - INFO -   Batch 1000/2527: Loss = 0.0708
2026-01-07 09:21:15,235 - INFO -   Batch 1050/2527: Loss = 0.1086
2026-01-07 09:21:38,820 - INFO -   Batch 1100/2527: Loss = 0.0866
2026-01-07 09:22:02,579 - INFO -   Batch 1150/2527: Loss = 0.0778
2026-01-07 09:22:26,184 - INFO -   Batch 1200/2527: Loss = 0.0630
2026-01-07 09:22:50,024 - INFO -   Batch 1250/2527: Loss = 0.0462
2026-01-07 09:23:13,900 - INFO -   Batch 1300/2527: Loss = 0.0524
2026-01-07 09:23:37,686 - INFO -   Batch 1350/2527: Loss = 0.0558
2026-01-07 09:24:01,310 - INFO -   Batch 1400/2527: Loss = 0.0658
2026-01-07 09:24:24,930 - INFO -   Batch 1450/2527: Loss = 0.0799
2026-01-07 09:24:48,880 - INFO -   Batch 1500/2527: Loss = 0.0597
2026-01-07 09:25:12,443 - INFO -   Batch 1550/2527: Loss = 0.0716
2026-01-07 09:25:36,194 - INFO -   Batch 1600/2527: Loss = 0.0529
2026-01-07 09:26:00,176 - INFO -   Batch 1650/2527: Loss = 0.0806
2026-01-07 09:26:23,992 - INFO -   Batch 1700/2527: Loss = 0.0634
2026-01-07 09:26:47,843 - INFO -   Batch 1750/2527: Loss = 0.0725
2026-01-07 09:27:11,621 - INFO -   Batch 1800/2527: Loss = 0.0859
2026-01-07 09:27:35,208 - INFO -   Batch 1850/2527: Loss = 0.0639
2026-01-07 09:27:58,844 - INFO -   Batch 1900/2527: Loss = 0.0912
2026-01-07 09:28:22,803 - INFO -   Batch 1950/2527: Loss = 0.0511
2026-01-07 09:28:46,410 - INFO -   Batch 2000/2527: Loss = 0.0709
2026-01-07 09:29:10,521 - INFO -   Batch 2050/2527: Loss = 0.0600
2026-01-07 09:29:34,118 - INFO -   Batch 2100/2527: Loss = 0.0872
2026-01-07 09:29:57,954 - INFO -   Batch 2150/2527: Loss = 0.0595
2026-01-07 09:30:21,817 - INFO -   Batch 2200/2527: Loss = 0.0722
2026-01-07 09:30:45,629 - INFO -   Batch 2250/2527: Loss = 0.0870
2026-01-07 09:31:09,241 - INFO -   Batch 2300/2527: Loss = 0.0533
2026-01-07 09:31:33,240 - INFO -   Batch 2350/2527: Loss = 0.0757
2026-01-07 09:31:57,203 - INFO -   Batch 2400/2527: Loss = 0.0611
2026-01-07 09:32:20,778 - INFO -   Batch 2450/2527: Loss = 0.0665
2026-01-07 09:32:44,572 - INFO -   Batch 2500/2527: Loss = 0.0712
2026-01-07 09:37:36,519 - INFO - Epoch 80/160: Train Loss: 0.0702, Val Loss: 1.4263, Val mIoU: 40.80%, Val PixelAcc: 79.67%
2026-01-07 09:37:38,737 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_80.pth
2026-01-07 09:37:38,740 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_80.pth
2026-01-07 09:38:04,650 - INFO -   Batch 50/2527: Loss = 0.0782
2026-01-07 09:38:29,718 - INFO -   Batch 100/2527: Loss = 0.0611
2026-01-07 09:38:54,655 - INFO -   Batch 150/2527: Loss = 0.0469
2026-01-07 09:39:19,488 - INFO -   Batch 200/2527: Loss = 0.0675
2026-01-07 09:39:44,315 - INFO -   Batch 250/2527: Loss = 0.0746
2026-01-07 09:40:09,316 - INFO -   Batch 300/2527: Loss = 0.0746
2026-01-07 09:40:34,173 - INFO -   Batch 350/2527: Loss = 0.0631
2026-01-07 09:40:59,004 - INFO -   Batch 400/2527: Loss = 0.0538
2026-01-07 09:41:24,130 - INFO -   Batch 450/2527: Loss = 0.0678
2026-01-07 09:41:49,110 - INFO -   Batch 500/2527: Loss = 0.0465
2026-01-07 09:42:14,295 - INFO -   Batch 550/2527: Loss = 0.0696
2026-01-07 09:42:39,125 - INFO -   Batch 600/2527: Loss = 0.0755
2026-01-07 09:43:03,867 - INFO -   Batch 650/2527: Loss = 0.0555
2026-01-07 09:43:28,725 - INFO -   Batch 700/2527: Loss = 0.0576
2026-01-07 09:43:53,813 - INFO -   Batch 750/2527: Loss = 0.0567
2026-01-07 09:44:18,783 - INFO -   Batch 800/2527: Loss = 0.0521
2026-01-07 09:44:43,625 - INFO -   Batch 850/2527: Loss = 0.0712
2026-01-07 09:45:08,470 - INFO -   Batch 900/2527: Loss = 0.0733
2026-01-07 09:45:33,464 - INFO -   Batch 950/2527: Loss = 0.1035
2026-01-07 09:45:58,636 - INFO -   Batch 1000/2527: Loss = 0.0671
2026-01-07 09:46:23,485 - INFO -   Batch 1050/2527: Loss = 0.0714
2026-01-07 09:46:48,597 - INFO -   Batch 1100/2527: Loss = 0.0891
2026-01-07 09:47:13,575 - INFO -   Batch 1150/2527: Loss = 0.0633
2026-01-07 09:47:38,399 - INFO -   Batch 1200/2527: Loss = 0.0600
2026-01-07 09:48:03,249 - INFO -   Batch 1250/2527: Loss = 0.0690
2026-01-07 09:48:28,329 - INFO -   Batch 1300/2527: Loss = 0.0938
2026-01-07 09:48:53,169 - INFO -   Batch 1350/2527: Loss = 0.0829
2026-01-07 09:49:18,295 - INFO -   Batch 1400/2527: Loss = 0.0654
2026-01-07 09:49:43,279 - INFO -   Batch 1450/2527: Loss = 0.0914
2026-01-07 09:50:08,103 - INFO -   Batch 1500/2527: Loss = 0.0553
2026-01-07 09:50:32,965 - INFO -   Batch 1550/2527: Loss = 0.0822
2026-01-07 09:50:57,983 - INFO -   Batch 1600/2527: Loss = 0.0750
2026-01-07 09:51:22,796 - INFO -   Batch 1650/2527: Loss = 0.0665
2026-01-07 09:51:47,624 - INFO -   Batch 1700/2527: Loss = 0.1038
2026-01-07 09:52:12,742 - INFO -   Batch 1750/2527: Loss = 0.0452
2026-01-07 09:52:37,723 - INFO -   Batch 1800/2527: Loss = 0.0580
2026-01-07 09:53:02,536 - INFO -   Batch 1850/2527: Loss = 0.0847
2026-01-07 09:53:27,374 - INFO -   Batch 1900/2527: Loss = 0.0832
2026-01-07 09:53:52,095 - INFO -   Batch 1950/2527: Loss = 0.0926
2026-01-07 09:54:17,296 - INFO -   Batch 2000/2527: Loss = 0.0610
2026-01-07 09:54:42,407 - INFO -   Batch 2050/2527: Loss = 0.0685
2026-01-07 09:55:07,375 - INFO -   Batch 2100/2527: Loss = 0.0644
2026-01-07 09:55:32,194 - INFO -   Batch 2150/2527: Loss = 0.0635
2026-01-07 09:55:57,029 - INFO -   Batch 2200/2527: Loss = 0.0688
2026-01-07 09:56:22,060 - INFO -   Batch 2250/2527: Loss = 0.0843
2026-01-07 09:56:46,880 - INFO -   Batch 2300/2527: Loss = 0.0690
2026-01-07 09:57:11,712 - INFO -   Batch 2350/2527: Loss = 0.0768
2026-01-07 09:57:36,828 - INFO -   Batch 2400/2527: Loss = 0.0744
2026-01-07 09:58:02,160 - INFO -   Batch 2450/2527: Loss = 0.0628
2026-01-07 09:58:26,983 - INFO -   Batch 2500/2527: Loss = 0.0680
2026-01-07 10:03:07,319 - INFO - Epoch 81/160: Train Loss: 0.0705, Val Loss: 1.4432, Val mIoU: 40.80%, Val PixelAcc: 79.61%
2026-01-07 10:03:33,623 - INFO -   Batch 50/2527: Loss = 0.0621
2026-01-07 10:03:58,584 - INFO -   Batch 100/2527: Loss = 0.0869
2026-01-07 10:04:23,370 - INFO -   Batch 150/2527: Loss = 0.1014
2026-01-07 10:04:48,548 - INFO -   Batch 200/2527: Loss = 0.0671
2026-01-07 10:05:13,647 - INFO -   Batch 250/2527: Loss = 0.0679
2026-01-07 10:05:38,606 - INFO -   Batch 300/2527: Loss = 0.0428
2026-01-07 10:06:03,452 - INFO -   Batch 350/2527: Loss = 0.0630
2026-01-07 10:06:28,313 - INFO -   Batch 400/2527: Loss = 0.0657
2026-01-07 10:06:53,035 - INFO -   Batch 450/2527: Loss = 0.0643
2026-01-07 10:07:17,871 - INFO -   Batch 500/2527: Loss = 0.0733
2026-01-07 10:07:43,007 - INFO -   Batch 550/2527: Loss = 0.0399
2026-01-07 10:08:07,979 - INFO -   Batch 600/2527: Loss = 0.0398
2026-01-07 10:08:32,830 - INFO -   Batch 650/2527: Loss = 0.0816
2026-01-07 10:08:57,666 - INFO -   Batch 700/2527: Loss = 0.0659
2026-01-07 10:09:22,669 - INFO -   Batch 750/2527: Loss = 0.0956
2026-01-07 10:09:47,523 - INFO -   Batch 800/2527: Loss = 0.0557
2026-01-07 10:10:12,380 - INFO -   Batch 850/2527: Loss = 0.0597
2026-01-07 10:10:37,491 - INFO -   Batch 900/2527: Loss = 0.0661
2026-01-07 10:11:02,815 - INFO -   Batch 950/2527: Loss = 0.0680
2026-01-07 10:11:27,667 - INFO -   Batch 1000/2527: Loss = 0.0466
2026-01-07 10:11:52,508 - INFO -   Batch 1050/2527: Loss = 0.0596
2026-01-07 10:12:17,221 - INFO -   Batch 1100/2527: Loss = 0.0597
2026-01-07 10:12:42,092 - INFO -   Batch 1150/2527: Loss = 0.0857
2026-01-07 10:13:07,215 - INFO -   Batch 1200/2527: Loss = 0.0677
2026-01-07 10:13:32,202 - INFO -   Batch 1250/2527: Loss = 0.0286
2026-01-07 10:13:57,025 - INFO -   Batch 1300/2527: Loss = 0.0681
2026-01-07 10:14:22,262 - INFO -   Batch 1350/2527: Loss = 0.0530
2026-01-07 10:14:47,271 - INFO -   Batch 1400/2527: Loss = 0.0577
2026-01-07 10:15:12,114 - INFO -   Batch 1450/2527: Loss = 0.0595
2026-01-07 10:15:36,965 - INFO -   Batch 1500/2527: Loss = 0.0805
2026-01-07 10:16:02,085 - INFO -   Batch 1550/2527: Loss = 0.0773
2026-01-07 10:16:27,070 - INFO -   Batch 1600/2527: Loss = 0.0890
2026-01-07 10:16:52,261 - INFO -   Batch 1650/2527: Loss = 0.0517
2026-01-07 10:17:17,234 - INFO -   Batch 1700/2527: Loss = 0.0919
2026-01-07 10:17:41,975 - INFO -   Batch 1750/2527: Loss = 0.0618
2026-01-07 10:18:06,833 - INFO -   Batch 1800/2527: Loss = 0.0864
2026-01-07 10:18:31,972 - INFO -   Batch 1850/2527: Loss = 0.0684
2026-01-07 10:18:56,937 - INFO -   Batch 1900/2527: Loss = 0.1125
2026-01-07 10:19:21,761 - INFO -   Batch 1950/2527: Loss = 0.0744
2026-01-07 10:19:46,600 - INFO -   Batch 2000/2527: Loss = 0.0337
2026-01-07 10:20:11,606 - INFO -   Batch 2050/2527: Loss = 0.0993
2026-01-07 10:20:36,437 - INFO -   Batch 2100/2527: Loss = 0.0746
2026-01-07 10:21:01,249 - INFO -   Batch 2150/2527: Loss = 0.0642
2026-01-07 10:21:26,356 - INFO -   Batch 2200/2527: Loss = 0.0570
2026-01-07 10:21:51,346 - INFO -   Batch 2250/2527: Loss = 0.0679
2026-01-07 10:22:16,187 - INFO -   Batch 2300/2527: Loss = 0.0732
2026-01-07 10:22:41,013 - INFO -   Batch 2350/2527: Loss = 0.0694
2026-01-07 10:23:06,111 - INFO -   Batch 2400/2527: Loss = 0.0726
2026-01-07 10:23:30,971 - INFO -   Batch 2450/2527: Loss = 0.0484
2026-01-07 10:23:56,085 - INFO -   Batch 2500/2527: Loss = 0.0654
2026-01-07 10:28:48,922 - INFO - Epoch 82/160: Train Loss: 0.0661, Val Loss: 1.4801, Val mIoU: 40.10%, Val PixelAcc: 79.45%
2026-01-07 10:29:13,696 - INFO -   Batch 50/2527: Loss = 0.0783
2026-01-07 10:29:36,605 - INFO -   Batch 100/2527: Loss = 0.0819
2026-01-07 10:29:59,531 - INFO -   Batch 150/2527: Loss = 0.0690
2026-01-07 10:30:22,470 - INFO -   Batch 200/2527: Loss = 0.0623
2026-01-07 10:30:45,805 - INFO -   Batch 250/2527: Loss = 0.0363
2026-01-07 10:31:08,754 - INFO -   Batch 300/2527: Loss = 0.0799
2026-01-07 10:31:31,713 - INFO -   Batch 350/2527: Loss = 0.0639
2026-01-07 10:31:54,663 - INFO -   Batch 400/2527: Loss = 0.0772
2026-01-07 10:32:17,622 - INFO -   Batch 450/2527: Loss = 0.0416
2026-01-07 10:32:40,574 - INFO -   Batch 500/2527: Loss = 0.0441
2026-01-07 10:33:03,905 - INFO -   Batch 550/2527: Loss = 0.0532
2026-01-07 10:33:26,860 - INFO -   Batch 600/2527: Loss = 0.0464
2026-01-07 10:33:49,822 - INFO -   Batch 650/2527: Loss = 0.0408
2026-01-07 10:34:12,789 - INFO -   Batch 700/2527: Loss = 0.0825
2026-01-07 10:34:35,746 - INFO -   Batch 750/2527: Loss = 0.0603
2026-01-07 10:34:58,710 - INFO -   Batch 800/2527: Loss = 0.1531
2026-01-07 10:35:21,668 - INFO -   Batch 850/2527: Loss = 0.0769
2026-01-07 10:35:44,624 - INFO -   Batch 900/2527: Loss = 0.0962
2026-01-07 10:36:07,572 - INFO -   Batch 950/2527: Loss = 0.0539
2026-01-07 10:36:30,530 - INFO -   Batch 1000/2527: Loss = 0.0576
2026-01-07 10:36:53,490 - INFO -   Batch 1050/2527: Loss = 0.0643
2026-01-07 10:37:16,459 - INFO -   Batch 1100/2527: Loss = 0.0484
2026-01-07 10:37:39,413 - INFO -   Batch 1150/2527: Loss = 0.0733
2026-01-07 10:38:02,370 - INFO -   Batch 1200/2527: Loss = 0.0672
2026-01-07 10:38:25,330 - INFO -   Batch 1250/2527: Loss = 0.0531
2026-01-07 10:38:48,650 - INFO -   Batch 1300/2527: Loss = 0.0640
2026-01-07 10:39:11,594 - INFO -   Batch 1350/2527: Loss = 0.0511
2026-01-07 10:39:34,530 - INFO -   Batch 1400/2527: Loss = 0.0953
2026-01-07 10:39:57,475 - INFO -   Batch 1450/2527: Loss = 0.0630
2026-01-07 10:40:20,423 - INFO -   Batch 1500/2527: Loss = 0.0487
2026-01-07 10:40:43,386 - INFO -   Batch 1550/2527: Loss = 0.0662
2026-01-07 10:41:06,339 - INFO -   Batch 1600/2527: Loss = 0.0769
2026-01-07 10:41:29,287 - INFO -   Batch 1650/2527: Loss = 0.0569
2026-01-07 10:41:52,610 - INFO -   Batch 1700/2527: Loss = 0.0785
2026-01-07 10:42:15,559 - INFO -   Batch 1750/2527: Loss = 0.0912
2026-01-07 10:42:38,515 - INFO -   Batch 1800/2527: Loss = 0.0552
2026-01-07 10:43:01,466 - INFO -   Batch 1850/2527: Loss = 0.0663
2026-01-07 10:43:24,417 - INFO -   Batch 1900/2527: Loss = 0.0516
2026-01-07 10:43:47,378 - INFO -   Batch 1950/2527: Loss = 0.0895
2026-01-07 10:44:10,692 - INFO -   Batch 2000/2527: Loss = 0.0415
2026-01-07 10:44:33,637 - INFO -   Batch 2050/2527: Loss = 0.0624
2026-01-07 10:44:56,585 - INFO -   Batch 2100/2527: Loss = 0.0619
2026-01-07 10:45:19,538 - INFO -   Batch 2150/2527: Loss = 0.0537
2026-01-07 10:45:42,640 - INFO -   Batch 2200/2527: Loss = 0.0813
2026-01-07 10:46:05,592 - INFO -   Batch 2250/2527: Loss = 0.0544
2026-01-07 10:46:28,537 - INFO -   Batch 2300/2527: Loss = 0.0768
2026-01-07 10:46:51,493 - INFO -   Batch 2350/2527: Loss = 0.0683
2026-01-07 10:47:14,504 - INFO -   Batch 2400/2527: Loss = 0.0502
2026-01-07 10:47:37,450 - INFO -   Batch 2450/2527: Loss = 0.0615
2026-01-07 10:48:00,398 - INFO -   Batch 2500/2527: Loss = 0.0848
2026-01-07 10:52:40,610 - INFO - Epoch 83/160: Train Loss: 0.0660, Val Loss: 1.4906, Val mIoU: 40.86%, Val PixelAcc: 79.64%
2026-01-07 10:53:05,946 - INFO -   Batch 50/2527: Loss = 0.0482
2026-01-07 10:53:29,511 - INFO -   Batch 100/2527: Loss = 0.0651
2026-01-07 10:53:53,433 - INFO -   Batch 150/2527: Loss = 0.0731
2026-01-07 10:54:16,992 - INFO -   Batch 200/2527: Loss = 0.0647
2026-01-07 10:54:41,100 - INFO -   Batch 250/2527: Loss = 0.0558
2026-01-07 10:55:04,721 - INFO -   Batch 300/2527: Loss = 0.0446
2026-01-07 10:55:28,549 - INFO -   Batch 350/2527: Loss = 0.0545
2026-01-07 10:55:52,388 - INFO -   Batch 400/2527: Loss = 0.0747
2026-01-07 10:56:16,160 - INFO -   Batch 450/2527: Loss = 0.0657
2026-01-07 10:56:39,774 - INFO -   Batch 500/2527: Loss = 0.0606
2026-01-07 10:57:03,810 - INFO -   Batch 550/2527: Loss = 0.0592
2026-01-07 10:57:27,777 - INFO -   Batch 600/2527: Loss = 0.0555
2026-01-07 10:57:51,709 - INFO -   Batch 650/2527: Loss = 0.0639
2026-01-07 10:58:15,458 - INFO -   Batch 700/2527: Loss = 0.0473
2026-01-07 10:58:39,060 - INFO -   Batch 750/2527: Loss = 0.0642
2026-01-07 10:59:02,903 - INFO -   Batch 800/2527: Loss = 0.0839
2026-01-07 10:59:26,770 - INFO -   Batch 850/2527: Loss = 0.0732
2026-01-07 10:59:50,583 - INFO -   Batch 900/2527: Loss = 0.0709
2026-01-07 11:00:14,546 - INFO -   Batch 950/2527: Loss = 0.0550
2026-01-07 11:00:38,144 - INFO -   Batch 1000/2527: Loss = 0.0879
2026-01-07 11:01:02,598 - INFO -   Batch 1050/2527: Loss = 0.0852
2026-01-07 11:01:26,192 - INFO -   Batch 1100/2527: Loss = 0.0621
2026-01-07 11:01:49,935 - INFO -   Batch 1150/2527: Loss = 0.0672
2026-01-07 11:02:13,549 - INFO -   Batch 1200/2527: Loss = 0.0545
2026-01-07 11:02:37,356 - INFO -   Batch 1250/2527: Loss = 0.0543
2026-01-07 11:03:01,232 - INFO -   Batch 1300/2527: Loss = 0.0718
2026-01-07 11:03:25,006 - INFO -   Batch 1350/2527: Loss = 0.0869
2026-01-07 11:03:48,595 - INFO -   Batch 1400/2527: Loss = 0.0569
2026-01-07 11:04:12,213 - INFO -   Batch 1450/2527: Loss = 0.0797
2026-01-07 11:04:36,184 - INFO -   Batch 1500/2527: Loss = 0.0657
2026-01-07 11:04:59,765 - INFO -   Batch 1550/2527: Loss = 0.0753
2026-01-07 11:05:23,986 - INFO -   Batch 1600/2527: Loss = 0.0804
2026-01-07 11:05:47,960 - INFO -   Batch 1650/2527: Loss = 0.0548
2026-01-07 11:06:11,764 - INFO -   Batch 1700/2527: Loss = 0.0842
2026-01-07 11:06:35,620 - INFO -   Batch 1750/2527: Loss = 0.0551
2026-01-07 11:06:59,411 - INFO -   Batch 1800/2527: Loss = 0.0469
2026-01-07 11:07:23,012 - INFO -   Batch 1850/2527: Loss = 0.0734
2026-01-07 11:07:46,630 - INFO -   Batch 1900/2527: Loss = 0.0647
2026-01-07 11:08:10,613 - INFO -   Batch 1950/2527: Loss = 0.0413
2026-01-07 11:08:34,211 - INFO -   Batch 2000/2527: Loss = 0.0893
2026-01-07 11:08:57,962 - INFO -   Batch 2050/2527: Loss = 0.0641
2026-01-07 11:09:21,932 - INFO -   Batch 2100/2527: Loss = 0.0567
2026-01-07 11:09:45,737 - INFO -   Batch 2150/2527: Loss = 0.0628
2026-01-07 11:10:09,607 - INFO -   Batch 2200/2527: Loss = 0.0548
2026-01-07 11:10:33,406 - INFO -   Batch 2250/2527: Loss = 0.0642
2026-01-07 11:10:57,000 - INFO -   Batch 2300/2527: Loss = 0.0426
2026-01-07 11:11:20,670 - INFO -   Batch 2350/2527: Loss = 0.0717
2026-01-07 11:11:44,997 - INFO -   Batch 2400/2527: Loss = 0.0408
2026-01-07 11:12:08,598 - INFO -   Batch 2450/2527: Loss = 0.0784
2026-01-07 11:12:32,365 - INFO -   Batch 2500/2527: Loss = 0.0840
2026-01-07 11:17:23,757 - INFO - Epoch 84/160: Train Loss: 0.0653, Val Loss: 1.4795, Val mIoU: 40.72%, Val PixelAcc: 79.70%
2026-01-07 11:17:50,015 - INFO -   Batch 50/2527: Loss = 0.0595
2026-01-07 11:18:15,092 - INFO -   Batch 100/2527: Loss = 0.0704
2026-01-07 11:18:40,020 - INFO -   Batch 150/2527: Loss = 0.0725
2026-01-07 11:19:04,857 - INFO -   Batch 200/2527: Loss = 0.0488
2026-01-07 11:19:29,773 - INFO -   Batch 250/2527: Loss = 0.0687
2026-01-07 11:19:54,771 - INFO -   Batch 300/2527: Loss = 0.0470
2026-01-07 11:20:19,576 - INFO -   Batch 350/2527: Loss = 0.0503
2026-01-07 11:20:44,380 - INFO -   Batch 400/2527: Loss = 0.0678
2026-01-07 11:21:09,477 - INFO -   Batch 450/2527: Loss = 0.0493
2026-01-07 11:21:34,472 - INFO -   Batch 500/2527: Loss = 0.0525
2026-01-07 11:21:59,332 - INFO -   Batch 550/2527: Loss = 0.0788
2026-01-07 11:22:24,520 - INFO -   Batch 600/2527: Loss = 0.0917
2026-01-07 11:22:49,225 - INFO -   Batch 650/2527: Loss = 0.0781
2026-01-07 11:23:14,056 - INFO -   Batch 700/2527: Loss = 0.0401
2026-01-07 11:23:39,205 - INFO -   Batch 750/2527: Loss = 0.0613
2026-01-07 11:24:04,163 - INFO -   Batch 800/2527: Loss = 0.0441
2026-01-07 11:24:29,000 - INFO -   Batch 850/2527: Loss = 0.0730
2026-01-07 11:24:53,831 - INFO -   Batch 900/2527: Loss = 0.0591
2026-01-07 11:25:18,833 - INFO -   Batch 950/2527: Loss = 0.0670
2026-01-07 11:25:44,102 - INFO -   Batch 1000/2527: Loss = 0.0534
2026-01-07 11:26:08,973 - INFO -   Batch 1050/2527: Loss = 0.0711
2026-01-07 11:26:34,084 - INFO -   Batch 1100/2527: Loss = 0.0478
2026-01-07 11:26:59,067 - INFO -   Batch 1150/2527: Loss = 0.0647
2026-01-07 11:27:23,889 - INFO -   Batch 1200/2527: Loss = 0.0537
2026-01-07 11:27:48,743 - INFO -   Batch 1250/2527: Loss = 0.0691
2026-01-07 11:28:13,881 - INFO -   Batch 1300/2527: Loss = 0.0792
2026-01-07 11:28:38,701 - INFO -   Batch 1350/2527: Loss = 0.0481
2026-01-07 11:29:03,804 - INFO -   Batch 1400/2527: Loss = 0.0499
2026-01-07 11:29:28,757 - INFO -   Batch 1450/2527: Loss = 0.0644
2026-01-07 11:29:53,561 - INFO -   Batch 1500/2527: Loss = 0.0835
2026-01-07 11:30:18,417 - INFO -   Batch 1550/2527: Loss = 0.0942
2026-01-07 11:30:43,433 - INFO -   Batch 1600/2527: Loss = 0.0447
2026-01-07 11:31:08,237 - INFO -   Batch 1650/2527: Loss = 0.0521
2026-01-07 11:31:33,088 - INFO -   Batch 1700/2527: Loss = 0.0760
2026-01-07 11:31:58,183 - INFO -   Batch 1750/2527: Loss = 0.0584
2026-01-07 11:32:23,143 - INFO -   Batch 1800/2527: Loss = 0.0656
2026-01-07 11:32:47,957 - INFO -   Batch 1850/2527: Loss = 0.0817
2026-01-07 11:33:12,776 - INFO -   Batch 1900/2527: Loss = 0.0898
2026-01-07 11:33:37,785 - INFO -   Batch 1950/2527: Loss = 0.0614
2026-01-07 11:34:02,649 - INFO -   Batch 2000/2527: Loss = 0.0510
2026-01-07 11:34:28,112 - INFO -   Batch 2050/2527: Loss = 0.0600
2026-01-07 11:34:53,072 - INFO -   Batch 2100/2527: Loss = 0.1065
2026-01-07 11:35:17,917 - INFO -   Batch 2150/2527: Loss = 0.0623
2026-01-07 11:35:42,725 - INFO -   Batch 2200/2527: Loss = 0.0655
2026-01-07 11:36:07,723 - INFO -   Batch 2250/2527: Loss = 0.0558
2026-01-07 11:36:32,531 - INFO -   Batch 2300/2527: Loss = 0.0830
2026-01-07 11:36:57,355 - INFO -   Batch 2350/2527: Loss = 0.0570
2026-01-07 11:37:22,468 - INFO -   Batch 2400/2527: Loss = 0.0682
2026-01-07 11:37:47,830 - INFO -   Batch 2450/2527: Loss = 0.0448
2026-01-07 11:38:12,670 - INFO -   Batch 2500/2527: Loss = 0.0713
2026-01-07 11:42:52,262 - INFO - Epoch 85/160: Train Loss: 0.0643, Val Loss: 1.4830, Val mIoU: 40.63%, Val PixelAcc: 79.72%
2026-01-07 11:43:18,613 - INFO -   Batch 50/2527: Loss = 0.0694
2026-01-07 11:43:43,564 - INFO -   Batch 100/2527: Loss = 0.0780
2026-01-07 11:44:08,383 - INFO -   Batch 150/2527: Loss = 0.0616
2026-01-07 11:44:33,603 - INFO -   Batch 200/2527: Loss = 0.0827
2026-01-07 11:44:58,711 - INFO -   Batch 250/2527: Loss = 0.0637
2026-01-07 11:45:23,725 - INFO -   Batch 300/2527: Loss = 0.0457
2026-01-07 11:45:48,530 - INFO -   Batch 350/2527: Loss = 0.0856
2026-01-07 11:46:13,430 - INFO -   Batch 400/2527: Loss = 0.0694
2026-01-07 11:46:38,117 - INFO -   Batch 450/2527: Loss = 0.0496
2026-01-07 11:47:02,945 - INFO -   Batch 500/2527: Loss = 0.0527
2026-01-07 11:47:28,237 - INFO -   Batch 550/2527: Loss = 0.0587
2026-01-07 11:47:53,189 - INFO -   Batch 600/2527: Loss = 0.0649
2026-01-07 11:48:18,264 - INFO -   Batch 650/2527: Loss = 0.0410
2026-01-07 11:48:43,115 - INFO -   Batch 700/2527: Loss = 0.0731
2026-01-07 11:49:08,206 - INFO -   Batch 750/2527: Loss = 0.0658
2026-01-07 11:49:33,074 - INFO -   Batch 800/2527: Loss = 0.0433
2026-01-07 11:49:57,937 - INFO -   Batch 850/2527: Loss = 0.0568
2026-01-07 11:50:23,051 - INFO -   Batch 900/2527: Loss = 0.0644
2026-01-07 11:50:48,355 - INFO -   Batch 950/2527: Loss = 0.0721
2026-01-07 11:51:13,166 - INFO -   Batch 1000/2527: Loss = 0.0767
2026-01-07 11:51:37,988 - INFO -   Batch 1050/2527: Loss = 0.0604
2026-01-07 11:52:02,704 - INFO -   Batch 1100/2527: Loss = 0.0419
2026-01-07 11:52:27,558 - INFO -   Batch 1150/2527: Loss = 0.0619
2026-01-07 11:52:52,694 - INFO -   Batch 1200/2527: Loss = 0.0736
2026-01-07 11:53:17,671 - INFO -   Batch 1250/2527: Loss = 0.0601
2026-01-07 11:53:42,481 - INFO -   Batch 1300/2527: Loss = 0.0688
2026-01-07 11:54:07,671 - INFO -   Batch 1350/2527: Loss = 0.0431
2026-01-07 11:54:32,709 - INFO -   Batch 1400/2527: Loss = 0.0596
2026-01-07 11:54:57,564 - INFO -   Batch 1450/2527: Loss = 0.0784
2026-01-07 11:55:22,386 - INFO -   Batch 1500/2527: Loss = 0.0472
2026-01-07 11:55:47,489 - INFO -   Batch 1550/2527: Loss = 0.0462
2026-01-07 11:56:12,480 - INFO -   Batch 1600/2527: Loss = 0.0498
2026-01-07 11:56:37,714 - INFO -   Batch 1650/2527: Loss = 0.0740
2026-01-07 11:57:02,543 - INFO -   Batch 1700/2527: Loss = 0.0716
2026-01-07 11:57:27,257 - INFO -   Batch 1750/2527: Loss = 0.0639
2026-01-07 11:57:52,090 - INFO -   Batch 1800/2527: Loss = 0.0494
2026-01-07 11:58:17,341 - INFO -   Batch 1850/2527: Loss = 0.0637
2026-01-07 11:58:42,369 - INFO -   Batch 1900/2527: Loss = 0.0538
2026-01-07 11:59:07,282 - INFO -   Batch 1950/2527: Loss = 0.0679
2026-01-07 11:59:32,130 - INFO -   Batch 2000/2527: Loss = 0.0518
2026-01-07 11:59:57,157 - INFO -   Batch 2050/2527: Loss = 0.0641
2026-01-07 12:00:21,966 - INFO -   Batch 2100/2527: Loss = 0.0449
2026-01-07 12:00:46,788 - INFO -   Batch 2150/2527: Loss = 0.0894
2026-01-07 12:01:11,954 - INFO -   Batch 2200/2527: Loss = 0.0353
2026-01-07 12:01:36,947 - INFO -   Batch 2250/2527: Loss = 0.0477
2026-01-07 12:02:01,757 - INFO -   Batch 2300/2527: Loss = 0.0638
2026-01-07 12:02:26,593 - INFO -   Batch 2350/2527: Loss = 0.0755
2026-01-07 12:02:51,681 - INFO -   Batch 2400/2527: Loss = 0.0591
2026-01-07 12:03:16,528 - INFO -   Batch 2450/2527: Loss = 0.0607
2026-01-07 12:03:41,695 - INFO -   Batch 2500/2527: Loss = 0.0506
2026-01-07 12:08:34,283 - INFO - Epoch 86/160: Train Loss: 0.0637, Val Loss: 1.4926, Val mIoU: 40.58%, Val PixelAcc: 79.72%
2026-01-07 12:09:00,004 - INFO -   Batch 50/2527: Loss = 0.0821
2026-01-07 12:09:23,198 - INFO -   Batch 100/2527: Loss = 0.0691
2026-01-07 12:09:46,130 - INFO -   Batch 150/2527: Loss = 0.0531
2026-01-07 12:10:09,079 - INFO -   Batch 200/2527: Loss = 0.0378
2026-01-07 12:10:32,029 - INFO -   Batch 250/2527: Loss = 0.0597
2026-01-07 12:10:55,350 - INFO -   Batch 300/2527: Loss = 0.0463
2026-01-07 12:11:18,235 - INFO -   Batch 350/2527: Loss = 0.0606
2026-01-07 12:11:41,181 - INFO -   Batch 400/2527: Loss = 0.0648
2026-01-07 12:12:04,157 - INFO -   Batch 450/2527: Loss = 0.0500
2026-01-07 12:12:27,112 - INFO -   Batch 500/2527: Loss = 0.0486
2026-01-07 12:12:50,142 - INFO -   Batch 550/2527: Loss = 0.0618
2026-01-07 12:13:13,480 - INFO -   Batch 600/2527: Loss = 0.0676
2026-01-07 12:13:36,431 - INFO -   Batch 650/2527: Loss = 0.0563
2026-01-07 12:13:59,398 - INFO -   Batch 700/2527: Loss = 0.0784
2026-01-07 12:14:22,335 - INFO -   Batch 750/2527: Loss = 0.0746
2026-01-07 12:14:45,329 - INFO -   Batch 800/2527: Loss = 0.0668
2026-01-07 12:15:08,323 - INFO -   Batch 850/2527: Loss = 0.0608
2026-01-07 12:15:31,275 - INFO -   Batch 900/2527: Loss = 0.0840
2026-01-07 12:15:54,235 - INFO -   Batch 950/2527: Loss = 0.0701
2026-01-07 12:16:17,291 - INFO -   Batch 1000/2527: Loss = 0.0537
2026-01-07 12:16:40,278 - INFO -   Batch 1050/2527: Loss = 0.0581
2026-01-07 12:17:03,227 - INFO -   Batch 1100/2527: Loss = 0.0517
2026-01-07 12:17:26,182 - INFO -   Batch 1150/2527: Loss = 0.0495
2026-01-07 12:17:49,699 - INFO -   Batch 1200/2527: Loss = 0.0674
2026-01-07 12:18:12,737 - INFO -   Batch 1250/2527: Loss = 0.0546
2026-01-07 12:18:36,053 - INFO -   Batch 1300/2527: Loss = 0.0641
2026-01-07 12:18:59,528 - INFO -   Batch 1350/2527: Loss = 0.0572
2026-01-07 12:19:22,526 - INFO -   Batch 1400/2527: Loss = 0.0532
2026-01-07 12:19:45,486 - INFO -   Batch 1450/2527: Loss = 0.0678
2026-01-07 12:20:08,435 - INFO -   Batch 1500/2527: Loss = 0.0691
2026-01-07 12:20:31,392 - INFO -   Batch 1550/2527: Loss = 0.0705
2026-01-07 12:20:54,352 - INFO -   Batch 1600/2527: Loss = 0.0742
2026-01-07 12:21:17,701 - INFO -   Batch 1650/2527: Loss = 0.0835
2026-01-07 12:21:40,649 - INFO -   Batch 1700/2527: Loss = 0.0705
2026-01-07 12:22:03,979 - INFO -   Batch 1750/2527: Loss = 0.0707
2026-01-07 12:22:26,940 - INFO -   Batch 1800/2527: Loss = 0.0531
2026-01-07 12:22:49,939 - INFO -   Batch 1850/2527: Loss = 0.0328
2026-01-07 12:23:12,887 - INFO -   Batch 1900/2527: Loss = 0.0857
2026-01-07 12:23:35,938 - INFO -   Batch 1950/2527: Loss = 0.0468
2026-01-07 12:23:59,035 - INFO -   Batch 2000/2527: Loss = 0.0514
2026-01-07 12:24:22,401 - INFO -   Batch 2050/2527: Loss = 0.0653
2026-01-07 12:24:45,393 - INFO -   Batch 2100/2527: Loss = 0.0558
2026-01-07 12:25:08,421 - INFO -   Batch 2150/2527: Loss = 0.0554
2026-01-07 12:25:31,575 - INFO -   Batch 2200/2527: Loss = 0.0644
2026-01-07 12:25:54,541 - INFO -   Batch 2250/2527: Loss = 0.0422
2026-01-07 12:26:17,494 - INFO -   Batch 2300/2527: Loss = 0.0426
2026-01-07 12:26:40,524 - INFO -   Batch 2350/2527: Loss = 0.0534
2026-01-07 12:27:03,515 - INFO -   Batch 2400/2527: Loss = 0.0785
2026-01-07 12:27:26,487 - INFO -   Batch 2450/2527: Loss = 0.0890
2026-01-07 12:27:49,562 - INFO -   Batch 2500/2527: Loss = 0.0670
2026-01-07 12:32:28,890 - INFO - Epoch 87/160: Train Loss: 0.0658, Val Loss: 1.4736, Val mIoU: 40.75%, Val PixelAcc: 79.58%
2026-01-07 12:32:54,192 - INFO -   Batch 50/2527: Loss = 0.0513
2026-01-07 12:33:17,793 - INFO -   Batch 100/2527: Loss = 0.0519
2026-01-07 12:33:41,712 - INFO -   Batch 150/2527: Loss = 0.0866
2026-01-07 12:34:05,282 - INFO -   Batch 200/2527: Loss = 0.0697
2026-01-07 12:34:29,423 - INFO -   Batch 250/2527: Loss = 0.0879
2026-01-07 12:34:53,024 - INFO -   Batch 300/2527: Loss = 0.0485
2026-01-07 12:35:16,862 - INFO -   Batch 350/2527: Loss = 0.1011
2026-01-07 12:35:40,710 - INFO -   Batch 400/2527: Loss = 0.0401
2026-01-07 12:36:04,505 - INFO -   Batch 450/2527: Loss = 0.0635
2026-01-07 12:36:28,096 - INFO -   Batch 500/2527: Loss = 0.0457
2026-01-07 12:36:51,712 - INFO -   Batch 550/2527: Loss = 0.0473
2026-01-07 12:37:15,653 - INFO -   Batch 600/2527: Loss = 0.0358
2026-01-07 12:37:39,629 - INFO -   Batch 650/2527: Loss = 0.0603
2026-01-07 12:38:03,401 - INFO -   Batch 700/2527: Loss = 0.0497
2026-01-07 12:38:27,001 - INFO -   Batch 750/2527: Loss = 0.0656
2026-01-07 12:38:50,818 - INFO -   Batch 800/2527: Loss = 0.0644
2026-01-07 12:39:14,678 - INFO -   Batch 850/2527: Loss = 0.0727
2026-01-07 12:39:38,486 - INFO -   Batch 900/2527: Loss = 0.0776
2026-01-07 12:40:02,487 - INFO -   Batch 950/2527: Loss = 0.0531
2026-01-07 12:40:26,102 - INFO -   Batch 1000/2527: Loss = 0.0624
2026-01-07 12:40:50,040 - INFO -   Batch 1050/2527: Loss = 0.0721
2026-01-07 12:41:13,812 - INFO -   Batch 1100/2527: Loss = 0.0626
2026-01-07 12:41:37,611 - INFO -   Batch 1150/2527: Loss = 0.0660
2026-01-07 12:42:01,246 - INFO -   Batch 1200/2527: Loss = 0.0650
2026-01-07 12:42:25,047 - INFO -   Batch 1250/2527: Loss = 0.0558
2026-01-07 12:42:48,944 - INFO -   Batch 1300/2527: Loss = 0.0558
2026-01-07 12:43:12,767 - INFO -   Batch 1350/2527: Loss = 0.0604
2026-01-07 12:43:36,385 - INFO -   Batch 1400/2527: Loss = 0.0719
2026-01-07 12:44:00,007 - INFO -   Batch 1450/2527: Loss = 0.0566
2026-01-07 12:44:24,065 - INFO -   Batch 1500/2527: Loss = 0.0435
2026-01-07 12:44:47,625 - INFO -   Batch 1550/2527: Loss = 0.0587
2026-01-07 12:45:11,368 - INFO -   Batch 1600/2527: Loss = 0.0529
2026-01-07 12:45:34,985 - INFO -   Batch 1650/2527: Loss = 0.0506
2026-01-07 12:45:59,161 - INFO -   Batch 1700/2527: Loss = 0.0666
2026-01-07 12:46:23,012 - INFO -   Batch 1750/2527: Loss = 0.0647
2026-01-07 12:46:46,775 - INFO -   Batch 1800/2527: Loss = 0.0380
2026-01-07 12:47:11,678 - INFO -   Batch 1850/2527: Loss = 0.0568
2026-01-07 12:47:35,293 - INFO -   Batch 1900/2527: Loss = 0.0612
2026-01-07 12:47:59,236 - INFO -   Batch 1950/2527: Loss = 0.0729
2026-01-07 12:48:22,810 - INFO -   Batch 2000/2527: Loss = 0.0693
2026-01-07 12:48:46,605 - INFO -   Batch 2050/2527: Loss = 0.0383
2026-01-07 12:49:10,591 - INFO -   Batch 2100/2527: Loss = 0.0680
2026-01-07 12:49:34,403 - INFO -   Batch 2150/2527: Loss = 0.0523
2026-01-07 12:49:58,505 - INFO -   Batch 2200/2527: Loss = 0.0819
2026-01-07 12:50:22,397 - INFO -   Batch 2250/2527: Loss = 0.0607
2026-01-07 12:50:45,980 - INFO -   Batch 2300/2527: Loss = 0.0734
2026-01-07 12:51:09,611 - INFO -   Batch 2350/2527: Loss = 0.0625
2026-01-07 12:51:33,932 - INFO -   Batch 2400/2527: Loss = 0.0436
2026-01-07 12:51:57,492 - INFO -   Batch 2450/2527: Loss = 0.0591
2026-01-07 12:52:21,236 - INFO -   Batch 2500/2527: Loss = 0.0584
2026-01-07 12:57:13,252 - INFO - Epoch 88/160: Train Loss: 0.0594, Val Loss: 1.5336, Val mIoU: 40.66%, Val PixelAcc: 79.67%
2026-01-07 12:57:39,271 - INFO -   Batch 50/2527: Loss = 0.0550
2026-01-07 12:58:04,313 - INFO -   Batch 100/2527: Loss = 0.0895
2026-01-07 12:58:29,253 - INFO -   Batch 150/2527: Loss = 0.0724
2026-01-07 12:58:54,037 - INFO -   Batch 200/2527: Loss = 0.0438
2026-01-07 12:59:18,881 - INFO -   Batch 250/2527: Loss = 0.0591
2026-01-07 12:59:43,881 - INFO -   Batch 300/2527: Loss = 0.0501
2026-01-07 13:00:08,695 - INFO -   Batch 350/2527: Loss = 0.0526
2026-01-07 13:00:33,521 - INFO -   Batch 400/2527: Loss = 0.0659
2026-01-07 13:00:58,644 - INFO -   Batch 450/2527: Loss = 0.0692
2026-01-07 13:01:24,016 - INFO -   Batch 500/2527: Loss = 0.0643
2026-01-07 13:01:48,823 - INFO -   Batch 550/2527: Loss = 0.0424
2026-01-07 13:02:14,018 - INFO -   Batch 600/2527: Loss = 0.0534
2026-01-07 13:02:38,760 - INFO -   Batch 650/2527: Loss = 0.0619
2026-01-07 13:03:03,604 - INFO -   Batch 700/2527: Loss = 0.0553
2026-01-07 13:03:28,702 - INFO -   Batch 750/2527: Loss = 0.0642
2026-01-07 13:03:53,664 - INFO -   Batch 800/2527: Loss = 0.0543
2026-01-07 13:04:18,464 - INFO -   Batch 850/2527: Loss = 0.0542
2026-01-07 13:04:43,283 - INFO -   Batch 900/2527: Loss = 0.0608
2026-01-07 13:05:08,314 - INFO -   Batch 950/2527: Loss = 0.0721
2026-01-07 13:05:33,522 - INFO -   Batch 1000/2527: Loss = 0.0644
2026-01-07 13:05:58,350 - INFO -   Batch 1050/2527: Loss = 0.0513
2026-01-07 13:06:23,448 - INFO -   Batch 1100/2527: Loss = 0.0543
2026-01-07 13:06:48,399 - INFO -   Batch 1150/2527: Loss = 0.0386
2026-01-07 13:07:13,228 - INFO -   Batch 1200/2527: Loss = 0.0556
2026-01-07 13:07:38,049 - INFO -   Batch 1250/2527: Loss = 0.0581
2026-01-07 13:08:03,126 - INFO -   Batch 1300/2527: Loss = 0.0793
2026-01-07 13:08:27,956 - INFO -   Batch 1350/2527: Loss = 0.0629
2026-01-07 13:08:53,050 - INFO -   Batch 1400/2527: Loss = 0.0506
2026-01-07 13:09:18,120 - INFO -   Batch 1450/2527: Loss = 0.0703
2026-01-07 13:09:42,954 - INFO -   Batch 1500/2527: Loss = 0.0643
2026-01-07 13:10:07,774 - INFO -   Batch 1550/2527: Loss = 0.0537
2026-01-07 13:10:32,765 - INFO -   Batch 1600/2527: Loss = 0.0511
2026-01-07 13:10:57,570 - INFO -   Batch 1650/2527: Loss = 0.0647
2026-01-07 13:11:22,383 - INFO -   Batch 1700/2527: Loss = 0.0476
2026-01-07 13:11:47,499 - INFO -   Batch 1750/2527: Loss = 0.0674
2026-01-07 13:12:12,454 - INFO -   Batch 1800/2527: Loss = 0.0909
2026-01-07 13:12:37,262 - INFO -   Batch 1850/2527: Loss = 0.0841
2026-01-07 13:13:02,113 - INFO -   Batch 1900/2527: Loss = 0.0668
2026-01-07 13:13:26,813 - INFO -   Batch 1950/2527: Loss = 0.0575
2026-01-07 13:13:51,645 - INFO -   Batch 2000/2527: Loss = 0.0805
2026-01-07 13:14:17,127 - INFO -   Batch 2050/2527: Loss = 0.0331
2026-01-07 13:14:42,111 - INFO -   Batch 2100/2527: Loss = 0.0783
2026-01-07 13:15:06,911 - INFO -   Batch 2150/2527: Loss = 0.0648
2026-01-07 13:15:31,766 - INFO -   Batch 2200/2527: Loss = 0.0419
2026-01-07 13:15:56,867 - INFO -   Batch 2250/2527: Loss = 0.0750
2026-01-07 13:16:21,687 - INFO -   Batch 2300/2527: Loss = 0.0532
2026-01-07 13:16:46,504 - INFO -   Batch 2350/2527: Loss = 0.0618
2026-01-07 13:17:11,627 - INFO -   Batch 2400/2527: Loss = 0.0693
2026-01-07 13:17:36,938 - INFO -   Batch 2450/2527: Loss = 0.0402
2026-01-07 13:18:01,737 - INFO -   Batch 2500/2527: Loss = 0.0528
2026-01-07 13:22:41,481 - INFO - Epoch 89/160: Train Loss: 0.0596, Val Loss: 1.5161, Val mIoU: 40.70%, Val PixelAcc: 79.63%
2026-01-07 13:23:07,801 - INFO -   Batch 50/2527: Loss = 0.0770
2026-01-07 13:23:32,767 - INFO -   Batch 100/2527: Loss = 0.0686
2026-01-07 13:23:58,114 - INFO -   Batch 150/2527: Loss = 0.0520
2026-01-07 13:24:22,917 - INFO -   Batch 200/2527: Loss = 0.0544
2026-01-07 13:24:48,395 - INFO -   Batch 250/2527: Loss = 0.0731
2026-01-07 13:25:13,773 - INFO -   Batch 300/2527: Loss = 0.0289
2026-01-07 13:25:38,573 - INFO -   Batch 350/2527: Loss = 0.0709
2026-01-07 13:26:03,426 - INFO -   Batch 400/2527: Loss = 0.0471
2026-01-07 13:26:28,172 - INFO -   Batch 450/2527: Loss = 0.0689
2026-01-07 13:26:52,981 - INFO -   Batch 500/2527: Loss = 0.0670
2026-01-07 13:27:18,072 - INFO -   Batch 550/2527: Loss = 0.0429
2026-01-07 13:27:43,030 - INFO -   Batch 600/2527: Loss = 0.0427
2026-01-07 13:28:07,837 - INFO -   Batch 650/2527: Loss = 0.0847
2026-01-07 13:28:32,679 - INFO -   Batch 700/2527: Loss = 0.0745
2026-01-07 13:28:57,758 - INFO -   Batch 750/2527: Loss = 0.0474
2026-01-07 13:29:22,565 - INFO -   Batch 800/2527: Loss = 0.0624
2026-01-07 13:29:47,378 - INFO -   Batch 850/2527: Loss = 0.0538
2026-01-07 13:30:12,487 - INFO -   Batch 900/2527: Loss = 0.0561
2026-01-07 13:30:37,788 - INFO -   Batch 950/2527: Loss = 0.0684
2026-01-07 13:31:02,598 - INFO -   Batch 1000/2527: Loss = 0.0645
2026-01-07 13:31:27,436 - INFO -   Batch 1050/2527: Loss = 0.0731
2026-01-07 13:31:52,147 - INFO -   Batch 1100/2527: Loss = 0.0473
2026-01-07 13:32:16,972 - INFO -   Batch 1150/2527: Loss = 0.0514
2026-01-07 13:32:42,085 - INFO -   Batch 1200/2527: Loss = 0.0604
2026-01-07 13:33:07,053 - INFO -   Batch 1250/2527: Loss = 0.0742
2026-01-07 13:33:31,862 - INFO -   Batch 1300/2527: Loss = 0.0454
2026-01-07 13:33:56,685 - INFO -   Batch 1350/2527: Loss = 0.0532
2026-01-07 13:34:22,481 - INFO -   Batch 1400/2527: Loss = 0.0595
2026-01-07 13:34:47,291 - INFO -   Batch 1450/2527: Loss = 0.0716
2026-01-07 13:35:12,120 - INFO -   Batch 1500/2527: Loss = 0.0546
2026-01-07 13:35:37,232 - INFO -   Batch 1550/2527: Loss = 0.0409
2026-01-07 13:36:02,188 - INFO -   Batch 1600/2527: Loss = 0.0606
2026-01-07 13:36:27,007 - INFO -   Batch 1650/2527: Loss = 0.0489
2026-01-07 13:36:52,225 - INFO -   Batch 1700/2527: Loss = 0.0600
2026-01-07 13:37:16,944 - INFO -   Batch 1750/2527: Loss = 0.0987
2026-01-07 13:37:42,229 - INFO -   Batch 1800/2527: Loss = 0.0658
2026-01-07 13:38:07,319 - INFO -   Batch 1850/2527: Loss = 0.0538
2026-01-07 13:38:32,284 - INFO -   Batch 1900/2527: Loss = 0.0557
2026-01-07 13:38:57,081 - INFO -   Batch 1950/2527: Loss = 0.0577
2026-01-07 13:39:21,935 - INFO -   Batch 2000/2527: Loss = 0.0654
2026-01-07 13:39:46,945 - INFO -   Batch 2050/2527: Loss = 0.0450
2026-01-07 13:40:11,749 - INFO -   Batch 2100/2527: Loss = 0.0554
2026-01-07 13:40:36,566 - INFO -   Batch 2150/2527: Loss = 0.0407
2026-01-07 13:41:01,707 - INFO -   Batch 2200/2527: Loss = 0.0714
2026-01-07 13:41:26,650 - INFO -   Batch 2250/2527: Loss = 0.0399
2026-01-07 13:41:51,453 - INFO -   Batch 2300/2527: Loss = 0.0519
2026-01-07 13:42:16,265 - INFO -   Batch 2350/2527: Loss = 0.0846
2026-01-07 13:42:41,356 - INFO -   Batch 2400/2527: Loss = 0.0693
2026-01-07 13:43:06,175 - INFO -   Batch 2450/2527: Loss = 0.0494
2026-01-07 13:43:31,299 - INFO -   Batch 2500/2527: Loss = 0.0436
2026-01-07 13:48:23,919 - INFO - Epoch 90/160: Train Loss: 0.0606, Val Loss: 1.5206, Val mIoU: 40.48%, Val PixelAcc: 79.57%
2026-01-07 13:48:25,953 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_90.pth
2026-01-07 13:48:25,955 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_90.pth
2026-01-07 13:48:50,998 - INFO -   Batch 50/2527: Loss = 0.0464
2026-01-07 13:49:13,909 - INFO -   Batch 100/2527: Loss = 0.0572
2026-01-07 13:49:36,838 - INFO -   Batch 150/2527: Loss = 0.0721
2026-01-07 13:49:59,812 - INFO -   Batch 200/2527: Loss = 0.0504
2026-01-07 13:50:22,753 - INFO -   Batch 250/2527: Loss = 0.0693
2026-01-07 13:50:46,067 - INFO -   Batch 300/2527: Loss = 0.0594
2026-01-07 13:51:09,044 - INFO -   Batch 350/2527: Loss = 0.0703
2026-01-07 13:51:32,000 - INFO -   Batch 400/2527: Loss = 0.0505
2026-01-07 13:51:54,965 - INFO -   Batch 450/2527: Loss = 0.0583
2026-01-07 13:52:17,930 - INFO -   Batch 500/2527: Loss = 0.0586
2026-01-07 13:52:40,893 - INFO -   Batch 550/2527: Loss = 0.0814
2026-01-07 13:53:04,228 - INFO -   Batch 600/2527: Loss = 0.0662
2026-01-07 13:53:27,184 - INFO -   Batch 650/2527: Loss = 0.0753
2026-01-07 13:53:50,137 - INFO -   Batch 700/2527: Loss = 0.0641
2026-01-07 13:54:13,391 - INFO -   Batch 750/2527: Loss = 0.0549
2026-01-07 13:54:36,367 - INFO -   Batch 800/2527: Loss = 0.0503
2026-01-07 13:54:59,330 - INFO -   Batch 850/2527: Loss = 0.0527
2026-01-07 13:55:22,288 - INFO -   Batch 900/2527: Loss = 0.0665
2026-01-07 13:55:45,252 - INFO -   Batch 950/2527: Loss = 0.0433
2026-01-07 13:56:08,223 - INFO -   Batch 1000/2527: Loss = 0.0647
2026-01-07 13:56:31,217 - INFO -   Batch 1050/2527: Loss = 0.0731
2026-01-07 13:56:54,172 - INFO -   Batch 1100/2527: Loss = 0.0596
2026-01-07 13:57:17,146 - INFO -   Batch 1150/2527: Loss = 0.0704
2026-01-07 13:57:40,106 - INFO -   Batch 1200/2527: Loss = 0.0801
2026-01-07 13:58:03,076 - INFO -   Batch 1250/2527: Loss = 0.0591
2026-01-07 13:58:26,037 - INFO -   Batch 1300/2527: Loss = 0.0543
2026-01-07 13:58:49,410 - INFO -   Batch 1350/2527: Loss = 0.0818
2026-01-07 13:59:12,361 - INFO -   Batch 1400/2527: Loss = 0.0519
2026-01-07 13:59:35,353 - INFO -   Batch 1450/2527: Loss = 0.0638
2026-01-07 13:59:58,784 - INFO -   Batch 1500/2527: Loss = 0.0803
2026-01-07 14:00:21,738 - INFO -   Batch 1550/2527: Loss = 0.0444
2026-01-07 14:00:44,693 - INFO -   Batch 1600/2527: Loss = 0.0815
2026-01-07 14:01:07,672 - INFO -   Batch 1650/2527: Loss = 0.0475
2026-01-07 14:01:30,663 - INFO -   Batch 1700/2527: Loss = 0.0404
2026-01-07 14:01:53,985 - INFO -   Batch 1750/2527: Loss = 0.0734
2026-01-07 14:02:16,945 - INFO -   Batch 1800/2527: Loss = 0.0299
2026-01-07 14:02:40,022 - INFO -   Batch 1850/2527: Loss = 0.0502
2026-01-07 14:03:02,977 - INFO -   Batch 1900/2527: Loss = 0.0473
2026-01-07 14:03:25,932 - INFO -   Batch 1950/2527: Loss = 0.0664
2026-01-07 14:03:48,891 - INFO -   Batch 2000/2527: Loss = 0.0539
2026-01-07 14:04:12,222 - INFO -   Batch 2050/2527: Loss = 0.0508
2026-01-07 14:04:35,184 - INFO -   Batch 2100/2527: Loss = 0.0765
2026-01-07 14:04:58,573 - INFO -   Batch 2150/2527: Loss = 0.0476
2026-01-07 14:05:21,532 - INFO -   Batch 2200/2527: Loss = 0.0445
2026-01-07 14:05:44,524 - INFO -   Batch 2250/2527: Loss = 0.0660
2026-01-07 14:06:07,476 - INFO -   Batch 2300/2527: Loss = 0.0494
2026-01-07 14:06:30,429 - INFO -   Batch 2350/2527: Loss = 0.0551
2026-01-07 14:06:53,385 - INFO -   Batch 2400/2527: Loss = 0.0459
2026-01-07 14:07:16,369 - INFO -   Batch 2450/2527: Loss = 0.0506
2026-01-07 14:07:39,330 - INFO -   Batch 2500/2527: Loss = 0.0523
2026-01-07 14:12:18,724 - INFO - Epoch 91/160: Train Loss: 0.0593, Val Loss: 1.5370, Val mIoU: 40.80%, Val PixelAcc: 79.67%
2026-01-07 14:12:43,930 - INFO -   Batch 50/2527: Loss = 0.0568
2026-01-07 14:13:07,502 - INFO -   Batch 100/2527: Loss = 0.0516
2026-01-07 14:13:31,435 - INFO -   Batch 150/2527: Loss = 0.0759
2026-01-07 14:13:54,980 - INFO -   Batch 200/2527: Loss = 0.0675
2026-01-07 14:14:19,097 - INFO -   Batch 250/2527: Loss = 0.0455
2026-01-07 14:14:42,699 - INFO -   Batch 300/2527: Loss = 0.0652
2026-01-07 14:15:06,514 - INFO -   Batch 350/2527: Loss = 0.0407
2026-01-07 14:15:30,396 - INFO -   Batch 400/2527: Loss = 0.0720
2026-01-07 14:15:54,176 - INFO -   Batch 450/2527: Loss = 0.0559
2026-01-07 14:16:17,774 - INFO -   Batch 500/2527: Loss = 0.0502
2026-01-07 14:16:41,426 - INFO -   Batch 550/2527: Loss = 0.0642
2026-01-07 14:17:05,382 - INFO -   Batch 600/2527: Loss = 0.0553
2026-01-07 14:17:29,282 - INFO -   Batch 650/2527: Loss = 0.0399
2026-01-07 14:17:53,389 - INFO -   Batch 700/2527: Loss = 0.0572
2026-01-07 14:18:16,995 - INFO -   Batch 750/2527: Loss = 0.0633
2026-01-07 14:18:40,818 - INFO -   Batch 800/2527: Loss = 0.0699
2026-01-07 14:19:04,687 - INFO -   Batch 850/2527: Loss = 0.0659
2026-01-07 14:19:28,503 - INFO -   Batch 900/2527: Loss = 0.0556
2026-01-07 14:19:52,477 - INFO -   Batch 950/2527: Loss = 0.0615
2026-01-07 14:20:16,092 - INFO -   Batch 1000/2527: Loss = 0.0679
2026-01-07 14:20:40,048 - INFO -   Batch 1050/2527: Loss = 0.0520
2026-01-07 14:21:03,643 - INFO -   Batch 1100/2527: Loss = 0.0588
2026-01-07 14:21:27,393 - INFO -   Batch 1150/2527: Loss = 0.0547
2026-01-07 14:21:51,034 - INFO -   Batch 1200/2527: Loss = 0.0335
2026-01-07 14:22:14,839 - INFO -   Batch 1250/2527: Loss = 0.0331
2026-01-07 14:22:38,680 - INFO -   Batch 1300/2527: Loss = 0.0472
2026-01-07 14:23:02,478 - INFO -   Batch 1350/2527: Loss = 0.0579
2026-01-07 14:23:26,060 - INFO -   Batch 1400/2527: Loss = 0.0744
2026-01-07 14:23:49,698 - INFO -   Batch 1450/2527: Loss = 0.0477
2026-01-07 14:24:13,680 - INFO -   Batch 1500/2527: Loss = 0.0434
2026-01-07 14:24:37,278 - INFO -   Batch 1550/2527: Loss = 0.0370
2026-01-07 14:25:01,024 - INFO -   Batch 1600/2527: Loss = 0.0710
2026-01-07 14:25:24,613 - INFO -   Batch 1650/2527: Loss = 0.1048
2026-01-07 14:25:48,790 - INFO -   Batch 1700/2527: Loss = 0.0822
2026-01-07 14:26:12,674 - INFO -   Batch 1750/2527: Loss = 0.0671
2026-01-07 14:26:36,478 - INFO -   Batch 1800/2527: Loss = 0.0701
2026-01-07 14:27:00,077 - INFO -   Batch 1850/2527: Loss = 0.0575
2026-01-07 14:27:23,690 - INFO -   Batch 1900/2527: Loss = 0.0575
2026-01-07 14:27:47,636 - INFO -   Batch 1950/2527: Loss = 0.0519
2026-01-07 14:28:11,196 - INFO -   Batch 2000/2527: Loss = 0.0353
2026-01-07 14:28:34,935 - INFO -   Batch 2050/2527: Loss = 0.0488
2026-01-07 14:28:58,905 - INFO -   Batch 2100/2527: Loss = 0.0706
2026-01-07 14:29:22,721 - INFO -   Batch 2150/2527: Loss = 0.0621
2026-01-07 14:29:46,574 - INFO -   Batch 2200/2527: Loss = 0.0405
2026-01-07 14:30:10,361 - INFO -   Batch 2250/2527: Loss = 0.0384
2026-01-07 14:30:33,950 - INFO -   Batch 2300/2527: Loss = 0.1146
2026-01-07 14:30:57,572 - INFO -   Batch 2350/2527: Loss = 0.0588
2026-01-07 14:31:21,893 - INFO -   Batch 2400/2527: Loss = 0.0700
2026-01-07 14:31:45,465 - INFO -   Batch 2450/2527: Loss = 0.0546
2026-01-07 14:32:09,212 - INFO -   Batch 2500/2527: Loss = 0.0687
2026-01-07 14:37:00,711 - INFO - Epoch 92/160: Train Loss: 0.0581, Val Loss: 1.5418, Val mIoU: 40.47%, Val PixelAcc: 79.59%
2026-01-07 14:37:26,649 - INFO -   Batch 50/2527: Loss = 0.0284
2026-01-07 14:37:51,698 - INFO -   Batch 100/2527: Loss = 0.0456
2026-01-07 14:38:16,626 - INFO -   Batch 150/2527: Loss = 0.0501
2026-01-07 14:38:41,442 - INFO -   Batch 200/2527: Loss = 0.0532
2026-01-07 14:39:06,256 - INFO -   Batch 250/2527: Loss = 0.0449
2026-01-07 14:39:31,252 - INFO -   Batch 300/2527: Loss = 0.0402
2026-01-07 14:39:56,060 - INFO -   Batch 350/2527: Loss = 0.0991
2026-01-07 14:40:20,869 - INFO -   Batch 400/2527: Loss = 0.0577
2026-01-07 14:40:45,978 - INFO -   Batch 450/2527: Loss = 0.0687
2026-01-07 14:41:10,951 - INFO -   Batch 500/2527: Loss = 0.0447
2026-01-07 14:41:35,774 - INFO -   Batch 550/2527: Loss = 0.0770
2026-01-07 14:42:00,976 - INFO -   Batch 600/2527: Loss = 0.0499
2026-01-07 14:42:25,688 - INFO -   Batch 650/2527: Loss = 0.0605
2026-01-07 14:42:50,503 - INFO -   Batch 700/2527: Loss = 0.0872
2026-01-07 14:43:15,639 - INFO -   Batch 750/2527: Loss = 0.0697
2026-01-07 14:43:40,603 - INFO -   Batch 800/2527: Loss = 0.0344
2026-01-07 14:44:05,411 - INFO -   Batch 850/2527: Loss = 0.0662
2026-01-07 14:44:30,244 - INFO -   Batch 900/2527: Loss = 0.0484
2026-01-07 14:44:55,243 - INFO -   Batch 950/2527: Loss = 0.0429
2026-01-07 14:45:20,048 - INFO -   Batch 1000/2527: Loss = 0.0836
2026-01-07 14:45:45,231 - INFO -   Batch 1050/2527: Loss = 0.0364
2026-01-07 14:46:10,351 - INFO -   Batch 1100/2527: Loss = 0.0517
2026-01-07 14:46:35,332 - INFO -   Batch 1150/2527: Loss = 0.0663
2026-01-07 14:47:00,161 - INFO -   Batch 1200/2527: Loss = 0.0457
2026-01-07 14:47:24,985 - INFO -   Batch 1250/2527: Loss = 0.0514
2026-01-07 14:47:49,709 - INFO -   Batch 1300/2527: Loss = 0.0553
2026-01-07 14:48:14,890 - INFO -   Batch 1350/2527: Loss = 0.0980
2026-01-07 14:48:39,989 - INFO -   Batch 1400/2527: Loss = 0.0896
2026-01-07 14:49:04,972 - INFO -   Batch 1450/2527: Loss = 0.0616
2026-01-07 14:49:29,787 - INFO -   Batch 1500/2527: Loss = 0.0709
2026-01-07 14:49:54,606 - INFO -   Batch 1550/2527: Loss = 0.0440
2026-01-07 14:50:19,612 - INFO -   Batch 1600/2527: Loss = 0.0304
2026-01-07 14:50:44,463 - INFO -   Batch 1650/2527: Loss = 0.0636
2026-01-07 14:51:09,302 - INFO -   Batch 1700/2527: Loss = 0.0488
2026-01-07 14:51:34,451 - INFO -   Batch 1750/2527: Loss = 0.0487
2026-01-07 14:51:59,409 - INFO -   Batch 1800/2527: Loss = 0.0452
2026-01-07 14:52:24,234 - INFO -   Batch 1850/2527: Loss = 0.0558
2026-01-07 14:52:49,164 - INFO -   Batch 1900/2527: Loss = 0.0465
2026-01-07 14:53:13,929 - INFO -   Batch 1950/2527: Loss = 0.0970
2026-01-07 14:53:38,792 - INFO -   Batch 2000/2527: Loss = 0.0427
2026-01-07 14:54:04,253 - INFO -   Batch 2050/2527: Loss = 0.0544
2026-01-07 14:54:29,235 - INFO -   Batch 2100/2527: Loss = 0.0982
2026-01-07 14:54:54,045 - INFO -   Batch 2150/2527: Loss = 0.0641
2026-01-07 14:55:18,877 - INFO -   Batch 2200/2527: Loss = 0.0584
2026-01-07 14:55:43,878 - INFO -   Batch 2250/2527: Loss = 0.0594
2026-01-07 14:56:08,688 - INFO -   Batch 2300/2527: Loss = 0.0566
2026-01-07 14:56:33,895 - INFO -   Batch 2350/2527: Loss = 0.0463
2026-01-07 14:56:59,041 - INFO -   Batch 2400/2527: Loss = 0.0452
2026-01-07 14:57:23,995 - INFO -   Batch 2450/2527: Loss = 0.0515
2026-01-07 14:57:48,837 - INFO -   Batch 2500/2527: Loss = 0.0447
2026-01-07 15:02:28,897 - INFO - Epoch 93/160: Train Loss: 0.0575, Val Loss: 1.5476, Val mIoU: 40.67%, Val PixelAcc: 79.73%
2026-01-07 15:02:55,351 - INFO -   Batch 50/2527: Loss = 0.0322
2026-01-07 15:03:20,333 - INFO -   Batch 100/2527: Loss = 0.0596
2026-01-07 15:03:45,328 - INFO -   Batch 150/2527: Loss = 0.0735
2026-01-07 15:04:10,138 - INFO -   Batch 200/2527: Loss = 0.0688
2026-01-07 15:04:35,237 - INFO -   Batch 250/2527: Loss = 0.0532
2026-01-07 15:05:00,204 - INFO -   Batch 300/2527: Loss = 0.0626
2026-01-07 15:05:25,013 - INFO -   Batch 350/2527: Loss = 0.0633
2026-01-07 15:05:49,829 - INFO -   Batch 400/2527: Loss = 0.0513
2026-01-07 15:06:14,543 - INFO -   Batch 450/2527: Loss = 0.0908
2026-01-07 15:06:39,389 - INFO -   Batch 500/2527: Loss = 0.0598
2026-01-07 15:07:04,863 - INFO -   Batch 550/2527: Loss = 0.0404
2026-01-07 15:07:29,831 - INFO -   Batch 600/2527: Loss = 0.0408
2026-01-07 15:07:54,645 - INFO -   Batch 650/2527: Loss = 0.0643
2026-01-07 15:08:19,484 - INFO -   Batch 700/2527: Loss = 0.0450
2026-01-07 15:08:44,645 - INFO -   Batch 750/2527: Loss = 0.0398
2026-01-07 15:09:09,468 - INFO -   Batch 800/2527: Loss = 0.0923
2026-01-07 15:09:34,368 - INFO -   Batch 850/2527: Loss = 0.0520
2026-01-07 15:09:59,478 - INFO -   Batch 900/2527: Loss = 0.0954
2026-01-07 15:10:24,804 - INFO -   Batch 950/2527: Loss = 0.0554
2026-01-07 15:10:49,611 - INFO -   Batch 1000/2527: Loss = 0.0739
2026-01-07 15:11:14,433 - INFO -   Batch 1050/2527: Loss = 0.0590
2026-01-07 15:11:39,225 - INFO -   Batch 1100/2527: Loss = 0.0351
2026-01-07 15:12:04,044 - INFO -   Batch 1150/2527: Loss = 0.0503
2026-01-07 15:12:29,155 - INFO -   Batch 1200/2527: Loss = 0.0416
2026-01-07 15:12:54,489 - INFO -   Batch 1250/2527: Loss = 0.0360
2026-01-07 15:13:19,303 - INFO -   Batch 1300/2527: Loss = 0.0421
2026-01-07 15:13:44,132 - INFO -   Batch 1350/2527: Loss = 0.0619
2026-01-07 15:14:09,182 - INFO -   Batch 1400/2527: Loss = 0.0462
2026-01-07 15:14:34,009 - INFO -   Batch 1450/2527: Loss = 0.0486
2026-01-07 15:14:58,826 - INFO -   Batch 1500/2527: Loss = 0.0523
2026-01-07 15:15:23,922 - INFO -   Batch 1550/2527: Loss = 0.0390
2026-01-07 15:15:48,872 - INFO -   Batch 1600/2527: Loss = 0.0744
2026-01-07 15:16:13,691 - INFO -   Batch 1650/2527: Loss = 0.0765
2026-01-07 15:16:38,548 - INFO -   Batch 1700/2527: Loss = 0.0447
2026-01-07 15:17:03,264 - INFO -   Batch 1750/2527: Loss = 0.0571
2026-01-07 15:17:28,110 - INFO -   Batch 1800/2527: Loss = 0.0608
2026-01-07 15:17:53,227 - INFO -   Batch 1850/2527: Loss = 0.0475
2026-01-07 15:18:18,187 - INFO -   Batch 1900/2527: Loss = 0.0398
2026-01-07 15:18:43,049 - INFO -   Batch 1950/2527: Loss = 0.0638
2026-01-07 15:19:08,267 - INFO -   Batch 2000/2527: Loss = 0.0747
2026-01-07 15:19:33,286 - INFO -   Batch 2050/2527: Loss = 0.0641
2026-01-07 15:19:58,391 - INFO -   Batch 2100/2527: Loss = 0.0383
2026-01-07 15:20:23,225 - INFO -   Batch 2150/2527: Loss = 0.0524
2026-01-07 15:20:49,890 - INFO -   Batch 2200/2527: Loss = 0.0723
2026-01-07 15:21:14,877 - INFO -   Batch 2250/2527: Loss = 0.0640
2026-01-07 15:21:39,705 - INFO -   Batch 2300/2527: Loss = 0.0558
2026-01-07 15:22:04,543 - INFO -   Batch 2350/2527: Loss = 0.0469
2026-01-07 15:22:29,641 - INFO -   Batch 2400/2527: Loss = 0.0867
2026-01-07 15:22:54,490 - INFO -   Batch 2450/2527: Loss = 0.0554
2026-01-07 15:23:20,131 - INFO -   Batch 2500/2527: Loss = 0.0323
2026-01-07 15:28:12,653 - INFO - Epoch 94/160: Train Loss: 0.0566, Val Loss: 1.5789, Val mIoU: 40.58%, Val PixelAcc: 79.71%
2026-01-07 15:28:37,773 - INFO -   Batch 50/2527: Loss = 0.0370
2026-01-07 15:29:00,727 - INFO -   Batch 100/2527: Loss = 0.0564
2026-01-07 15:29:24,017 - INFO -   Batch 150/2527: Loss = 0.0464
2026-01-07 15:29:46,958 - INFO -   Batch 200/2527: Loss = 0.0498
2026-01-07 15:30:09,944 - INFO -   Batch 250/2527: Loss = 0.0456
2026-01-07 15:30:32,895 - INFO -   Batch 300/2527: Loss = 0.0485
2026-01-07 15:30:56,068 - INFO -   Batch 350/2527: Loss = 0.0601
2026-01-07 15:31:19,054 - INFO -   Batch 400/2527: Loss = 0.0615
2026-01-07 15:31:42,020 - INFO -   Batch 450/2527: Loss = 0.0359
2026-01-07 15:32:04,993 - INFO -   Batch 500/2527: Loss = 0.0684
2026-01-07 15:32:27,997 - INFO -   Batch 550/2527: Loss = 0.0428
2026-01-07 15:32:50,952 - INFO -   Batch 600/2527: Loss = 0.0604
2026-01-07 15:33:13,911 - INFO -   Batch 650/2527: Loss = 0.0568
2026-01-07 15:33:36,873 - INFO -   Batch 700/2527: Loss = 0.0857
2026-01-07 15:33:59,861 - INFO -   Batch 750/2527: Loss = 0.0702
2026-01-07 15:34:22,818 - INFO -   Batch 800/2527: Loss = 0.0575
2026-01-07 15:34:45,860 - INFO -   Batch 850/2527: Loss = 0.0633
2026-01-07 15:35:09,194 - INFO -   Batch 900/2527: Loss = 0.0630
2026-01-07 15:35:32,167 - INFO -   Batch 950/2527: Loss = 0.0688
2026-01-07 15:35:55,131 - INFO -   Batch 1000/2527: Loss = 0.0628
2026-01-07 15:36:18,133 - INFO -   Batch 1050/2527: Loss = 0.0535
2026-01-07 15:36:41,095 - INFO -   Batch 1100/2527: Loss = 0.0463
2026-01-07 15:37:04,054 - INFO -   Batch 1150/2527: Loss = 0.0587
2026-01-07 15:37:27,012 - INFO -   Batch 1200/2527: Loss = 0.0501
2026-01-07 15:37:50,005 - INFO -   Batch 1250/2527: Loss = 0.0594
2026-01-07 15:38:12,969 - INFO -   Batch 1300/2527: Loss = 0.0401
2026-01-07 15:38:36,303 - INFO -   Batch 1350/2527: Loss = 0.0419
2026-01-07 15:38:59,261 - INFO -   Batch 1400/2527: Loss = 0.0652
2026-01-07 15:39:22,229 - INFO -   Batch 1450/2527: Loss = 0.0445
2026-01-07 15:39:45,202 - INFO -   Batch 1500/2527: Loss = 0.0399
2026-01-07 15:40:08,173 - INFO -   Batch 1550/2527: Loss = 0.0408
2026-01-07 15:40:31,498 - INFO -   Batch 1600/2527: Loss = 0.0638
2026-01-07 15:40:54,584 - INFO -   Batch 1650/2527: Loss = 0.0430
2026-01-07 15:41:17,547 - INFO -   Batch 1700/2527: Loss = 0.0666
2026-01-07 15:41:40,516 - INFO -   Batch 1750/2527: Loss = 0.0460
2026-01-07 15:42:03,508 - INFO -   Batch 1800/2527: Loss = 0.0513
2026-01-07 15:42:26,468 - INFO -   Batch 1850/2527: Loss = 0.0483
2026-01-07 15:42:49,420 - INFO -   Batch 1900/2527: Loss = 0.0570
2026-01-07 15:43:12,419 - INFO -   Batch 1950/2527: Loss = 0.0540
2026-01-07 15:43:35,406 - INFO -   Batch 2000/2527: Loss = 0.0760
2026-01-07 15:43:58,371 - INFO -   Batch 2050/2527: Loss = 0.0646
2026-01-07 15:44:21,339 - INFO -   Batch 2100/2527: Loss = 0.0512
2026-01-07 15:44:44,292 - INFO -   Batch 2150/2527: Loss = 0.0431
2026-01-07 15:45:07,265 - INFO -   Batch 2200/2527: Loss = 0.0487
2026-01-07 15:45:30,221 - INFO -   Batch 2250/2527: Loss = 0.0479
2026-01-07 15:45:53,661 - INFO -   Batch 2300/2527: Loss = 0.0419
2026-01-07 15:46:16,996 - INFO -   Batch 2350/2527: Loss = 0.0527
2026-01-07 15:46:39,976 - INFO -   Batch 2400/2527: Loss = 0.0692
2026-01-07 15:47:02,935 - INFO -   Batch 2450/2527: Loss = 0.0647
2026-01-07 15:47:25,891 - INFO -   Batch 2500/2527: Loss = 0.0446
2026-01-07 15:52:05,114 - INFO - Epoch 95/160: Train Loss: 0.0560, Val Loss: 1.5573, Val mIoU: 40.67%, Val PixelAcc: 79.62%
2026-01-07 15:52:30,405 - INFO -   Batch 50/2527: Loss = 0.0629
2026-01-07 15:52:53,983 - INFO -   Batch 100/2527: Loss = 0.0521
2026-01-07 15:53:17,912 - INFO -   Batch 150/2527: Loss = 0.0629
2026-01-07 15:53:41,473 - INFO -   Batch 200/2527: Loss = 0.0462
2026-01-07 15:54:05,578 - INFO -   Batch 250/2527: Loss = 0.0656
2026-01-07 15:54:29,212 - INFO -   Batch 300/2527: Loss = 0.0385
2026-01-07 15:54:53,043 - INFO -   Batch 350/2527: Loss = 0.0422
2026-01-07 15:55:16,913 - INFO -   Batch 400/2527: Loss = 0.0563
2026-01-07 15:55:40,708 - INFO -   Batch 450/2527: Loss = 0.0672
2026-01-07 15:56:04,341 - INFO -   Batch 500/2527: Loss = 0.0640
2026-01-07 15:56:28,338 - INFO -   Batch 550/2527: Loss = 0.0501
2026-01-07 15:56:52,302 - INFO -   Batch 600/2527: Loss = 0.0564
2026-01-07 15:57:15,879 - INFO -   Batch 650/2527: Loss = 0.0953
2026-01-07 15:57:39,640 - INFO -   Batch 700/2527: Loss = 0.0616
2026-01-07 15:58:03,267 - INFO -   Batch 750/2527: Loss = 0.0421
2026-01-07 15:58:27,095 - INFO -   Batch 800/2527: Loss = 0.0641
2026-01-07 15:58:50,958 - INFO -   Batch 850/2527: Loss = 0.0780
2026-01-07 15:59:14,747 - INFO -   Batch 900/2527: Loss = 0.0709
2026-01-07 15:59:38,336 - INFO -   Batch 950/2527: Loss = 0.0428
2026-01-07 16:00:01,961 - INFO -   Batch 1000/2527: Loss = 0.0684
2026-01-07 16:00:25,926 - INFO -   Batch 1050/2527: Loss = 0.0659
2026-01-07 16:00:49,508 - INFO -   Batch 1100/2527: Loss = 0.0477
2026-01-07 16:01:13,269 - INFO -   Batch 1150/2527: Loss = 0.0516
2026-01-07 16:01:36,898 - INFO -   Batch 1200/2527: Loss = 0.0438
2026-01-07 16:02:00,765 - INFO -   Batch 1250/2527: Loss = 0.0618
2026-01-07 16:02:24,994 - INFO -   Batch 1300/2527: Loss = 0.0404
2026-01-07 16:02:48,791 - INFO -   Batch 1350/2527: Loss = 0.0475
2026-01-07 16:03:12,397 - INFO -   Batch 1400/2527: Loss = 0.0517
2026-01-07 16:03:36,052 - INFO -   Batch 1450/2527: Loss = 0.0359
2026-01-07 16:04:00,010 - INFO -   Batch 1500/2527: Loss = 0.0416
2026-01-07 16:04:23,582 - INFO -   Batch 1550/2527: Loss = 0.0480
2026-01-07 16:04:47,345 - INFO -   Batch 1600/2527: Loss = 0.0543
2026-01-07 16:05:10,962 - INFO -   Batch 1650/2527: Loss = 0.0399
2026-01-07 16:05:35,150 - INFO -   Batch 1700/2527: Loss = 0.0913
2026-01-07 16:05:59,028 - INFO -   Batch 1750/2527: Loss = 0.0555
2026-01-07 16:06:22,812 - INFO -   Batch 1800/2527: Loss = 0.0762
2026-01-07 16:06:46,438 - INFO -   Batch 1850/2527: Loss = 0.0648
2026-01-07 16:07:10,084 - INFO -   Batch 1900/2527: Loss = 0.0370
2026-01-07 16:07:34,040 - INFO -   Batch 1950/2527: Loss = 0.0360
2026-01-07 16:07:57,997 - INFO -   Batch 2000/2527: Loss = 0.0861
2026-01-07 16:08:21,746 - INFO -   Batch 2050/2527: Loss = 0.0500
2026-01-07 16:08:45,378 - INFO -   Batch 2100/2527: Loss = 0.0311
2026-01-07 16:09:09,203 - INFO -   Batch 2150/2527: Loss = 0.0507
2026-01-07 16:09:33,101 - INFO -   Batch 2200/2527: Loss = 0.0384
2026-01-07 16:09:56,893 - INFO -   Batch 2250/2527: Loss = 0.0701
2026-01-07 16:10:20,598 - INFO -   Batch 2300/2527: Loss = 0.0554
2026-01-07 16:10:44,248 - INFO -   Batch 2350/2527: Loss = 0.0604
2026-01-07 16:11:08,215 - INFO -   Batch 2400/2527: Loss = 0.0416
2026-01-07 16:11:31,792 - INFO -   Batch 2450/2527: Loss = 0.0645
2026-01-07 16:11:55,578 - INFO -   Batch 2500/2527: Loss = 0.0616
2026-01-07 16:16:47,650 - INFO - Epoch 96/160: Train Loss: 0.0553, Val Loss: 1.5825, Val mIoU: 40.57%, Val PixelAcc: 79.63%
2026-01-07 16:17:13,817 - INFO -   Batch 50/2527: Loss = 0.0638
2026-01-07 16:17:38,926 - INFO -   Batch 100/2527: Loss = 0.0599
2026-01-07 16:18:03,913 - INFO -   Batch 150/2527: Loss = 0.0533
2026-01-07 16:18:29,131 - INFO -   Batch 200/2527: Loss = 0.0827
2026-01-07 16:18:53,986 - INFO -   Batch 250/2527: Loss = 0.0566
2026-01-07 16:19:19,011 - INFO -   Batch 300/2527: Loss = 0.0395
2026-01-07 16:19:43,848 - INFO -   Batch 350/2527: Loss = 0.0396
2026-01-07 16:20:08,684 - INFO -   Batch 400/2527: Loss = 0.0640
2026-01-07 16:20:33,834 - INFO -   Batch 450/2527: Loss = 0.0807
2026-01-07 16:20:58,849 - INFO -   Batch 500/2527: Loss = 0.0633
2026-01-07 16:21:23,683 - INFO -   Batch 550/2527: Loss = 0.0507
2026-01-07 16:21:48,898 - INFO -   Batch 600/2527: Loss = 0.0636
2026-01-07 16:22:13,667 - INFO -   Batch 650/2527: Loss = 0.0358
2026-01-07 16:22:38,533 - INFO -   Batch 700/2527: Loss = 0.0386
2026-01-07 16:23:03,655 - INFO -   Batch 750/2527: Loss = 0.0497
2026-01-07 16:23:28,626 - INFO -   Batch 800/2527: Loss = 0.0462
2026-01-07 16:23:53,553 - INFO -   Batch 850/2527: Loss = 0.0489
2026-01-07 16:24:18,801 - INFO -   Batch 900/2527: Loss = 0.0490
2026-01-07 16:24:43,844 - INFO -   Batch 950/2527: Loss = 0.0474
2026-01-07 16:25:08,665 - INFO -   Batch 1000/2527: Loss = 0.0477
2026-01-07 16:25:33,536 - INFO -   Batch 1050/2527: Loss = 0.0366
2026-01-07 16:25:58,695 - INFO -   Batch 1100/2527: Loss = 0.0693
2026-01-07 16:26:23,667 - INFO -   Batch 1150/2527: Loss = 0.0589
2026-01-07 16:26:48,490 - INFO -   Batch 1200/2527: Loss = 0.0493
2026-01-07 16:27:13,317 - INFO -   Batch 1250/2527: Loss = 0.0888
2026-01-07 16:27:38,052 - INFO -   Batch 1300/2527: Loss = 0.0496
2026-01-07 16:28:02,910 - INFO -   Batch 1350/2527: Loss = 0.0758
2026-01-07 16:28:28,032 - INFO -   Batch 1400/2527: Loss = 0.0674
2026-01-07 16:28:53,015 - INFO -   Batch 1450/2527: Loss = 0.0469
2026-01-07 16:29:17,875 - INFO -   Batch 1500/2527: Loss = 0.0677
2026-01-07 16:29:42,741 - INFO -   Batch 1550/2527: Loss = 0.0662
2026-01-07 16:30:07,769 - INFO -   Batch 1600/2527: Loss = 0.0513
2026-01-07 16:30:33,003 - INFO -   Batch 1650/2527: Loss = 0.0440
2026-01-07 16:30:57,907 - INFO -   Batch 1700/2527: Loss = 0.0538
2026-01-07 16:31:23,043 - INFO -   Batch 1750/2527: Loss = 0.0532
2026-01-07 16:31:48,072 - INFO -   Batch 1800/2527: Loss = 0.0631
2026-01-07 16:32:12,907 - INFO -   Batch 1850/2527: Loss = 0.0420
2026-01-07 16:32:37,770 - INFO -   Batch 1900/2527: Loss = 0.0511
2026-01-07 16:33:02,504 - INFO -   Batch 1950/2527: Loss = 0.0331
2026-01-07 16:33:27,365 - INFO -   Batch 2000/2527: Loss = 0.0648
2026-01-07 16:33:52,871 - INFO -   Batch 2050/2527: Loss = 0.0409
2026-01-07 16:34:17,879 - INFO -   Batch 2100/2527: Loss = 0.0533
2026-01-07 16:34:42,732 - INFO -   Batch 2150/2527: Loss = 0.0655
2026-01-07 16:35:07,572 - INFO -   Batch 2200/2527: Loss = 0.0527
2026-01-07 16:35:32,611 - INFO -   Batch 2250/2527: Loss = 0.0390
2026-01-07 16:35:57,455 - INFO -   Batch 2300/2527: Loss = 0.0597
2026-01-07 16:36:22,700 - INFO -   Batch 2350/2527: Loss = 0.0405
2026-01-07 16:36:47,817 - INFO -   Batch 2400/2527: Loss = 0.0595
2026-01-07 16:37:12,800 - INFO -   Batch 2450/2527: Loss = 0.0349
2026-01-07 16:37:37,698 - INFO -   Batch 2500/2527: Loss = 0.0521
2026-01-07 16:42:17,947 - INFO - Epoch 97/160: Train Loss: 0.0552, Val Loss: 1.6219, Val mIoU: 40.66%, Val PixelAcc: 79.63%
2026-01-07 16:42:44,420 - INFO -   Batch 50/2527: Loss = 0.0574
2026-01-07 16:43:09,417 - INFO -   Batch 100/2527: Loss = 0.0451
2026-01-07 16:43:34,239 - INFO -   Batch 150/2527: Loss = 0.0368
2026-01-07 16:43:59,061 - INFO -   Batch 200/2527: Loss = 0.0512
2026-01-07 16:44:24,162 - INFO -   Batch 250/2527: Loss = 0.0306
2026-01-07 16:44:49,138 - INFO -   Batch 300/2527: Loss = 0.0682
2026-01-07 16:45:13,956 - INFO -   Batch 350/2527: Loss = 0.0330
2026-01-07 16:45:38,822 - INFO -   Batch 400/2527: Loss = 0.0611
2026-01-07 16:46:03,575 - INFO -   Batch 450/2527: Loss = 0.0505
2026-01-07 16:46:28,419 - INFO -   Batch 500/2527: Loss = 0.0397
2026-01-07 16:46:53,959 - INFO -   Batch 550/2527: Loss = 0.0499
2026-01-07 16:47:18,968 - INFO -   Batch 600/2527: Loss = 0.0407
2026-01-07 16:47:43,801 - INFO -   Batch 650/2527: Loss = 0.0590
2026-01-07 16:48:08,633 - INFO -   Batch 700/2527: Loss = 0.0464
2026-01-07 16:48:33,677 - INFO -   Batch 750/2527: Loss = 0.1009
2026-01-07 16:48:58,538 - INFO -   Batch 800/2527: Loss = 0.0660
2026-01-07 16:49:23,443 - INFO -   Batch 850/2527: Loss = 0.0593
2026-01-07 16:49:48,574 - INFO -   Batch 900/2527: Loss = 0.0625
2026-01-07 16:50:13,901 - INFO -   Batch 950/2527: Loss = 0.0388
2026-01-07 16:50:38,726 - INFO -   Batch 1000/2527: Loss = 0.0305
2026-01-07 16:51:03,600 - INFO -   Batch 1050/2527: Loss = 0.0432
2026-01-07 16:51:28,354 - INFO -   Batch 1100/2527: Loss = 0.0521
2026-01-07 16:51:53,203 - INFO -   Batch 1150/2527: Loss = 0.0448
2026-01-07 16:52:18,316 - INFO -   Batch 1200/2527: Loss = 0.0506
2026-01-07 16:52:43,692 - INFO -   Batch 1250/2527: Loss = 0.0369
2026-01-07 16:53:08,516 - INFO -   Batch 1300/2527: Loss = 0.0610
2026-01-07 16:53:33,375 - INFO -   Batch 1350/2527: Loss = 0.0615
2026-01-07 16:53:58,405 - INFO -   Batch 1400/2527: Loss = 0.0536
2026-01-07 16:54:23,241 - INFO -   Batch 1450/2527: Loss = 0.0321
2026-01-07 16:54:48,125 - INFO -   Batch 1500/2527: Loss = 0.0517
2026-01-07 16:55:13,248 - INFO -   Batch 1550/2527: Loss = 0.0286
2026-01-07 16:55:38,259 - INFO -   Batch 1600/2527: Loss = 0.0417
2026-01-07 16:56:03,085 - INFO -   Batch 1650/2527: Loss = 0.0565
2026-01-07 16:56:27,922 - INFO -   Batch 1700/2527: Loss = 0.0384
2026-01-07 16:56:52,650 - INFO -   Batch 1750/2527: Loss = 0.0650
2026-01-07 16:57:17,482 - INFO -   Batch 1800/2527: Loss = 0.0665
2026-01-07 16:57:42,621 - INFO -   Batch 1850/2527: Loss = 0.0637
2026-01-07 16:58:07,630 - INFO -   Batch 1900/2527: Loss = 0.0427
2026-01-07 16:58:32,473 - INFO -   Batch 1950/2527: Loss = 0.0424
2026-01-07 16:58:57,699 - INFO -   Batch 2000/2527: Loss = 0.0518
2026-01-07 16:59:22,714 - INFO -   Batch 2050/2527: Loss = 0.0543
2026-01-07 16:59:47,556 - INFO -   Batch 2100/2527: Loss = 0.0644
2026-01-07 17:00:12,415 - INFO -   Batch 2150/2527: Loss = 0.0635
2026-01-07 17:00:37,555 - INFO -   Batch 2200/2527: Loss = 0.1037
2026-01-07 17:01:02,527 - INFO -   Batch 2250/2527: Loss = 0.0384
2026-01-07 17:01:27,354 - INFO -   Batch 2300/2527: Loss = 0.0317
2026-01-07 17:01:52,239 - INFO -   Batch 2350/2527: Loss = 0.0351
2026-01-07 17:02:17,337 - INFO -   Batch 2400/2527: Loss = 0.0385
2026-01-07 17:02:42,194 - INFO -   Batch 2450/2527: Loss = 0.0338
2026-01-07 17:03:07,334 - INFO -   Batch 2500/2527: Loss = 0.0481
2026-01-07 17:08:00,042 - INFO - Epoch 98/160: Train Loss: 0.0525, Val Loss: 1.6144, Val mIoU: 40.79%, Val PixelAcc: 79.69%
2026-01-07 17:08:24,932 - INFO -   Batch 50/2527: Loss = 0.0469
2026-01-07 17:08:47,836 - INFO -   Batch 100/2527: Loss = 0.0608
2026-01-07 17:09:10,807 - INFO -   Batch 150/2527: Loss = 0.0477
2026-01-07 17:09:34,203 - INFO -   Batch 200/2527: Loss = 0.0711
2026-01-07 17:09:57,161 - INFO -   Batch 250/2527: Loss = 0.0532
2026-01-07 17:10:20,121 - INFO -   Batch 300/2527: Loss = 0.0479
2026-01-07 17:10:43,076 - INFO -   Batch 350/2527: Loss = 0.0561
2026-01-07 17:11:06,039 - INFO -   Batch 400/2527: Loss = 0.0535
2026-01-07 17:11:29,010 - INFO -   Batch 450/2527: Loss = 0.0628
2026-01-07 17:11:51,958 - INFO -   Batch 500/2527: Loss = 0.0418
2026-01-07 17:12:14,921 - INFO -   Batch 550/2527: Loss = 0.0488
2026-01-07 17:12:37,873 - INFO -   Batch 600/2527: Loss = 0.0650
2026-01-07 17:13:00,828 - INFO -   Batch 650/2527: Loss = 0.0383
2026-01-07 17:13:23,779 - INFO -   Batch 700/2527: Loss = 0.0432
2026-01-07 17:13:46,763 - INFO -   Batch 750/2527: Loss = 0.0525
2026-01-07 17:14:09,941 - INFO -   Batch 800/2527: Loss = 0.0564
2026-01-07 17:14:32,919 - INFO -   Batch 850/2527: Loss = 0.0291
2026-01-07 17:14:56,428 - INFO -   Batch 900/2527: Loss = 0.0475
2026-01-07 17:15:19,698 - INFO -   Batch 950/2527: Loss = 0.0563
2026-01-07 17:15:42,793 - INFO -   Batch 1000/2527: Loss = 0.0386
2026-01-07 17:16:06,193 - INFO -   Batch 1050/2527: Loss = 0.0562
2026-01-07 17:16:29,497 - INFO -   Batch 1100/2527: Loss = 0.0784
2026-01-07 17:16:52,504 - INFO -   Batch 1150/2527: Loss = 0.0413
2026-01-07 17:17:15,721 - INFO -   Batch 1200/2527: Loss = 0.0592
2026-01-07 17:17:39,214 - INFO -   Batch 1250/2527: Loss = 0.0390
2026-01-07 17:18:02,188 - INFO -   Batch 1300/2527: Loss = 0.0472
2026-01-07 17:18:25,741 - INFO -   Batch 1350/2527: Loss = 0.0638
2026-01-07 17:18:48,621 - INFO -   Batch 1400/2527: Loss = 0.0328
2026-01-07 17:19:11,824 - INFO -   Batch 1450/2527: Loss = 0.0651
2026-01-07 17:19:35,503 - INFO -   Batch 1500/2527: Loss = 0.0582
2026-01-07 17:19:59,162 - INFO -   Batch 1550/2527: Loss = 0.0913
2026-01-07 17:20:22,873 - INFO -   Batch 1600/2527: Loss = 0.0654
2026-01-07 17:20:46,052 - INFO -   Batch 1650/2527: Loss = 0.0549
2026-01-07 17:21:09,676 - INFO -   Batch 1700/2527: Loss = 0.0378
2026-01-07 17:21:33,891 - INFO -   Batch 1750/2527: Loss = 0.0318
2026-01-07 17:21:57,450 - INFO -   Batch 1800/2527: Loss = 0.0514
2026-01-07 17:22:20,664 - INFO -   Batch 1850/2527: Loss = 0.0601
2026-01-07 17:22:44,451 - INFO -   Batch 1900/2527: Loss = 0.0699
2026-01-07 17:23:07,834 - INFO -   Batch 1950/2527: Loss = 0.0542
2026-01-07 17:23:31,537 - INFO -   Batch 2000/2527: Loss = 0.0503
2026-01-07 17:23:55,212 - INFO -   Batch 2050/2527: Loss = 0.0574
2026-01-07 17:24:18,527 - INFO -   Batch 2100/2527: Loss = 0.0732
2026-01-07 17:24:41,981 - INFO -   Batch 2150/2527: Loss = 0.0684
2026-01-07 17:25:05,266 - INFO -   Batch 2200/2527: Loss = 0.0819
2026-01-07 17:25:28,424 - INFO -   Batch 2250/2527: Loss = 0.0600
2026-01-07 17:25:52,018 - INFO -   Batch 2300/2527: Loss = 0.0392
2026-01-07 17:26:15,208 - INFO -   Batch 2350/2527: Loss = 0.0645
2026-01-07 17:26:39,450 - INFO -   Batch 2400/2527: Loss = 0.0507
2026-01-07 17:27:02,853 - INFO -   Batch 2450/2527: Loss = 0.0469
2026-01-07 17:27:25,895 - INFO -   Batch 2500/2527: Loss = 0.0596
2026-01-07 17:32:14,669 - INFO - Epoch 99/160: Train Loss: 0.0528, Val Loss: 1.6049, Val mIoU: 40.38%, Val PixelAcc: 79.61%
2026-01-07 17:32:41,313 - INFO -   Batch 50/2527: Loss = 0.0424
2026-01-07 17:33:05,233 - INFO -   Batch 100/2527: Loss = 0.0745
2026-01-07 17:33:29,788 - INFO -   Batch 150/2527: Loss = 0.0581
2026-01-07 17:33:53,514 - INFO -   Batch 200/2527: Loss = 0.0544
2026-01-07 17:34:17,476 - INFO -   Batch 250/2527: Loss = 0.0465
2026-01-07 17:34:41,276 - INFO -   Batch 300/2527: Loss = 0.0740
2026-01-07 17:35:05,343 - INFO -   Batch 350/2527: Loss = 0.0529
2026-01-07 17:35:29,261 - INFO -   Batch 400/2527: Loss = 0.0474
2026-01-07 17:35:53,139 - INFO -   Batch 450/2527: Loss = 0.0396
2026-01-07 17:36:17,061 - INFO -   Batch 500/2527: Loss = 0.0430
2026-01-07 17:36:40,860 - INFO -   Batch 550/2527: Loss = 0.0532
2026-01-07 17:37:04,946 - INFO -   Batch 600/2527: Loss = 0.0571
2026-01-07 17:37:28,960 - INFO -   Batch 650/2527: Loss = 0.0451
2026-01-07 17:37:52,959 - INFO -   Batch 700/2527: Loss = 0.0514
2026-01-07 17:38:16,776 - INFO -   Batch 750/2527: Loss = 0.0524
2026-01-07 17:38:40,906 - INFO -   Batch 800/2527: Loss = 0.0627
2026-01-07 17:39:04,942 - INFO -   Batch 850/2527: Loss = 0.0429
2026-01-07 17:39:28,909 - INFO -   Batch 900/2527: Loss = 0.0641
2026-01-07 17:39:53,214 - INFO -   Batch 950/2527: Loss = 0.0554
2026-01-07 17:40:16,891 - INFO -   Batch 1000/2527: Loss = 0.0806
2026-01-07 17:40:41,174 - INFO -   Batch 1050/2527: Loss = 0.0654
2026-01-07 17:41:04,782 - INFO -   Batch 1100/2527: Loss = 0.0509
2026-01-07 17:41:28,690 - INFO -   Batch 1150/2527: Loss = 0.0502
2026-01-07 17:41:52,570 - INFO -   Batch 1200/2527: Loss = 0.0381
2026-01-07 17:42:16,529 - INFO -   Batch 1250/2527: Loss = 0.0526
2026-01-07 17:42:40,782 - INFO -   Batch 1300/2527: Loss = 0.0523
2026-01-07 17:43:04,663 - INFO -   Batch 1350/2527: Loss = 0.0472
2026-01-07 17:43:28,339 - INFO -   Batch 1400/2527: Loss = 0.0392
2026-01-07 17:43:52,010 - INFO -   Batch 1450/2527: Loss = 0.0428
2026-01-07 17:44:16,039 - INFO -   Batch 1500/2527: Loss = 0.0490
2026-01-07 17:44:39,681 - INFO -   Batch 1550/2527: Loss = 0.0681
2026-01-07 17:45:03,484 - INFO -   Batch 1600/2527: Loss = 0.0454
2026-01-07 17:45:27,587 - INFO -   Batch 1650/2527: Loss = 0.0353
2026-01-07 17:45:51,477 - INFO -   Batch 1700/2527: Loss = 0.0449
2026-01-07 17:46:15,485 - INFO -   Batch 1750/2527: Loss = 0.0589
2026-01-07 17:46:39,387 - INFO -   Batch 1800/2527: Loss = 0.0447
2026-01-07 17:47:03,102 - INFO -   Batch 1850/2527: Loss = 0.0719
2026-01-07 17:47:26,955 - INFO -   Batch 1900/2527: Loss = 0.0489
2026-01-07 17:47:51,247 - INFO -   Batch 1950/2527: Loss = 0.0596
2026-01-07 17:48:15,128 - INFO -   Batch 2000/2527: Loss = 0.0533
2026-01-07 17:48:39,077 - INFO -   Batch 2050/2527: Loss = 0.0457
2026-01-07 17:49:03,140 - INFO -   Batch 2100/2527: Loss = 0.0521
2026-01-07 17:49:27,081 - INFO -   Batch 2150/2527: Loss = 0.0527
2026-01-07 17:49:51,119 - INFO -   Batch 2200/2527: Loss = 0.0374
2026-01-07 17:50:15,177 - INFO -   Batch 2250/2527: Loss = 0.0574
2026-01-07 17:50:38,966 - INFO -   Batch 2300/2527: Loss = 0.0698
2026-01-07 17:51:02,782 - INFO -   Batch 2350/2527: Loss = 0.0705
2026-01-07 17:51:27,209 - INFO -   Batch 2400/2527: Loss = 0.0401
2026-01-07 17:51:50,803 - INFO -   Batch 2450/2527: Loss = 0.0266
2026-01-07 17:52:14,617 - INFO -   Batch 2500/2527: Loss = 0.0826
2026-01-07 17:57:06,522 - INFO - Epoch 100/160: Train Loss: 0.0526, Val Loss: 1.6125, Val mIoU: 40.55%, Val PixelAcc: 79.62%
2026-01-07 17:57:08,711 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_100.pth
2026-01-07 17:57:08,713 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_100.pth
2026-01-07 17:57:34,782 - INFO -   Batch 50/2527: Loss = 0.0501
2026-01-07 17:57:59,857 - INFO -   Batch 100/2527: Loss = 0.0619
2026-01-07 17:58:24,802 - INFO -   Batch 150/2527: Loss = 0.0372
2026-01-07 17:58:49,603 - INFO -   Batch 200/2527: Loss = 0.0479
2026-01-07 17:59:14,432 - INFO -   Batch 250/2527: Loss = 0.0475
2026-01-07 17:59:39,500 - INFO -   Batch 300/2527: Loss = 0.0592
2026-01-07 18:00:04,330 - INFO -   Batch 350/2527: Loss = 0.0533
2026-01-07 18:00:29,165 - INFO -   Batch 400/2527: Loss = 0.0352
2026-01-07 18:00:54,279 - INFO -   Batch 450/2527: Loss = 0.0780
2026-01-07 18:01:19,256 - INFO -   Batch 500/2527: Loss = 0.0630
2026-01-07 18:01:44,113 - INFO -   Batch 550/2527: Loss = 0.0534
2026-01-07 18:02:09,354 - INFO -   Batch 600/2527: Loss = 0.0454
2026-01-07 18:02:34,084 - INFO -   Batch 650/2527: Loss = 0.0489
2026-01-07 18:02:58,916 - INFO -   Batch 700/2527: Loss = 0.0567
2026-01-07 18:03:24,122 - INFO -   Batch 750/2527: Loss = 0.0437
2026-01-07 18:03:49,120 - INFO -   Batch 800/2527: Loss = 0.0549
2026-01-07 18:04:13,993 - INFO -   Batch 850/2527: Loss = 0.0620
2026-01-07 18:04:38,917 - INFO -   Batch 900/2527: Loss = 0.0500
2026-01-07 18:05:03,930 - INFO -   Batch 950/2527: Loss = 0.0463
2026-01-07 18:05:28,760 - INFO -   Batch 1000/2527: Loss = 0.0470
2026-01-07 18:05:53,980 - INFO -   Batch 1050/2527: Loss = 0.0446
2026-01-07 18:06:19,154 - INFO -   Batch 1100/2527: Loss = 0.0605
2026-01-07 18:06:44,133 - INFO -   Batch 1150/2527: Loss = 0.0612
2026-01-07 18:07:08,965 - INFO -   Batch 1200/2527: Loss = 0.0509
2026-01-07 18:07:33,804 - INFO -   Batch 1250/2527: Loss = 0.0480
2026-01-07 18:07:58,538 - INFO -   Batch 1300/2527: Loss = 0.0402
2026-01-07 18:08:23,767 - INFO -   Batch 1350/2527: Loss = 0.0669
2026-01-07 18:08:48,978 - INFO -   Batch 1400/2527: Loss = 0.0460
2026-01-07 18:09:13,938 - INFO -   Batch 1450/2527: Loss = 0.0449
2026-01-07 18:09:38,762 - INFO -   Batch 1500/2527: Loss = 0.0428
2026-01-07 18:10:03,642 - INFO -   Batch 1550/2527: Loss = 0.0445
2026-01-07 18:10:28,665 - INFO -   Batch 1600/2527: Loss = 0.0407
2026-01-07 18:10:53,506 - INFO -   Batch 1650/2527: Loss = 0.0608
2026-01-07 18:11:18,354 - INFO -   Batch 1700/2527: Loss = 0.0483
2026-01-07 18:11:43,479 - INFO -   Batch 1750/2527: Loss = 0.0557
2026-01-07 18:12:08,454 - INFO -   Batch 1800/2527: Loss = 0.0507
2026-01-07 18:12:33,276 - INFO -   Batch 1850/2527: Loss = 0.0309
2026-01-07 18:12:58,116 - INFO -   Batch 1900/2527: Loss = 0.0591
2026-01-07 18:13:22,860 - INFO -   Batch 1950/2527: Loss = 0.1075
2026-01-07 18:13:47,696 - INFO -   Batch 2000/2527: Loss = 0.0632
2026-01-07 18:14:13,194 - INFO -   Batch 2050/2527: Loss = 0.0382
2026-01-07 18:14:38,183 - INFO -   Batch 2100/2527: Loss = 0.0626
2026-01-07 18:15:03,018 - INFO -   Batch 2150/2527: Loss = 0.0621
2026-01-07 18:15:27,867 - INFO -   Batch 2200/2527: Loss = 0.0337
2026-01-07 18:15:52,887 - INFO -   Batch 2250/2527: Loss = 0.0471
2026-01-07 18:16:17,723 - INFO -   Batch 2300/2527: Loss = 0.0499
2026-01-07 18:16:42,576 - INFO -   Batch 2350/2527: Loss = 0.0415
2026-01-07 18:17:07,722 - INFO -   Batch 2400/2527: Loss = 0.0474
2026-01-07 18:17:32,702 - INFO -   Batch 2450/2527: Loss = 0.0397
2026-01-07 18:17:57,896 - INFO -   Batch 2500/2527: Loss = 0.0487
2026-01-07 18:22:37,781 - INFO - Epoch 101/160: Train Loss: 0.0518, Val Loss: 1.6273, Val mIoU: 40.46%, Val PixelAcc: 79.61%
2026-01-07 18:23:04,035 - INFO -   Batch 50/2527: Loss = 0.0620
2026-01-07 18:23:29,015 - INFO -   Batch 100/2527: Loss = 0.0577
2026-01-07 18:23:53,831 - INFO -   Batch 150/2527: Loss = 0.0509
2026-01-07 18:24:18,665 - INFO -   Batch 200/2527: Loss = 0.0556
2026-01-07 18:24:44,170 - INFO -   Batch 250/2527: Loss = 0.0514
2026-01-07 18:25:09,150 - INFO -   Batch 300/2527: Loss = 0.0424
2026-01-07 18:25:33,978 - INFO -   Batch 350/2527: Loss = 0.0408
2026-01-07 18:25:58,818 - INFO -   Batch 400/2527: Loss = 0.0402
2026-01-07 18:26:23,543 - INFO -   Batch 450/2527: Loss = 0.0468
2026-01-07 18:26:48,394 - INFO -   Batch 500/2527: Loss = 0.0392
2026-01-07 18:27:13,537 - INFO -   Batch 550/2527: Loss = 0.0558
2026-01-07 18:27:38,531 - INFO -   Batch 600/2527: Loss = 0.0305
2026-01-07 18:28:03,366 - INFO -   Batch 650/2527: Loss = 0.0519
2026-01-07 18:28:28,248 - INFO -   Batch 700/2527: Loss = 0.0451
2026-01-07 18:28:53,253 - INFO -   Batch 750/2527: Loss = 0.0650
2026-01-07 18:29:18,082 - INFO -   Batch 800/2527: Loss = 0.0441
2026-01-07 18:29:42,921 - INFO -   Batch 850/2527: Loss = 0.0756
2026-01-07 18:30:08,073 - INFO -   Batch 900/2527: Loss = 0.0390
2026-01-07 18:30:33,051 - INFO -   Batch 950/2527: Loss = 0.0419
2026-01-07 18:30:58,234 - INFO -   Batch 1000/2527: Loss = 0.0460
2026-01-07 18:31:23,074 - INFO -   Batch 1050/2527: Loss = 0.0382
2026-01-07 18:31:47,845 - INFO -   Batch 1100/2527: Loss = 0.0587
2026-01-07 18:32:12,707 - INFO -   Batch 1150/2527: Loss = 0.0704
2026-01-07 18:32:37,847 - INFO -   Batch 1200/2527: Loss = 0.0710
2026-01-07 18:33:02,816 - INFO -   Batch 1250/2527: Loss = 0.0511
2026-01-07 18:33:27,644 - INFO -   Batch 1300/2527: Loss = 0.0556
2026-01-07 18:33:52,491 - INFO -   Batch 1350/2527: Loss = 0.0703
2026-01-07 18:34:17,888 - INFO -   Batch 1400/2527: Loss = 0.0418
2026-01-07 18:34:42,768 - INFO -   Batch 1450/2527: Loss = 0.0745
2026-01-07 18:35:07,615 - INFO -   Batch 1500/2527: Loss = 0.0497
2026-01-07 18:35:32,770 - INFO -   Batch 1550/2527: Loss = 0.0418
2026-01-07 18:35:57,775 - INFO -   Batch 1600/2527: Loss = 0.0505
2026-01-07 18:36:22,612 - INFO -   Batch 1650/2527: Loss = 0.0417
2026-01-07 18:36:47,852 - INFO -   Batch 1700/2527: Loss = 0.0521
2026-01-07 18:37:12,571 - INFO -   Batch 1750/2527: Loss = 0.0369
2026-01-07 18:37:37,420 - INFO -   Batch 1800/2527: Loss = 0.0661
2026-01-07 18:38:02,525 - INFO -   Batch 1850/2527: Loss = 0.0502
2026-01-07 18:38:27,496 - INFO -   Batch 1900/2527: Loss = 0.0675
2026-01-07 18:38:52,323 - INFO -   Batch 1950/2527: Loss = 0.0436
2026-01-07 18:39:17,171 - INFO -   Batch 2000/2527: Loss = 0.0612
2026-01-07 18:39:42,191 - INFO -   Batch 2050/2527: Loss = 0.0557
2026-01-07 18:40:07,054 - INFO -   Batch 2100/2527: Loss = 0.0432
2026-01-07 18:40:31,911 - INFO -   Batch 2150/2527: Loss = 0.0657
2026-01-07 18:40:57,028 - INFO -   Batch 2200/2527: Loss = 0.0582
2026-01-07 18:41:22,000 - INFO -   Batch 2250/2527: Loss = 0.0492
2026-01-07 18:41:46,815 - INFO -   Batch 2300/2527: Loss = 0.0311
2026-01-07 18:42:11,669 - INFO -   Batch 2350/2527: Loss = 0.0539
2026-01-07 18:42:36,390 - INFO -   Batch 2400/2527: Loss = 0.0521
2026-01-07 18:43:01,599 - INFO -   Batch 2450/2527: Loss = 0.0567
2026-01-07 18:43:26,711 - INFO -   Batch 2500/2527: Loss = 0.0729
2026-01-07 18:48:19,436 - INFO - Epoch 102/160: Train Loss: 0.0512, Val Loss: 1.6290, Val mIoU: 40.64%, Val PixelAcc: 79.62%
2026-01-07 18:48:44,510 - INFO -   Batch 50/2527: Loss = 0.0561
2026-01-07 18:49:07,408 - INFO -   Batch 100/2527: Loss = 0.0733
2026-01-07 18:49:30,347 - INFO -   Batch 150/2527: Loss = 0.0586
2026-01-07 18:49:53,281 - INFO -   Batch 200/2527: Loss = 0.0408
2026-01-07 18:50:16,259 - INFO -   Batch 250/2527: Loss = 0.0508
2026-01-07 18:50:39,206 - INFO -   Batch 300/2527: Loss = 0.0319
2026-01-07 18:51:02,550 - INFO -   Batch 350/2527: Loss = 0.0494
2026-01-07 18:51:25,507 - INFO -   Batch 400/2527: Loss = 0.0677
2026-01-07 18:51:48,455 - INFO -   Batch 450/2527: Loss = 0.0480
2026-01-07 18:52:11,421 - INFO -   Batch 500/2527: Loss = 0.0462
2026-01-07 18:52:34,415 - INFO -   Batch 550/2527: Loss = 0.0469
2026-01-07 18:52:57,740 - INFO -   Batch 600/2527: Loss = 0.0367
2026-01-07 18:53:20,691 - INFO -   Batch 650/2527: Loss = 0.0547
2026-01-07 18:53:43,650 - INFO -   Batch 700/2527: Loss = 0.0476
2026-01-07 18:54:06,602 - INFO -   Batch 750/2527: Loss = 0.0404
2026-01-07 18:54:29,553 - INFO -   Batch 800/2527: Loss = 0.0615
2026-01-07 18:54:52,514 - INFO -   Batch 850/2527: Loss = 0.0649
2026-01-07 18:55:15,523 - INFO -   Batch 900/2527: Loss = 0.0565
2026-01-07 18:55:38,524 - INFO -   Batch 950/2527: Loss = 0.0401
2026-01-07 18:56:01,480 - INFO -   Batch 1000/2527: Loss = 0.0414
2026-01-07 18:56:24,433 - INFO -   Batch 1050/2527: Loss = 0.1111
2026-01-07 18:56:47,392 - INFO -   Batch 1100/2527: Loss = 0.0477
2026-01-07 18:57:10,378 - INFO -   Batch 1150/2527: Loss = 0.0703
2026-01-07 18:57:33,340 - INFO -   Batch 1200/2527: Loss = 0.0493
2026-01-07 18:57:56,292 - INFO -   Batch 1250/2527: Loss = 0.0382
2026-01-07 18:58:19,241 - INFO -   Batch 1300/2527: Loss = 0.0341
2026-01-07 18:58:42,564 - INFO -   Batch 1350/2527: Loss = 0.0362
2026-01-07 18:59:05,511 - INFO -   Batch 1400/2527: Loss = 0.0487
2026-01-07 18:59:28,488 - INFO -   Batch 1450/2527: Loss = 0.0268
2026-01-07 18:59:51,443 - INFO -   Batch 1500/2527: Loss = 0.0618
2026-01-07 19:00:14,431 - INFO -   Batch 1550/2527: Loss = 0.0304
2026-01-07 19:00:37,377 - INFO -   Batch 1600/2527: Loss = 0.0369
2026-01-07 19:01:00,333 - INFO -   Batch 1650/2527: Loss = 0.0328
2026-01-07 19:01:23,302 - INFO -   Batch 1700/2527: Loss = 0.0435
2026-01-07 19:01:46,295 - INFO -   Batch 1750/2527: Loss = 0.0632
2026-01-07 19:02:09,602 - INFO -   Batch 1800/2527: Loss = 0.0569
2026-01-07 19:02:32,590 - INFO -   Batch 1850/2527: Loss = 0.0551
2026-01-07 19:02:55,556 - INFO -   Batch 1900/2527: Loss = 0.0452
2026-01-07 19:03:18,511 - INFO -   Batch 1950/2527: Loss = 0.0485
2026-01-07 19:03:41,469 - INFO -   Batch 2000/2527: Loss = 0.0588
2026-01-07 19:04:04,829 - INFO -   Batch 2050/2527: Loss = 0.0475
2026-01-07 19:04:27,785 - INFO -   Batch 2100/2527: Loss = 0.0468
2026-01-07 19:04:50,754 - INFO -   Batch 2150/2527: Loss = 0.0477
2026-01-07 19:05:13,745 - INFO -   Batch 2200/2527: Loss = 0.0389
2026-01-07 19:05:36,739 - INFO -   Batch 2250/2527: Loss = 0.0557
2026-01-07 19:05:59,728 - INFO -   Batch 2300/2527: Loss = 0.0515
2026-01-07 19:06:22,683 - INFO -   Batch 2350/2527: Loss = 0.0325
2026-01-07 19:06:45,641 - INFO -   Batch 2400/2527: Loss = 0.0570
2026-01-07 19:07:08,665 - INFO -   Batch 2450/2527: Loss = 0.0413
2026-01-07 19:07:31,655 - INFO -   Batch 2500/2527: Loss = 0.0395
2026-01-07 19:12:10,125 - INFO - Epoch 103/160: Train Loss: 0.0506, Val Loss: 1.6696, Val mIoU: 40.76%, Val PixelAcc: 79.65%
2026-01-07 19:12:35,438 - INFO -   Batch 50/2527: Loss = 0.0435
2026-01-07 19:12:59,008 - INFO -   Batch 100/2527: Loss = 0.0556
2026-01-07 19:13:22,930 - INFO -   Batch 150/2527: Loss = 0.0691
2026-01-07 19:13:46,576 - INFO -   Batch 200/2527: Loss = 0.0335
2026-01-07 19:14:10,339 - INFO -   Batch 250/2527: Loss = 0.0475
2026-01-07 19:14:34,296 - INFO -   Batch 300/2527: Loss = 0.0470
2026-01-07 19:14:58,104 - INFO -   Batch 350/2527: Loss = 0.0866
2026-01-07 19:15:21,964 - INFO -   Batch 400/2527: Loss = 0.0563
2026-01-07 19:15:45,746 - INFO -   Batch 450/2527: Loss = 0.0636
2026-01-07 19:16:09,368 - INFO -   Batch 500/2527: Loss = 0.0542
2026-01-07 19:16:33,025 - INFO -   Batch 550/2527: Loss = 0.0524
2026-01-07 19:16:57,002 - INFO -   Batch 600/2527: Loss = 0.0530
2026-01-07 19:17:20,576 - INFO -   Batch 650/2527: Loss = 0.0310
2026-01-07 19:17:44,702 - INFO -   Batch 700/2527: Loss = 0.0413
2026-01-07 19:18:08,388 - INFO -   Batch 750/2527: Loss = 0.0385
2026-01-07 19:18:32,218 - INFO -   Batch 800/2527: Loss = 0.0753
2026-01-07 19:18:56,074 - INFO -   Batch 850/2527: Loss = 0.0450
2026-01-07 19:19:19,844 - INFO -   Batch 900/2527: Loss = 0.0511
2026-01-07 19:19:43,498 - INFO -   Batch 950/2527: Loss = 0.0542
2026-01-07 19:20:07,484 - INFO -   Batch 1000/2527: Loss = 0.0434
2026-01-07 19:20:31,438 - INFO -   Batch 1050/2527: Loss = 0.0407
2026-01-07 19:20:55,022 - INFO -   Batch 1100/2527: Loss = 0.0471
2026-01-07 19:21:18,756 - INFO -   Batch 1150/2527: Loss = 0.0553
2026-01-07 19:21:43,030 - INFO -   Batch 1200/2527: Loss = 0.0298
2026-01-07 19:22:06,874 - INFO -   Batch 1250/2527: Loss = 0.0538
2026-01-07 19:22:30,733 - INFO -   Batch 1300/2527: Loss = 0.0524
2026-01-07 19:22:54,512 - INFO -   Batch 1350/2527: Loss = 0.0475
2026-01-07 19:23:18,112 - INFO -   Batch 1400/2527: Loss = 0.0419
2026-01-07 19:23:41,724 - INFO -   Batch 1450/2527: Loss = 0.0579
2026-01-07 19:24:05,705 - INFO -   Batch 1500/2527: Loss = 0.0553
2026-01-07 19:24:29,318 - INFO -   Batch 1550/2527: Loss = 0.0418
2026-01-07 19:24:53,072 - INFO -   Batch 1600/2527: Loss = 0.0380
2026-01-07 19:25:16,672 - INFO -   Batch 1650/2527: Loss = 0.0392
2026-01-07 19:25:40,483 - INFO -   Batch 1700/2527: Loss = 0.0358
2026-01-07 19:26:04,736 - INFO -   Batch 1750/2527: Loss = 0.0449
2026-01-07 19:26:28,546 - INFO -   Batch 1800/2527: Loss = 0.0606
2026-01-07 19:26:52,629 - INFO -   Batch 1850/2527: Loss = 0.0562
2026-01-07 19:27:16,293 - INFO -   Batch 1900/2527: Loss = 0.0443
2026-01-07 19:27:40,250 - INFO -   Batch 1950/2527: Loss = 0.0639
2026-01-07 19:28:03,824 - INFO -   Batch 2000/2527: Loss = 0.0494
2026-01-07 19:28:27,581 - INFO -   Batch 2050/2527: Loss = 0.0400
2026-01-07 19:28:51,188 - INFO -   Batch 2100/2527: Loss = 0.0507
2026-01-07 19:29:15,381 - INFO -   Batch 2150/2527: Loss = 0.0568
2026-01-07 19:29:39,237 - INFO -   Batch 2200/2527: Loss = 0.0493
2026-01-07 19:30:03,036 - INFO -   Batch 2250/2527: Loss = 0.0486
2026-01-07 19:30:26,641 - INFO -   Batch 2300/2527: Loss = 0.0295
2026-01-07 19:30:50,262 - INFO -   Batch 2350/2527: Loss = 0.0411
2026-01-07 19:31:14,249 - INFO -   Batch 2400/2527: Loss = 0.0360
2026-01-07 19:31:38,185 - INFO -   Batch 2450/2527: Loss = 0.0437
2026-01-07 19:32:01,946 - INFO -   Batch 2500/2527: Loss = 0.0524
2026-01-07 19:36:54,000 - INFO - Epoch 104/160: Train Loss: 0.0501, Val Loss: 1.6347, Val mIoU: 40.58%, Val PixelAcc: 79.63%
2026-01-07 19:37:20,150 - INFO -   Batch 50/2527: Loss = 0.0575
2026-01-07 19:37:45,208 - INFO -   Batch 100/2527: Loss = 0.0342
2026-01-07 19:38:10,151 - INFO -   Batch 150/2527: Loss = 0.0582
2026-01-07 19:38:34,979 - INFO -   Batch 200/2527: Loss = 0.0575
2026-01-07 19:38:59,797 - INFO -   Batch 250/2527: Loss = 0.0690
2026-01-07 19:39:24,805 - INFO -   Batch 300/2527: Loss = 0.0555
2026-01-07 19:39:49,616 - INFO -   Batch 350/2527: Loss = 0.0604
2026-01-07 19:40:14,435 - INFO -   Batch 400/2527: Loss = 0.0589
2026-01-07 19:40:39,509 - INFO -   Batch 450/2527: Loss = 0.0392
2026-01-07 19:41:04,972 - INFO -   Batch 500/2527: Loss = 0.0364
2026-01-07 19:41:29,818 - INFO -   Batch 550/2527: Loss = 0.0532
2026-01-07 19:41:54,660 - INFO -   Batch 600/2527: Loss = 0.0630
2026-01-07 19:42:19,749 - INFO -   Batch 650/2527: Loss = 0.0391
2026-01-07 19:42:44,576 - INFO -   Batch 700/2527: Loss = 0.0715
2026-01-07 19:43:09,715 - INFO -   Batch 750/2527: Loss = 0.0606
2026-01-07 19:43:34,702 - INFO -   Batch 800/2527: Loss = 0.0354
2026-01-07 19:43:59,520 - INFO -   Batch 850/2527: Loss = 0.0316
2026-01-07 19:44:24,418 - INFO -   Batch 900/2527: Loss = 0.0574
2026-01-07 19:44:49,702 - INFO -   Batch 950/2527: Loss = 0.0542
2026-01-07 19:45:14,595 - INFO -   Batch 1000/2527: Loss = 0.0498
2026-01-07 19:45:39,923 - INFO -   Batch 1050/2527: Loss = 0.0375
2026-01-07 19:46:05,232 - INFO -   Batch 1100/2527: Loss = 0.0782
2026-01-07 19:46:30,235 - INFO -   Batch 1150/2527: Loss = 0.0597
2026-01-07 19:46:55,099 - INFO -   Batch 1200/2527: Loss = 0.0311
2026-01-07 19:47:20,181 - INFO -   Batch 1250/2527: Loss = 0.0430
2026-01-07 19:47:44,958 - INFO -   Batch 1300/2527: Loss = 0.0386
2026-01-07 19:48:09,829 - INFO -   Batch 1350/2527: Loss = 0.0491
2026-01-07 19:48:35,164 - INFO -   Batch 1400/2527: Loss = 0.0512
2026-01-07 19:49:00,492 - INFO -   Batch 1450/2527: Loss = 0.0621
2026-01-07 19:49:25,917 - INFO -   Batch 1500/2527: Loss = 0.0700
2026-01-07 19:49:50,998 - INFO -   Batch 1550/2527: Loss = 0.0420
2026-01-07 19:50:16,373 - INFO -   Batch 1600/2527: Loss = 0.0287
2026-01-07 19:50:41,736 - INFO -   Batch 1650/2527: Loss = 0.0652
2026-01-07 19:51:07,079 - INFO -   Batch 1700/2527: Loss = 0.0419
2026-01-07 19:51:32,537 - INFO -   Batch 1750/2527: Loss = 0.0445
2026-01-07 19:51:58,057 - INFO -   Batch 1800/2527: Loss = 0.0299
2026-01-07 19:52:23,064 - INFO -   Batch 1850/2527: Loss = 0.0353
2026-01-07 19:52:48,200 - INFO -   Batch 1900/2527: Loss = 0.0646
2026-01-07 19:53:13,722 - INFO -   Batch 1950/2527: Loss = 0.0551
2026-01-07 19:53:38,923 - INFO -   Batch 2000/2527: Loss = 0.0531
2026-01-07 19:54:04,603 - INFO -   Batch 2050/2527: Loss = 0.0405
2026-01-07 19:54:30,084 - INFO -   Batch 2100/2527: Loss = 0.0455
2026-01-07 19:54:55,199 - INFO -   Batch 2150/2527: Loss = 0.0354
2026-01-07 19:55:20,511 - INFO -   Batch 2200/2527: Loss = 0.0284
2026-01-07 19:55:46,411 - INFO -   Batch 2250/2527: Loss = 0.0318
2026-01-07 19:56:11,635 - INFO -   Batch 2300/2527: Loss = 0.0458
2026-01-07 19:56:36,851 - INFO -   Batch 2350/2527: Loss = 0.0331
2026-01-07 19:57:02,450 - INFO -   Batch 2400/2527: Loss = 0.0506
2026-01-07 19:57:28,101 - INFO -   Batch 2450/2527: Loss = 0.0550
2026-01-07 19:57:53,314 - INFO -   Batch 2500/2527: Loss = 0.0469
2026-01-07 20:02:44,840 - INFO - Epoch 105/160: Train Loss: 0.0494, Val Loss: 1.6730, Val mIoU: 40.43%, Val PixelAcc: 79.59%
2026-01-07 20:03:12,878 - INFO -   Batch 50/2527: Loss = 0.0448
2026-01-07 20:03:38,286 - INFO -   Batch 100/2527: Loss = 0.0426
2026-01-07 20:04:03,556 - INFO -   Batch 150/2527: Loss = 0.0587
2026-01-07 20:04:28,696 - INFO -   Batch 200/2527: Loss = 0.0666
2026-01-07 20:04:54,433 - INFO -   Batch 250/2527: Loss = 0.0397
2026-01-07 20:05:19,968 - INFO -   Batch 300/2527: Loss = 0.0343
2026-01-07 20:05:45,069 - INFO -   Batch 350/2527: Loss = 0.0347
2026-01-07 20:06:10,165 - INFO -   Batch 400/2527: Loss = 0.0402
2026-01-07 20:06:35,534 - INFO -   Batch 450/2527: Loss = 0.0563
2026-01-07 20:07:00,575 - INFO -   Batch 500/2527: Loss = 0.0500
2026-01-07 20:07:25,996 - INFO -   Batch 550/2527: Loss = 0.0420
2026-01-07 20:07:51,355 - INFO -   Batch 600/2527: Loss = 0.0461
2026-01-07 20:08:16,668 - INFO -   Batch 650/2527: Loss = 0.0405
2026-01-07 20:08:41,927 - INFO -   Batch 700/2527: Loss = 0.0546
2026-01-07 20:09:07,437 - INFO -   Batch 750/2527: Loss = 0.0590
2026-01-07 20:09:32,681 - INFO -   Batch 800/2527: Loss = 0.0356
2026-01-07 20:09:58,398 - INFO -   Batch 850/2527: Loss = 0.0470
2026-01-07 20:10:23,915 - INFO -   Batch 900/2527: Loss = 0.0499
2026-01-07 20:10:49,429 - INFO -   Batch 950/2527: Loss = 0.0901
2026-01-07 20:11:14,623 - INFO -   Batch 1000/2527: Loss = 0.0416
2026-01-07 20:11:39,753 - INFO -   Batch 1050/2527: Loss = 0.0546
2026-01-07 20:12:04,848 - INFO -   Batch 1100/2527: Loss = 0.0492
2026-01-07 20:12:30,268 - INFO -   Batch 1150/2527: Loss = 0.0449
2026-01-07 20:12:55,917 - INFO -   Batch 1200/2527: Loss = 0.0682
2026-01-07 20:13:21,335 - INFO -   Batch 1250/2527: Loss = 0.0637
2026-01-07 20:13:46,771 - INFO -   Batch 1300/2527: Loss = 0.0567
2026-01-07 20:14:12,257 - INFO -   Batch 1350/2527: Loss = 0.0610
2026-01-07 20:14:37,756 - INFO -   Batch 1400/2527: Loss = 0.0617
2026-01-07 20:15:03,178 - INFO -   Batch 1450/2527: Loss = 0.0535
2026-01-07 20:15:28,245 - INFO -   Batch 1500/2527: Loss = 0.0523
2026-01-07 20:15:53,767 - INFO -   Batch 1550/2527: Loss = 0.0366
2026-01-07 20:16:18,926 - INFO -   Batch 1600/2527: Loss = 0.0302
2026-01-07 20:16:44,454 - INFO -   Batch 1650/2527: Loss = 0.0607
2026-01-07 20:17:09,829 - INFO -   Batch 1700/2527: Loss = 0.0475
2026-01-07 20:17:34,878 - INFO -   Batch 1750/2527: Loss = 0.0400
2026-01-07 20:18:00,160 - INFO -   Batch 1800/2527: Loss = 0.0606
2026-01-07 20:18:25,735 - INFO -   Batch 1850/2527: Loss = 0.0558
2026-01-07 20:18:51,364 - INFO -   Batch 1900/2527: Loss = 0.0498
2026-01-07 20:19:16,514 - INFO -   Batch 1950/2527: Loss = 0.0439
2026-01-07 20:19:41,630 - INFO -   Batch 2000/2527: Loss = 0.0375
2026-01-07 20:20:07,085 - INFO -   Batch 2050/2527: Loss = 0.0535
2026-01-07 20:20:32,318 - INFO -   Batch 2100/2527: Loss = 0.0429
2026-01-07 20:20:57,469 - INFO -   Batch 2150/2527: Loss = 0.0481
2026-01-07 20:21:22,997 - INFO -   Batch 2200/2527: Loss = 0.0385
2026-01-07 20:21:48,346 - INFO -   Batch 2250/2527: Loss = 0.0464
2026-01-07 20:22:13,979 - INFO -   Batch 2300/2527: Loss = 0.0485
2026-01-07 20:22:39,211 - INFO -   Batch 2350/2527: Loss = 0.0268
2026-01-07 20:23:04,303 - INFO -   Batch 2400/2527: Loss = 0.0443
2026-01-07 20:23:29,426 - INFO -   Batch 2450/2527: Loss = 0.0298
2026-01-07 20:23:55,094 - INFO -   Batch 2500/2527: Loss = 0.0356
2026-01-07 20:28:56,558 - INFO - Epoch 106/160: Train Loss: 0.0488, Val Loss: 1.6592, Val mIoU: 40.54%, Val PixelAcc: 79.68%
2026-01-07 20:29:21,447 - INFO -   Batch 50/2527: Loss = 0.0507
2026-01-07 20:29:44,389 - INFO -   Batch 100/2527: Loss = 0.0557
2026-01-07 20:30:07,433 - INFO -   Batch 150/2527: Loss = 0.0361
2026-01-07 20:30:30,357 - INFO -   Batch 200/2527: Loss = 0.0470
2026-01-07 20:30:53,303 - INFO -   Batch 250/2527: Loss = 0.0519
2026-01-07 20:31:16,624 - INFO -   Batch 300/2527: Loss = 0.0543
2026-01-07 20:31:39,530 - INFO -   Batch 350/2527: Loss = 0.0439
2026-01-07 20:32:02,494 - INFO -   Batch 400/2527: Loss = 0.0365
2026-01-07 20:32:25,379 - INFO -   Batch 450/2527: Loss = 0.0366
2026-01-07 20:32:48,388 - INFO -   Batch 500/2527: Loss = 0.0500
2026-01-07 20:33:11,312 - INFO -   Batch 550/2527: Loss = 0.0401
2026-01-07 20:33:34,322 - INFO -   Batch 600/2527: Loss = 0.0375
2026-01-07 20:33:57,301 - INFO -   Batch 650/2527: Loss = 0.0539
2026-01-07 20:34:20,641 - INFO -   Batch 700/2527: Loss = 0.0422
2026-01-07 20:34:43,775 - INFO -   Batch 750/2527: Loss = 0.0412
2026-01-07 20:35:06,687 - INFO -   Batch 800/2527: Loss = 0.0518
2026-01-07 20:35:29,632 - INFO -   Batch 850/2527: Loss = 0.0331
2026-01-07 20:35:52,538 - INFO -   Batch 900/2527: Loss = 0.0367
2026-01-07 20:36:16,286 - INFO -   Batch 950/2527: Loss = 0.0543
2026-01-07 20:36:39,677 - INFO -   Batch 1000/2527: Loss = 0.0423
2026-01-07 20:37:02,592 - INFO -   Batch 1050/2527: Loss = 0.0585
2026-01-07 20:37:25,500 - INFO -   Batch 1100/2527: Loss = 0.0636
2026-01-07 20:37:48,428 - INFO -   Batch 1150/2527: Loss = 0.0621
2026-01-07 20:38:11,334 - INFO -   Batch 1200/2527: Loss = 0.0569
2026-01-07 20:38:34,307 - INFO -   Batch 1250/2527: Loss = 0.0447
2026-01-07 20:38:57,208 - INFO -   Batch 1300/2527: Loss = 0.0320
2026-01-07 20:39:20,127 - INFO -   Batch 1350/2527: Loss = 0.2575
2026-01-07 20:39:43,023 - INFO -   Batch 1400/2527: Loss = 0.0502
2026-01-07 20:40:05,933 - INFO -   Batch 1450/2527: Loss = 0.0679
2026-01-07 20:40:28,921 - INFO -   Batch 1500/2527: Loss = 0.0396
2026-01-07 20:40:51,837 - INFO -   Batch 1550/2527: Loss = 0.0265
2026-01-07 20:41:14,773 - INFO -   Batch 1600/2527: Loss = 0.0409
2026-01-07 20:41:37,691 - INFO -   Batch 1650/2527: Loss = 0.0503
2026-01-07 20:42:00,739 - INFO -   Batch 1700/2527: Loss = 0.0556
2026-01-07 20:42:23,976 - INFO -   Batch 1750/2527: Loss = 0.0502
2026-01-07 20:42:47,424 - INFO -   Batch 1800/2527: Loss = 0.0426
2026-01-07 20:43:10,679 - INFO -   Batch 1850/2527: Loss = 0.0535
2026-01-07 20:43:33,574 - INFO -   Batch 1900/2527: Loss = 0.0401
2026-01-07 20:43:56,467 - INFO -   Batch 1950/2527: Loss = 0.0708
2026-01-07 20:44:19,463 - INFO -   Batch 2000/2527: Loss = 0.0614
2026-01-07 20:44:42,398 - INFO -   Batch 2050/2527: Loss = 0.0450
2026-01-07 20:45:05,353 - INFO -   Batch 2100/2527: Loss = 0.0836
2026-01-07 20:45:28,624 - INFO -   Batch 2150/2527: Loss = 0.0552
2026-01-07 20:45:51,529 - INFO -   Batch 2200/2527: Loss = 0.0384
2026-01-07 20:46:14,486 - INFO -   Batch 2250/2527: Loss = 0.0311
2026-01-07 20:46:37,515 - INFO -   Batch 2300/2527: Loss = 0.0356
2026-01-07 20:47:00,454 - INFO -   Batch 2350/2527: Loss = 0.0603
2026-01-07 20:47:23,406 - INFO -   Batch 2400/2527: Loss = 0.0724
2026-01-07 20:47:46,938 - INFO -   Batch 2450/2527: Loss = 0.0346
2026-01-07 20:48:10,296 - INFO -   Batch 2500/2527: Loss = 0.0411
2026-01-07 20:52:49,081 - INFO - Epoch 107/160: Train Loss: 0.0484, Val Loss: 1.7016, Val mIoU: 40.33%, Val PixelAcc: 79.54%
2026-01-07 20:53:14,409 - INFO -   Batch 50/2527: Loss = 0.0454
2026-01-07 20:53:38,002 - INFO -   Batch 100/2527: Loss = 0.0435
2026-01-07 20:54:01,922 - INFO -   Batch 150/2527: Loss = 0.0454
2026-01-07 20:54:25,467 - INFO -   Batch 200/2527: Loss = 0.0216
2026-01-07 20:54:49,239 - INFO -   Batch 250/2527: Loss = 0.0706
2026-01-07 20:55:12,999 - INFO -   Batch 300/2527: Loss = 0.0478
2026-01-07 20:55:36,845 - INFO -   Batch 350/2527: Loss = 0.0566
2026-01-07 20:56:00,831 - INFO -   Batch 400/2527: Loss = 0.0502
2026-01-07 20:56:24,617 - INFO -   Batch 450/2527: Loss = 0.1061
2026-01-07 20:56:48,205 - INFO -   Batch 500/2527: Loss = 0.0658
2026-01-07 20:57:11,822 - INFO -   Batch 550/2527: Loss = 0.0472
2026-01-07 20:57:35,807 - INFO -   Batch 600/2527: Loss = 0.0419
2026-01-07 20:57:59,760 - INFO -   Batch 650/2527: Loss = 0.0390
2026-01-07 20:58:23,515 - INFO -   Batch 700/2527: Loss = 0.0397
2026-01-07 20:58:47,132 - INFO -   Batch 750/2527: Loss = 0.0453
2026-01-07 20:59:10,957 - INFO -   Batch 800/2527: Loss = 0.0550
2026-01-07 20:59:34,881 - INFO -   Batch 850/2527: Loss = 0.0544
2026-01-07 20:59:58,701 - INFO -   Batch 900/2527: Loss = 0.0464
2026-01-07 21:00:22,300 - INFO -   Batch 950/2527: Loss = 0.0452
2026-01-07 21:00:45,930 - INFO -   Batch 1000/2527: Loss = 0.0473
2026-01-07 21:01:09,920 - INFO -   Batch 1050/2527: Loss = 0.0512
2026-01-07 21:01:33,871 - INFO -   Batch 1100/2527: Loss = 0.0493
2026-01-07 21:01:57,627 - INFO -   Batch 1150/2527: Loss = 0.0379
2026-01-07 21:02:21,238 - INFO -   Batch 1200/2527: Loss = 0.0264
2026-01-07 21:02:45,132 - INFO -   Batch 1250/2527: Loss = 0.0500
2026-01-07 21:03:09,006 - INFO -   Batch 1300/2527: Loss = 0.0460
2026-01-07 21:03:32,801 - INFO -   Batch 1350/2527: Loss = 0.0548
2026-01-07 21:03:56,781 - INFO -   Batch 1400/2527: Loss = 0.0416
2026-01-07 21:04:20,403 - INFO -   Batch 1450/2527: Loss = 0.0592
2026-01-07 21:04:44,360 - INFO -   Batch 1500/2527: Loss = 0.0383
2026-01-07 21:05:07,930 - INFO -   Batch 1550/2527: Loss = 0.0358
2026-01-07 21:05:31,677 - INFO -   Batch 1600/2527: Loss = 0.0477
2026-01-07 21:05:55,281 - INFO -   Batch 1650/2527: Loss = 0.0409
2026-01-07 21:06:19,131 - INFO -   Batch 1700/2527: Loss = 0.0879
2026-01-07 21:06:42,999 - INFO -   Batch 1750/2527: Loss = 0.0474
2026-01-07 21:07:06,785 - INFO -   Batch 1800/2527: Loss = 0.0469
2026-01-07 21:07:30,386 - INFO -   Batch 1850/2527: Loss = 0.0297
2026-01-07 21:07:54,035 - INFO -   Batch 1900/2527: Loss = 0.0671
2026-01-07 21:08:17,999 - INFO -   Batch 1950/2527: Loss = 0.0450
2026-01-07 21:08:41,606 - INFO -   Batch 2000/2527: Loss = 0.0721
2026-01-07 21:09:05,382 - INFO -   Batch 2050/2527: Loss = 0.0551
2026-01-07 21:09:29,370 - INFO -   Batch 2100/2527: Loss = 0.0643
2026-01-07 21:09:53,197 - INFO -   Batch 2150/2527: Loss = 0.0467
2026-01-07 21:10:17,052 - INFO -   Batch 2200/2527: Loss = 0.0404
2026-01-07 21:10:40,882 - INFO -   Batch 2250/2527: Loss = 0.0362
2026-01-07 21:11:04,488 - INFO -   Batch 2300/2527: Loss = 0.0487
2026-01-07 21:11:28,113 - INFO -   Batch 2350/2527: Loss = 0.0711
2026-01-07 21:11:52,162 - INFO -   Batch 2400/2527: Loss = 0.0613
2026-01-07 21:12:15,770 - INFO -   Batch 2450/2527: Loss = 0.0397
2026-01-07 21:12:39,521 - INFO -   Batch 2500/2527: Loss = 0.0410
2026-01-07 21:17:32,371 - INFO - Epoch 108/160: Train Loss: 0.0479, Val Loss: 1.6942, Val mIoU: 40.37%, Val PixelAcc: 79.62%
2026-01-07 21:17:58,400 - INFO -   Batch 50/2527: Loss = 0.0415
2026-01-07 21:18:23,477 - INFO -   Batch 100/2527: Loss = 0.0430
2026-01-07 21:18:48,442 - INFO -   Batch 150/2527: Loss = 0.0440
2026-01-07 21:19:13,256 - INFO -   Batch 200/2527: Loss = 0.0362
2026-01-07 21:19:38,237 - INFO -   Batch 250/2527: Loss = 0.0464
2026-01-07 21:20:03,247 - INFO -   Batch 300/2527: Loss = 0.0327
2026-01-07 21:20:28,117 - INFO -   Batch 350/2527: Loss = 0.0603
2026-01-07 21:20:53,315 - INFO -   Batch 400/2527: Loss = 0.0378
2026-01-07 21:21:18,462 - INFO -   Batch 450/2527: Loss = 0.0523
2026-01-07 21:21:43,445 - INFO -   Batch 500/2527: Loss = 0.0471
2026-01-07 21:22:08,281 - INFO -   Batch 550/2527: Loss = 0.0678
2026-01-07 21:22:33,144 - INFO -   Batch 600/2527: Loss = 0.0442
2026-01-07 21:22:57,871 - INFO -   Batch 650/2527: Loss = 0.0547
2026-01-07 21:23:23,119 - INFO -   Batch 700/2527: Loss = 0.0535
2026-01-07 21:23:48,278 - INFO -   Batch 750/2527: Loss = 0.0374
2026-01-07 21:24:13,276 - INFO -   Batch 800/2527: Loss = 0.0513
2026-01-07 21:24:38,115 - INFO -   Batch 850/2527: Loss = 0.0767
2026-01-07 21:25:02,978 - INFO -   Batch 900/2527: Loss = 0.0480
2026-01-07 21:25:27,966 - INFO -   Batch 950/2527: Loss = 0.0529
2026-01-07 21:25:52,782 - INFO -   Batch 1000/2527: Loss = 0.0732
2026-01-07 21:26:17,616 - INFO -   Batch 1050/2527: Loss = 0.0499
2026-01-07 21:26:42,773 - INFO -   Batch 1100/2527: Loss = 0.0599
2026-01-07 21:27:07,749 - INFO -   Batch 1150/2527: Loss = 0.0316
2026-01-07 21:27:32,628 - INFO -   Batch 1200/2527: Loss = 0.0522
2026-01-07 21:27:57,446 - INFO -   Batch 1250/2527: Loss = 0.0368
2026-01-07 21:28:22,168 - INFO -   Batch 1300/2527: Loss = 0.0513
2026-01-07 21:28:47,016 - INFO -   Batch 1350/2527: Loss = 0.0481
2026-01-07 21:29:12,131 - INFO -   Batch 1400/2527: Loss = 0.0270
2026-01-07 21:29:37,471 - INFO -   Batch 1450/2527: Loss = 0.0392
2026-01-07 21:30:02,290 - INFO -   Batch 1500/2527: Loss = 0.0490
2026-01-07 21:30:27,182 - INFO -   Batch 1550/2527: Loss = 0.0516
2026-01-07 21:30:52,199 - INFO -   Batch 1600/2527: Loss = 0.0447
2026-01-07 21:31:17,041 - INFO -   Batch 1650/2527: Loss = 0.0607
2026-01-07 21:31:41,877 - INFO -   Batch 1700/2527: Loss = 0.0532
2026-01-07 21:32:07,000 - INFO -   Batch 1750/2527: Loss = 0.0456
2026-01-07 21:32:31,973 - INFO -   Batch 1800/2527: Loss = 0.0357
2026-01-07 21:32:57,157 - INFO -   Batch 1850/2527: Loss = 0.0583
2026-01-07 21:33:22,019 - INFO -   Batch 1900/2527: Loss = 0.0321
2026-01-07 21:33:46,810 - INFO -   Batch 1950/2527: Loss = 0.0390
2026-01-07 21:34:11,650 - INFO -   Batch 2000/2527: Loss = 0.0458
2026-01-07 21:34:36,758 - INFO -   Batch 2050/2527: Loss = 0.0482
2026-01-07 21:35:01,706 - INFO -   Batch 2100/2527: Loss = 0.0418
2026-01-07 21:35:26,890 - INFO -   Batch 2150/2527: Loss = 0.0337
2026-01-07 21:35:51,727 - INFO -   Batch 2200/2527: Loss = 0.0351
2026-01-07 21:36:16,765 - INFO -   Batch 2250/2527: Loss = 0.0495
2026-01-07 21:36:41,604 - INFO -   Batch 2300/2527: Loss = 0.0480
2026-01-07 21:37:06,441 - INFO -   Batch 2350/2527: Loss = 0.0344
2026-01-07 21:37:31,598 - INFO -   Batch 2400/2527: Loss = 0.0433
2026-01-07 21:37:56,563 - INFO -   Batch 2450/2527: Loss = 0.0561
2026-01-07 21:38:21,377 - INFO -   Batch 2500/2527: Loss = 0.0546
2026-01-07 21:43:00,752 - INFO - Epoch 109/160: Train Loss: 0.0474, Val Loss: 1.6715, Val mIoU: 40.46%, Val PixelAcc: 79.58%
2026-01-07 21:43:27,045 - INFO -   Batch 50/2527: Loss = 0.0465
2026-01-07 21:43:52,029 - INFO -   Batch 100/2527: Loss = 0.0498
2026-01-07 21:44:16,821 - INFO -   Batch 150/2527: Loss = 0.0535
2026-01-07 21:44:41,660 - INFO -   Batch 200/2527: Loss = 0.0452
2026-01-07 21:45:06,765 - INFO -   Batch 250/2527: Loss = 0.0684
2026-01-07 21:45:31,760 - INFO -   Batch 300/2527: Loss = 0.0765
2026-01-07 21:45:56,946 - INFO -   Batch 350/2527: Loss = 0.0370
2026-01-07 21:46:21,806 - INFO -   Batch 400/2527: Loss = 0.0552
2026-01-07 21:46:46,556 - INFO -   Batch 450/2527: Loss = 0.0698
2026-01-07 21:47:11,414 - INFO -   Batch 500/2527: Loss = 0.0612
2026-01-07 21:47:36,534 - INFO -   Batch 550/2527: Loss = 0.0203
2026-01-07 21:48:01,524 - INFO -   Batch 600/2527: Loss = 0.0447
2026-01-07 21:48:26,346 - INFO -   Batch 650/2527: Loss = 0.0671
2026-01-07 21:48:51,201 - INFO -   Batch 700/2527: Loss = 0.0411
2026-01-07 21:49:16,243 - INFO -   Batch 750/2527: Loss = 0.0476
2026-01-07 21:49:41,451 - INFO -   Batch 800/2527: Loss = 0.0350
2026-01-07 21:50:06,317 - INFO -   Batch 850/2527: Loss = 0.0390
2026-01-07 21:50:31,434 - INFO -   Batch 900/2527: Loss = 0.0496
2026-01-07 21:50:56,409 - INFO -   Batch 950/2527: Loss = 0.0409
2026-01-07 21:51:21,238 - INFO -   Batch 1000/2527: Loss = 0.0479
2026-01-07 21:51:46,103 - INFO -   Batch 1050/2527: Loss = 0.0528
2026-01-07 21:52:11,212 - INFO -   Batch 1100/2527: Loss = 0.0416
2026-01-07 21:52:36,042 - INFO -   Batch 1150/2527: Loss = 0.0525
2026-01-07 21:53:01,153 - INFO -   Batch 1200/2527: Loss = 0.0344
2026-01-07 21:53:26,104 - INFO -   Batch 1250/2527: Loss = 0.0533
2026-01-07 21:53:50,938 - INFO -   Batch 1300/2527: Loss = 0.0538
2026-01-07 21:54:15,794 - INFO -   Batch 1350/2527: Loss = 0.0409
2026-01-07 21:54:40,810 - INFO -   Batch 1400/2527: Loss = 0.0462
2026-01-07 21:55:05,655 - INFO -   Batch 1450/2527: Loss = 0.0509
2026-01-07 21:55:30,497 - INFO -   Batch 1500/2527: Loss = 0.0546
2026-01-07 21:55:55,590 - INFO -   Batch 1550/2527: Loss = 0.0320
2026-01-07 21:56:20,553 - INFO -   Batch 1600/2527: Loss = 0.0379
2026-01-07 21:56:45,392 - INFO -   Batch 1650/2527: Loss = 0.0412
2026-01-07 21:57:10,245 - INFO -   Batch 1700/2527: Loss = 0.0341
2026-01-07 21:57:34,988 - INFO -   Batch 1750/2527: Loss = 0.0488
2026-01-07 21:58:00,208 - INFO -   Batch 1800/2527: Loss = 0.0317
2026-01-07 21:58:25,326 - INFO -   Batch 1850/2527: Loss = 0.0491
2026-01-07 21:58:50,285 - INFO -   Batch 1900/2527: Loss = 0.0479
2026-01-07 21:59:15,128 - INFO -   Batch 1950/2527: Loss = 0.0503
2026-01-07 21:59:39,997 - INFO -   Batch 2000/2527: Loss = 0.0343
2026-01-07 22:00:05,011 - INFO -   Batch 2050/2527: Loss = 0.0256
2026-01-07 22:00:29,845 - INFO -   Batch 2100/2527: Loss = 0.0458
2026-01-07 22:00:54,731 - INFO -   Batch 2150/2527: Loss = 0.0377
2026-01-07 22:01:19,840 - INFO -   Batch 2200/2527: Loss = 0.0428
2026-01-07 22:01:45,157 - INFO -   Batch 2250/2527: Loss = 0.0537
2026-01-07 22:02:10,020 - INFO -   Batch 2300/2527: Loss = 0.0592
2026-01-07 22:02:34,872 - INFO -   Batch 2350/2527: Loss = 0.0306
2026-01-07 22:02:59,600 - INFO -   Batch 2400/2527: Loss = 0.1421
2026-01-07 22:03:24,428 - INFO -   Batch 2450/2527: Loss = 0.0412
2026-01-07 22:03:49,930 - INFO -   Batch 2500/2527: Loss = 0.0472
2026-01-07 22:08:43,193 - INFO - Epoch 110/160: Train Loss: 0.0468, Val Loss: 1.6918, Val mIoU: 40.41%, Val PixelAcc: 79.56%
2026-01-07 22:08:45,305 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_110.pth
2026-01-07 22:08:45,307 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_110.pth
2026-01-07 22:09:10,275 - INFO -   Batch 50/2527: Loss = 0.0397
2026-01-07 22:09:33,180 - INFO -   Batch 100/2527: Loss = 0.0532
2026-01-07 22:09:56,112 - INFO -   Batch 150/2527: Loss = 0.0397
2026-01-07 22:10:19,093 - INFO -   Batch 200/2527: Loss = 0.0576
2026-01-07 22:10:42,048 - INFO -   Batch 250/2527: Loss = 0.0534
2026-01-07 22:11:05,374 - INFO -   Batch 300/2527: Loss = 0.0471
2026-01-07 22:11:28,322 - INFO -   Batch 350/2527: Loss = 0.0304
2026-01-07 22:11:51,273 - INFO -   Batch 400/2527: Loss = 0.0350
2026-01-07 22:12:14,235 - INFO -   Batch 450/2527: Loss = 0.0333
2026-01-07 22:12:37,211 - INFO -   Batch 500/2527: Loss = 0.0398
2026-01-07 22:13:00,162 - INFO -   Batch 550/2527: Loss = 0.0449
2026-01-07 22:13:23,123 - INFO -   Batch 600/2527: Loss = 0.0438
2026-01-07 22:13:46,071 - INFO -   Batch 650/2527: Loss = 0.0458
2026-01-07 22:14:09,042 - INFO -   Batch 700/2527: Loss = 0.0286
2026-01-07 22:14:32,001 - INFO -   Batch 750/2527: Loss = 0.0597
2026-01-07 22:14:54,964 - INFO -   Batch 800/2527: Loss = 0.0658
2026-01-07 22:15:17,933 - INFO -   Batch 850/2527: Loss = 0.0273
2026-01-07 22:15:40,893 - INFO -   Batch 900/2527: Loss = 0.0543
2026-01-07 22:16:03,857 - INFO -   Batch 950/2527: Loss = 0.0708
2026-01-07 22:16:26,821 - INFO -   Batch 1000/2527: Loss = 0.0493
2026-01-07 22:16:50,141 - INFO -   Batch 1050/2527: Loss = 0.0390
2026-01-07 22:17:13,136 - INFO -   Batch 1100/2527: Loss = 0.0615
2026-01-07 22:17:36,097 - INFO -   Batch 1150/2527: Loss = 0.0417
2026-01-07 22:17:59,060 - INFO -   Batch 1200/2527: Loss = 0.0482
2026-01-07 22:18:22,017 - INFO -   Batch 1250/2527: Loss = 0.0331
2026-01-07 22:18:45,000 - INFO -   Batch 1300/2527: Loss = 0.0384
2026-01-07 22:19:08,003 - INFO -   Batch 1350/2527: Loss = 0.0446
2026-01-07 22:19:30,970 - INFO -   Batch 1400/2527: Loss = 0.0461
2026-01-07 22:19:54,291 - INFO -   Batch 1450/2527: Loss = 0.0445
2026-01-07 22:20:17,287 - INFO -   Batch 1500/2527: Loss = 0.0314
2026-01-07 22:20:40,260 - INFO -   Batch 1550/2527: Loss = 0.0530
2026-01-07 22:21:03,221 - INFO -   Batch 1600/2527: Loss = 0.0640
2026-01-07 22:21:26,188 - INFO -   Batch 1650/2527: Loss = 0.0385
2026-01-07 22:21:49,158 - INFO -   Batch 1700/2527: Loss = 0.0468
2026-01-07 22:22:12,499 - INFO -   Batch 1750/2527: Loss = 0.0519
2026-01-07 22:22:35,469 - INFO -   Batch 1800/2527: Loss = 0.0563
2026-01-07 22:22:58,435 - INFO -   Batch 1850/2527: Loss = 0.0487
2026-01-07 22:23:21,390 - INFO -   Batch 1900/2527: Loss = 0.0451
2026-01-07 22:23:44,349 - INFO -   Batch 1950/2527: Loss = 0.0454
2026-01-07 22:24:07,308 - INFO -   Batch 2000/2527: Loss = 0.0560
2026-01-07 22:24:30,266 - INFO -   Batch 2050/2527: Loss = 0.0485
2026-01-07 22:24:53,225 - INFO -   Batch 2100/2527: Loss = 0.0378
2026-01-07 22:25:16,191 - INFO -   Batch 2150/2527: Loss = 0.0467
2026-01-07 22:25:39,147 - INFO -   Batch 2200/2527: Loss = 0.0700
2026-01-07 22:26:02,120 - INFO -   Batch 2250/2527: Loss = 0.0579
2026-01-07 22:26:25,083 - INFO -   Batch 2300/2527: Loss = 0.0430
2026-01-07 22:26:48,046 - INFO -   Batch 2350/2527: Loss = 0.0447
2026-01-07 22:27:11,009 - INFO -   Batch 2400/2527: Loss = 0.0384
2026-01-07 22:27:33,990 - INFO -   Batch 2450/2527: Loss = 0.0528
2026-01-07 22:27:57,327 - INFO -   Batch 2500/2527: Loss = 0.0295
2026-01-07 22:32:36,471 - INFO - Epoch 111/160: Train Loss: 0.0465, Val Loss: 1.7125, Val mIoU: 40.60%, Val PixelAcc: 79.60%
2026-01-07 22:33:01,703 - INFO -   Batch 50/2527: Loss = 0.0388
2026-01-07 22:33:25,278 - INFO -   Batch 100/2527: Loss = 0.0279
2026-01-07 22:33:49,220 - INFO -   Batch 150/2527: Loss = 0.0321
2026-01-07 22:34:12,834 - INFO -   Batch 200/2527: Loss = 0.0678
2026-01-07 22:34:36,573 - INFO -   Batch 250/2527: Loss = 0.0512
2026-01-07 22:35:00,187 - INFO -   Batch 300/2527: Loss = 0.0305
2026-01-07 22:35:24,368 - INFO -   Batch 350/2527: Loss = 0.0264
2026-01-07 22:35:48,243 - INFO -   Batch 400/2527: Loss = 0.0418
2026-01-07 22:36:12,212 - INFO -   Batch 450/2527: Loss = 0.0540
2026-01-07 22:36:35,988 - INFO -   Batch 500/2527: Loss = 0.0476
2026-01-07 22:36:59,751 - INFO -   Batch 550/2527: Loss = 0.0257
2026-01-07 22:37:23,873 - INFO -   Batch 600/2527: Loss = 0.0545
2026-01-07 22:37:48,185 - INFO -   Batch 650/2527: Loss = 0.0334
2026-01-07 22:38:12,126 - INFO -   Batch 700/2527: Loss = 0.0385
2026-01-07 22:38:35,851 - INFO -   Batch 750/2527: Loss = 0.0421
2026-01-07 22:38:59,972 - INFO -   Batch 800/2527: Loss = 0.0430
2026-01-07 22:39:23,936 - INFO -   Batch 850/2527: Loss = 0.0417
2026-01-07 22:39:48,300 - INFO -   Batch 900/2527: Loss = 0.0430
2026-01-07 22:40:12,103 - INFO -   Batch 950/2527: Loss = 0.0351
2026-01-07 22:40:36,152 - INFO -   Batch 1000/2527: Loss = 0.0397
2026-01-07 22:41:00,458 - INFO -   Batch 1050/2527: Loss = 0.0634
2026-01-07 22:41:24,326 - INFO -   Batch 1100/2527: Loss = 0.0433
2026-01-07 22:41:48,442 - INFO -   Batch 1150/2527: Loss = 0.0381
2026-01-07 22:42:12,444 - INFO -   Batch 1200/2527: Loss = 0.0570
2026-01-07 22:42:36,692 - INFO -   Batch 1250/2527: Loss = 0.0321
2026-01-07 22:43:00,738 - INFO -   Batch 1300/2527: Loss = 0.0427
2026-01-07 22:43:24,797 - INFO -   Batch 1350/2527: Loss = 0.0408
2026-01-07 22:43:49,352 - INFO -   Batch 1400/2527: Loss = 0.0483
2026-01-07 22:44:13,375 - INFO -   Batch 1450/2527: Loss = 0.0431
2026-01-07 22:44:37,779 - INFO -   Batch 1500/2527: Loss = 0.0397
2026-01-07 22:45:01,528 - INFO -   Batch 1550/2527: Loss = 0.0319
2026-01-07 22:45:25,822 - INFO -   Batch 1600/2527: Loss = 0.0430
2026-01-07 22:45:49,788 - INFO -   Batch 1650/2527: Loss = 0.0470
2026-01-07 22:46:13,982 - INFO -   Batch 1700/2527: Loss = 0.0344
2026-01-07 22:46:39,098 - INFO -   Batch 1750/2527: Loss = 0.0478
2026-01-07 22:47:03,265 - INFO -   Batch 1800/2527: Loss = 0.0466
2026-01-07 22:47:27,174 - INFO -   Batch 1850/2527: Loss = 0.0418
2026-01-07 22:47:50,996 - INFO -   Batch 1900/2527: Loss = 0.0327
2026-01-07 22:48:15,349 - INFO -   Batch 1950/2527: Loss = 0.0399
2026-01-07 22:48:39,381 - INFO -   Batch 2000/2527: Loss = 0.0490
2026-01-07 22:49:03,356 - INFO -   Batch 2050/2527: Loss = 0.0511
2026-01-07 22:49:27,347 - INFO -   Batch 2100/2527: Loss = 0.0446
2026-01-07 22:49:51,623 - INFO -   Batch 2150/2527: Loss = 0.0442
2026-01-07 22:50:15,742 - INFO -   Batch 2200/2527: Loss = 0.0470
2026-01-07 22:50:39,802 - INFO -   Batch 2250/2527: Loss = 0.0431
2026-01-07 22:51:03,849 - INFO -   Batch 2300/2527: Loss = 0.0447
2026-01-07 22:51:28,052 - INFO -   Batch 2350/2527: Loss = 0.0398
2026-01-07 22:51:52,586 - INFO -   Batch 2400/2527: Loss = 0.0588
2026-01-07 22:52:16,968 - INFO -   Batch 2450/2527: Loss = 0.0323
2026-01-07 22:52:41,183 - INFO -   Batch 2500/2527: Loss = 0.0372
2026-01-07 22:57:47,081 - INFO - Epoch 112/160: Train Loss: 0.0459, Val Loss: 1.7201, Val mIoU: 40.26%, Val PixelAcc: 79.58%
2026-01-07 22:58:13,554 - INFO -   Batch 50/2527: Loss = 0.0473
2026-01-07 22:58:38,829 - INFO -   Batch 100/2527: Loss = 0.0499
2026-01-07 22:59:03,921 - INFO -   Batch 150/2527: Loss = 0.0304
2026-01-07 22:59:28,896 - INFO -   Batch 200/2527: Loss = 0.0425
2026-01-07 22:59:54,187 - INFO -   Batch 250/2527: Loss = 0.0493
2026-01-07 23:00:19,298 - INFO -   Batch 300/2527: Loss = 0.0434
2026-01-07 23:00:44,749 - INFO -   Batch 350/2527: Loss = 0.0441
2026-01-07 23:01:09,734 - INFO -   Batch 400/2527: Loss = 0.0500
2026-01-07 23:01:35,042 - INFO -   Batch 450/2527: Loss = 0.0395
2026-01-07 23:02:00,116 - INFO -   Batch 500/2527: Loss = 0.0545
2026-01-07 23:02:25,109 - INFO -   Batch 550/2527: Loss = 0.0371
2026-01-07 23:02:49,976 - INFO -   Batch 600/2527: Loss = 0.0416
2026-01-07 23:03:15,146 - INFO -   Batch 650/2527: Loss = 0.0291
2026-01-07 23:03:40,157 - INFO -   Batch 700/2527: Loss = 0.0559
2026-01-07 23:04:05,351 - INFO -   Batch 750/2527: Loss = 0.0361
2026-01-07 23:04:30,505 - INFO -   Batch 800/2527: Loss = 0.0551
2026-01-07 23:04:55,376 - INFO -   Batch 850/2527: Loss = 0.0564
2026-01-07 23:05:20,266 - INFO -   Batch 900/2527: Loss = 0.0309
2026-01-07 23:05:45,555 - INFO -   Batch 950/2527: Loss = 0.0509
2026-01-07 23:06:10,483 - INFO -   Batch 1000/2527: Loss = 0.0359
2026-01-07 23:06:35,502 - INFO -   Batch 1050/2527: Loss = 0.0423
2026-01-07 23:07:00,643 - INFO -   Batch 1100/2527: Loss = 0.0544
2026-01-07 23:07:25,756 - INFO -   Batch 1150/2527: Loss = 0.0501
2026-01-07 23:07:50,646 - INFO -   Batch 1200/2527: Loss = 0.0358
2026-01-07 23:08:15,577 - INFO -   Batch 1250/2527: Loss = 0.0447
2026-01-07 23:08:40,417 - INFO -   Batch 1300/2527: Loss = 0.0409
2026-01-07 23:09:05,350 - INFO -   Batch 1350/2527: Loss = 0.0708
2026-01-07 23:09:31,288 - INFO -   Batch 1400/2527: Loss = 0.0567
2026-01-07 23:09:56,322 - INFO -   Batch 1450/2527: Loss = 0.0586
2026-01-07 23:10:21,189 - INFO -   Batch 1500/2527: Loss = 0.0248
2026-01-07 23:10:46,332 - INFO -   Batch 1550/2527: Loss = 0.0409
2026-01-07 23:11:11,393 - INFO -   Batch 1600/2527: Loss = 0.0238
2026-01-07 23:11:36,293 - INFO -   Batch 1650/2527: Loss = 0.0451
2026-01-07 23:12:01,301 - INFO -   Batch 1700/2527: Loss = 0.0579
2026-01-07 23:12:26,674 - INFO -   Batch 1750/2527: Loss = 0.0446
2026-01-07 23:12:52,076 - INFO -   Batch 1800/2527: Loss = 0.0300
2026-01-07 23:13:17,236 - INFO -   Batch 1850/2527: Loss = 0.0486
2026-01-07 23:13:42,358 - INFO -   Batch 1900/2527: Loss = 0.0411
2026-01-07 23:14:07,251 - INFO -   Batch 1950/2527: Loss = 0.0677
2026-01-07 23:14:32,135 - INFO -   Batch 2000/2527: Loss = 0.0198
2026-01-07 23:14:57,265 - INFO -   Batch 2050/2527: Loss = 0.0611
2026-01-07 23:15:22,687 - INFO -   Batch 2100/2527: Loss = 0.0509
2026-01-07 23:15:47,583 - INFO -   Batch 2150/2527: Loss = 0.0596
2026-01-07 23:16:12,509 - INFO -   Batch 2200/2527: Loss = 0.0568
2026-01-07 23:16:37,609 - INFO -   Batch 2250/2527: Loss = 0.0576
2026-01-07 23:17:02,537 - INFO -   Batch 2300/2527: Loss = 0.0431
2026-01-07 23:17:27,345 - INFO -   Batch 2350/2527: Loss = 0.0462
2026-01-07 23:17:52,555 - INFO -   Batch 2400/2527: Loss = 0.0626
2026-01-07 23:18:17,558 - INFO -   Batch 2450/2527: Loss = 0.0435
2026-01-07 23:18:42,499 - INFO -   Batch 2500/2527: Loss = 0.0426
2026-01-07 23:23:21,564 - INFO - Epoch 113/160: Train Loss: 0.0455, Val Loss: 1.7572, Val mIoU: 40.38%, Val PixelAcc: 79.62%
2026-01-07 23:23:47,978 - INFO -   Batch 50/2527: Loss = 0.0679
2026-01-07 23:24:12,947 - INFO -   Batch 100/2527: Loss = 0.0355
2026-01-07 23:24:37,747 - INFO -   Batch 150/2527: Loss = 0.0463
2026-01-07 23:25:02,619 - INFO -   Batch 200/2527: Loss = 0.0257
2026-01-07 23:25:27,703 - INFO -   Batch 250/2527: Loss = 0.0633
2026-01-07 23:25:53,034 - INFO -   Batch 300/2527: Loss = 0.0559
2026-01-07 23:26:17,849 - INFO -   Batch 350/2527: Loss = 0.0290
2026-01-07 23:26:42,690 - INFO -   Batch 400/2527: Loss = 0.0342
2026-01-07 23:27:07,441 - INFO -   Batch 450/2527: Loss = 0.0426
2026-01-07 23:27:32,295 - INFO -   Batch 500/2527: Loss = 0.0408
2026-01-07 23:27:57,417 - INFO -   Batch 550/2527: Loss = 0.0295
2026-01-07 23:28:22,378 - INFO -   Batch 600/2527: Loss = 0.0391
2026-01-07 23:28:47,180 - INFO -   Batch 650/2527: Loss = 0.0357
2026-01-07 23:29:12,018 - INFO -   Batch 700/2527: Loss = 0.0581
2026-01-07 23:29:37,395 - INFO -   Batch 750/2527: Loss = 0.0488
2026-01-07 23:30:02,225 - INFO -   Batch 800/2527: Loss = 0.0820
2026-01-07 23:30:27,060 - INFO -   Batch 850/2527: Loss = 0.0505
2026-01-07 23:30:52,172 - INFO -   Batch 900/2527: Loss = 0.0466
2026-01-07 23:31:17,182 - INFO -   Batch 950/2527: Loss = 0.0431
2026-01-07 23:31:42,382 - INFO -   Batch 1000/2527: Loss = 0.0355
2026-01-07 23:32:07,247 - INFO -   Batch 1050/2527: Loss = 0.0454
2026-01-07 23:32:31,997 - INFO -   Batch 1100/2527: Loss = 0.0553
2026-01-07 23:32:56,824 - INFO -   Batch 1150/2527: Loss = 0.0466
2026-01-07 23:33:21,955 - INFO -   Batch 1200/2527: Loss = 0.0526
2026-01-07 23:33:46,953 - INFO -   Batch 1250/2527: Loss = 0.0349
2026-01-07 23:34:11,777 - INFO -   Batch 1300/2527: Loss = 0.0412
2026-01-07 23:34:36,621 - INFO -   Batch 1350/2527: Loss = 0.0467
2026-01-07 23:35:01,664 - INFO -   Batch 1400/2527: Loss = 0.0321
2026-01-07 23:35:26,499 - INFO -   Batch 1450/2527: Loss = 0.0346
2026-01-07 23:35:51,348 - INFO -   Batch 1500/2527: Loss = 0.0383
2026-01-07 23:36:16,473 - INFO -   Batch 1550/2527: Loss = 0.0440
2026-01-07 23:36:41,450 - INFO -   Batch 1600/2527: Loss = 0.0389
2026-01-07 23:37:06,283 - INFO -   Batch 1650/2527: Loss = 0.0389
2026-01-07 23:37:31,136 - INFO -   Batch 1700/2527: Loss = 0.0341
2026-01-07 23:37:56,226 - INFO -   Batch 1750/2527: Loss = 0.0456
2026-01-07 23:38:21,077 - INFO -   Batch 1800/2527: Loss = 0.0466
2026-01-07 23:38:46,229 - INFO -   Batch 1850/2527: Loss = 0.0410
2026-01-07 23:39:11,193 - INFO -   Batch 1900/2527: Loss = 0.0597
2026-01-07 23:39:36,013 - INFO -   Batch 1950/2527: Loss = 0.0473
2026-01-07 23:40:00,860 - INFO -   Batch 2000/2527: Loss = 0.0426
2026-01-07 23:40:25,892 - INFO -   Batch 2050/2527: Loss = 0.0530
2026-01-07 23:40:50,716 - INFO -   Batch 2100/2527: Loss = 0.0432
2026-01-07 23:41:15,542 - INFO -   Batch 2150/2527: Loss = 0.0499
2026-01-07 23:41:41,041 - INFO -   Batch 2200/2527: Loss = 0.0553
2026-01-07 23:42:06,016 - INFO -   Batch 2250/2527: Loss = 0.0616
2026-01-07 23:42:30,841 - INFO -   Batch 2300/2527: Loss = 0.0781
2026-01-07 23:42:55,699 - INFO -   Batch 2350/2527: Loss = 0.0560
2026-01-07 23:43:20,428 - INFO -   Batch 2400/2527: Loss = 0.0350
2026-01-07 23:43:45,663 - INFO -   Batch 2450/2527: Loss = 0.0557
2026-01-07 23:44:10,777 - INFO -   Batch 2500/2527: Loss = 0.0602
2026-01-07 23:49:03,212 - INFO - Epoch 114/160: Train Loss: 0.0452, Val Loss: 1.7279, Val mIoU: 40.57%, Val PixelAcc: 79.61%
2026-01-07 23:49:28,263 - INFO -   Batch 50/2527: Loss = 0.0388
2026-01-07 23:49:51,163 - INFO -   Batch 100/2527: Loss = 0.0348
2026-01-07 23:50:14,100 - INFO -   Batch 150/2527: Loss = 0.0341
2026-01-07 23:50:37,092 - INFO -   Batch 200/2527: Loss = 0.0241
2026-01-07 23:51:00,042 - INFO -   Batch 250/2527: Loss = 0.0479
2026-01-07 23:51:22,999 - INFO -   Batch 300/2527: Loss = 0.0398
2026-01-07 23:51:45,957 - INFO -   Batch 350/2527: Loss = 0.0292
2026-01-07 23:52:08,957 - INFO -   Batch 400/2527: Loss = 0.0326
2026-01-07 23:52:31,911 - INFO -   Batch 450/2527: Loss = 0.0286
2026-01-07 23:52:54,870 - INFO -   Batch 500/2527: Loss = 0.0415
2026-01-07 23:53:17,830 - INFO -   Batch 550/2527: Loss = 0.0560
2026-01-07 23:53:40,825 - INFO -   Batch 600/2527: Loss = 0.0586
2026-01-07 23:54:03,781 - INFO -   Batch 650/2527: Loss = 0.0398
2026-01-07 23:54:27,113 - INFO -   Batch 700/2527: Loss = 0.0411
2026-01-07 23:54:50,079 - INFO -   Batch 750/2527: Loss = 0.0412
2026-01-07 23:55:13,046 - INFO -   Batch 800/2527: Loss = 0.0398
2026-01-07 23:55:36,007 - INFO -   Batch 850/2527: Loss = 0.0321
2026-01-07 23:55:58,971 - INFO -   Batch 900/2527: Loss = 0.0380
2026-01-07 23:56:21,933 - INFO -   Batch 950/2527: Loss = 0.0593
2026-01-07 23:56:44,907 - INFO -   Batch 1000/2527: Loss = 0.0494
2026-01-07 23:57:07,883 - INFO -   Batch 1050/2527: Loss = 0.0300
2026-01-07 23:57:31,242 - INFO -   Batch 1100/2527: Loss = 0.0427
2026-01-07 23:57:54,213 - INFO -   Batch 1150/2527: Loss = 0.0349
2026-01-07 23:58:17,215 - INFO -   Batch 1200/2527: Loss = 0.0338
2026-01-07 23:58:40,173 - INFO -   Batch 1250/2527: Loss = 0.0458
2026-01-07 23:59:03,131 - INFO -   Batch 1300/2527: Loss = 0.0298
2026-01-07 23:59:26,094 - INFO -   Batch 1350/2527: Loss = 0.0447
2026-01-07 23:59:49,447 - INFO -   Batch 1400/2527: Loss = 0.0439
2026-01-08 00:00:12,409 - INFO -   Batch 1450/2527: Loss = 0.0645
2026-01-08 00:00:35,390 - INFO -   Batch 1500/2527: Loss = 0.0403
2026-01-08 00:00:58,352 - INFO -   Batch 1550/2527: Loss = 0.0299
2026-01-08 00:01:21,411 - INFO -   Batch 1600/2527: Loss = 0.0391
2026-01-08 00:01:44,378 - INFO -   Batch 1650/2527: Loss = 0.0456
2026-01-08 00:02:07,337 - INFO -   Batch 1700/2527: Loss = 0.0535
2026-01-08 00:02:30,324 - INFO -   Batch 1750/2527: Loss = 0.0510
2026-01-08 00:02:53,282 - INFO -   Batch 1800/2527: Loss = 0.0513
2026-01-08 00:03:16,279 - INFO -   Batch 1850/2527: Loss = 0.0360
2026-01-08 00:03:39,241 - INFO -   Batch 1900/2527: Loss = 0.0357
2026-01-08 00:04:02,205 - INFO -   Batch 1950/2527: Loss = 0.0485
2026-01-08 00:04:25,167 - INFO -   Batch 2000/2527: Loss = 0.0454
2026-01-08 00:04:48,173 - INFO -   Batch 2050/2527: Loss = 0.0344
2026-01-08 00:05:11,134 - INFO -   Batch 2100/2527: Loss = 0.0320
2026-01-08 00:05:34,468 - INFO -   Batch 2150/2527: Loss = 0.0524
2026-01-08 00:05:57,428 - INFO -   Batch 2200/2527: Loss = 0.0426
2026-01-08 00:06:20,392 - INFO -   Batch 2250/2527: Loss = 0.0421
2026-01-08 00:06:43,355 - INFO -   Batch 2300/2527: Loss = 0.0260
2026-01-08 00:07:06,316 - INFO -   Batch 2350/2527: Loss = 0.0437
2026-01-08 00:07:29,268 - INFO -   Batch 2400/2527: Loss = 0.0425
2026-01-08 00:07:52,230 - INFO -   Batch 2450/2527: Loss = 0.0499
2026-01-08 00:08:15,194 - INFO -   Batch 2500/2527: Loss = 0.0433
2026-01-08 00:12:53,928 - INFO - Epoch 115/160: Train Loss: 0.0444, Val Loss: 1.7379, Val mIoU: 40.31%, Val PixelAcc: 79.54%
2026-01-08 00:13:19,249 - INFO -   Batch 50/2527: Loss = 0.0438
2026-01-08 00:13:42,825 - INFO -   Batch 100/2527: Loss = 0.0480
2026-01-08 00:14:06,730 - INFO -   Batch 150/2527: Loss = 0.0272
2026-01-08 00:14:30,284 - INFO -   Batch 200/2527: Loss = 0.0518
2026-01-08 00:14:54,017 - INFO -   Batch 250/2527: Loss = 0.0457
2026-01-08 00:15:17,608 - INFO -   Batch 300/2527: Loss = 0.0376
2026-01-08 00:15:41,453 - INFO -   Batch 350/2527: Loss = 0.0467
2026-01-08 00:16:05,329 - INFO -   Batch 400/2527: Loss = 0.0494
2026-01-08 00:16:29,466 - INFO -   Batch 450/2527: Loss = 0.0474
2026-01-08 00:16:53,066 - INFO -   Batch 500/2527: Loss = 0.0497
2026-01-08 00:17:16,690 - INFO -   Batch 550/2527: Loss = 0.0443
2026-01-08 00:17:40,778 - INFO -   Batch 600/2527: Loss = 0.0369
2026-01-08 00:18:04,354 - INFO -   Batch 650/2527: Loss = 0.0386
2026-01-08 00:18:28,136 - INFO -   Batch 700/2527: Loss = 0.0316
2026-01-08 00:18:52,106 - INFO -   Batch 750/2527: Loss = 0.0462
2026-01-08 00:19:15,922 - INFO -   Batch 800/2527: Loss = 0.0447
2026-01-08 00:19:39,785 - INFO -   Batch 850/2527: Loss = 0.0494
2026-01-08 00:20:03,569 - INFO -   Batch 900/2527: Loss = 0.0480
2026-01-08 00:20:27,171 - INFO -   Batch 950/2527: Loss = 0.0348
2026-01-08 00:20:50,827 - INFO -   Batch 1000/2527: Loss = 0.0391
2026-01-08 00:21:14,803 - INFO -   Batch 1050/2527: Loss = 0.0631
2026-01-08 00:21:38,380 - INFO -   Batch 1100/2527: Loss = 0.0487
2026-01-08 00:22:02,164 - INFO -   Batch 1150/2527: Loss = 0.0502
2026-01-08 00:22:25,759 - INFO -   Batch 1200/2527: Loss = 0.0329
2026-01-08 00:22:49,564 - INFO -   Batch 1250/2527: Loss = 0.0420
2026-01-08 00:23:13,416 - INFO -   Batch 1300/2527: Loss = 0.0689
2026-01-08 00:23:37,200 - INFO -   Batch 1350/2527: Loss = 0.0297
2026-01-08 00:24:00,803 - INFO -   Batch 1400/2527: Loss = 0.0405
2026-01-08 00:24:24,822 - INFO -   Batch 1450/2527: Loss = 0.0461
2026-01-08 00:24:48,775 - INFO -   Batch 1500/2527: Loss = 0.0490
2026-01-08 00:25:12,344 - INFO -   Batch 1550/2527: Loss = 0.0442
2026-01-08 00:25:36,108 - INFO -   Batch 1600/2527: Loss = 0.0429
2026-01-08 00:25:59,724 - INFO -   Batch 1650/2527: Loss = 0.0385
2026-01-08 00:26:23,543 - INFO -   Batch 1700/2527: Loss = 0.0462
2026-01-08 00:26:47,443 - INFO -   Batch 1750/2527: Loss = 0.0441
2026-01-08 00:27:11,227 - INFO -   Batch 1800/2527: Loss = 0.0391
2026-01-08 00:27:34,833 - INFO -   Batch 1850/2527: Loss = 0.0337
2026-01-08 00:27:58,845 - INFO -   Batch 1900/2527: Loss = 0.0386
2026-01-08 00:28:22,794 - INFO -   Batch 1950/2527: Loss = 0.0468
2026-01-08 00:28:46,408 - INFO -   Batch 2000/2527: Loss = 0.0496
2026-01-08 00:29:10,163 - INFO -   Batch 2050/2527: Loss = 0.0221
2026-01-08 00:29:33,769 - INFO -   Batch 2100/2527: Loss = 0.0466
2026-01-08 00:29:57,603 - INFO -   Batch 2150/2527: Loss = 0.0568
2026-01-08 00:30:21,820 - INFO -   Batch 2200/2527: Loss = 0.0473
2026-01-08 00:30:45,632 - INFO -   Batch 2250/2527: Loss = 0.0447
2026-01-08 00:31:09,225 - INFO -   Batch 2300/2527: Loss = 0.0335
2026-01-08 00:31:32,839 - INFO -   Batch 2350/2527: Loss = 0.0575
2026-01-08 00:31:56,780 - INFO -   Batch 2400/2527: Loss = 0.0369
2026-01-08 00:32:20,354 - INFO -   Batch 2450/2527: Loss = 0.0540
2026-01-08 00:32:44,113 - INFO -   Batch 2500/2527: Loss = 0.0263
2026-01-08 00:37:36,138 - INFO - Epoch 116/160: Train Loss: 0.0442, Val Loss: 1.7492, Val mIoU: 40.22%, Val PixelAcc: 79.51%
2026-01-08 00:38:02,173 - INFO -   Batch 50/2527: Loss = 0.0461
2026-01-08 00:38:27,240 - INFO -   Batch 100/2527: Loss = 0.0423
2026-01-08 00:38:52,168 - INFO -   Batch 150/2527: Loss = 0.0458
2026-01-08 00:39:16,978 - INFO -   Batch 200/2527: Loss = 0.0491
2026-01-08 00:39:41,802 - INFO -   Batch 250/2527: Loss = 0.0593
2026-01-08 00:40:06,805 - INFO -   Batch 300/2527: Loss = 0.0547
2026-01-08 00:40:31,637 - INFO -   Batch 350/2527: Loss = 0.0485
2026-01-08 00:40:56,841 - INFO -   Batch 400/2527: Loss = 0.0488
2026-01-08 00:41:21,960 - INFO -   Batch 450/2527: Loss = 0.0553
2026-01-08 00:41:46,944 - INFO -   Batch 500/2527: Loss = 0.0451
2026-01-08 00:42:11,799 - INFO -   Batch 550/2527: Loss = 0.0285
2026-01-08 00:42:36,656 - INFO -   Batch 600/2527: Loss = 0.0535
2026-01-08 00:43:01,402 - INFO -   Batch 650/2527: Loss = 0.0703
2026-01-08 00:43:26,257 - INFO -   Batch 700/2527: Loss = 0.0461
2026-01-08 00:43:51,386 - INFO -   Batch 750/2527: Loss = 0.0537
2026-01-08 00:44:16,702 - INFO -   Batch 800/2527: Loss = 0.0390
2026-01-08 00:44:41,519 - INFO -   Batch 850/2527: Loss = 0.0423
2026-01-08 00:45:06,351 - INFO -   Batch 900/2527: Loss = 0.0384
2026-01-08 00:45:31,391 - INFO -   Batch 950/2527: Loss = 0.0434
2026-01-08 00:45:56,214 - INFO -   Batch 1000/2527: Loss = 0.0370
2026-01-08 00:46:21,076 - INFO -   Batch 1050/2527: Loss = 0.0357
2026-01-08 00:46:46,603 - INFO -   Batch 1100/2527: Loss = 0.0624
2026-01-08 00:47:11,606 - INFO -   Batch 1150/2527: Loss = 0.0542
2026-01-08 00:47:36,439 - INFO -   Batch 1200/2527: Loss = 0.0442
2026-01-08 00:48:01,295 - INFO -   Batch 1250/2527: Loss = 0.0375
2026-01-08 00:48:26,014 - INFO -   Batch 1300/2527: Loss = 0.0453
2026-01-08 00:48:50,848 - INFO -   Batch 1350/2527: Loss = 0.0627
2026-01-08 00:49:15,999 - INFO -   Batch 1400/2527: Loss = 0.0606
2026-01-08 00:49:40,974 - INFO -   Batch 1450/2527: Loss = 0.0406
2026-01-08 00:50:05,817 - INFO -   Batch 1500/2527: Loss = 0.0573
2026-01-08 00:50:30,660 - INFO -   Batch 1550/2527: Loss = 0.0454
2026-01-08 00:50:55,665 - INFO -   Batch 1600/2527: Loss = 0.0474
2026-01-08 00:51:20,495 - INFO -   Batch 1650/2527: Loss = 0.0510
2026-01-08 00:51:45,351 - INFO -   Batch 1700/2527: Loss = 0.0339
2026-01-08 00:52:10,475 - INFO -   Batch 1750/2527: Loss = 0.0354
2026-01-08 00:52:35,448 - INFO -   Batch 1800/2527: Loss = 0.0542
2026-01-08 00:53:00,686 - INFO -   Batch 1850/2527: Loss = 0.0336
2026-01-08 00:53:25,517 - INFO -   Batch 1900/2527: Loss = 0.0786
2026-01-08 00:53:50,253 - INFO -   Batch 1950/2527: Loss = 0.0350
2026-01-08 00:54:15,094 - INFO -   Batch 2000/2527: Loss = 0.0351
2026-01-08 00:54:40,199 - INFO -   Batch 2050/2527: Loss = 0.0479
2026-01-08 00:55:05,159 - INFO -   Batch 2100/2527: Loss = 0.0505
2026-01-08 00:55:30,008 - INFO -   Batch 2150/2527: Loss = 0.0398
2026-01-08 00:55:54,856 - INFO -   Batch 2200/2527: Loss = 0.0388
2026-01-08 00:56:20,239 - INFO -   Batch 2250/2527: Loss = 0.0359
2026-01-08 00:56:45,048 - INFO -   Batch 2300/2527: Loss = 0.0441
2026-01-08 00:57:09,902 - INFO -   Batch 2350/2527: Loss = 0.0661
2026-01-08 00:57:35,019 - INFO -   Batch 2400/2527: Loss = 0.0550
2026-01-08 00:58:00,024 - INFO -   Batch 2450/2527: Loss = 0.0319
2026-01-08 00:58:24,849 - INFO -   Batch 2500/2527: Loss = 0.0392
2026-01-08 01:03:04,327 - INFO - Epoch 117/160: Train Loss: 0.0439, Val Loss: 1.7942, Val mIoU: 40.22%, Val PixelAcc: 79.52%
2026-01-08 01:03:30,707 - INFO -   Batch 50/2527: Loss = 0.0371
2026-01-08 01:03:55,913 - INFO -   Batch 100/2527: Loss = 0.0543
2026-01-08 01:04:20,823 - INFO -   Batch 150/2527: Loss = 0.0448
2026-01-08 01:04:45,696 - INFO -   Batch 200/2527: Loss = 0.0392
2026-01-08 01:05:10,828 - INFO -   Batch 250/2527: Loss = 0.0393
2026-01-08 01:05:35,842 - INFO -   Batch 300/2527: Loss = 0.0496
2026-01-08 01:06:00,760 - INFO -   Batch 350/2527: Loss = 0.0495
2026-01-08 01:06:25,576 - INFO -   Batch 400/2527: Loss = 0.0369
2026-01-08 01:06:50,327 - INFO -   Batch 450/2527: Loss = 0.0403
2026-01-08 01:07:15,174 - INFO -   Batch 500/2527: Loss = 0.0330
2026-01-08 01:07:40,485 - INFO -   Batch 550/2527: Loss = 0.0344
2026-01-08 01:08:05,479 - INFO -   Batch 600/2527: Loss = 0.0445
2026-01-08 01:08:30,785 - INFO -   Batch 650/2527: Loss = 0.0519
2026-01-08 01:08:55,982 - INFO -   Batch 700/2527: Loss = 0.0424
2026-01-08 01:09:21,004 - INFO -   Batch 750/2527: Loss = 0.0483
2026-01-08 01:09:45,841 - INFO -   Batch 800/2527: Loss = 0.0380
2026-01-08 01:10:10,634 - INFO -   Batch 850/2527: Loss = 0.0361
2026-01-08 01:10:35,814 - INFO -   Batch 900/2527: Loss = 0.0415
2026-01-08 01:11:00,879 - INFO -   Batch 950/2527: Loss = 0.0266
2026-01-08 01:11:26,180 - INFO -   Batch 1000/2527: Loss = 0.0513
2026-01-08 01:11:51,054 - INFO -   Batch 1050/2527: Loss = 0.0444
2026-01-08 01:12:15,944 - INFO -   Batch 1100/2527: Loss = 0.0493
2026-01-08 01:12:40,832 - INFO -   Batch 1150/2527: Loss = 0.0525
2026-01-08 01:13:06,026 - INFO -   Batch 1200/2527: Loss = 0.0445
2026-01-08 01:13:31,065 - INFO -   Batch 1250/2527: Loss = 0.0342
2026-01-08 01:13:55,875 - INFO -   Batch 1300/2527: Loss = 0.0463
2026-01-08 01:14:20,756 - INFO -   Batch 1350/2527: Loss = 0.0436
2026-01-08 01:14:45,754 - INFO -   Batch 1400/2527: Loss = 0.0506
2026-01-08 01:15:10,538 - INFO -   Batch 1450/2527: Loss = 0.0448
2026-01-08 01:15:35,378 - INFO -   Batch 1500/2527: Loss = 0.0285
2026-01-08 01:16:00,524 - INFO -   Batch 1550/2527: Loss = 0.0398
2026-01-08 01:16:25,636 - INFO -   Batch 1600/2527: Loss = 0.0625
2026-01-08 01:16:50,602 - INFO -   Batch 1650/2527: Loss = 0.0606
2026-01-08 01:17:15,492 - INFO -   Batch 1700/2527: Loss = 0.0457
2026-01-08 01:17:40,549 - INFO -   Batch 1750/2527: Loss = 0.0373
2026-01-08 01:18:05,407 - INFO -   Batch 1800/2527: Loss = 0.0547
2026-01-08 01:18:30,574 - INFO -   Batch 1850/2527: Loss = 0.0385
2026-01-08 01:18:55,553 - INFO -   Batch 1900/2527: Loss = 0.0233
2026-01-08 01:19:20,476 - INFO -   Batch 1950/2527: Loss = 0.0445
2026-01-08 01:19:45,370 - INFO -   Batch 2000/2527: Loss = 0.0464
2026-01-08 01:20:10,409 - INFO -   Batch 2050/2527: Loss = 0.0423
2026-01-08 01:20:35,257 - INFO -   Batch 2100/2527: Loss = 0.0333
2026-01-08 01:21:00,478 - INFO -   Batch 2150/2527: Loss = 0.0334
2026-01-08 01:21:25,562 - INFO -   Batch 2200/2527: Loss = 0.0298
2026-01-08 01:21:50,554 - INFO -   Batch 2250/2527: Loss = 0.0458
2026-01-08 01:22:15,440 - INFO -   Batch 2300/2527: Loss = 0.0376
2026-01-08 01:22:40,494 - INFO -   Batch 2350/2527: Loss = 0.0516
2026-01-08 01:23:05,208 - INFO -   Batch 2400/2527: Loss = 0.0459
2026-01-08 01:23:30,415 - INFO -   Batch 2450/2527: Loss = 0.0348
2026-01-08 01:23:55,527 - INFO -   Batch 2500/2527: Loss = 0.0333
2026-01-08 01:28:56,428 - INFO - Epoch 118/160: Train Loss: 0.0436, Val Loss: 1.7894, Val mIoU: 40.19%, Val PixelAcc: 79.49%
2026-01-08 01:29:21,447 - INFO -   Batch 50/2527: Loss = 0.0569
2026-01-08 01:29:44,390 - INFO -   Batch 100/2527: Loss = 0.0473
2026-01-08 01:30:07,310 - INFO -   Batch 150/2527: Loss = 0.0439
2026-01-08 01:30:30,256 - INFO -   Batch 200/2527: Loss = 0.0585
2026-01-08 01:30:53,196 - INFO -   Batch 250/2527: Loss = 0.0379
2026-01-08 01:31:16,154 - INFO -   Batch 300/2527: Loss = 0.0532
2026-01-08 01:31:39,111 - INFO -   Batch 350/2527: Loss = 0.0309
2026-01-08 01:32:02,072 - INFO -   Batch 400/2527: Loss = 0.0432
2026-01-08 01:32:25,051 - INFO -   Batch 450/2527: Loss = 0.0440
2026-01-08 01:32:48,019 - INFO -   Batch 500/2527: Loss = 0.0384
2026-01-08 01:33:10,976 - INFO -   Batch 550/2527: Loss = 0.0510
2026-01-08 01:33:33,947 - INFO -   Batch 600/2527: Loss = 0.0619
2026-01-08 01:33:57,265 - INFO -   Batch 650/2527: Loss = 0.0438
2026-01-08 01:34:20,230 - INFO -   Batch 700/2527: Loss = 0.0451
2026-01-08 01:34:43,192 - INFO -   Batch 750/2527: Loss = 0.0522
2026-01-08 01:35:06,182 - INFO -   Batch 800/2527: Loss = 0.0517
2026-01-08 01:35:29,148 - INFO -   Batch 850/2527: Loss = 0.0311
2026-01-08 01:35:52,101 - INFO -   Batch 900/2527: Loss = 0.0223
2026-01-08 01:36:15,062 - INFO -   Batch 950/2527: Loss = 0.0417
2026-01-08 01:36:38,061 - INFO -   Batch 1000/2527: Loss = 0.0391
2026-01-08 01:37:01,021 - INFO -   Batch 1050/2527: Loss = 0.0497
2026-01-08 01:37:24,341 - INFO -   Batch 1100/2527: Loss = 0.0593
2026-01-08 01:37:47,315 - INFO -   Batch 1150/2527: Loss = 0.0337
2026-01-08 01:38:10,295 - INFO -   Batch 1200/2527: Loss = 0.0355
2026-01-08 01:38:33,270 - INFO -   Batch 1250/2527: Loss = 0.0375
2026-01-08 01:38:56,246 - INFO -   Batch 1300/2527: Loss = 0.0664
2026-01-08 01:39:19,225 - INFO -   Batch 1350/2527: Loss = 0.0457
2026-01-08 01:39:42,597 - INFO -   Batch 1400/2527: Loss = 0.0453
2026-01-08 01:40:05,544 - INFO -   Batch 1450/2527: Loss = 0.0430
2026-01-08 01:40:28,498 - INFO -   Batch 1500/2527: Loss = 0.0557
2026-01-08 01:40:51,455 - INFO -   Batch 1550/2527: Loss = 0.0406
2026-01-08 01:41:14,452 - INFO -   Batch 1600/2527: Loss = 0.0430
2026-01-08 01:41:37,419 - INFO -   Batch 1650/2527: Loss = 0.0434
2026-01-08 01:42:00,377 - INFO -   Batch 1700/2527: Loss = 0.0645
2026-01-08 01:42:23,334 - INFO -   Batch 1750/2527: Loss = 0.0376
2026-01-08 01:42:46,325 - INFO -   Batch 1800/2527: Loss = 0.0361
2026-01-08 01:43:09,281 - INFO -   Batch 1850/2527: Loss = 0.0464
2026-01-08 01:43:32,235 - INFO -   Batch 1900/2527: Loss = 0.0365
2026-01-08 01:43:55,185 - INFO -   Batch 1950/2527: Loss = 0.0443
2026-01-08 01:44:18,172 - INFO -   Batch 2000/2527: Loss = 0.0413
2026-01-08 01:44:41,129 - INFO -   Batch 2050/2527: Loss = 0.0489
2026-01-08 01:45:04,468 - INFO -   Batch 2100/2527: Loss = 0.0401
2026-01-08 01:45:27,424 - INFO -   Batch 2150/2527: Loss = 0.0341
2026-01-08 01:45:50,379 - INFO -   Batch 2200/2527: Loss = 0.0404
2026-01-08 01:46:13,344 - INFO -   Batch 2250/2527: Loss = 0.0410
2026-01-08 01:46:36,338 - INFO -   Batch 2300/2527: Loss = 0.0358
2026-01-08 01:46:59,297 - INFO -   Batch 2350/2527: Loss = 0.0382
2026-01-08 01:47:22,265 - INFO -   Batch 2400/2527: Loss = 0.0491
2026-01-08 01:47:45,229 - INFO -   Batch 2450/2527: Loss = 0.0400
2026-01-08 01:48:08,205 - INFO -   Batch 2500/2527: Loss = 0.0630
2026-01-08 01:52:47,436 - INFO - Epoch 119/160: Train Loss: 0.0430, Val Loss: 1.7962, Val mIoU: 40.20%, Val PixelAcc: 79.51%
2026-01-08 01:53:12,674 - INFO -   Batch 50/2527: Loss = 0.0283
2026-01-08 01:53:36,265 - INFO -   Batch 100/2527: Loss = 0.0413
2026-01-08 01:54:00,187 - INFO -   Batch 150/2527: Loss = 0.0415
2026-01-08 01:54:23,752 - INFO -   Batch 200/2527: Loss = 0.0548
2026-01-08 01:54:47,530 - INFO -   Batch 250/2527: Loss = 0.0430
2026-01-08 01:55:11,487 - INFO -   Batch 300/2527: Loss = 0.0501
2026-01-08 01:55:35,292 - INFO -   Batch 350/2527: Loss = 0.0384
2026-01-08 01:55:59,146 - INFO -   Batch 400/2527: Loss = 0.0505
2026-01-08 01:56:22,921 - INFO -   Batch 450/2527: Loss = 0.0366
2026-01-08 01:56:46,553 - INFO -   Batch 500/2527: Loss = 0.0354
2026-01-08 01:57:10,170 - INFO -   Batch 550/2527: Loss = 0.0486
2026-01-08 01:57:34,118 - INFO -   Batch 600/2527: Loss = 0.0344
2026-01-08 01:57:57,685 - INFO -   Batch 650/2527: Loss = 0.0432
2026-01-08 01:58:21,452 - INFO -   Batch 700/2527: Loss = 0.0519
2026-01-08 01:58:45,056 - INFO -   Batch 750/2527: Loss = 0.0592
2026-01-08 01:59:08,853 - INFO -   Batch 800/2527: Loss = 0.0325
2026-01-08 01:59:32,706 - INFO -   Batch 850/2527: Loss = 0.0629
2026-01-08 01:59:56,484 - INFO -   Batch 900/2527: Loss = 0.0358
2026-01-08 02:00:20,082 - INFO -   Batch 950/2527: Loss = 0.0441
2026-01-08 02:00:43,720 - INFO -   Batch 1000/2527: Loss = 0.0414
2026-01-08 02:01:08,042 - INFO -   Batch 1050/2527: Loss = 0.0359
2026-01-08 02:01:31,624 - INFO -   Batch 1100/2527: Loss = 0.0352
2026-01-08 02:01:55,385 - INFO -   Batch 1150/2527: Loss = 0.0554
2026-01-08 02:02:19,000 - INFO -   Batch 1200/2527: Loss = 0.0397
2026-01-08 02:02:42,847 - INFO -   Batch 1250/2527: Loss = 0.0494
2026-01-08 02:03:06,705 - INFO -   Batch 1300/2527: Loss = 0.0246
2026-01-08 02:03:30,525 - INFO -   Batch 1350/2527: Loss = 0.0344
2026-01-08 02:03:54,118 - INFO -   Batch 1400/2527: Loss = 0.0613
2026-01-08 02:04:18,148 - INFO -   Batch 1450/2527: Loss = 0.0361
2026-01-08 02:04:42,100 - INFO -   Batch 1500/2527: Loss = 0.0351
2026-01-08 02:05:05,675 - INFO -   Batch 1550/2527: Loss = 0.0446
2026-01-08 02:05:29,464 - INFO -   Batch 1600/2527: Loss = 0.0411
2026-01-08 02:05:53,063 - INFO -   Batch 1650/2527: Loss = 0.0344
2026-01-08 02:06:16,913 - INFO -   Batch 1700/2527: Loss = 0.0457
2026-01-08 02:06:41,167 - INFO -   Batch 1750/2527: Loss = 0.0468
2026-01-08 02:07:04,947 - INFO -   Batch 1800/2527: Loss = 0.0460
2026-01-08 02:07:28,566 - INFO -   Batch 1850/2527: Loss = 0.0302
2026-01-08 02:07:52,196 - INFO -   Batch 1900/2527: Loss = 0.0382
2026-01-08 02:08:16,178 - INFO -   Batch 1950/2527: Loss = 0.0703
2026-01-08 02:08:39,779 - INFO -   Batch 2000/2527: Loss = 0.0346
2026-01-08 02:09:03,529 - INFO -   Batch 2050/2527: Loss = 0.0277
2026-01-08 02:09:27,580 - INFO -   Batch 2100/2527: Loss = 0.0434
2026-01-08 02:09:51,396 - INFO -   Batch 2150/2527: Loss = 0.0359
2026-01-08 02:10:15,258 - INFO -   Batch 2200/2527: Loss = 0.0502
2026-01-08 02:10:39,076 - INFO -   Batch 2250/2527: Loss = 0.0450
2026-01-08 02:11:02,704 - INFO -   Batch 2300/2527: Loss = 0.0294
2026-01-08 02:11:26,342 - INFO -   Batch 2350/2527: Loss = 0.0393
2026-01-08 02:11:50,304 - INFO -   Batch 2400/2527: Loss = 0.0763
2026-01-08 02:12:13,879 - INFO -   Batch 2450/2527: Loss = 0.0358
2026-01-08 02:12:38,051 - INFO -   Batch 2500/2527: Loss = 0.0391
2026-01-08 02:17:29,889 - INFO - Epoch 120/160: Train Loss: 0.0429, Val Loss: 1.8122, Val mIoU: 40.23%, Val PixelAcc: 79.49%
2026-01-08 02:17:31,781 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_120.pth
2026-01-08 02:17:31,782 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_120.pth
2026-01-08 02:17:57,770 - INFO -   Batch 50/2527: Loss = 0.0281
2026-01-08 02:18:22,830 - INFO -   Batch 100/2527: Loss = 0.0408
2026-01-08 02:18:47,795 - INFO -   Batch 150/2527: Loss = 0.0588
2026-01-08 02:19:12,615 - INFO -   Batch 200/2527: Loss = 0.0399
2026-01-08 02:19:37,446 - INFO -   Batch 250/2527: Loss = 0.0408
2026-01-08 02:20:02,463 - INFO -   Batch 300/2527: Loss = 0.0505
2026-01-08 02:20:27,272 - INFO -   Batch 350/2527: Loss = 0.0354
2026-01-08 02:20:52,096 - INFO -   Batch 400/2527: Loss = 0.0368
2026-01-08 02:21:17,202 - INFO -   Batch 450/2527: Loss = 0.0372
2026-01-08 02:21:42,175 - INFO -   Batch 500/2527: Loss = 0.0363
2026-01-08 02:22:07,011 - INFO -   Batch 550/2527: Loss = 0.0466
2026-01-08 02:22:31,856 - INFO -   Batch 600/2527: Loss = 0.0336
2026-01-08 02:22:56,954 - INFO -   Batch 650/2527: Loss = 0.0410
2026-01-08 02:23:21,803 - INFO -   Batch 700/2527: Loss = 0.0279
2026-01-08 02:23:46,929 - INFO -   Batch 750/2527: Loss = 0.0348
2026-01-08 02:24:11,897 - INFO -   Batch 800/2527: Loss = 0.0478
2026-01-08 02:24:36,704 - INFO -   Batch 850/2527: Loss = 0.0401
2026-01-08 02:25:01,633 - INFO -   Batch 900/2527: Loss = 0.0412
2026-01-08 02:25:26,646 - INFO -   Batch 950/2527: Loss = 0.0477
2026-01-08 02:25:51,469 - INFO -   Batch 1000/2527: Loss = 0.0424
2026-01-08 02:26:16,676 - INFO -   Batch 1050/2527: Loss = 0.0289
2026-01-08 02:26:41,799 - INFO -   Batch 1100/2527: Loss = 0.0574
2026-01-08 02:27:06,798 - INFO -   Batch 1150/2527: Loss = 0.0387
2026-01-08 02:27:31,641 - INFO -   Batch 1200/2527: Loss = 0.0490
2026-01-08 02:27:56,483 - INFO -   Batch 1250/2527: Loss = 0.0406
2026-01-08 02:28:21,222 - INFO -   Batch 1300/2527: Loss = 0.0477
2026-01-08 02:28:46,419 - INFO -   Batch 1350/2527: Loss = 0.0469
2026-01-08 02:29:11,526 - INFO -   Batch 1400/2527: Loss = 0.0335
2026-01-08 02:29:36,517 - INFO -   Batch 1450/2527: Loss = 0.0597
2026-01-08 02:30:01,344 - INFO -   Batch 1500/2527: Loss = 0.0592
2026-01-08 02:30:26,184 - INFO -   Batch 1550/2527: Loss = 0.0308
2026-01-08 02:30:51,199 - INFO -   Batch 1600/2527: Loss = 0.0431
2026-01-08 02:31:16,039 - INFO -   Batch 1650/2527: Loss = 0.0403
2026-01-08 02:31:40,886 - INFO -   Batch 1700/2527: Loss = 0.0349
2026-01-08 02:32:06,178 - INFO -   Batch 1750/2527: Loss = 0.0473
2026-01-08 02:32:31,167 - INFO -   Batch 1800/2527: Loss = 0.0506
2026-01-08 02:32:55,979 - INFO -   Batch 1850/2527: Loss = 0.0336
2026-01-08 02:33:20,810 - INFO -   Batch 1900/2527: Loss = 0.0404
2026-01-08 02:33:45,515 - INFO -   Batch 1950/2527: Loss = 0.0488
2026-01-08 02:34:10,372 - INFO -   Batch 2000/2527: Loss = 0.0285
2026-01-08 02:34:35,494 - INFO -   Batch 2050/2527: Loss = 0.0389
2026-01-08 02:35:00,860 - INFO -   Batch 2100/2527: Loss = 0.0494
2026-01-08 02:35:25,676 - INFO -   Batch 2150/2527: Loss = 0.0500
2026-01-08 02:35:50,520 - INFO -   Batch 2200/2527: Loss = 0.0468
2026-01-08 02:36:15,562 - INFO -   Batch 2250/2527: Loss = 0.0444
2026-01-08 02:36:40,398 - INFO -   Batch 2300/2527: Loss = 0.0505
2026-01-08 02:37:05,239 - INFO -   Batch 2350/2527: Loss = 0.0349
2026-01-08 02:37:30,365 - INFO -   Batch 2400/2527: Loss = 0.0395
2026-01-08 02:37:55,337 - INFO -   Batch 2450/2527: Loss = 0.0522
2026-01-08 02:38:20,532 - INFO -   Batch 2500/2527: Loss = 0.0593
2026-01-08 02:42:59,493 - INFO - Epoch 121/160: Train Loss: 0.0425, Val Loss: 1.8018, Val mIoU: 40.15%, Val PixelAcc: 79.51%
2026-01-08 02:43:25,795 - INFO -   Batch 50/2527: Loss = 0.0442
2026-01-08 02:43:50,749 - INFO -   Batch 100/2527: Loss = 0.0262
2026-01-08 02:44:15,539 - INFO -   Batch 150/2527: Loss = 0.0410
2026-01-08 02:44:40,357 - INFO -   Batch 200/2527: Loss = 0.0315
2026-01-08 02:45:05,881 - INFO -   Batch 250/2527: Loss = 0.0481
2026-01-08 02:45:30,851 - INFO -   Batch 300/2527: Loss = 0.0358
2026-01-08 02:45:55,673 - INFO -   Batch 350/2527: Loss = 0.0495
2026-01-08 02:46:20,498 - INFO -   Batch 400/2527: Loss = 0.0285
2026-01-08 02:46:45,241 - INFO -   Batch 450/2527: Loss = 0.0390
2026-01-08 02:47:10,095 - INFO -   Batch 500/2527: Loss = 0.0654
2026-01-08 02:47:35,224 - INFO -   Batch 550/2527: Loss = 0.0285
2026-01-08 02:48:00,191 - INFO -   Batch 600/2527: Loss = 0.0482
2026-01-08 02:48:25,009 - INFO -   Batch 650/2527: Loss = 0.0728
2026-01-08 02:48:49,831 - INFO -   Batch 700/2527: Loss = 0.0549
2026-01-08 02:49:14,889 - INFO -   Batch 750/2527: Loss = 0.0361
2026-01-08 02:49:39,703 - INFO -   Batch 800/2527: Loss = 0.0280
2026-01-08 02:50:04,545 - INFO -   Batch 850/2527: Loss = 0.0309
2026-01-08 02:50:29,677 - INFO -   Batch 900/2527: Loss = 0.0325
2026-01-08 02:50:54,641 - INFO -   Batch 950/2527: Loss = 0.0324
2026-01-08 02:51:19,810 - INFO -   Batch 1000/2527: Loss = 0.0377
2026-01-08 02:51:44,642 - INFO -   Batch 1050/2527: Loss = 0.0361
2026-01-08 02:52:09,368 - INFO -   Batch 1100/2527: Loss = 0.0282
2026-01-08 02:52:34,201 - INFO -   Batch 1150/2527: Loss = 0.0453
2026-01-08 02:52:59,345 - INFO -   Batch 1200/2527: Loss = 0.0352
2026-01-08 02:53:24,317 - INFO -   Batch 1250/2527: Loss = 0.0390
2026-01-08 02:53:49,169 - INFO -   Batch 1300/2527: Loss = 0.0283
2026-01-08 02:54:14,006 - INFO -   Batch 1350/2527: Loss = 0.0612
2026-01-08 02:54:39,367 - INFO -   Batch 1400/2527: Loss = 0.0518
2026-01-08 02:55:04,226 - INFO -   Batch 1450/2527: Loss = 0.0554
2026-01-08 02:55:29,048 - INFO -   Batch 1500/2527: Loss = 0.0350
2026-01-08 02:55:54,169 - INFO -   Batch 1550/2527: Loss = 0.0305
2026-01-08 02:56:19,154 - INFO -   Batch 1600/2527: Loss = 0.0418
2026-01-08 02:56:43,990 - INFO -   Batch 1650/2527: Loss = 0.0338
2026-01-08 02:57:09,251 - INFO -   Batch 1700/2527: Loss = 0.0578
2026-01-08 02:57:33,994 - INFO -   Batch 1750/2527: Loss = 0.0355
2026-01-08 02:57:58,834 - INFO -   Batch 1800/2527: Loss = 0.0450
2026-01-08 02:58:23,957 - INFO -   Batch 1850/2527: Loss = 0.0516
2026-01-08 02:58:48,953 - INFO -   Batch 1900/2527: Loss = 0.0385
2026-01-08 02:59:13,784 - INFO -   Batch 1950/2527: Loss = 0.0534
2026-01-08 02:59:38,609 - INFO -   Batch 2000/2527: Loss = 0.0354
2026-01-08 03:00:03,616 - INFO -   Batch 2050/2527: Loss = 0.0422
2026-01-08 03:00:28,449 - INFO -   Batch 2100/2527: Loss = 0.0436
2026-01-08 03:00:53,273 - INFO -   Batch 2150/2527: Loss = 0.0612
2026-01-08 03:01:18,430 - INFO -   Batch 2200/2527: Loss = 0.0397
2026-01-08 03:01:43,409 - INFO -   Batch 2250/2527: Loss = 0.0302
2026-01-08 03:02:08,226 - INFO -   Batch 2300/2527: Loss = 0.0289
2026-01-08 03:02:33,106 - INFO -   Batch 2350/2527: Loss = 0.0407
2026-01-08 03:02:57,824 - INFO -   Batch 2400/2527: Loss = 0.0559
2026-01-08 03:03:23,006 - INFO -   Batch 2450/2527: Loss = 0.0529
2026-01-08 03:03:48,126 - INFO -   Batch 2500/2527: Loss = 0.0452
2026-01-08 03:08:40,551 - INFO - Epoch 122/160: Train Loss: 0.0421, Val Loss: 1.8036, Val mIoU: 40.23%, Val PixelAcc: 79.53%
2026-01-08 03:09:05,471 - INFO -   Batch 50/2527: Loss = 0.0616
2026-01-08 03:09:28,378 - INFO -   Batch 100/2527: Loss = 0.0552
2026-01-08 03:09:51,317 - INFO -   Batch 150/2527: Loss = 0.0473
2026-01-08 03:10:14,257 - INFO -   Batch 200/2527: Loss = 0.0393
2026-01-08 03:10:37,212 - INFO -   Batch 250/2527: Loss = 0.0420
2026-01-08 03:11:00,168 - INFO -   Batch 300/2527: Loss = 0.0384
2026-01-08 03:11:23,489 - INFO -   Batch 350/2527: Loss = 0.0288
2026-01-08 03:11:46,456 - INFO -   Batch 400/2527: Loss = 0.0342
2026-01-08 03:12:09,419 - INFO -   Batch 450/2527: Loss = 0.0387
2026-01-08 03:12:32,389 - INFO -   Batch 500/2527: Loss = 0.0600
2026-01-08 03:12:55,361 - INFO -   Batch 550/2527: Loss = 0.0342
2026-01-08 03:13:18,319 - INFO -   Batch 600/2527: Loss = 0.0422
2026-01-08 03:13:41,693 - INFO -   Batch 650/2527: Loss = 0.0490
2026-01-08 03:14:04,652 - INFO -   Batch 700/2527: Loss = 0.0358
2026-01-08 03:14:27,618 - INFO -   Batch 750/2527: Loss = 0.0369
2026-01-08 03:14:50,569 - INFO -   Batch 800/2527: Loss = 0.0483
2026-01-08 03:15:13,558 - INFO -   Batch 850/2527: Loss = 0.0333
2026-01-08 03:15:36,516 - INFO -   Batch 900/2527: Loss = 0.0302
2026-01-08 03:15:59,481 - INFO -   Batch 950/2527: Loss = 0.0385
2026-01-08 03:16:22,454 - INFO -   Batch 1000/2527: Loss = 0.0370
2026-01-08 03:16:45,445 - INFO -   Batch 1050/2527: Loss = 0.0478
2026-01-08 03:17:08,409 - INFO -   Batch 1100/2527: Loss = 0.0402
2026-01-08 03:17:31,366 - INFO -   Batch 1150/2527: Loss = 0.0293
2026-01-08 03:17:54,326 - INFO -   Batch 1200/2527: Loss = 0.0414
2026-01-08 03:18:17,323 - INFO -   Batch 1250/2527: Loss = 0.0506
2026-01-08 03:18:40,321 - INFO -   Batch 1300/2527: Loss = 0.0393
2026-01-08 03:19:03,670 - INFO -   Batch 1350/2527: Loss = 0.0384
2026-01-08 03:19:26,627 - INFO -   Batch 1400/2527: Loss = 0.0490
2026-01-08 03:19:49,606 - INFO -   Batch 1450/2527: Loss = 0.0382
2026-01-08 03:20:12,602 - INFO -   Batch 1500/2527: Loss = 0.0324
2026-01-08 03:20:35,555 - INFO -   Batch 1550/2527: Loss = 0.0436
2026-01-08 03:20:58,517 - INFO -   Batch 1600/2527: Loss = 0.0484
2026-01-08 03:21:21,481 - INFO -   Batch 1650/2527: Loss = 0.0506
2026-01-08 03:21:44,473 - INFO -   Batch 1700/2527: Loss = 0.0292
2026-01-08 03:22:07,433 - INFO -   Batch 1750/2527: Loss = 0.0551
2026-01-08 03:22:30,776 - INFO -   Batch 1800/2527: Loss = 0.0431
2026-01-08 03:22:53,736 - INFO -   Batch 1850/2527: Loss = 0.0340
2026-01-08 03:23:16,699 - INFO -   Batch 1900/2527: Loss = 0.0327
2026-01-08 03:23:39,660 - INFO -   Batch 1950/2527: Loss = 0.0392
2026-01-08 03:24:02,632 - INFO -   Batch 2000/2527: Loss = 0.0323
2026-01-08 03:24:25,954 - INFO -   Batch 2050/2527: Loss = 0.0390
2026-01-08 03:24:48,908 - INFO -   Batch 2100/2527: Loss = 0.0252
2026-01-08 03:25:11,869 - INFO -   Batch 2150/2527: Loss = 0.0337
2026-01-08 03:25:34,828 - INFO -   Batch 2200/2527: Loss = 0.0201
2026-01-08 03:25:57,782 - INFO -   Batch 2250/2527: Loss = 0.0325
2026-01-08 03:26:20,749 - INFO -   Batch 2300/2527: Loss = 0.0289
2026-01-08 03:26:43,709 - INFO -   Batch 2350/2527: Loss = 0.0372
2026-01-08 03:27:06,670 - INFO -   Batch 2400/2527: Loss = 0.0752
2026-01-08 03:27:29,634 - INFO -   Batch 2450/2527: Loss = 0.0315
2026-01-08 03:27:52,595 - INFO -   Batch 2500/2527: Loss = 0.0394
2026-01-08 03:32:30,909 - INFO - Epoch 123/160: Train Loss: 0.0419, Val Loss: 1.8212, Val mIoU: 40.12%, Val PixelAcc: 79.44%
2026-01-08 03:32:56,127 - INFO -   Batch 50/2527: Loss = 0.0329
2026-01-08 03:33:19,699 - INFO -   Batch 100/2527: Loss = 0.0430
2026-01-08 03:33:43,655 - INFO -   Batch 150/2527: Loss = 0.0463
2026-01-08 03:34:07,248 - INFO -   Batch 200/2527: Loss = 0.0307
2026-01-08 03:34:30,999 - INFO -   Batch 250/2527: Loss = 0.0416
2026-01-08 03:34:54,962 - INFO -   Batch 300/2527: Loss = 0.0380
2026-01-08 03:35:18,787 - INFO -   Batch 350/2527: Loss = 0.0313
2026-01-08 03:35:42,674 - INFO -   Batch 400/2527: Loss = 0.0435
2026-01-08 03:36:06,480 - INFO -   Batch 450/2527: Loss = 0.0364
2026-01-08 03:36:30,064 - INFO -   Batch 500/2527: Loss = 0.0488
2026-01-08 03:36:53,680 - INFO -   Batch 550/2527: Loss = 0.0406
2026-01-08 03:37:17,639 - INFO -   Batch 600/2527: Loss = 0.0396
2026-01-08 03:37:41,213 - INFO -   Batch 650/2527: Loss = 0.0272
2026-01-08 03:38:05,363 - INFO -   Batch 700/2527: Loss = 0.0509
2026-01-08 03:38:28,962 - INFO -   Batch 750/2527: Loss = 0.0432
2026-01-08 03:38:52,786 - INFO -   Batch 800/2527: Loss = 0.0400
2026-01-08 03:39:16,657 - INFO -   Batch 850/2527: Loss = 0.0483
2026-01-08 03:39:40,450 - INFO -   Batch 900/2527: Loss = 0.0447
2026-01-08 03:40:04,052 - INFO -   Batch 950/2527: Loss = 0.0452
2026-01-08 03:40:28,049 - INFO -   Batch 1000/2527: Loss = 0.0420
2026-01-08 03:40:52,001 - INFO -   Batch 1050/2527: Loss = 0.0443
2026-01-08 03:41:15,574 - INFO -   Batch 1100/2527: Loss = 0.0473
2026-01-08 03:41:39,334 - INFO -   Batch 1150/2527: Loss = 0.0211
2026-01-08 03:42:02,963 - INFO -   Batch 1200/2527: Loss = 0.0333
2026-01-08 03:42:26,780 - INFO -   Batch 1250/2527: Loss = 0.0397
2026-01-08 03:42:50,640 - INFO -   Batch 1300/2527: Loss = 0.0544
2026-01-08 03:43:14,429 - INFO -   Batch 1350/2527: Loss = 0.0381
2026-01-08 03:43:38,055 - INFO -   Batch 1400/2527: Loss = 0.0333
2026-01-08 03:44:01,680 - INFO -   Batch 1450/2527: Loss = 0.0278
2026-01-08 03:44:25,639 - INFO -   Batch 1500/2527: Loss = 0.0375
2026-01-08 03:44:49,222 - INFO -   Batch 1550/2527: Loss = 0.0547
2026-01-08 03:45:12,976 - INFO -   Batch 1600/2527: Loss = 0.0224
2026-01-08 03:45:36,577 - INFO -   Batch 1650/2527: Loss = 0.0338
2026-01-08 03:46:00,405 - INFO -   Batch 1700/2527: Loss = 0.0476
2026-01-08 03:46:24,619 - INFO -   Batch 1750/2527: Loss = 0.0429
2026-01-08 03:46:48,403 - INFO -   Batch 1800/2527: Loss = 0.0437
2026-01-08 03:47:11,998 - INFO -   Batch 1850/2527: Loss = 0.0412
2026-01-08 03:47:35,652 - INFO -   Batch 1900/2527: Loss = 0.0300
2026-01-08 03:47:59,618 - INFO -   Batch 1950/2527: Loss = 0.0313
2026-01-08 03:48:23,189 - INFO -   Batch 2000/2527: Loss = 0.0375
2026-01-08 03:48:46,939 - INFO -   Batch 2050/2527: Loss = 0.0255
2026-01-08 03:49:10,544 - INFO -   Batch 2100/2527: Loss = 0.0220
2026-01-08 03:49:34,726 - INFO -   Batch 2150/2527: Loss = 0.0517
2026-01-08 03:49:58,592 - INFO -   Batch 2200/2527: Loss = 0.0298
2026-01-08 03:50:22,369 - INFO -   Batch 2250/2527: Loss = 0.0767
2026-01-08 03:50:45,960 - INFO -   Batch 2300/2527: Loss = 0.0524
2026-01-08 03:51:09,576 - INFO -   Batch 2350/2527: Loss = 0.0612
2026-01-08 03:51:33,538 - INFO -   Batch 2400/2527: Loss = 0.0379
2026-01-08 03:51:57,484 - INFO -   Batch 2450/2527: Loss = 0.0400
2026-01-08 03:52:21,233 - INFO -   Batch 2500/2527: Loss = 0.0529
2026-01-08 03:57:12,857 - INFO - Epoch 124/160: Train Loss: 0.0415, Val Loss: 1.8225, Val mIoU: 40.07%, Val PixelAcc: 79.46%
2026-01-08 03:57:38,999 - INFO -   Batch 50/2527: Loss = 0.0329
2026-01-08 03:58:04,060 - INFO -   Batch 100/2527: Loss = 0.0252
2026-01-08 03:58:29,023 - INFO -   Batch 150/2527: Loss = 0.0495
2026-01-08 03:58:53,823 - INFO -   Batch 200/2527: Loss = 0.0455
2026-01-08 03:59:18,647 - INFO -   Batch 250/2527: Loss = 0.0493
2026-01-08 03:59:43,684 - INFO -   Batch 300/2527: Loss = 0.0343
2026-01-08 04:00:08,517 - INFO -   Batch 350/2527: Loss = 0.0643
2026-01-08 04:00:33,357 - INFO -   Batch 400/2527: Loss = 0.0259
2026-01-08 04:00:58,481 - INFO -   Batch 450/2527: Loss = 0.0322
2026-01-08 04:01:23,449 - INFO -   Batch 500/2527: Loss = 0.0497
2026-01-08 04:01:48,298 - INFO -   Batch 550/2527: Loss = 0.0356
2026-01-08 04:02:13,120 - INFO -   Batch 600/2527: Loss = 0.0350
2026-01-08 04:02:38,223 - INFO -   Batch 650/2527: Loss = 0.0438
2026-01-08 04:03:03,046 - INFO -   Batch 700/2527: Loss = 0.0389
2026-01-08 04:03:28,171 - INFO -   Batch 750/2527: Loss = 0.0379
2026-01-08 04:03:53,134 - INFO -   Batch 800/2527: Loss = 0.0426
2026-01-08 04:04:17,956 - INFO -   Batch 850/2527: Loss = 0.0327
2026-01-08 04:04:42,806 - INFO -   Batch 900/2527: Loss = 0.0408
2026-01-08 04:05:07,855 - INFO -   Batch 950/2527: Loss = 0.0440
2026-01-08 04:05:32,702 - INFO -   Batch 1000/2527: Loss = 0.0368
2026-01-08 04:05:57,898 - INFO -   Batch 1050/2527: Loss = 0.0568
2026-01-08 04:06:23,012 - INFO -   Batch 1100/2527: Loss = 0.0390
2026-01-08 04:06:48,002 - INFO -   Batch 1150/2527: Loss = 0.0403
2026-01-08 04:07:12,839 - INFO -   Batch 1200/2527: Loss = 0.0317
2026-01-08 04:07:37,711 - INFO -   Batch 1250/2527: Loss = 0.0333
2026-01-08 04:08:02,433 - INFO -   Batch 1300/2527: Loss = 0.0517
2026-01-08 04:08:27,649 - INFO -   Batch 1350/2527: Loss = 0.0489
2026-01-08 04:08:52,771 - INFO -   Batch 1400/2527: Loss = 0.0447
2026-01-08 04:09:17,735 - INFO -   Batch 1450/2527: Loss = 0.0429
2026-01-08 04:09:42,578 - INFO -   Batch 1500/2527: Loss = 0.0386
2026-01-08 04:10:07,425 - INFO -   Batch 1550/2527: Loss = 0.0319
2026-01-08 04:10:32,429 - INFO -   Batch 1600/2527: Loss = 0.0602
2026-01-08 04:10:57,254 - INFO -   Batch 1650/2527: Loss = 0.0366
2026-01-08 04:11:22,090 - INFO -   Batch 1700/2527: Loss = 0.0348
2026-01-08 04:11:47,211 - INFO -   Batch 1750/2527: Loss = 0.0393
2026-01-08 04:12:12,191 - INFO -   Batch 1800/2527: Loss = 0.0404
2026-01-08 04:12:37,034 - INFO -   Batch 1850/2527: Loss = 0.0609
2026-01-08 04:13:01,901 - INFO -   Batch 1900/2527: Loss = 0.0359
2026-01-08 04:13:26,622 - INFO -   Batch 1950/2527: Loss = 0.0526
2026-01-08 04:13:51,481 - INFO -   Batch 2000/2527: Loss = 0.0470
2026-01-08 04:14:16,593 - INFO -   Batch 2050/2527: Loss = 0.0414
2026-01-08 04:14:41,942 - INFO -   Batch 2100/2527: Loss = 0.0548
2026-01-08 04:15:06,758 - INFO -   Batch 2150/2527: Loss = 0.0293
2026-01-08 04:15:31,634 - INFO -   Batch 2200/2527: Loss = 0.0433
2026-01-08 04:15:56,648 - INFO -   Batch 2250/2527: Loss = 0.0557
2026-01-08 04:16:21,500 - INFO -   Batch 2300/2527: Loss = 0.0327
2026-01-08 04:16:46,348 - INFO -   Batch 2350/2527: Loss = 0.0323
2026-01-08 04:17:11,461 - INFO -   Batch 2400/2527: Loss = 0.0362
2026-01-08 04:17:36,462 - INFO -   Batch 2450/2527: Loss = 0.0262
2026-01-08 04:18:01,657 - INFO -   Batch 2500/2527: Loss = 0.0439
2026-01-08 04:22:40,253 - INFO - Epoch 125/160: Train Loss: 0.0413, Val Loss: 1.8153, Val mIoU: 40.13%, Val PixelAcc: 79.44%
2026-01-08 04:23:06,632 - INFO -   Batch 50/2527: Loss = 0.0338
2026-01-08 04:23:31,613 - INFO -   Batch 100/2527: Loss = 0.0475
2026-01-08 04:23:56,420 - INFO -   Batch 150/2527: Loss = 0.0353
2026-01-08 04:24:21,257 - INFO -   Batch 200/2527: Loss = 0.0374
2026-01-08 04:24:46,359 - INFO -   Batch 250/2527: Loss = 0.0334
2026-01-08 04:25:11,714 - INFO -   Batch 300/2527: Loss = 0.0483
2026-01-08 04:25:36,533 - INFO -   Batch 350/2527: Loss = 0.0418
2026-01-08 04:26:01,389 - INFO -   Batch 400/2527: Loss = 0.0348
2026-01-08 04:26:26,118 - INFO -   Batch 450/2527: Loss = 0.0348
2026-01-08 04:26:50,963 - INFO -   Batch 500/2527: Loss = 0.0382
2026-01-08 04:27:16,089 - INFO -   Batch 550/2527: Loss = 0.0360
2026-01-08 04:27:41,078 - INFO -   Batch 600/2527: Loss = 0.0358
2026-01-08 04:28:05,901 - INFO -   Batch 650/2527: Loss = 0.0315
2026-01-08 04:28:30,733 - INFO -   Batch 700/2527: Loss = 0.0195
2026-01-08 04:28:55,747 - INFO -   Batch 750/2527: Loss = 0.0406
2026-01-08 04:29:20,560 - INFO -   Batch 800/2527: Loss = 0.0440
2026-01-08 04:29:45,432 - INFO -   Batch 850/2527: Loss = 0.0310
2026-01-08 04:30:10,539 - INFO -   Batch 900/2527: Loss = 0.0526
2026-01-08 04:30:35,546 - INFO -   Batch 950/2527: Loss = 0.0313
2026-01-08 04:31:00,747 - INFO -   Batch 1000/2527: Loss = 0.0466
2026-01-08 04:31:25,575 - INFO -   Batch 1050/2527: Loss = 0.0290
2026-01-08 04:31:50,319 - INFO -   Batch 1100/2527: Loss = 0.0424
2026-01-08 04:32:15,144 - INFO -   Batch 1150/2527: Loss = 0.0390
2026-01-08 04:32:40,267 - INFO -   Batch 1200/2527: Loss = 0.0406
2026-01-08 04:33:05,243 - INFO -   Batch 1250/2527: Loss = 0.0545
2026-01-08 04:33:30,105 - INFO -   Batch 1300/2527: Loss = 0.0282
2026-01-08 04:33:54,963 - INFO -   Batch 1350/2527: Loss = 0.0326
2026-01-08 04:34:19,969 - INFO -   Batch 1400/2527: Loss = 0.0378
2026-01-08 04:34:45,175 - INFO -   Batch 1450/2527: Loss = 0.0468
2026-01-08 04:35:10,022 - INFO -   Batch 1500/2527: Loss = 0.0351
2026-01-08 04:35:35,155 - INFO -   Batch 1550/2527: Loss = 0.0502
2026-01-08 04:36:00,121 - INFO -   Batch 1600/2527: Loss = 0.0441
2026-01-08 04:36:24,927 - INFO -   Batch 1650/2527: Loss = 0.0295
2026-01-08 04:36:50,146 - INFO -   Batch 1700/2527: Loss = 0.0361
2026-01-08 04:37:14,894 - INFO -   Batch 1750/2527: Loss = 0.0304
2026-01-08 04:37:39,734 - INFO -   Batch 1800/2527: Loss = 0.0443
2026-01-08 04:38:04,848 - INFO -   Batch 1850/2527: Loss = 0.0329
2026-01-08 04:38:29,818 - INFO -   Batch 1900/2527: Loss = 0.0417
2026-01-08 04:38:54,629 - INFO -   Batch 1950/2527: Loss = 0.0431
2026-01-08 04:39:19,482 - INFO -   Batch 2000/2527: Loss = 0.0312
2026-01-08 04:39:44,553 - INFO -   Batch 2050/2527: Loss = 0.0426
2026-01-08 04:40:09,430 - INFO -   Batch 2100/2527: Loss = 0.0665
2026-01-08 04:40:34,259 - INFO -   Batch 2150/2527: Loss = 0.0363
2026-01-08 04:40:59,378 - INFO -   Batch 2200/2527: Loss = 0.0499
2026-01-08 04:41:24,342 - INFO -   Batch 2250/2527: Loss = 0.0325
2026-01-08 04:41:49,164 - INFO -   Batch 2300/2527: Loss = 0.0467
2026-01-08 04:42:14,011 - INFO -   Batch 2350/2527: Loss = 0.0328
2026-01-08 04:42:38,763 - INFO -   Batch 2400/2527: Loss = 0.0212
2026-01-08 04:43:03,984 - INFO -   Batch 2450/2527: Loss = 0.0263
2026-01-08 04:43:29,130 - INFO -   Batch 2500/2527: Loss = 0.0500
2026-01-08 04:48:20,861 - INFO - Epoch 126/160: Train Loss: 0.0411, Val Loss: 1.8654, Val mIoU: 39.83%, Val PixelAcc: 79.38%
2026-01-08 04:48:45,766 - INFO -   Batch 50/2527: Loss = 0.0533
2026-01-08 04:49:08,675 - INFO -   Batch 100/2527: Loss = 0.0400
2026-01-08 04:49:31,639 - INFO -   Batch 150/2527: Loss = 0.0532
2026-01-08 04:49:54,583 - INFO -   Batch 200/2527: Loss = 0.0438
2026-01-08 04:50:17,532 - INFO -   Batch 250/2527: Loss = 0.0421
2026-01-08 04:50:40,488 - INFO -   Batch 300/2527: Loss = 0.0436
2026-01-08 04:51:03,822 - INFO -   Batch 350/2527: Loss = 0.0378
2026-01-08 04:51:26,778 - INFO -   Batch 400/2527: Loss = 0.0327
2026-01-08 04:51:49,750 - INFO -   Batch 450/2527: Loss = 0.0469
2026-01-08 04:52:12,727 - INFO -   Batch 500/2527: Loss = 0.0383
2026-01-08 04:52:35,685 - INFO -   Batch 550/2527: Loss = 0.0494
2026-01-08 04:52:58,644 - INFO -   Batch 600/2527: Loss = 0.0427
2026-01-08 04:53:21,967 - INFO -   Batch 650/2527: Loss = 0.0371
2026-01-08 04:53:44,929 - INFO -   Batch 700/2527: Loss = 0.0353
2026-01-08 04:54:07,889 - INFO -   Batch 750/2527: Loss = 0.0379
2026-01-08 04:54:30,854 - INFO -   Batch 800/2527: Loss = 0.0466
2026-01-08 04:54:53,814 - INFO -   Batch 850/2527: Loss = 0.0384
2026-01-08 04:55:16,791 - INFO -   Batch 900/2527: Loss = 0.0541
2026-01-08 04:55:39,746 - INFO -   Batch 950/2527: Loss = 0.0445
2026-01-08 04:56:02,711 - INFO -   Batch 1000/2527: Loss = 0.0305
2026-01-08 04:56:25,671 - INFO -   Batch 1050/2527: Loss = 0.0475
2026-01-08 04:56:48,628 - INFO -   Batch 1100/2527: Loss = 0.0464
2026-01-08 04:57:11,585 - INFO -   Batch 1150/2527: Loss = 0.0300
2026-01-08 04:57:34,541 - INFO -   Batch 1200/2527: Loss = 0.0403
2026-01-08 04:57:57,500 - INFO -   Batch 1250/2527: Loss = 0.0373
2026-01-08 04:58:20,465 - INFO -   Batch 1300/2527: Loss = 0.0405
2026-01-08 04:58:43,424 - INFO -   Batch 1350/2527: Loss = 0.0604
2026-01-08 04:59:06,755 - INFO -   Batch 1400/2527: Loss = 0.0442
2026-01-08 04:59:29,761 - INFO -   Batch 1450/2527: Loss = 0.0297
2026-01-08 04:59:52,728 - INFO -   Batch 1500/2527: Loss = 0.0372
2026-01-08 05:00:15,688 - INFO -   Batch 1550/2527: Loss = 0.0310
2026-01-08 05:00:38,652 - INFO -   Batch 1600/2527: Loss = 0.0491
2026-01-08 05:01:01,644 - INFO -   Batch 1650/2527: Loss = 0.0519
2026-01-08 05:01:24,611 - INFO -   Batch 1700/2527: Loss = 0.0371
2026-01-08 05:01:47,569 - INFO -   Batch 1750/2527: Loss = 0.0589
2026-01-08 05:02:10,919 - INFO -   Batch 1800/2527: Loss = 0.0439
2026-01-08 05:02:33,904 - INFO -   Batch 1850/2527: Loss = 0.0364
2026-01-08 05:02:56,869 - INFO -   Batch 1900/2527: Loss = 0.0330
2026-01-08 05:03:19,834 - INFO -   Batch 1950/2527: Loss = 0.0481
2026-01-08 05:03:42,806 - INFO -   Batch 2000/2527: Loss = 0.0241
2026-01-08 05:04:05,777 - INFO -   Batch 2050/2527: Loss = 0.0279
2026-01-08 05:04:29,112 - INFO -   Batch 2100/2527: Loss = 0.0404
2026-01-08 05:04:52,069 - INFO -   Batch 2150/2527: Loss = 0.0497
2026-01-08 05:05:15,021 - INFO -   Batch 2200/2527: Loss = 0.0364
2026-01-08 05:05:38,006 - INFO -   Batch 2250/2527: Loss = 0.0473
2026-01-08 05:06:00,964 - INFO -   Batch 2300/2527: Loss = 0.0431
2026-01-08 05:06:23,926 - INFO -   Batch 2350/2527: Loss = 0.0464
2026-01-08 05:06:46,894 - INFO -   Batch 2400/2527: Loss = 0.0480
2026-01-08 05:07:09,895 - INFO -   Batch 2450/2527: Loss = 0.0212
2026-01-08 05:07:32,857 - INFO -   Batch 2500/2527: Loss = 0.0357
2026-01-08 05:12:10,618 - INFO - Epoch 127/160: Train Loss: 0.0408, Val Loss: 1.8510, Val mIoU: 40.00%, Val PixelAcc: 79.42%
2026-01-08 05:12:35,943 - INFO -   Batch 50/2527: Loss = 0.0486
2026-01-08 05:12:59,518 - INFO -   Batch 100/2527: Loss = 0.0458
2026-01-08 05:13:23,435 - INFO -   Batch 150/2527: Loss = 0.0338
2026-01-08 05:13:46,991 - INFO -   Batch 200/2527: Loss = 0.0403
2026-01-08 05:14:10,731 - INFO -   Batch 250/2527: Loss = 0.0322
2026-01-08 05:14:34,715 - INFO -   Batch 300/2527: Loss = 0.0352
2026-01-08 05:14:58,559 - INFO -   Batch 350/2527: Loss = 0.0365
2026-01-08 05:15:22,417 - INFO -   Batch 400/2527: Loss = 0.0327
2026-01-08 05:15:46,206 - INFO -   Batch 450/2527: Loss = 0.0409
2026-01-08 05:16:09,824 - INFO -   Batch 500/2527: Loss = 0.0368
2026-01-08 05:16:33,446 - INFO -   Batch 550/2527: Loss = 0.0423
2026-01-08 05:16:57,398 - INFO -   Batch 600/2527: Loss = 0.0357
2026-01-08 05:17:20,970 - INFO -   Batch 650/2527: Loss = 0.0432
2026-01-08 05:17:45,093 - INFO -   Batch 700/2527: Loss = 0.0353
2026-01-08 05:18:08,716 - INFO -   Batch 750/2527: Loss = 0.0258
2026-01-08 05:18:32,539 - INFO -   Batch 800/2527: Loss = 0.0469
2026-01-08 05:18:56,407 - INFO -   Batch 850/2527: Loss = 0.0313
2026-01-08 05:19:20,182 - INFO -   Batch 900/2527: Loss = 0.0450
2026-01-08 05:19:43,781 - INFO -   Batch 950/2527: Loss = 0.0289
2026-01-08 05:20:07,771 - INFO -   Batch 1000/2527: Loss = 0.0508
2026-01-08 05:20:31,729 - INFO -   Batch 1050/2527: Loss = 0.0410
2026-01-08 05:20:55,298 - INFO -   Batch 1100/2527: Loss = 0.0417
2026-01-08 05:21:19,050 - INFO -   Batch 1150/2527: Loss = 0.0322
2026-01-08 05:21:42,653 - INFO -   Batch 1200/2527: Loss = 0.0336
2026-01-08 05:22:06,495 - INFO -   Batch 1250/2527: Loss = 0.0172
2026-01-08 05:22:30,379 - INFO -   Batch 1300/2527: Loss = 0.0467
2026-01-08 05:22:54,176 - INFO -   Batch 1350/2527: Loss = 0.0321
2026-01-08 05:23:17,768 - INFO -   Batch 1400/2527: Loss = 0.0303
2026-01-08 05:23:41,400 - INFO -   Batch 1450/2527: Loss = 0.0341
2026-01-08 05:24:05,365 - INFO -   Batch 1500/2527: Loss = 0.0346
2026-01-08 05:24:28,941 - INFO -   Batch 1550/2527: Loss = 0.0398
2026-01-08 05:24:52,684 - INFO -   Batch 1600/2527: Loss = 0.0434
2026-01-08 05:25:16,313 - INFO -   Batch 1650/2527: Loss = 0.0434
2026-01-08 05:25:40,126 - INFO -   Batch 1700/2527: Loss = 0.0343
2026-01-08 05:26:04,345 - INFO -   Batch 1750/2527: Loss = 0.0308
2026-01-08 05:26:28,131 - INFO -   Batch 1800/2527: Loss = 0.0666
2026-01-08 05:26:51,769 - INFO -   Batch 1850/2527: Loss = 0.0433
2026-01-08 05:27:15,426 - INFO -   Batch 1900/2527: Loss = 0.0311
2026-01-08 05:27:39,380 - INFO -   Batch 1950/2527: Loss = 0.0468
2026-01-08 05:28:02,971 - INFO -   Batch 2000/2527: Loss = 0.0421
2026-01-08 05:28:26,728 - INFO -   Batch 2050/2527: Loss = 0.0431
2026-01-08 05:28:50,363 - INFO -   Batch 2100/2527: Loss = 0.0225
2026-01-08 05:29:14,537 - INFO -   Batch 2150/2527: Loss = 0.0327
2026-01-08 05:29:38,388 - INFO -   Batch 2200/2527: Loss = 0.0558
2026-01-08 05:30:02,178 - INFO -   Batch 2250/2527: Loss = 0.0369
2026-01-08 05:30:25,777 - INFO -   Batch 2300/2527: Loss = 0.0363
2026-01-08 05:30:49,443 - INFO -   Batch 2350/2527: Loss = 0.0337
2026-01-08 05:31:13,391 - INFO -   Batch 2400/2527: Loss = 0.0378
2026-01-08 05:31:37,350 - INFO -   Batch 2450/2527: Loss = 0.0503
2026-01-08 05:32:01,104 - INFO -   Batch 2500/2527: Loss = 0.0547
2026-01-08 05:36:52,167 - INFO - Epoch 128/160: Train Loss: 0.0405, Val Loss: 1.8477, Val mIoU: 39.88%, Val PixelAcc: 79.39%
2026-01-08 05:37:18,163 - INFO -   Batch 50/2527: Loss = 0.0346
2026-01-08 05:37:43,221 - INFO -   Batch 100/2527: Loss = 0.0338
2026-01-08 05:38:08,149 - INFO -   Batch 150/2527: Loss = 0.0537
2026-01-08 05:38:32,952 - INFO -   Batch 200/2527: Loss = 0.0363
2026-01-08 05:38:57,780 - INFO -   Batch 250/2527: Loss = 0.0744
2026-01-08 05:39:22,785 - INFO -   Batch 300/2527: Loss = 0.0421
2026-01-08 05:39:47,646 - INFO -   Batch 350/2527: Loss = 0.0442
2026-01-08 05:40:12,504 - INFO -   Batch 400/2527: Loss = 0.0286
2026-01-08 05:40:37,620 - INFO -   Batch 450/2527: Loss = 0.0342
2026-01-08 05:41:02,576 - INFO -   Batch 500/2527: Loss = 0.0249
2026-01-08 05:41:27,398 - INFO -   Batch 550/2527: Loss = 0.0377
2026-01-08 05:41:52,225 - INFO -   Batch 600/2527: Loss = 0.0332
2026-01-08 05:42:17,308 - INFO -   Batch 650/2527: Loss = 0.0383
2026-01-08 05:42:42,136 - INFO -   Batch 700/2527: Loss = 0.0440
2026-01-08 05:43:07,247 - INFO -   Batch 750/2527: Loss = 0.0351
2026-01-08 05:43:32,216 - INFO -   Batch 800/2527: Loss = 0.0298
2026-01-08 05:43:57,040 - INFO -   Batch 850/2527: Loss = 0.0429
2026-01-08 05:44:21,898 - INFO -   Batch 900/2527: Loss = 0.0356
2026-01-08 05:44:46,939 - INFO -   Batch 950/2527: Loss = 0.0354
2026-01-08 05:45:11,761 - INFO -   Batch 1000/2527: Loss = 0.0589
2026-01-08 05:45:36,588 - INFO -   Batch 1050/2527: Loss = 0.0393
2026-01-08 05:46:02,055 - INFO -   Batch 1100/2527: Loss = 0.0423
2026-01-08 05:46:27,014 - INFO -   Batch 1150/2527: Loss = 0.0454
2026-01-08 05:46:51,853 - INFO -   Batch 1200/2527: Loss = 0.0504
2026-01-08 05:47:16,677 - INFO -   Batch 1250/2527: Loss = 0.0380
2026-01-08 05:47:41,434 - INFO -   Batch 1300/2527: Loss = 0.0320
2026-01-08 05:48:06,300 - INFO -   Batch 1350/2527: Loss = 0.0400
2026-01-08 05:48:31,797 - INFO -   Batch 1400/2527: Loss = 0.0622
2026-01-08 05:48:56,769 - INFO -   Batch 1450/2527: Loss = 0.0576
2026-01-08 05:49:21,605 - INFO -   Batch 1500/2527: Loss = 0.0359
2026-01-08 05:49:46,473 - INFO -   Batch 1550/2527: Loss = 0.0458
2026-01-08 05:50:11,475 - INFO -   Batch 1600/2527: Loss = 0.0314
2026-01-08 05:50:36,297 - INFO -   Batch 1650/2527: Loss = 0.0405
2026-01-08 05:51:01,129 - INFO -   Batch 1700/2527: Loss = 0.0365
2026-01-08 05:51:26,235 - INFO -   Batch 1750/2527: Loss = 0.0450
2026-01-08 05:51:51,225 - INFO -   Batch 1800/2527: Loss = 0.0415
2026-01-08 05:52:16,063 - INFO -   Batch 1850/2527: Loss = 0.0388
2026-01-08 05:52:40,899 - INFO -   Batch 1900/2527: Loss = 0.0314
2026-01-08 05:53:05,611 - INFO -   Batch 1950/2527: Loss = 0.0301
2026-01-08 05:53:30,452 - INFO -   Batch 2000/2527: Loss = 0.0394
2026-01-08 05:53:55,566 - INFO -   Batch 2050/2527: Loss = 0.0333
2026-01-08 05:54:20,914 - INFO -   Batch 2100/2527: Loss = 0.0246
2026-01-08 05:54:45,750 - INFO -   Batch 2150/2527: Loss = 0.0481
2026-01-08 05:55:10,568 - INFO -   Batch 2200/2527: Loss = 0.0345
2026-01-08 05:55:35,602 - INFO -   Batch 2250/2527: Loss = 0.0368
2026-01-08 05:56:00,446 - INFO -   Batch 2300/2527: Loss = 0.0362
2026-01-08 05:56:25,268 - INFO -   Batch 2350/2527: Loss = 0.0317
2026-01-08 05:56:50,409 - INFO -   Batch 2400/2527: Loss = 0.0413
2026-01-08 05:57:15,379 - INFO -   Batch 2450/2527: Loss = 0.0311
2026-01-08 05:57:40,197 - INFO -   Batch 2500/2527: Loss = 0.0407
2026-01-08 06:02:18,092 - INFO - Epoch 129/160: Train Loss: 0.0403, Val Loss: 1.8585, Val mIoU: 39.97%, Val PixelAcc: 79.44%
2026-01-08 06:02:44,566 - INFO -   Batch 50/2527: Loss = 0.0510
2026-01-08 06:03:09,540 - INFO -   Batch 100/2527: Loss = 0.0426
2026-01-08 06:03:34,346 - INFO -   Batch 150/2527: Loss = 0.0505
2026-01-08 06:03:59,165 - INFO -   Batch 200/2527: Loss = 0.0470
2026-01-08 06:04:24,273 - INFO -   Batch 250/2527: Loss = 0.0286
2026-01-08 06:04:49,608 - INFO -   Batch 300/2527: Loss = 0.0424
2026-01-08 06:05:14,430 - INFO -   Batch 350/2527: Loss = 0.0387
2026-01-08 06:05:39,283 - INFO -   Batch 400/2527: Loss = 0.0482
2026-01-08 06:06:04,032 - INFO -   Batch 450/2527: Loss = 0.0525
2026-01-08 06:06:28,889 - INFO -   Batch 500/2527: Loss = 0.0488
2026-01-08 06:06:54,008 - INFO -   Batch 550/2527: Loss = 0.0398
2026-01-08 06:07:19,000 - INFO -   Batch 600/2527: Loss = 0.0341
2026-01-08 06:07:43,825 - INFO -   Batch 650/2527: Loss = 0.0448
2026-01-08 06:08:08,651 - INFO -   Batch 700/2527: Loss = 0.0440
2026-01-08 06:08:33,653 - INFO -   Batch 750/2527: Loss = 0.0432
2026-01-08 06:08:58,480 - INFO -   Batch 800/2527: Loss = 0.0386
2026-01-08 06:09:23,303 - INFO -   Batch 850/2527: Loss = 0.0215
2026-01-08 06:09:48,447 - INFO -   Batch 900/2527: Loss = 0.0422
2026-01-08 06:10:13,482 - INFO -   Batch 950/2527: Loss = 0.0325
2026-01-08 06:10:38,304 - INFO -   Batch 1000/2527: Loss = 0.0292
2026-01-08 06:11:03,526 - INFO -   Batch 1050/2527: Loss = 0.0548
2026-01-08 06:11:28,245 - INFO -   Batch 1100/2527: Loss = 0.0546
2026-01-08 06:11:53,079 - INFO -   Batch 1150/2527: Loss = 0.0393
2026-01-08 06:12:18,198 - INFO -   Batch 1200/2527: Loss = 0.0302
2026-01-08 06:12:43,154 - INFO -   Batch 1250/2527: Loss = 0.0332
2026-01-08 06:13:07,982 - INFO -   Batch 1300/2527: Loss = 0.0374
2026-01-08 06:13:32,850 - INFO -   Batch 1350/2527: Loss = 0.0301
2026-01-08 06:13:57,866 - INFO -   Batch 1400/2527: Loss = 0.0397
2026-01-08 06:14:23,040 - INFO -   Batch 1450/2527: Loss = 0.0642
2026-01-08 06:14:47,867 - INFO -   Batch 1500/2527: Loss = 0.0348
2026-01-08 06:15:13,023 - INFO -   Batch 1550/2527: Loss = 0.0239
2026-01-08 06:15:38,013 - INFO -   Batch 1600/2527: Loss = 0.0660
2026-01-08 06:16:02,827 - INFO -   Batch 1650/2527: Loss = 0.0364
2026-01-08 06:16:27,649 - INFO -   Batch 1700/2527: Loss = 0.0456
2026-01-08 06:16:52,724 - INFO -   Batch 1750/2527: Loss = 0.0305
2026-01-08 06:17:17,556 - INFO -   Batch 1800/2527: Loss = 0.0403
2026-01-08 06:17:42,656 - INFO -   Batch 1850/2527: Loss = 0.0351
2026-01-08 06:18:07,640 - INFO -   Batch 1900/2527: Loss = 0.0394
2026-01-08 06:18:32,450 - INFO -   Batch 1950/2527: Loss = 0.0403
2026-01-08 06:18:57,296 - INFO -   Batch 2000/2527: Loss = 0.0391
2026-01-08 06:19:22,340 - INFO -   Batch 2050/2527: Loss = 0.0367
2026-01-08 06:19:47,207 - INFO -   Batch 2100/2527: Loss = 0.0295
2026-01-08 06:20:12,019 - INFO -   Batch 2150/2527: Loss = 0.0444
2026-01-08 06:20:37,148 - INFO -   Batch 2200/2527: Loss = 0.0400
2026-01-08 06:21:02,099 - INFO -   Batch 2250/2527: Loss = 0.0252
2026-01-08 06:21:26,911 - INFO -   Batch 2300/2527: Loss = 0.0574
2026-01-08 06:21:51,754 - INFO -   Batch 2350/2527: Loss = 0.0518
2026-01-08 06:22:16,482 - INFO -   Batch 2400/2527: Loss = 0.0421
2026-01-08 06:22:41,307 - INFO -   Batch 2450/2527: Loss = 0.0337
2026-01-08 06:23:06,799 - INFO -   Batch 2500/2527: Loss = 0.0458
2026-01-08 06:27:57,865 - INFO - Epoch 130/160: Train Loss: 0.0400, Val Loss: 1.8605, Val mIoU: 39.97%, Val PixelAcc: 79.40%
2026-01-08 06:27:59,914 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_130.pth
2026-01-08 06:27:59,916 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_130.pth
2026-01-08 06:28:24,851 - INFO -   Batch 50/2527: Loss = 0.0419
2026-01-08 06:28:47,761 - INFO -   Batch 100/2527: Loss = 0.0200
2026-01-08 06:29:10,713 - INFO -   Batch 150/2527: Loss = 0.0225
2026-01-08 06:29:33,649 - INFO -   Batch 200/2527: Loss = 0.0449
2026-01-08 06:29:56,593 - INFO -   Batch 250/2527: Loss = 0.0298
2026-01-08 06:30:19,551 - INFO -   Batch 300/2527: Loss = 0.0232
2026-01-08 06:30:42,876 - INFO -   Batch 350/2527: Loss = 0.0282
2026-01-08 06:31:05,832 - INFO -   Batch 400/2527: Loss = 0.0466
2026-01-08 06:31:28,824 - INFO -   Batch 450/2527: Loss = 0.0526
2026-01-08 06:31:51,786 - INFO -   Batch 500/2527: Loss = 0.0518
2026-01-08 06:32:14,745 - INFO -   Batch 550/2527: Loss = 0.0353
2026-01-08 06:32:37,702 - INFO -   Batch 600/2527: Loss = 0.0350
2026-01-08 06:33:01,051 - INFO -   Batch 650/2527: Loss = 0.0352
2026-01-08 06:33:23,999 - INFO -   Batch 700/2527: Loss = 0.0301
2026-01-08 06:33:46,952 - INFO -   Batch 750/2527: Loss = 0.0412
2026-01-08 06:34:09,941 - INFO -   Batch 800/2527: Loss = 0.0418
2026-01-08 06:34:32,907 - INFO -   Batch 850/2527: Loss = 0.0451
2026-01-08 06:34:55,865 - INFO -   Batch 900/2527: Loss = 0.0650
2026-01-08 06:35:18,818 - INFO -   Batch 950/2527: Loss = 0.0416
2026-01-08 06:35:41,803 - INFO -   Batch 1000/2527: Loss = 0.0366
2026-01-08 06:36:04,766 - INFO -   Batch 1050/2527: Loss = 0.0532
2026-01-08 06:36:27,731 - INFO -   Batch 1100/2527: Loss = 0.0420
2026-01-08 06:36:50,687 - INFO -   Batch 1150/2527: Loss = 0.0406
2026-01-08 06:37:13,679 - INFO -   Batch 1200/2527: Loss = 0.0306
2026-01-08 06:37:36,634 - INFO -   Batch 1250/2527: Loss = 0.0429
2026-01-08 06:37:59,596 - INFO -   Batch 1300/2527: Loss = 0.0515
2026-01-08 06:38:22,556 - INFO -   Batch 1350/2527: Loss = 0.0262
2026-01-08 06:38:45,903 - INFO -   Batch 1400/2527: Loss = 0.0373
2026-01-08 06:39:08,892 - INFO -   Batch 1450/2527: Loss = 0.0343
2026-01-08 06:39:31,848 - INFO -   Batch 1500/2527: Loss = 0.0326
2026-01-08 06:39:54,809 - INFO -   Batch 1550/2527: Loss = 0.0434
2026-01-08 06:40:17,770 - INFO -   Batch 1600/2527: Loss = 0.0364
2026-01-08 06:40:40,770 - INFO -   Batch 1650/2527: Loss = 0.0417
2026-01-08 06:41:03,729 - INFO -   Batch 1700/2527: Loss = 0.0404
2026-01-08 06:41:26,688 - INFO -   Batch 1750/2527: Loss = 0.0471
2026-01-08 06:41:50,021 - INFO -   Batch 1800/2527: Loss = 0.0327
2026-01-08 06:42:12,983 - INFO -   Batch 1850/2527: Loss = 0.0333
2026-01-08 06:42:35,952 - INFO -   Batch 1900/2527: Loss = 0.0283
2026-01-08 06:42:58,946 - INFO -   Batch 1950/2527: Loss = 0.0389
2026-01-08 06:43:21,915 - INFO -   Batch 2000/2527: Loss = 0.0294
2026-01-08 06:43:44,880 - INFO -   Batch 2050/2527: Loss = 0.0281
2026-01-08 06:44:08,204 - INFO -   Batch 2100/2527: Loss = 0.0367
2026-01-08 06:44:31,179 - INFO -   Batch 2150/2527: Loss = 0.0442
2026-01-08 06:44:54,129 - INFO -   Batch 2200/2527: Loss = 0.0495
2026-01-08 06:45:17,091 - INFO -   Batch 2250/2527: Loss = 0.0447
2026-01-08 06:45:40,045 - INFO -   Batch 2300/2527: Loss = 0.0314
2026-01-08 06:46:03,000 - INFO -   Batch 2350/2527: Loss = 0.0449
2026-01-08 06:46:25,952 - INFO -   Batch 2400/2527: Loss = 0.0397
2026-01-08 06:46:48,915 - INFO -   Batch 2450/2527: Loss = 0.0378
2026-01-08 06:47:11,882 - INFO -   Batch 2500/2527: Loss = 0.0307
2026-01-08 06:51:48,761 - INFO - Epoch 131/160: Train Loss: 0.0398, Val Loss: 1.8641, Val mIoU: 39.90%, Val PixelAcc: 79.37%
2026-01-08 06:52:14,103 - INFO -   Batch 50/2527: Loss = 0.0470
2026-01-08 06:52:37,716 - INFO -   Batch 100/2527: Loss = 0.0311
2026-01-08 06:53:01,621 - INFO -   Batch 150/2527: Loss = 0.0630
2026-01-08 06:53:25,174 - INFO -   Batch 200/2527: Loss = 0.0406
2026-01-08 06:53:48,917 - INFO -   Batch 250/2527: Loss = 0.0240
2026-01-08 06:54:12,906 - INFO -   Batch 300/2527: Loss = 0.0663
2026-01-08 06:54:36,708 - INFO -   Batch 350/2527: Loss = 0.0495
2026-01-08 06:55:00,552 - INFO -   Batch 400/2527: Loss = 0.0441
2026-01-08 06:55:24,336 - INFO -   Batch 450/2527: Loss = 0.0514
2026-01-08 06:55:47,945 - INFO -   Batch 500/2527: Loss = 0.0475
2026-01-08 06:56:11,603 - INFO -   Batch 550/2527: Loss = 0.0594
2026-01-08 06:56:35,570 - INFO -   Batch 600/2527: Loss = 0.0379
2026-01-08 06:56:59,147 - INFO -   Batch 650/2527: Loss = 0.0383
2026-01-08 06:57:22,899 - INFO -   Batch 700/2527: Loss = 0.0568
2026-01-08 06:57:46,871 - INFO -   Batch 750/2527: Loss = 0.0384
2026-01-08 06:58:10,722 - INFO -   Batch 800/2527: Loss = 0.0416
2026-01-08 06:58:34,579 - INFO -   Batch 850/2527: Loss = 0.0294
2026-01-08 06:58:58,362 - INFO -   Batch 900/2527: Loss = 0.0276
2026-01-08 06:59:21,958 - INFO -   Batch 950/2527: Loss = 0.0403
2026-01-08 06:59:45,582 - INFO -   Batch 1000/2527: Loss = 0.0492
2026-01-08 07:00:09,946 - INFO -   Batch 1050/2527: Loss = 0.0414
2026-01-08 07:00:33,535 - INFO -   Batch 1100/2527: Loss = 0.0427
2026-01-08 07:00:57,287 - INFO -   Batch 1150/2527: Loss = 0.0493
2026-01-08 07:01:20,908 - INFO -   Batch 1200/2527: Loss = 0.0373
2026-01-08 07:01:44,734 - INFO -   Batch 1250/2527: Loss = 0.0257
2026-01-08 07:02:08,587 - INFO -   Batch 1300/2527: Loss = 0.0318
2026-01-08 07:02:32,381 - INFO -   Batch 1350/2527: Loss = 0.0538
2026-01-08 07:02:55,980 - INFO -   Batch 1400/2527: Loss = 0.0287
2026-01-08 07:03:19,601 - INFO -   Batch 1450/2527: Loss = 0.0326
2026-01-08 07:03:43,566 - INFO -   Batch 1500/2527: Loss = 0.0443
2026-01-08 07:04:07,165 - INFO -   Batch 1550/2527: Loss = 0.0235
2026-01-08 07:04:30,922 - INFO -   Batch 1600/2527: Loss = 0.0386
2026-01-08 07:04:54,518 - INFO -   Batch 1650/2527: Loss = 0.0257
2026-01-08 07:05:18,325 - INFO -   Batch 1700/2527: Loss = 0.0566
2026-01-08 07:05:42,561 - INFO -   Batch 1750/2527: Loss = 0.0466
2026-01-08 07:06:06,372 - INFO -   Batch 1800/2527: Loss = 0.0494
2026-01-08 07:06:29,966 - INFO -   Batch 1850/2527: Loss = 0.0539
2026-01-08 07:06:53,589 - INFO -   Batch 1900/2527: Loss = 0.0409
2026-01-08 07:07:17,543 - INFO -   Batch 1950/2527: Loss = 0.0351
2026-01-08 07:07:41,110 - INFO -   Batch 2000/2527: Loss = 0.0488
2026-01-08 07:08:04,860 - INFO -   Batch 2050/2527: Loss = 0.0402
2026-01-08 07:08:28,491 - INFO -   Batch 2100/2527: Loss = 0.0434
2026-01-08 07:08:52,312 - INFO -   Batch 2150/2527: Loss = 0.0461
2026-01-08 07:09:16,528 - INFO -   Batch 2200/2527: Loss = 0.0450
2026-01-08 07:09:40,340 - INFO -   Batch 2250/2527: Loss = 0.0328
2026-01-08 07:10:04,010 - INFO -   Batch 2300/2527: Loss = 0.0446
2026-01-08 07:10:27,652 - INFO -   Batch 2350/2527: Loss = 0.0348
2026-01-08 07:10:51,637 - INFO -   Batch 2400/2527: Loss = 0.0297
2026-01-08 07:11:16,465 - INFO -   Batch 2450/2527: Loss = 0.0395
2026-01-08 07:11:40,178 - INFO -   Batch 2500/2527: Loss = 0.0389
2026-01-08 07:16:30,955 - INFO - Epoch 132/160: Train Loss: 0.0396, Val Loss: 1.8669, Val mIoU: 39.83%, Val PixelAcc: 79.40%
2026-01-08 07:16:57,077 - INFO -   Batch 50/2527: Loss = 0.0389
2026-01-08 07:17:22,166 - INFO -   Batch 100/2527: Loss = 0.0373
2026-01-08 07:17:47,191 - INFO -   Batch 150/2527: Loss = 0.0396
2026-01-08 07:18:12,017 - INFO -   Batch 200/2527: Loss = 0.0354
2026-01-08 07:18:36,843 - INFO -   Batch 250/2527: Loss = 0.0459
2026-01-08 07:19:02,091 - INFO -   Batch 300/2527: Loss = 0.0480
2026-01-08 07:19:26,956 - INFO -   Batch 350/2527: Loss = 0.0403
2026-01-08 07:19:51,878 - INFO -   Batch 400/2527: Loss = 0.0353
2026-01-08 07:20:17,074 - INFO -   Batch 450/2527: Loss = 0.0347
2026-01-08 07:20:42,127 - INFO -   Batch 500/2527: Loss = 0.0440
2026-01-08 07:21:06,973 - INFO -   Batch 550/2527: Loss = 0.0314
2026-01-08 07:21:32,075 - INFO -   Batch 600/2527: Loss = 0.0320
2026-01-08 07:21:56,901 - INFO -   Batch 650/2527: Loss = 0.0303
2026-01-08 07:22:22,110 - INFO -   Batch 700/2527: Loss = 0.0345
2026-01-08 07:22:47,340 - INFO -   Batch 750/2527: Loss = 0.0436
2026-01-08 07:23:12,321 - INFO -   Batch 800/2527: Loss = 0.0253
2026-01-08 07:23:37,161 - INFO -   Batch 850/2527: Loss = 0.0332
2026-01-08 07:24:01,998 - INFO -   Batch 900/2527: Loss = 0.0390
2026-01-08 07:24:27,108 - INFO -   Batch 950/2527: Loss = 0.0563
2026-01-08 07:24:52,019 - INFO -   Batch 1000/2527: Loss = 0.0380
2026-01-08 07:25:16,959 - INFO -   Batch 1050/2527: Loss = 0.0466
2026-01-08 07:25:42,448 - INFO -   Batch 1100/2527: Loss = 0.0518
2026-01-08 07:26:07,404 - INFO -   Batch 1150/2527: Loss = 0.0466
2026-01-08 07:26:32,226 - INFO -   Batch 1200/2527: Loss = 0.0353
2026-01-08 07:26:57,048 - INFO -   Batch 1250/2527: Loss = 0.0539
2026-01-08 07:27:21,912 - INFO -   Batch 1300/2527: Loss = 0.0540
2026-01-08 07:27:46,742 - INFO -   Batch 1350/2527: Loss = 0.0296
2026-01-08 07:28:12,288 - INFO -   Batch 1400/2527: Loss = 0.0417
2026-01-08 07:28:37,263 - INFO -   Batch 1450/2527: Loss = 0.0367
2026-01-08 07:29:02,106 - INFO -   Batch 1500/2527: Loss = 0.0295
2026-01-08 07:29:26,916 - INFO -   Batch 1550/2527: Loss = 0.0464
2026-01-08 07:29:52,060 - INFO -   Batch 1600/2527: Loss = 0.0347
2026-01-08 07:30:16,930 - INFO -   Batch 1650/2527: Loss = 0.0539
2026-01-08 07:30:41,951 - INFO -   Batch 1700/2527: Loss = 0.0416
2026-01-08 07:31:07,020 - INFO -   Batch 1750/2527: Loss = 0.0473
2026-01-08 07:31:31,951 - INFO -   Batch 1800/2527: Loss = 0.0339
2026-01-08 07:31:56,752 - INFO -   Batch 1850/2527: Loss = 0.0263
2026-01-08 07:32:21,592 - INFO -   Batch 1900/2527: Loss = 0.0431
2026-01-08 07:32:46,380 - INFO -   Batch 1950/2527: Loss = 0.0413
2026-01-08 07:33:11,220 - INFO -   Batch 2000/2527: Loss = 0.0537
2026-01-08 07:33:36,414 - INFO -   Batch 2050/2527: Loss = 0.0354
2026-01-08 07:34:01,333 - INFO -   Batch 2100/2527: Loss = 0.0574
2026-01-08 07:34:26,504 - INFO -   Batch 2150/2527: Loss = 0.0368
2026-01-08 07:34:51,318 - INFO -   Batch 2200/2527: Loss = 0.0380
2026-01-08 07:35:16,330 - INFO -   Batch 2250/2527: Loss = 0.0267
2026-01-08 07:35:41,198 - INFO -   Batch 2300/2527: Loss = 0.0455
2026-01-08 07:36:06,088 - INFO -   Batch 2350/2527: Loss = 0.0368
2026-01-08 07:36:31,180 - INFO -   Batch 2400/2527: Loss = 0.0548
2026-01-08 07:36:56,448 - INFO -   Batch 2450/2527: Loss = 0.0487
2026-01-08 07:37:21,290 - INFO -   Batch 2500/2527: Loss = 0.0377
2026-01-08 07:42:00,403 - INFO - Epoch 133/160: Train Loss: 0.0393, Val Loss: 1.8820, Val mIoU: 39.79%, Val PixelAcc: 79.37%
2026-01-08 07:42:26,702 - INFO -   Batch 50/2527: Loss = 0.0313
2026-01-08 07:42:51,670 - INFO -   Batch 100/2527: Loss = 0.0440
2026-01-08 07:43:16,513 - INFO -   Batch 150/2527: Loss = 0.0330
2026-01-08 07:43:41,345 - INFO -   Batch 200/2527: Loss = 0.0507
2026-01-08 07:44:06,492 - INFO -   Batch 250/2527: Loss = 0.0409
2026-01-08 07:44:31,484 - INFO -   Batch 300/2527: Loss = 0.0423
2026-01-08 07:44:56,295 - INFO -   Batch 350/2527: Loss = 0.0347
2026-01-08 07:45:21,117 - INFO -   Batch 400/2527: Loss = 0.0428
2026-01-08 07:45:46,198 - INFO -   Batch 450/2527: Loss = 0.0372
2026-01-08 07:46:11,054 - INFO -   Batch 500/2527: Loss = 0.0370
2026-01-08 07:46:36,176 - INFO -   Batch 550/2527: Loss = 0.0369
2026-01-08 07:47:01,199 - INFO -   Batch 600/2527: Loss = 0.0234
2026-01-08 07:47:26,029 - INFO -   Batch 650/2527: Loss = 0.0451
2026-01-08 07:47:51,249 - INFO -   Batch 700/2527: Loss = 0.0355
2026-01-08 07:48:16,270 - INFO -   Batch 750/2527: Loss = 0.0413
2026-01-08 07:48:41,100 - INFO -   Batch 800/2527: Loss = 0.0455
2026-01-08 07:49:05,963 - INFO -   Batch 850/2527: Loss = 0.0398
2026-01-08 07:49:31,117 - INFO -   Batch 900/2527: Loss = 0.0490
2026-01-08 07:49:56,093 - INFO -   Batch 950/2527: Loss = 0.0258
2026-01-08 07:50:20,917 - INFO -   Batch 1000/2527: Loss = 0.0414
2026-01-08 07:50:45,759 - INFO -   Batch 1050/2527: Loss = 0.0447
2026-01-08 07:51:10,506 - INFO -   Batch 1100/2527: Loss = 0.0278
2026-01-08 07:51:35,345 - INFO -   Batch 1150/2527: Loss = 0.0394
2026-01-08 07:52:00,502 - INFO -   Batch 1200/2527: Loss = 0.0272
2026-01-08 07:52:25,485 - INFO -   Batch 1250/2527: Loss = 0.0479
2026-01-08 07:52:50,300 - INFO -   Batch 1300/2527: Loss = 0.0355
2026-01-08 07:53:15,136 - INFO -   Batch 1350/2527: Loss = 0.0322
2026-01-08 07:53:40,146 - INFO -   Batch 1400/2527: Loss = 0.0553
2026-01-08 07:54:05,341 - INFO -   Batch 1450/2527: Loss = 0.0468
2026-01-08 07:54:30,188 - INFO -   Batch 1500/2527: Loss = 0.0447
2026-01-08 07:54:55,331 - INFO -   Batch 1550/2527: Loss = 0.0296
2026-01-08 07:55:20,321 - INFO -   Batch 1600/2527: Loss = 0.0454
2026-01-08 07:55:45,162 - INFO -   Batch 1650/2527: Loss = 0.0477
2026-01-08 07:56:10,026 - INFO -   Batch 1700/2527: Loss = 0.0507
2026-01-08 07:56:34,749 - INFO -   Batch 1750/2527: Loss = 0.0494
2026-01-08 07:56:59,655 - INFO -   Batch 1800/2527: Loss = 0.0339
2026-01-08 07:57:24,747 - INFO -   Batch 1850/2527: Loss = 0.0370
2026-01-08 07:57:50,101 - INFO -   Batch 1900/2527: Loss = 0.0367
2026-01-08 07:58:14,949 - INFO -   Batch 1950/2527: Loss = 0.0446
2026-01-08 07:58:39,824 - INFO -   Batch 2000/2527: Loss = 0.0435
2026-01-08 07:59:04,849 - INFO -   Batch 2050/2527: Loss = 0.0285
2026-01-08 07:59:29,735 - INFO -   Batch 2100/2527: Loss = 0.0393
2026-01-08 07:59:54,942 - INFO -   Batch 2150/2527: Loss = 0.0406
2026-01-08 08:00:20,062 - INFO -   Batch 2200/2527: Loss = 0.0402
2026-01-08 08:00:45,044 - INFO -   Batch 2250/2527: Loss = 0.0390
2026-01-08 08:01:09,898 - INFO -   Batch 2300/2527: Loss = 0.0590
2026-01-08 08:01:34,757 - INFO -   Batch 2350/2527: Loss = 0.0462
2026-01-08 08:01:59,464 - INFO -   Batch 2400/2527: Loss = 0.0275
2026-01-08 08:02:24,299 - INFO -   Batch 2450/2527: Loss = 0.0532
2026-01-08 08:02:49,435 - INFO -   Batch 2500/2527: Loss = 0.0365
2026-01-08 08:07:40,457 - INFO - Epoch 134/160: Train Loss: 0.0392, Val Loss: 1.8937, Val mIoU: 39.85%, Val PixelAcc: 79.37%
2026-01-08 08:08:05,497 - INFO -   Batch 50/2527: Loss = 0.0319
2026-01-08 08:08:28,400 - INFO -   Batch 100/2527: Loss = 0.0204
2026-01-08 08:08:51,333 - INFO -   Batch 150/2527: Loss = 0.0268
2026-01-08 08:09:14,268 - INFO -   Batch 200/2527: Loss = 0.0313
2026-01-08 08:09:37,220 - INFO -   Batch 250/2527: Loss = 0.0485
2026-01-08 08:10:00,167 - INFO -   Batch 300/2527: Loss = 0.0391
2026-01-08 08:10:23,115 - INFO -   Batch 350/2527: Loss = 0.0160
2026-01-08 08:10:46,441 - INFO -   Batch 400/2527: Loss = 0.0251
2026-01-08 08:11:09,395 - INFO -   Batch 450/2527: Loss = 0.0407
2026-01-08 08:11:32,351 - INFO -   Batch 500/2527: Loss = 0.0358
2026-01-08 08:11:55,304 - INFO -   Batch 550/2527: Loss = 0.0428
2026-01-08 08:12:18,272 - INFO -   Batch 600/2527: Loss = 0.0365
2026-01-08 08:12:41,228 - INFO -   Batch 650/2527: Loss = 0.0222
2026-01-08 08:13:04,183 - INFO -   Batch 700/2527: Loss = 0.0346
2026-01-08 08:13:27,139 - INFO -   Batch 750/2527: Loss = 0.0341
2026-01-08 08:13:50,468 - INFO -   Batch 800/2527: Loss = 0.0360
2026-01-08 08:14:13,429 - INFO -   Batch 850/2527: Loss = 0.0555
2026-01-08 08:14:36,390 - INFO -   Batch 900/2527: Loss = 0.0409
2026-01-08 08:14:59,348 - INFO -   Batch 950/2527: Loss = 0.0262
2026-01-08 08:15:22,305 - INFO -   Batch 1000/2527: Loss = 0.0538
2026-01-08 08:15:45,260 - INFO -   Batch 1050/2527: Loss = 0.0788
2026-01-08 08:16:08,591 - INFO -   Batch 1100/2527: Loss = 0.0428
2026-01-08 08:16:31,582 - INFO -   Batch 1150/2527: Loss = 0.0440
2026-01-08 08:16:54,538 - INFO -   Batch 1200/2527: Loss = 0.0343
2026-01-08 08:17:17,503 - INFO -   Batch 1250/2527: Loss = 0.0374
2026-01-08 08:17:40,466 - INFO -   Batch 1300/2527: Loss = 0.0426
2026-01-08 08:18:03,467 - INFO -   Batch 1350/2527: Loss = 0.0470
2026-01-08 08:18:26,420 - INFO -   Batch 1400/2527: Loss = 0.0340
2026-01-08 08:18:49,376 - INFO -   Batch 1450/2527: Loss = 0.0403
2026-01-08 08:19:12,333 - INFO -   Batch 1500/2527: Loss = 0.0451
2026-01-08 08:19:35,329 - INFO -   Batch 1550/2527: Loss = 0.0306
2026-01-08 08:19:58,297 - INFO -   Batch 1600/2527: Loss = 0.0455
2026-01-08 08:20:21,258 - INFO -   Batch 1650/2527: Loss = 0.0290
2026-01-08 08:20:44,224 - INFO -   Batch 1700/2527: Loss = 0.0606
2026-01-08 08:21:07,203 - INFO -   Batch 1750/2527: Loss = 0.0321
2026-01-08 08:21:30,166 - INFO -   Batch 1800/2527: Loss = 0.0215
2026-01-08 08:21:53,479 - INFO -   Batch 1850/2527: Loss = 0.0389
2026-01-08 08:22:16,454 - INFO -   Batch 1900/2527: Loss = 0.0327
2026-01-08 08:22:39,412 - INFO -   Batch 1950/2527: Loss = 0.0384
2026-01-08 08:23:02,373 - INFO -   Batch 2000/2527: Loss = 0.0382
2026-01-08 08:23:25,332 - INFO -   Batch 2050/2527: Loss = 0.0427
2026-01-08 08:23:48,302 - INFO -   Batch 2100/2527: Loss = 0.0456
2026-01-08 08:24:11,270 - INFO -   Batch 2150/2527: Loss = 0.0286
2026-01-08 08:24:34,227 - INFO -   Batch 2200/2527: Loss = 0.0293
2026-01-08 08:24:57,535 - INFO -   Batch 2250/2527: Loss = 0.0277
2026-01-08 08:25:20,495 - INFO -   Batch 2300/2527: Loss = 0.0213
2026-01-08 08:25:43,462 - INFO -   Batch 2350/2527: Loss = 0.0384
2026-01-08 08:26:06,418 - INFO -   Batch 2400/2527: Loss = 0.0335
2026-01-08 08:26:29,397 - INFO -   Batch 2450/2527: Loss = 0.0463
2026-01-08 08:26:52,356 - INFO -   Batch 2500/2527: Loss = 0.0386
2026-01-08 08:31:29,617 - INFO - Epoch 135/160: Train Loss: 0.0390, Val Loss: 1.8903, Val mIoU: 39.94%, Val PixelAcc: 79.38%
2026-01-08 08:31:55,233 - INFO -   Batch 50/2527: Loss = 0.0372
2026-01-08 08:32:18,814 - INFO -   Batch 100/2527: Loss = 0.0305
2026-01-08 08:32:42,778 - INFO -   Batch 150/2527: Loss = 0.0339
2026-01-08 08:33:06,351 - INFO -   Batch 200/2527: Loss = 0.0262
2026-01-08 08:33:30,092 - INFO -   Batch 250/2527: Loss = 0.0331
2026-01-08 08:33:53,695 - INFO -   Batch 300/2527: Loss = 0.0442
2026-01-08 08:34:17,516 - INFO -   Batch 350/2527: Loss = 0.0416
2026-01-08 08:34:41,375 - INFO -   Batch 400/2527: Loss = 0.0318
2026-01-08 08:35:05,157 - INFO -   Batch 450/2527: Loss = 0.0559
2026-01-08 08:35:28,746 - INFO -   Batch 500/2527: Loss = 0.0356
2026-01-08 08:35:52,374 - INFO -   Batch 550/2527: Loss = 0.0285
2026-01-08 08:36:16,339 - INFO -   Batch 600/2527: Loss = 0.0419
2026-01-08 08:36:39,928 - INFO -   Batch 650/2527: Loss = 0.0436
2026-01-08 08:37:03,719 - INFO -   Batch 700/2527: Loss = 0.0348
2026-01-08 08:37:27,675 - INFO -   Batch 750/2527: Loss = 0.0347
2026-01-08 08:37:51,489 - INFO -   Batch 800/2527: Loss = 0.0296
2026-01-08 08:38:15,351 - INFO -   Batch 850/2527: Loss = 0.0544
2026-01-08 08:38:39,166 - INFO -   Batch 900/2527: Loss = 0.0258
2026-01-08 08:39:02,781 - INFO -   Batch 950/2527: Loss = 0.0269
2026-01-08 08:39:26,415 - INFO -   Batch 1000/2527: Loss = 0.0296
2026-01-08 08:39:50,401 - INFO -   Batch 1050/2527: Loss = 0.0409
2026-01-08 08:40:14,012 - INFO -   Batch 1100/2527: Loss = 0.0454
2026-01-08 08:40:38,149 - INFO -   Batch 1150/2527: Loss = 0.0343
2026-01-08 08:41:01,768 - INFO -   Batch 1200/2527: Loss = 0.0394
2026-01-08 08:41:25,608 - INFO -   Batch 1250/2527: Loss = 0.0421
2026-01-08 08:41:49,492 - INFO -   Batch 1300/2527: Loss = 0.0397
2026-01-08 08:42:13,330 - INFO -   Batch 1350/2527: Loss = 0.0362
2026-01-08 08:42:36,964 - INFO -   Batch 1400/2527: Loss = 0.0258
2026-01-08 08:43:00,950 - INFO -   Batch 1450/2527: Loss = 0.0398
2026-01-08 08:43:24,909 - INFO -   Batch 1500/2527: Loss = 0.0364
2026-01-08 08:43:48,513 - INFO -   Batch 1550/2527: Loss = 0.0238
2026-01-08 08:44:12,297 - INFO -   Batch 1600/2527: Loss = 0.0377
2026-01-08 08:44:35,937 - INFO -   Batch 1650/2527: Loss = 0.0458
2026-01-08 08:44:59,797 - INFO -   Batch 1700/2527: Loss = 0.0292
2026-01-08 08:45:23,663 - INFO -   Batch 1750/2527: Loss = 0.0268
2026-01-08 08:45:47,450 - INFO -   Batch 1800/2527: Loss = 0.0436
2026-01-08 08:46:11,043 - INFO -   Batch 1850/2527: Loss = 0.0504
2026-01-08 08:46:34,657 - INFO -   Batch 1900/2527: Loss = 0.0432
2026-01-08 08:46:58,612 - INFO -   Batch 1950/2527: Loss = 0.0303
2026-01-08 08:47:22,190 - INFO -   Batch 2000/2527: Loss = 0.0388
2026-01-08 08:47:45,976 - INFO -   Batch 2050/2527: Loss = 0.0470
2026-01-08 08:48:09,579 - INFO -   Batch 2100/2527: Loss = 0.0385
2026-01-08 08:48:33,409 - INFO -   Batch 2150/2527: Loss = 0.0587
2026-01-08 08:48:57,620 - INFO -   Batch 2200/2527: Loss = 0.0350
2026-01-08 08:49:21,404 - INFO -   Batch 2250/2527: Loss = 0.0345
2026-01-08 08:49:45,029 - INFO -   Batch 2300/2527: Loss = 0.0558
2026-01-08 08:50:08,662 - INFO -   Batch 2350/2527: Loss = 0.0378
2026-01-08 08:50:32,621 - INFO -   Batch 2400/2527: Loss = 0.0329
2026-01-08 08:50:56,198 - INFO -   Batch 2450/2527: Loss = 0.0492
2026-01-08 08:51:19,952 - INFO -   Batch 2500/2527: Loss = 0.0338
2026-01-08 08:56:10,054 - INFO - Epoch 136/160: Train Loss: 0.0388, Val Loss: 1.8930, Val mIoU: 39.92%, Val PixelAcc: 79.37%
2026-01-08 08:56:36,214 - INFO -   Batch 50/2527: Loss = 0.0480
2026-01-08 08:57:01,653 - INFO -   Batch 100/2527: Loss = 0.0505
2026-01-08 08:57:26,601 - INFO -   Batch 150/2527: Loss = 0.0453
2026-01-08 08:57:51,430 - INFO -   Batch 200/2527: Loss = 0.0341
2026-01-08 08:58:16,253 - INFO -   Batch 250/2527: Loss = 0.0334
2026-01-08 08:58:41,283 - INFO -   Batch 300/2527: Loss = 0.0475
2026-01-08 08:59:06,124 - INFO -   Batch 350/2527: Loss = 0.0361
2026-01-08 08:59:31,333 - INFO -   Batch 400/2527: Loss = 0.0352
2026-01-08 08:59:56,451 - INFO -   Batch 450/2527: Loss = 0.0325
2026-01-08 09:00:21,412 - INFO -   Batch 500/2527: Loss = 0.0229
2026-01-08 09:00:46,243 - INFO -   Batch 550/2527: Loss = 0.0313
2026-01-08 09:01:11,083 - INFO -   Batch 600/2527: Loss = 0.0384
2026-01-08 09:01:35,846 - INFO -   Batch 650/2527: Loss = 0.0401
2026-01-08 09:02:00,740 - INFO -   Batch 700/2527: Loss = 0.0382
2026-01-08 09:02:25,868 - INFO -   Batch 750/2527: Loss = 0.0632
2026-01-08 09:02:50,826 - INFO -   Batch 800/2527: Loss = 0.0411
2026-01-08 09:03:15,676 - INFO -   Batch 850/2527: Loss = 0.0347
2026-01-08 09:03:40,515 - INFO -   Batch 900/2527: Loss = 0.0255
2026-01-08 09:04:05,646 - INFO -   Batch 950/2527: Loss = 0.0407
2026-01-08 09:04:30,493 - INFO -   Batch 1000/2527: Loss = 0.0215
2026-01-08 09:04:55,335 - INFO -   Batch 1050/2527: Loss = 0.0348
2026-01-08 09:05:20,816 - INFO -   Batch 1100/2527: Loss = 0.0408
2026-01-08 09:05:45,786 - INFO -   Batch 1150/2527: Loss = 0.0350
2026-01-08 09:06:10,666 - INFO -   Batch 1200/2527: Loss = 0.0442
2026-01-08 09:06:35,520 - INFO -   Batch 1250/2527: Loss = 0.0452
2026-01-08 09:07:00,250 - INFO -   Batch 1300/2527: Loss = 0.0311
2026-01-08 09:07:25,151 - INFO -   Batch 1350/2527: Loss = 0.0286
2026-01-08 09:07:50,286 - INFO -   Batch 1400/2527: Loss = 0.0400
2026-01-08 09:08:15,273 - INFO -   Batch 1450/2527: Loss = 0.0543
2026-01-08 09:08:40,110 - INFO -   Batch 1500/2527: Loss = 0.0344
2026-01-08 09:09:05,302 - INFO -   Batch 1550/2527: Loss = 0.0363
2026-01-08 09:09:30,328 - INFO -   Batch 1600/2527: Loss = 0.0477
2026-01-08 09:09:55,166 - INFO -   Batch 1650/2527: Loss = 0.0436
2026-01-08 09:10:20,050 - INFO -   Batch 1700/2527: Loss = 0.0395
2026-01-08 09:10:45,176 - INFO -   Batch 1750/2527: Loss = 0.0280
2026-01-08 09:11:10,521 - INFO -   Batch 1800/2527: Loss = 0.0505
2026-01-08 09:11:35,377 - INFO -   Batch 1850/2527: Loss = 0.0354
2026-01-08 09:12:00,228 - INFO -   Batch 1900/2527: Loss = 0.0222
2026-01-08 09:12:24,962 - INFO -   Batch 1950/2527: Loss = 0.0359
2026-01-08 09:12:49,843 - INFO -   Batch 2000/2527: Loss = 0.0231
2026-01-08 09:13:15,008 - INFO -   Batch 2050/2527: Loss = 0.0349
2026-01-08 09:13:40,021 - INFO -   Batch 2100/2527: Loss = 0.0428
2026-01-08 09:14:04,923 - INFO -   Batch 2150/2527: Loss = 0.0413
2026-01-08 09:14:29,788 - INFO -   Batch 2200/2527: Loss = 0.0440
2026-01-08 09:14:54,805 - INFO -   Batch 2250/2527: Loss = 0.0275
2026-01-08 09:15:19,624 - INFO -   Batch 2300/2527: Loss = 0.0405
2026-01-08 09:15:44,475 - INFO -   Batch 2350/2527: Loss = 0.0472
2026-01-08 09:16:09,611 - INFO -   Batch 2400/2527: Loss = 0.0531
2026-01-08 09:16:34,608 - INFO -   Batch 2450/2527: Loss = 0.0198
2026-01-08 09:16:59,447 - INFO -   Batch 2500/2527: Loss = 0.0291
2026-01-08 09:21:42,472 - INFO - Epoch 137/160: Train Loss: 0.0387, Val Loss: 1.9108, Val mIoU: 39.84%, Val PixelAcc: 79.34%
2026-01-08 09:22:09,252 - INFO -   Batch 50/2527: Loss = 0.0559
2026-01-08 09:22:34,238 - INFO -   Batch 100/2527: Loss = 0.0441
2026-01-08 09:22:59,042 - INFO -   Batch 150/2527: Loss = 0.0457
2026-01-08 09:23:23,849 - INFO -   Batch 200/2527: Loss = 0.0292
2026-01-08 09:23:48,960 - INFO -   Batch 250/2527: Loss = 0.0365
2026-01-08 09:24:13,923 - INFO -   Batch 300/2527: Loss = 0.0399
2026-01-08 09:24:38,771 - INFO -   Batch 350/2527: Loss = 0.0333
2026-01-08 09:25:03,596 - INFO -   Batch 400/2527: Loss = 0.0326
2026-01-08 09:25:28,672 - INFO -   Batch 450/2527: Loss = 0.0336
2026-01-08 09:25:53,507 - INFO -   Batch 500/2527: Loss = 0.0363
2026-01-08 09:26:18,650 - INFO -   Batch 550/2527: Loss = 0.0184
2026-01-08 09:26:43,646 - INFO -   Batch 600/2527: Loss = 0.0336
2026-01-08 09:27:08,488 - INFO -   Batch 650/2527: Loss = 0.0365
2026-01-08 09:27:33,352 - INFO -   Batch 700/2527: Loss = 0.0598
2026-01-08 09:27:58,770 - INFO -   Batch 750/2527: Loss = 0.0304
2026-01-08 09:28:23,607 - INFO -   Batch 800/2527: Loss = 0.0383
2026-01-08 09:28:48,438 - INFO -   Batch 850/2527: Loss = 0.0334
2026-01-08 09:29:13,556 - INFO -   Batch 900/2527: Loss = 0.0600
2026-01-08 09:29:38,525 - INFO -   Batch 950/2527: Loss = 0.0440
2026-01-08 09:30:03,343 - INFO -   Batch 1000/2527: Loss = 0.0414
2026-01-08 09:30:28,197 - INFO -   Batch 1050/2527: Loss = 0.0297
2026-01-08 09:30:52,915 - INFO -   Batch 1100/2527: Loss = 0.0308
2026-01-08 09:31:17,750 - INFO -   Batch 1150/2527: Loss = 0.0348
2026-01-08 09:31:42,896 - INFO -   Batch 1200/2527: Loss = 0.0326
2026-01-08 09:32:07,860 - INFO -   Batch 1250/2527: Loss = 0.0285
2026-01-08 09:32:32,707 - INFO -   Batch 1300/2527: Loss = 0.0278
2026-01-08 09:32:57,551 - INFO -   Batch 1350/2527: Loss = 0.0321
2026-01-08 09:33:22,581 - INFO -   Batch 1400/2527: Loss = 0.0293
2026-01-08 09:33:47,425 - INFO -   Batch 1450/2527: Loss = 0.0453
2026-01-08 09:34:12,664 - INFO -   Batch 1500/2527: Loss = 0.0449
2026-01-08 09:34:37,827 - INFO -   Batch 1550/2527: Loss = 0.0236
2026-01-08 09:35:02,811 - INFO -   Batch 1600/2527: Loss = 0.0420
2026-01-08 09:35:27,666 - INFO -   Batch 1650/2527: Loss = 0.0337
2026-01-08 09:35:52,508 - INFO -   Batch 1700/2527: Loss = 0.0422
2026-01-08 09:36:17,238 - INFO -   Batch 1750/2527: Loss = 0.0469
2026-01-08 09:36:42,123 - INFO -   Batch 1800/2527: Loss = 0.0318
2026-01-08 09:37:07,244 - INFO -   Batch 1850/2527: Loss = 0.0290
2026-01-08 09:37:32,577 - INFO -   Batch 1900/2527: Loss = 0.0518
2026-01-08 09:37:57,413 - INFO -   Batch 1950/2527: Loss = 0.0381
2026-01-08 09:38:22,284 - INFO -   Batch 2000/2527: Loss = 0.0333
2026-01-08 09:38:47,295 - INFO -   Batch 2050/2527: Loss = 0.0597
2026-01-08 09:39:12,129 - INFO -   Batch 2100/2527: Loss = 0.0354
2026-01-08 09:39:36,977 - INFO -   Batch 2150/2527: Loss = 0.0484
2026-01-08 09:40:02,473 - INFO -   Batch 2200/2527: Loss = 0.0474
2026-01-08 09:40:27,437 - INFO -   Batch 2250/2527: Loss = 0.0471
2026-01-08 09:40:52,255 - INFO -   Batch 2300/2527: Loss = 0.0347
2026-01-08 09:41:17,102 - INFO -   Batch 2350/2527: Loss = 0.0323
2026-01-08 09:41:41,831 - INFO -   Batch 2400/2527: Loss = 0.0287
2026-01-08 09:42:06,705 - INFO -   Batch 2450/2527: Loss = 0.0329
2026-01-08 09:42:31,830 - INFO -   Batch 2500/2527: Loss = 0.0472
2026-01-08 09:47:22,458 - INFO - Epoch 138/160: Train Loss: 0.0385, Val Loss: 1.9035, Val mIoU: 39.75%, Val PixelAcc: 79.35%
2026-01-08 09:47:47,476 - INFO -   Batch 50/2527: Loss = 0.0521
2026-01-08 09:48:10,402 - INFO -   Batch 100/2527: Loss = 0.0292
2026-01-08 09:48:33,348 - INFO -   Batch 150/2527: Loss = 0.0387
2026-01-08 09:48:56,286 - INFO -   Batch 200/2527: Loss = 0.0480
2026-01-08 09:49:19,597 - INFO -   Batch 250/2527: Loss = 0.0415
2026-01-08 09:49:42,584 - INFO -   Batch 300/2527: Loss = 0.0361
2026-01-08 09:50:05,557 - INFO -   Batch 350/2527: Loss = 0.0384
2026-01-08 09:50:28,519 - INFO -   Batch 400/2527: Loss = 0.0302
2026-01-08 09:50:51,476 - INFO -   Batch 450/2527: Loss = 0.0371
2026-01-08 09:51:14,478 - INFO -   Batch 500/2527: Loss = 0.0336
2026-01-08 09:51:37,813 - INFO -   Batch 550/2527: Loss = 0.0309
2026-01-08 09:52:00,786 - INFO -   Batch 600/2527: Loss = 0.0371
2026-01-08 09:52:23,745 - INFO -   Batch 650/2527: Loss = 0.0477
2026-01-08 09:52:46,718 - INFO -   Batch 700/2527: Loss = 0.0410
2026-01-08 09:53:09,672 - INFO -   Batch 750/2527: Loss = 0.0468
2026-01-08 09:53:32,638 - INFO -   Batch 800/2527: Loss = 0.0275
2026-01-08 09:53:55,607 - INFO -   Batch 850/2527: Loss = 0.0425
2026-01-08 09:54:18,569 - INFO -   Batch 900/2527: Loss = 0.0722
2026-01-08 09:54:41,535 - INFO -   Batch 950/2527: Loss = 0.0549
2026-01-08 09:55:04,513 - INFO -   Batch 1000/2527: Loss = 0.0439
2026-01-08 09:55:27,474 - INFO -   Batch 1050/2527: Loss = 0.0273
2026-01-08 09:55:50,436 - INFO -   Batch 1100/2527: Loss = 0.0573
2026-01-08 09:56:13,393 - INFO -   Batch 1150/2527: Loss = 0.0377
2026-01-08 09:56:36,367 - INFO -   Batch 1200/2527: Loss = 0.0309
2026-01-08 09:56:59,740 - INFO -   Batch 1250/2527: Loss = 0.0419
2026-01-08 09:57:22,738 - INFO -   Batch 1300/2527: Loss = 0.0460
2026-01-08 09:57:45,685 - INFO -   Batch 1350/2527: Loss = 0.0362
2026-01-08 09:58:08,707 - INFO -   Batch 1400/2527: Loss = 0.0232
2026-01-08 09:58:31,662 - INFO -   Batch 1450/2527: Loss = 0.0480
2026-01-08 09:58:54,617 - INFO -   Batch 1500/2527: Loss = 0.0321
2026-01-08 09:59:17,580 - INFO -   Batch 1550/2527: Loss = 0.0676
2026-01-08 09:59:40,529 - INFO -   Batch 1600/2527: Loss = 0.0308
2026-01-08 10:00:03,749 - INFO -   Batch 1650/2527: Loss = 0.0365
2026-01-08 10:00:27,482 - INFO -   Batch 1700/2527: Loss = 0.0388
2026-01-08 10:00:50,425 - INFO -   Batch 1750/2527: Loss = 0.0410
2026-01-08 10:01:13,768 - INFO -   Batch 1800/2527: Loss = 0.0285
2026-01-08 10:01:36,981 - INFO -   Batch 1850/2527: Loss = 0.0448
2026-01-08 10:02:00,038 - INFO -   Batch 1900/2527: Loss = 0.0511
2026-01-08 10:02:23,111 - INFO -   Batch 1950/2527: Loss = 0.0358
2026-01-08 10:02:46,613 - INFO -   Batch 2000/2527: Loss = 0.0286
2026-01-08 10:03:09,657 - INFO -   Batch 2050/2527: Loss = 0.0225
2026-01-08 10:03:32,702 - INFO -   Batch 2100/2527: Loss = 0.0452
2026-01-08 10:03:55,810 - INFO -   Batch 2150/2527: Loss = 0.0495
2026-01-08 10:04:19,074 - INFO -   Batch 2200/2527: Loss = 0.0279
2026-01-08 10:04:42,442 - INFO -   Batch 2250/2527: Loss = 0.0349
2026-01-08 10:05:05,566 - INFO -   Batch 2300/2527: Loss = 0.0503
2026-01-08 10:05:28,653 - INFO -   Batch 2350/2527: Loss = 0.0389
2026-01-08 10:05:51,850 - INFO -   Batch 2400/2527: Loss = 0.0358
2026-01-08 10:06:15,257 - INFO -   Batch 2450/2527: Loss = 0.0347
2026-01-08 10:06:38,206 - INFO -   Batch 2500/2527: Loss = 0.0406
2026-01-08 10:11:25,051 - INFO - Epoch 139/160: Train Loss: 0.0384, Val Loss: 1.9028, Val mIoU: 39.81%, Val PixelAcc: 79.35%
2026-01-08 10:11:53,041 - INFO -   Batch 50/2527: Loss = 0.0432
2026-01-08 10:12:16,973 - INFO -   Batch 100/2527: Loss = 0.0434
2026-01-08 10:12:40,901 - INFO -   Batch 150/2527: Loss = 0.0413
2026-01-08 10:13:04,714 - INFO -   Batch 200/2527: Loss = 0.0339
2026-01-08 10:13:28,905 - INFO -   Batch 250/2527: Loss = 0.0259
2026-01-08 10:13:52,875 - INFO -   Batch 300/2527: Loss = 0.0480
2026-01-08 10:14:16,993 - INFO -   Batch 350/2527: Loss = 0.0328
2026-01-08 10:14:41,104 - INFO -   Batch 400/2527: Loss = 0.0328
2026-01-08 10:15:05,223 - INFO -   Batch 450/2527: Loss = 0.0234
2026-01-08 10:15:28,933 - INFO -   Batch 500/2527: Loss = 0.0346
2026-01-08 10:15:53,109 - INFO -   Batch 550/2527: Loss = 0.0193
2026-01-08 10:16:17,263 - INFO -   Batch 600/2527: Loss = 0.0398
2026-01-08 10:16:41,306 - INFO -   Batch 650/2527: Loss = 0.0440
2026-01-08 10:17:05,431 - INFO -   Batch 700/2527: Loss = 0.0469
2026-01-08 10:17:29,247 - INFO -   Batch 750/2527: Loss = 0.0425
2026-01-08 10:17:53,353 - INFO -   Batch 800/2527: Loss = 0.0273
2026-01-08 10:18:18,025 - INFO -   Batch 850/2527: Loss = 0.0528
2026-01-08 10:18:42,052 - INFO -   Batch 900/2527: Loss = 0.0392
2026-01-08 10:19:05,890 - INFO -   Batch 950/2527: Loss = 0.0301
2026-01-08 10:19:29,706 - INFO -   Batch 1000/2527: Loss = 0.0439
2026-01-08 10:19:53,908 - INFO -   Batch 1050/2527: Loss = 0.0379
2026-01-08 10:20:17,706 - INFO -   Batch 1100/2527: Loss = 0.0257
2026-01-08 10:20:41,723 - INFO -   Batch 1150/2527: Loss = 0.0326
2026-01-08 10:21:05,597 - INFO -   Batch 1200/2527: Loss = 0.0257
2026-01-08 10:21:29,734 - INFO -   Batch 1250/2527: Loss = 0.0374
2026-01-08 10:21:53,744 - INFO -   Batch 1300/2527: Loss = 0.0328
2026-01-08 10:22:17,754 - INFO -   Batch 1350/2527: Loss = 0.0290
2026-01-08 10:22:41,572 - INFO -   Batch 1400/2527: Loss = 0.0268
2026-01-08 10:23:05,390 - INFO -   Batch 1450/2527: Loss = 0.0418
2026-01-08 10:23:29,596 - INFO -   Batch 1500/2527: Loss = 0.0473
2026-01-08 10:23:53,745 - INFO -   Batch 1550/2527: Loss = 0.0480
2026-01-08 10:24:17,936 - INFO -   Batch 1600/2527: Loss = 0.0444
2026-01-08 10:24:41,809 - INFO -   Batch 1650/2527: Loss = 0.0278
2026-01-08 10:25:05,967 - INFO -   Batch 1700/2527: Loss = 0.0294
2026-01-08 10:25:30,081 - INFO -   Batch 1750/2527: Loss = 0.0397
2026-01-08 10:25:54,138 - INFO -   Batch 1800/2527: Loss = 0.0261
2026-01-08 10:26:18,154 - INFO -   Batch 1850/2527: Loss = 0.0401
2026-01-08 10:26:42,139 - INFO -   Batch 1900/2527: Loss = 0.0258
2026-01-08 10:27:06,332 - INFO -   Batch 1950/2527: Loss = 0.0181
2026-01-08 10:27:30,618 - INFO -   Batch 2000/2527: Loss = 0.0342
2026-01-08 10:27:54,612 - INFO -   Batch 2050/2527: Loss = 0.0375
2026-01-08 10:28:18,488 - INFO -   Batch 2100/2527: Loss = 0.0376
2026-01-08 10:28:42,625 - INFO -   Batch 2150/2527: Loss = 0.0309
2026-01-08 10:29:06,888 - INFO -   Batch 2200/2527: Loss = 0.0391
2026-01-08 10:29:30,898 - INFO -   Batch 2250/2527: Loss = 0.0352
2026-01-08 10:29:55,145 - INFO -   Batch 2300/2527: Loss = 0.0380
2026-01-08 10:30:19,145 - INFO -   Batch 2350/2527: Loss = 0.0450
2026-01-08 10:30:43,329 - INFO -   Batch 2400/2527: Loss = 0.0321
2026-01-08 10:31:07,267 - INFO -   Batch 2450/2527: Loss = 0.0421
2026-01-08 10:31:31,152 - INFO -   Batch 2500/2527: Loss = 0.0381
2026-01-08 10:36:33,352 - INFO - Epoch 140/160: Train Loss: 0.0383, Val Loss: 1.9071, Val mIoU: 39.74%, Val PixelAcc: 79.32%
2026-01-08 10:36:35,596 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_140.pth
2026-01-08 10:36:35,598 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_140.pth
2026-01-08 10:37:02,355 - INFO -   Batch 50/2527: Loss = 0.0328
2026-01-08 10:37:27,729 - INFO -   Batch 100/2527: Loss = 0.0451
2026-01-08 10:37:53,002 - INFO -   Batch 150/2527: Loss = 0.0328
2026-01-08 10:38:18,250 - INFO -   Batch 200/2527: Loss = 0.0408
2026-01-08 10:38:43,429 - INFO -   Batch 250/2527: Loss = 0.0419
2026-01-08 10:39:08,843 - INFO -   Batch 300/2527: Loss = 0.0341
2026-01-08 10:39:33,918 - INFO -   Batch 350/2527: Loss = 0.0422
2026-01-08 10:39:58,968 - INFO -   Batch 400/2527: Loss = 0.0452
2026-01-08 10:40:24,390 - INFO -   Batch 450/2527: Loss = 0.0300
2026-01-08 10:40:49,961 - INFO -   Batch 500/2527: Loss = 0.0526
2026-01-08 10:41:15,146 - INFO -   Batch 550/2527: Loss = 0.0300
2026-01-08 10:41:40,302 - INFO -   Batch 600/2527: Loss = 0.0336
2026-01-08 10:42:05,345 - INFO -   Batch 650/2527: Loss = 0.0327
2026-01-08 10:42:30,559 - INFO -   Batch 700/2527: Loss = 0.0290
2026-01-08 10:42:55,931 - INFO -   Batch 750/2527: Loss = 0.0557
2026-01-08 10:43:21,360 - INFO -   Batch 800/2527: Loss = 0.0238
2026-01-08 10:43:46,592 - INFO -   Batch 850/2527: Loss = 0.0318
2026-01-08 10:44:12,306 - INFO -   Batch 900/2527: Loss = 0.0217
2026-01-08 10:44:37,614 - INFO -   Batch 950/2527: Loss = 0.0478
2026-01-08 10:45:02,793 - INFO -   Batch 1000/2527: Loss = 0.0365
2026-01-08 10:45:27,880 - INFO -   Batch 1050/2527: Loss = 0.0199
2026-01-08 10:45:53,227 - INFO -   Batch 1100/2527: Loss = 0.0408
2026-01-08 10:46:18,558 - INFO -   Batch 1150/2527: Loss = 0.0541
2026-01-08 10:46:44,068 - INFO -   Batch 1200/2527: Loss = 0.0486
2026-01-08 10:47:09,144 - INFO -   Batch 1250/2527: Loss = 0.0396
2026-01-08 10:47:34,116 - INFO -   Batch 1300/2527: Loss = 0.0280
2026-01-08 10:47:59,242 - INFO -   Batch 1350/2527: Loss = 0.0370
2026-01-08 10:48:24,877 - INFO -   Batch 1400/2527: Loss = 0.0339
2026-01-08 10:48:50,106 - INFO -   Batch 1450/2527: Loss = 0.0626
2026-01-08 10:49:15,251 - INFO -   Batch 1500/2527: Loss = 0.0341
2026-01-08 10:49:40,514 - INFO -   Batch 1550/2527: Loss = 0.0409
2026-01-08 10:50:05,921 - INFO -   Batch 1600/2527: Loss = 0.0386
2026-01-08 10:50:30,971 - INFO -   Batch 1650/2527: Loss = 0.0430
2026-01-08 10:50:56,079 - INFO -   Batch 1700/2527: Loss = 0.0346
2026-01-08 10:51:21,611 - INFO -   Batch 1750/2527: Loss = 0.0374
2026-01-08 10:51:47,023 - INFO -   Batch 1800/2527: Loss = 0.0243
2026-01-08 10:52:12,081 - INFO -   Batch 1850/2527: Loss = 0.0268
2026-01-08 10:52:37,165 - INFO -   Batch 1900/2527: Loss = 0.0488
2026-01-08 10:53:02,676 - INFO -   Batch 1950/2527: Loss = 0.0434
2026-01-08 10:53:27,855 - INFO -   Batch 2000/2527: Loss = 0.0299
2026-01-08 10:53:53,229 - INFO -   Batch 2050/2527: Loss = 0.0434
2026-01-08 10:54:18,647 - INFO -   Batch 2100/2527: Loss = 0.0193
2026-01-08 10:54:43,884 - INFO -   Batch 2150/2527: Loss = 0.0273
2026-01-08 10:55:08,942 - INFO -   Batch 2200/2527: Loss = 0.0323
2026-01-08 10:55:34,155 - INFO -   Batch 2250/2527: Loss = 0.0395
2026-01-08 10:55:59,210 - INFO -   Batch 2300/2527: Loss = 0.0316
2026-01-08 10:56:24,867 - INFO -   Batch 2350/2527: Loss = 0.0393
2026-01-08 10:56:50,283 - INFO -   Batch 2400/2527: Loss = 0.0583
2026-01-08 10:57:15,729 - INFO -   Batch 2450/2527: Loss = 0.0431
2026-01-08 10:57:40,860 - INFO -   Batch 2500/2527: Loss = 0.0483
2026-01-08 11:02:29,318 - INFO - Epoch 141/160: Train Loss: 0.0381, Val Loss: 1.9187, Val mIoU: 39.75%, Val PixelAcc: 79.31%
2026-01-08 11:02:55,865 - INFO -   Batch 50/2527: Loss = 0.0387
2026-01-08 11:03:21,390 - INFO -   Batch 100/2527: Loss = 0.0706
2026-01-08 11:03:46,322 - INFO -   Batch 150/2527: Loss = 0.0300
2026-01-08 11:04:11,195 - INFO -   Batch 200/2527: Loss = 0.0631
2026-01-08 11:04:36,400 - INFO -   Batch 250/2527: Loss = 0.0481
2026-01-08 11:05:01,534 - INFO -   Batch 300/2527: Loss = 0.0255
2026-01-08 11:05:26,547 - INFO -   Batch 350/2527: Loss = 0.0371
2026-01-08 11:05:51,546 - INFO -   Batch 400/2527: Loss = 0.0295
2026-01-08 11:06:16,542 - INFO -   Batch 450/2527: Loss = 0.0248
2026-01-08 11:06:41,832 - INFO -   Batch 500/2527: Loss = 0.0386
2026-01-08 11:07:07,335 - INFO -   Batch 550/2527: Loss = 0.0516
2026-01-08 11:07:32,910 - INFO -   Batch 600/2527: Loss = 0.0289
2026-01-08 11:07:58,515 - INFO -   Batch 650/2527: Loss = 0.0333
2026-01-08 11:08:23,634 - INFO -   Batch 700/2527: Loss = 0.0311
2026-01-08 11:08:48,971 - INFO -   Batch 750/2527: Loss = 0.0292
2026-01-08 11:09:14,155 - INFO -   Batch 800/2527: Loss = 0.0391
2026-01-08 11:09:39,258 - INFO -   Batch 850/2527: Loss = 0.0353
2026-01-08 11:10:04,705 - INFO -   Batch 900/2527: Loss = 0.0262
2026-01-08 11:10:30,191 - INFO -   Batch 950/2527: Loss = 0.0296
2026-01-08 11:10:55,121 - INFO -   Batch 1000/2527: Loss = 0.0432
2026-01-08 11:11:20,150 - INFO -   Batch 1050/2527: Loss = 0.0202
2026-01-08 11:11:45,049 - INFO -   Batch 1100/2527: Loss = 0.0350
2026-01-08 11:12:10,028 - INFO -   Batch 1150/2527: Loss = 0.0419
2026-01-08 11:12:35,391 - INFO -   Batch 1200/2527: Loss = 0.0441
2026-01-08 11:13:00,494 - INFO -   Batch 1250/2527: Loss = 0.0227
2026-01-08 11:13:25,442 - INFO -   Batch 1300/2527: Loss = 0.0252
2026-01-08 11:13:50,378 - INFO -   Batch 1350/2527: Loss = 0.0353
2026-01-08 11:14:15,513 - INFO -   Batch 1400/2527: Loss = 0.0273
2026-01-08 11:14:40,447 - INFO -   Batch 1450/2527: Loss = 0.0372
2026-01-08 11:15:05,338 - INFO -   Batch 1500/2527: Loss = 0.0308
2026-01-08 11:15:31,044 - INFO -   Batch 1550/2527: Loss = 0.0362
2026-01-08 11:15:56,082 - INFO -   Batch 1600/2527: Loss = 0.0335
2026-01-08 11:16:21,138 - INFO -   Batch 1650/2527: Loss = 0.0294
2026-01-08 11:16:46,128 - INFO -   Batch 1700/2527: Loss = 0.0316
2026-01-08 11:17:10,913 - INFO -   Batch 1750/2527: Loss = 0.0438
2026-01-08 11:17:35,851 - INFO -   Batch 1800/2527: Loss = 0.0385
2026-01-08 11:18:01,161 - INFO -   Batch 1850/2527: Loss = 0.0411
2026-01-08 11:18:26,185 - INFO -   Batch 1900/2527: Loss = 0.0369
2026-01-08 11:18:51,094 - INFO -   Batch 1950/2527: Loss = 0.0438
2026-01-08 11:19:16,339 - INFO -   Batch 2000/2527: Loss = 0.0379
2026-01-08 11:19:41,599 - INFO -   Batch 2050/2527: Loss = 0.0292
2026-01-08 11:20:06,698 - INFO -   Batch 2100/2527: Loss = 0.0426
2026-01-08 11:20:31,748 - INFO -   Batch 2150/2527: Loss = 0.0423
2026-01-08 11:20:57,146 - INFO -   Batch 2200/2527: Loss = 0.0443
2026-01-08 11:21:22,233 - INFO -   Batch 2250/2527: Loss = 0.0236
2026-01-08 11:21:47,324 - INFO -   Batch 2300/2527: Loss = 0.0421
2026-01-08 11:22:12,444 - INFO -   Batch 2350/2527: Loss = 0.0426
2026-01-08 11:22:37,732 - INFO -   Batch 2400/2527: Loss = 0.0319
2026-01-08 11:23:02,614 - INFO -   Batch 2450/2527: Loss = 0.0504
2026-01-08 11:23:27,804 - INFO -   Batch 2500/2527: Loss = 0.0266
2026-01-08 11:28:24,398 - INFO - Epoch 142/160: Train Loss: 0.0380, Val Loss: 1.9205, Val mIoU: 39.77%, Val PixelAcc: 79.31%
2026-01-08 11:28:49,405 - INFO -   Batch 50/2527: Loss = 0.0453
2026-01-08 11:29:12,293 - INFO -   Batch 100/2527: Loss = 0.0402
2026-01-08 11:29:35,592 - INFO -   Batch 150/2527: Loss = 0.0363
2026-01-08 11:29:58,479 - INFO -   Batch 200/2527: Loss = 0.0346
2026-01-08 11:30:21,372 - INFO -   Batch 250/2527: Loss = 0.0307
2026-01-08 11:30:44,287 - INFO -   Batch 300/2527: Loss = 0.0280
2026-01-08 11:31:07,257 - INFO -   Batch 350/2527: Loss = 0.0460
2026-01-08 11:31:30,173 - INFO -   Batch 400/2527: Loss = 0.0455
2026-01-08 11:31:53,105 - INFO -   Batch 450/2527: Loss = 0.0437
2026-01-08 11:32:16,016 - INFO -   Batch 500/2527: Loss = 0.0305
2026-01-08 11:32:39,059 - INFO -   Batch 550/2527: Loss = 0.0702
2026-01-08 11:33:02,286 - INFO -   Batch 600/2527: Loss = 0.0308
2026-01-08 11:33:25,170 - INFO -   Batch 650/2527: Loss = 0.0263
2026-01-08 11:33:48,137 - INFO -   Batch 700/2527: Loss = 0.0350
2026-01-08 11:34:11,068 - INFO -   Batch 750/2527: Loss = 0.0554
2026-01-08 11:34:33,985 - INFO -   Batch 800/2527: Loss = 0.0452
2026-01-08 11:34:56,903 - INFO -   Batch 850/2527: Loss = 0.0413
2026-01-08 11:35:20,479 - INFO -   Batch 900/2527: Loss = 0.0425
2026-01-08 11:35:44,018 - INFO -   Batch 950/2527: Loss = 0.0429
2026-01-08 11:36:07,269 - INFO -   Batch 1000/2527: Loss = 0.0333
2026-01-08 11:36:30,155 - INFO -   Batch 1050/2527: Loss = 0.0554
2026-01-08 11:36:53,553 - INFO -   Batch 1100/2527: Loss = 0.0422
2026-01-08 11:37:16,763 - INFO -   Batch 1150/2527: Loss = 0.0303
2026-01-08 11:37:39,984 - INFO -   Batch 1200/2527: Loss = 0.0441
2026-01-08 11:38:03,158 - INFO -   Batch 1250/2527: Loss = 0.0239
2026-01-08 11:38:27,133 - INFO -   Batch 1300/2527: Loss = 0.0466
2026-01-08 11:38:50,263 - INFO -   Batch 1350/2527: Loss = 0.0422
2026-01-08 11:39:13,552 - INFO -   Batch 1400/2527: Loss = 0.0364
2026-01-08 11:39:36,770 - INFO -   Batch 1450/2527: Loss = 0.0384
2026-01-08 11:39:59,915 - INFO -   Batch 1500/2527: Loss = 0.0377
2026-01-08 11:40:22,952 - INFO -   Batch 1550/2527: Loss = 0.0389
2026-01-08 11:40:46,727 - INFO -   Batch 1600/2527: Loss = 0.0376
2026-01-08 11:41:09,949 - INFO -   Batch 1650/2527: Loss = 0.0393
2026-01-08 11:41:33,179 - INFO -   Batch 1700/2527: Loss = 0.0364
2026-01-08 11:41:56,583 - INFO -   Batch 1750/2527: Loss = 0.0464
2026-01-08 11:42:19,683 - INFO -   Batch 1800/2527: Loss = 0.0336
2026-01-08 11:42:42,734 - INFO -   Batch 1850/2527: Loss = 0.0456
2026-01-08 11:43:05,860 - INFO -   Batch 1900/2527: Loss = 0.0273
2026-01-08 11:43:28,963 - INFO -   Batch 1950/2527: Loss = 0.0267
2026-01-08 11:43:52,240 - INFO -   Batch 2000/2527: Loss = 0.0354
2026-01-08 11:44:15,534 - INFO -   Batch 2050/2527: Loss = 0.0412
2026-01-08 11:44:39,271 - INFO -   Batch 2100/2527: Loss = 0.0325
2026-01-08 11:45:02,404 - INFO -   Batch 2150/2527: Loss = 0.0427
2026-01-08 11:45:25,469 - INFO -   Batch 2200/2527: Loss = 0.0327
2026-01-08 11:45:48,939 - INFO -   Batch 2250/2527: Loss = 0.0477
2026-01-08 11:46:12,437 - INFO -   Batch 2300/2527: Loss = 0.0177
2026-01-08 11:46:36,598 - INFO -   Batch 2350/2527: Loss = 0.0331
2026-01-08 11:46:59,772 - INFO -   Batch 2400/2527: Loss = 0.0724
2026-01-08 11:47:23,016 - INFO -   Batch 2450/2527: Loss = 0.0538
2026-01-08 11:47:46,186 - INFO -   Batch 2500/2527: Loss = 0.0412
2026-01-08 11:52:36,594 - INFO - Epoch 143/160: Train Loss: 0.0379, Val Loss: 1.9316, Val mIoU: 39.75%, Val PixelAcc: 79.32%
2026-01-08 11:53:03,104 - INFO -   Batch 50/2527: Loss = 0.0439
2026-01-08 11:53:27,151 - INFO -   Batch 100/2527: Loss = 0.0224
2026-01-08 11:53:51,472 - INFO -   Batch 150/2527: Loss = 0.0296
2026-01-08 11:54:15,186 - INFO -   Batch 200/2527: Loss = 0.0310
2026-01-08 11:54:39,441 - INFO -   Batch 250/2527: Loss = 0.0555
2026-01-08 11:55:03,299 - INFO -   Batch 300/2527: Loss = 0.0466
2026-01-08 11:55:27,380 - INFO -   Batch 350/2527: Loss = 0.0644
2026-01-08 11:55:51,567 - INFO -   Batch 400/2527: Loss = 0.0242
2026-01-08 11:56:15,674 - INFO -   Batch 450/2527: Loss = 0.0359
2026-01-08 11:56:40,005 - INFO -   Batch 500/2527: Loss = 0.0363
2026-01-08 11:57:03,976 - INFO -   Batch 550/2527: Loss = 0.0377
2026-01-08 11:57:28,314 - INFO -   Batch 600/2527: Loss = 0.0310
2026-01-08 11:57:52,037 - INFO -   Batch 650/2527: Loss = 0.0311
2026-01-08 11:58:16,242 - INFO -   Batch 700/2527: Loss = 0.0427
2026-01-08 11:58:40,234 - INFO -   Batch 750/2527: Loss = 0.0540
2026-01-08 11:59:04,863 - INFO -   Batch 800/2527: Loss = 0.0562
2026-01-08 11:59:29,208 - INFO -   Batch 850/2527: Loss = 0.0336
2026-01-08 11:59:53,362 - INFO -   Batch 900/2527: Loss = 0.0454
2026-01-08 12:00:17,546 - INFO -   Batch 950/2527: Loss = 0.0275
2026-01-08 12:00:41,579 - INFO -   Batch 1000/2527: Loss = 0.0334
2026-01-08 12:01:05,906 - INFO -   Batch 1050/2527: Loss = 0.0341
2026-01-08 12:01:29,816 - INFO -   Batch 1100/2527: Loss = 0.0371
2026-01-08 12:01:53,862 - INFO -   Batch 1150/2527: Loss = 0.0643
2026-01-08 12:02:18,276 - INFO -   Batch 1200/2527: Loss = 0.0353
2026-01-08 12:02:42,423 - INFO -   Batch 1250/2527: Loss = 0.0327
2026-01-08 12:03:06,614 - INFO -   Batch 1300/2527: Loss = 0.0421
2026-01-08 12:03:30,722 - INFO -   Batch 1350/2527: Loss = 0.0269
2026-01-08 12:03:54,706 - INFO -   Batch 1400/2527: Loss = 0.0283
2026-01-08 12:04:18,631 - INFO -   Batch 1450/2527: Loss = 0.0321
2026-01-08 12:04:43,152 - INFO -   Batch 1500/2527: Loss = 0.0376
2026-01-08 12:05:06,976 - INFO -   Batch 1550/2527: Loss = 0.0422
2026-01-08 12:05:30,989 - INFO -   Batch 1600/2527: Loss = 0.0467
2026-01-08 12:05:54,857 - INFO -   Batch 1650/2527: Loss = 0.0325
2026-01-08 12:06:18,940 - INFO -   Batch 1700/2527: Loss = 0.0332
2026-01-08 12:06:43,108 - INFO -   Batch 1750/2527: Loss = 0.0327
2026-01-08 12:07:07,138 - INFO -   Batch 1800/2527: Loss = 0.0399
2026-01-08 12:07:31,010 - INFO -   Batch 1850/2527: Loss = 0.0307
2026-01-08 12:07:54,911 - INFO -   Batch 1900/2527: Loss = 0.0429
2026-01-08 12:08:19,303 - INFO -   Batch 1950/2527: Loss = 0.0281
2026-01-08 12:08:43,012 - INFO -   Batch 2000/2527: Loss = 0.0393
2026-01-08 12:09:07,109 - INFO -   Batch 2050/2527: Loss = 0.0339
2026-01-08 12:09:31,135 - INFO -   Batch 2100/2527: Loss = 0.0526
2026-01-08 12:09:55,238 - INFO -   Batch 2150/2527: Loss = 0.0387
2026-01-08 12:10:19,340 - INFO -   Batch 2200/2527: Loss = 0.0353
2026-01-08 12:10:43,879 - INFO -   Batch 2250/2527: Loss = 0.0364
2026-01-08 12:11:07,660 - INFO -   Batch 2300/2527: Loss = 0.0388
2026-01-08 12:11:31,579 - INFO -   Batch 2350/2527: Loss = 0.0460
2026-01-08 12:11:55,863 - INFO -   Batch 2400/2527: Loss = 0.0238
2026-01-08 12:12:19,699 - INFO -   Batch 2450/2527: Loss = 0.0446
2026-01-08 12:12:43,705 - INFO -   Batch 2500/2527: Loss = 0.0282
2026-01-08 12:17:43,274 - INFO - Epoch 144/160: Train Loss: 0.0378, Val Loss: 1.9342, Val mIoU: 39.76%, Val PixelAcc: 79.30%
2026-01-08 12:18:09,440 - INFO -   Batch 50/2527: Loss = 0.0410
2026-01-08 12:18:34,959 - INFO -   Batch 100/2527: Loss = 0.0349
2026-01-08 12:18:59,912 - INFO -   Batch 150/2527: Loss = 0.0280
2026-01-08 12:19:24,952 - INFO -   Batch 200/2527: Loss = 0.0386
2026-01-08 12:19:49,749 - INFO -   Batch 250/2527: Loss = 0.0281
2026-01-08 12:20:15,035 - INFO -   Batch 300/2527: Loss = 0.0336
2026-01-08 12:20:40,079 - INFO -   Batch 350/2527: Loss = 0.0392
2026-01-08 12:21:04,962 - INFO -   Batch 400/2527: Loss = 0.0378
2026-01-08 12:21:30,259 - INFO -   Batch 450/2527: Loss = 0.0451
2026-01-08 12:21:55,225 - INFO -   Batch 500/2527: Loss = 0.0438
2026-01-08 12:22:20,089 - INFO -   Batch 550/2527: Loss = 0.0266
2026-01-08 12:22:44,904 - INFO -   Batch 600/2527: Loss = 0.0310
2026-01-08 12:23:09,642 - INFO -   Batch 650/2527: Loss = 0.0326
2026-01-08 12:23:34,421 - INFO -   Batch 700/2527: Loss = 0.0371
2026-01-08 12:23:59,620 - INFO -   Batch 750/2527: Loss = 0.0267
2026-01-08 12:24:25,024 - INFO -   Batch 800/2527: Loss = 0.0288
2026-01-08 12:24:49,900 - INFO -   Batch 850/2527: Loss = 0.0437
2026-01-08 12:25:14,722 - INFO -   Batch 900/2527: Loss = 0.0470
2026-01-08 12:25:39,770 - INFO -   Batch 950/2527: Loss = 0.0417
2026-01-08 12:26:04,592 - INFO -   Batch 1000/2527: Loss = 0.0388
2026-01-08 12:26:29,458 - INFO -   Batch 1050/2527: Loss = 0.0424
2026-01-08 12:26:54,562 - INFO -   Batch 1100/2527: Loss = 0.0406
2026-01-08 12:27:19,546 - INFO -   Batch 1150/2527: Loss = 0.0413
2026-01-08 12:27:44,473 - INFO -   Batch 1200/2527: Loss = 0.0316
2026-01-08 12:28:09,693 - INFO -   Batch 1250/2527: Loss = 0.0357
2026-01-08 12:28:34,456 - INFO -   Batch 1300/2527: Loss = 0.0349
2026-01-08 12:28:59,398 - INFO -   Batch 1350/2527: Loss = 0.0314
2026-01-08 12:29:24,532 - INFO -   Batch 1400/2527: Loss = 0.0370
2026-01-08 12:29:49,596 - INFO -   Batch 1450/2527: Loss = 0.0435
2026-01-08 12:30:14,479 - INFO -   Batch 1500/2527: Loss = 0.0186
2026-01-08 12:30:39,789 - INFO -   Batch 1550/2527: Loss = 0.0261
2026-01-08 12:31:04,798 - INFO -   Batch 1600/2527: Loss = 0.0522
2026-01-08 12:31:29,712 - INFO -   Batch 1650/2527: Loss = 0.0489
2026-01-08 12:31:54,518 - INFO -   Batch 1700/2527: Loss = 0.0364
2026-01-08 12:32:19,618 - INFO -   Batch 1750/2527: Loss = 0.0330
2026-01-08 12:32:44,641 - INFO -   Batch 1800/2527: Loss = 0.0533
2026-01-08 12:33:09,480 - INFO -   Batch 1850/2527: Loss = 0.0300
2026-01-08 12:33:34,392 - INFO -   Batch 1900/2527: Loss = 0.0160
2026-01-08 12:33:59,082 - INFO -   Batch 1950/2527: Loss = 0.0351
2026-01-08 12:34:23,983 - INFO -   Batch 2000/2527: Loss = 0.0270
2026-01-08 12:34:49,119 - INFO -   Batch 2050/2527: Loss = 0.0432
2026-01-08 12:35:14,155 - INFO -   Batch 2100/2527: Loss = 0.0325
2026-01-08 12:35:38,991 - INFO -   Batch 2150/2527: Loss = 0.0401
2026-01-08 12:36:03,960 - INFO -   Batch 2200/2527: Loss = 0.0338
2026-01-08 12:36:29,348 - INFO -   Batch 2250/2527: Loss = 0.0357
2026-01-08 12:36:54,248 - INFO -   Batch 2300/2527: Loss = 0.0325
2026-01-08 12:37:19,178 - INFO -   Batch 2350/2527: Loss = 0.0436
2026-01-08 12:37:44,272 - INFO -   Batch 2400/2527: Loss = 0.0330
2026-01-08 12:38:09,289 - INFO -   Batch 2450/2527: Loss = 0.0332
2026-01-08 12:38:34,156 - INFO -   Batch 2500/2527: Loss = 0.0456
2026-01-08 12:43:12,867 - INFO - Epoch 145/160: Train Loss: 0.0377, Val Loss: 1.9379, Val mIoU: 39.66%, Val PixelAcc: 79.30%
2026-01-08 12:43:39,353 - INFO -   Batch 50/2527: Loss = 0.0228
2026-01-08 12:44:04,315 - INFO -   Batch 100/2527: Loss = 0.0350
2026-01-08 12:44:29,477 - INFO -   Batch 150/2527: Loss = 0.0504
2026-01-08 12:44:54,312 - INFO -   Batch 200/2527: Loss = 0.0446
2026-01-08 12:45:19,503 - INFO -   Batch 250/2527: Loss = 0.0323
2026-01-08 12:45:44,487 - INFO -   Batch 300/2527: Loss = 0.0550
2026-01-08 12:46:09,325 - INFO -   Batch 350/2527: Loss = 0.0305
2026-01-08 12:46:34,187 - INFO -   Batch 400/2527: Loss = 0.0574
2026-01-08 12:46:59,293 - INFO -   Batch 450/2527: Loss = 0.0296
2026-01-08 12:47:24,156 - INFO -   Batch 500/2527: Loss = 0.0629
2026-01-08 12:47:49,283 - INFO -   Batch 550/2527: Loss = 0.0285
2026-01-08 12:48:14,265 - INFO -   Batch 600/2527: Loss = 0.0289
2026-01-08 12:48:39,106 - INFO -   Batch 650/2527: Loss = 0.0315
2026-01-08 12:49:03,947 - INFO -   Batch 700/2527: Loss = 0.0241
2026-01-08 12:49:28,978 - INFO -   Batch 750/2527: Loss = 0.0374
2026-01-08 12:49:53,812 - INFO -   Batch 800/2527: Loss = 0.0453
2026-01-08 12:50:18,664 - INFO -   Batch 850/2527: Loss = 0.0332
2026-01-08 12:50:43,818 - INFO -   Batch 900/2527: Loss = 0.0303
2026-01-08 12:51:08,807 - INFO -   Batch 950/2527: Loss = 0.0506
2026-01-08 12:51:33,651 - INFO -   Batch 1000/2527: Loss = 0.0475
2026-01-08 12:51:58,512 - INFO -   Batch 1050/2527: Loss = 0.0348
2026-01-08 12:52:23,240 - INFO -   Batch 1100/2527: Loss = 0.0551
2026-01-08 12:52:48,082 - INFO -   Batch 1150/2527: Loss = 0.0470
2026-01-08 12:53:13,575 - INFO -   Batch 1200/2527: Loss = 0.0338
2026-01-08 12:53:38,580 - INFO -   Batch 1250/2527: Loss = 0.0277
2026-01-08 12:54:03,398 - INFO -   Batch 1300/2527: Loss = 0.0445
2026-01-08 12:54:28,276 - INFO -   Batch 1350/2527: Loss = 0.0665
2026-01-08 12:54:53,294 - INFO -   Batch 1400/2527: Loss = 0.0384
2026-01-08 12:55:18,137 - INFO -   Batch 1450/2527: Loss = 0.0236
2026-01-08 12:55:42,972 - INFO -   Batch 1500/2527: Loss = 0.0273
2026-01-08 12:56:08,126 - INFO -   Batch 1550/2527: Loss = 0.0328
2026-01-08 12:56:33,498 - INFO -   Batch 1600/2527: Loss = 0.0282
2026-01-08 12:56:58,319 - INFO -   Batch 1650/2527: Loss = 0.0310
2026-01-08 12:57:23,146 - INFO -   Batch 1700/2527: Loss = 0.0276
2026-01-08 12:57:47,924 - INFO -   Batch 1750/2527: Loss = 0.0376
2026-01-08 12:58:12,790 - INFO -   Batch 1800/2527: Loss = 0.0345
2026-01-08 12:58:37,902 - INFO -   Batch 1850/2527: Loss = 0.0246
2026-01-08 12:59:03,242 - INFO -   Batch 1900/2527: Loss = 0.0417
2026-01-08 12:59:28,106 - INFO -   Batch 1950/2527: Loss = 0.0559
2026-01-08 12:59:52,969 - INFO -   Batch 2000/2527: Loss = 0.0341
2026-01-08 13:00:17,990 - INFO -   Batch 2050/2527: Loss = 0.0285
2026-01-08 13:00:42,828 - INFO -   Batch 2100/2527: Loss = 0.0193
2026-01-08 13:01:07,664 - INFO -   Batch 2150/2527: Loss = 0.0428
2026-01-08 13:01:32,790 - INFO -   Batch 2200/2527: Loss = 0.0362
2026-01-08 13:01:57,789 - INFO -   Batch 2250/2527: Loss = 0.0427
2026-01-08 13:02:22,623 - INFO -   Batch 2300/2527: Loss = 0.0483
2026-01-08 13:02:47,464 - INFO -   Batch 2350/2527: Loss = 0.0446
2026-01-08 13:03:12,198 - INFO -   Batch 2400/2527: Loss = 0.0328
2026-01-08 13:03:37,041 - INFO -   Batch 2450/2527: Loss = 0.0346
2026-01-08 13:04:02,186 - INFO -   Batch 2500/2527: Loss = 0.0496
2026-01-08 13:08:53,112 - INFO - Epoch 146/160: Train Loss: 0.0376, Val Loss: 1.9245, Val mIoU: 39.69%, Val PixelAcc: 79.31%
2026-01-08 13:09:18,172 - INFO -   Batch 50/2527: Loss = 0.0451
2026-01-08 13:09:41,441 - INFO -   Batch 100/2527: Loss = 0.0314
2026-01-08 13:10:04,407 - INFO -   Batch 150/2527: Loss = 0.0380
2026-01-08 13:10:27,346 - INFO -   Batch 200/2527: Loss = 0.0389
2026-01-08 13:10:50,292 - INFO -   Batch 250/2527: Loss = 0.0303
2026-01-08 13:11:13,237 - INFO -   Batch 300/2527: Loss = 0.0389
2026-01-08 13:11:36,221 - INFO -   Batch 350/2527: Loss = 0.0431
2026-01-08 13:11:59,184 - INFO -   Batch 400/2527: Loss = 0.0411
2026-01-08 13:12:22,148 - INFO -   Batch 450/2527: Loss = 0.0473
2026-01-08 13:12:45,483 - INFO -   Batch 500/2527: Loss = 0.0354
2026-01-08 13:13:08,443 - INFO -   Batch 550/2527: Loss = 0.0336
2026-01-08 13:13:31,402 - INFO -   Batch 600/2527: Loss = 0.0460
2026-01-08 13:13:54,365 - INFO -   Batch 650/2527: Loss = 0.0318
2026-01-08 13:14:17,343 - INFO -   Batch 700/2527: Loss = 0.0482
2026-01-08 13:14:40,341 - INFO -   Batch 750/2527: Loss = 0.0283
2026-01-08 13:15:03,675 - INFO -   Batch 800/2527: Loss = 0.0290
2026-01-08 13:15:26,640 - INFO -   Batch 850/2527: Loss = 0.0276
2026-01-08 13:15:49,609 - INFO -   Batch 900/2527: Loss = 0.0519
2026-01-08 13:16:12,606 - INFO -   Batch 950/2527: Loss = 0.0468
2026-01-08 13:16:35,565 - INFO -   Batch 1000/2527: Loss = 0.0336
2026-01-08 13:16:58,526 - INFO -   Batch 1050/2527: Loss = 0.0739
2026-01-08 13:17:21,487 - INFO -   Batch 1100/2527: Loss = 0.0503
2026-01-08 13:17:44,486 - INFO -   Batch 1150/2527: Loss = 0.0468
2026-01-08 13:18:07,446 - INFO -   Batch 1200/2527: Loss = 0.0287
2026-01-08 13:18:30,411 - INFO -   Batch 1250/2527: Loss = 0.0422
2026-01-08 13:18:53,369 - INFO -   Batch 1300/2527: Loss = 0.0442
2026-01-08 13:19:16,367 - INFO -   Batch 1350/2527: Loss = 0.0534
2026-01-08 13:19:39,327 - INFO -   Batch 1400/2527: Loss = 0.0348
2026-01-08 13:20:02,277 - INFO -   Batch 1450/2527: Loss = 0.0383
2026-01-08 13:20:25,245 - INFO -   Batch 1500/2527: Loss = 0.0398
2026-01-08 13:20:48,588 - INFO -   Batch 1550/2527: Loss = 0.0479
2026-01-08 13:21:11,552 - INFO -   Batch 1600/2527: Loss = 0.0353
2026-01-08 13:21:34,524 - INFO -   Batch 1650/2527: Loss = 0.0200
2026-01-08 13:21:57,478 - INFO -   Batch 1700/2527: Loss = 0.0312
2026-01-08 13:22:20,441 - INFO -   Batch 1750/2527: Loss = 0.0339
2026-01-08 13:22:43,400 - INFO -   Batch 1800/2527: Loss = 0.0307
2026-01-08 13:23:06,368 - INFO -   Batch 1850/2527: Loss = 0.0399
2026-01-08 13:23:29,340 - INFO -   Batch 1900/2527: Loss = 0.0379
2026-01-08 13:23:52,659 - INFO -   Batch 1950/2527: Loss = 0.0296
2026-01-08 13:24:15,621 - INFO -   Batch 2000/2527: Loss = 0.0367
2026-01-08 13:24:38,584 - INFO -   Batch 2050/2527: Loss = 0.0386
2026-01-08 13:25:01,589 - INFO -   Batch 2100/2527: Loss = 0.0229
2026-01-08 13:25:24,542 - INFO -   Batch 2150/2527: Loss = 0.0281
2026-01-08 13:25:47,506 - INFO -   Batch 2200/2527: Loss = 0.0310
2026-01-08 13:26:10,832 - INFO -   Batch 2250/2527: Loss = 0.0544
2026-01-08 13:26:33,779 - INFO -   Batch 2300/2527: Loss = 0.0370
2026-01-08 13:26:56,733 - INFO -   Batch 2350/2527: Loss = 0.0450
2026-01-08 13:27:19,683 - INFO -   Batch 2400/2527: Loss = 0.0409
2026-01-08 13:27:42,672 - INFO -   Batch 2450/2527: Loss = 0.0404
2026-01-08 13:28:05,620 - INFO -   Batch 2500/2527: Loss = 0.0297
2026-01-08 13:32:42,674 - INFO - Epoch 147/160: Train Loss: 0.0375, Val Loss: 1.9197, Val mIoU: 39.75%, Val PixelAcc: 79.31%
2026-01-08 13:33:08,030 - INFO -   Batch 50/2527: Loss = 0.0302
2026-01-08 13:33:31,620 - INFO -   Batch 100/2527: Loss = 0.0437
2026-01-08 13:33:55,559 - INFO -   Batch 150/2527: Loss = 0.0306
2026-01-08 13:34:19,121 - INFO -   Batch 200/2527: Loss = 0.0484
2026-01-08 13:34:42,862 - INFO -   Batch 250/2527: Loss = 0.0328
2026-01-08 13:35:06,471 - INFO -   Batch 300/2527: Loss = 0.0437
2026-01-08 13:35:30,294 - INFO -   Batch 350/2527: Loss = 0.0318
2026-01-08 13:35:54,135 - INFO -   Batch 400/2527: Loss = 0.0344
2026-01-08 13:36:18,279 - INFO -   Batch 450/2527: Loss = 0.0388
2026-01-08 13:36:41,879 - INFO -   Batch 500/2527: Loss = 0.0388
2026-01-08 13:37:05,516 - INFO -   Batch 550/2527: Loss = 0.0298
2026-01-08 13:37:29,477 - INFO -   Batch 600/2527: Loss = 0.0611
2026-01-08 13:37:53,053 - INFO -   Batch 650/2527: Loss = 0.0462
2026-01-08 13:38:16,806 - INFO -   Batch 700/2527: Loss = 0.0301
2026-01-08 13:38:40,414 - INFO -   Batch 750/2527: Loss = 0.0344
2026-01-08 13:39:04,242 - INFO -   Batch 800/2527: Loss = 0.0138
2026-01-08 13:39:28,122 - INFO -   Batch 850/2527: Loss = 0.0340
2026-01-08 13:39:52,271 - INFO -   Batch 900/2527: Loss = 0.0423
2026-01-08 13:40:15,876 - INFO -   Batch 950/2527: Loss = 0.0364
2026-01-08 13:40:39,499 - INFO -   Batch 1000/2527: Loss = 0.0313
2026-01-08 13:41:03,456 - INFO -   Batch 1050/2527: Loss = 0.0391
2026-01-08 13:41:27,043 - INFO -   Batch 1100/2527: Loss = 0.0277
2026-01-08 13:41:50,809 - INFO -   Batch 1150/2527: Loss = 0.0264
2026-01-08 13:42:14,779 - INFO -   Batch 1200/2527: Loss = 0.0455
2026-01-08 13:42:38,601 - INFO -   Batch 1250/2527: Loss = 0.0315
2026-01-08 13:43:02,468 - INFO -   Batch 1300/2527: Loss = 0.0407
2026-01-08 13:43:26,258 - INFO -   Batch 1350/2527: Loss = 0.0534
2026-01-08 13:43:49,873 - INFO -   Batch 1400/2527: Loss = 0.0261
2026-01-08 13:44:13,536 - INFO -   Batch 1450/2527: Loss = 0.0330
2026-01-08 13:44:37,522 - INFO -   Batch 1500/2527: Loss = 0.0331
2026-01-08 13:45:01,108 - INFO -   Batch 1550/2527: Loss = 0.0310
2026-01-08 13:45:24,863 - INFO -   Batch 1600/2527: Loss = 0.0355
2026-01-08 13:45:48,503 - INFO -   Batch 1650/2527: Loss = 0.0249
2026-01-08 13:46:12,320 - INFO -   Batch 1700/2527: Loss = 0.0307
2026-01-08 13:46:36,209 - INFO -   Batch 1750/2527: Loss = 0.0418
2026-01-08 13:47:00,006 - INFO -   Batch 1800/2527: Loss = 0.0417
2026-01-08 13:47:23,602 - INFO -   Batch 1850/2527: Loss = 0.0342
2026-01-08 13:47:47,596 - INFO -   Batch 1900/2527: Loss = 0.0500
2026-01-08 13:48:11,554 - INFO -   Batch 1950/2527: Loss = 0.0382
2026-01-08 13:48:35,171 - INFO -   Batch 2000/2527: Loss = 0.0485
2026-01-08 13:48:58,921 - INFO -   Batch 2050/2527: Loss = 0.0427
2026-01-08 13:49:22,517 - INFO -   Batch 2100/2527: Loss = 0.0319
2026-01-08 13:49:46,346 - INFO -   Batch 2150/2527: Loss = 0.0455
2026-01-08 13:50:10,239 - INFO -   Batch 2200/2527: Loss = 0.0442
2026-01-08 13:50:34,021 - INFO -   Batch 2250/2527: Loss = 0.0635
2026-01-08 13:50:57,615 - INFO -   Batch 2300/2527: Loss = 0.0300
2026-01-08 13:51:21,591 - INFO -   Batch 2350/2527: Loss = 0.0409
2026-01-08 13:51:45,537 - INFO -   Batch 2400/2527: Loss = 0.0420
2026-01-08 13:52:09,188 - INFO -   Batch 2450/2527: Loss = 0.0436
2026-01-08 13:52:32,952 - INFO -   Batch 2500/2527: Loss = 0.0506
2026-01-08 13:57:23,112 - INFO - Epoch 148/160: Train Loss: 0.0374, Val Loss: 1.9275, Val mIoU: 39.68%, Val PixelAcc: 79.29%
2026-01-08 13:57:49,157 - INFO -   Batch 50/2527: Loss = 0.0504
2026-01-08 13:58:14,647 - INFO -   Batch 100/2527: Loss = 0.0414
2026-01-08 13:58:39,587 - INFO -   Batch 150/2527: Loss = 0.0367
2026-01-08 13:59:04,394 - INFO -   Batch 200/2527: Loss = 0.0405
2026-01-08 13:59:29,226 - INFO -   Batch 250/2527: Loss = 0.0352
2026-01-08 13:59:54,224 - INFO -   Batch 300/2527: Loss = 0.0382
2026-01-08 14:00:19,043 - INFO -   Batch 350/2527: Loss = 0.0372
2026-01-08 14:00:43,878 - INFO -   Batch 400/2527: Loss = 0.0363
2026-01-08 14:01:08,986 - INFO -   Batch 450/2527: Loss = 0.0362
2026-01-08 14:01:33,937 - INFO -   Batch 500/2527: Loss = 0.0473
2026-01-08 14:01:58,774 - INFO -   Batch 550/2527: Loss = 0.0446
2026-01-08 14:02:23,620 - INFO -   Batch 600/2527: Loss = 0.0321
2026-01-08 14:02:48,339 - INFO -   Batch 650/2527: Loss = 0.0249
2026-01-08 14:03:13,192 - INFO -   Batch 700/2527: Loss = 0.0269
2026-01-08 14:03:38,311 - INFO -   Batch 750/2527: Loss = 0.0466
2026-01-08 14:04:03,276 - INFO -   Batch 800/2527: Loss = 0.0298
2026-01-08 14:04:28,503 - INFO -   Batch 850/2527: Loss = 0.0440
2026-01-08 14:04:53,324 - INFO -   Batch 900/2527: Loss = 0.0376
2026-01-08 14:05:18,330 - INFO -   Batch 950/2527: Loss = 0.0476
2026-01-08 14:05:43,162 - INFO -   Batch 1000/2527: Loss = 0.0255
2026-01-08 14:06:08,012 - INFO -   Batch 1050/2527: Loss = 0.0338
2026-01-08 14:06:33,119 - INFO -   Batch 1100/2527: Loss = 0.0329
2026-01-08 14:06:58,123 - INFO -   Batch 1150/2527: Loss = 0.0481
2026-01-08 14:07:22,935 - INFO -   Batch 1200/2527: Loss = 0.0312
2026-01-08 14:07:48,137 - INFO -   Batch 1250/2527: Loss = 0.0298
2026-01-08 14:08:12,861 - INFO -   Batch 1300/2527: Loss = 0.0329
2026-01-08 14:08:37,726 - INFO -   Batch 1350/2527: Loss = 0.0386
2026-01-08 14:09:02,857 - INFO -   Batch 1400/2527: Loss = 0.0593
2026-01-08 14:09:27,836 - INFO -   Batch 1450/2527: Loss = 0.0390
2026-01-08 14:09:52,652 - INFO -   Batch 1500/2527: Loss = 0.0365
2026-01-08 14:10:17,892 - INFO -   Batch 1550/2527: Loss = 0.0420
2026-01-08 14:10:42,914 - INFO -   Batch 1600/2527: Loss = 0.0491
2026-01-08 14:11:07,747 - INFO -   Batch 1650/2527: Loss = 0.0622
2026-01-08 14:11:32,585 - INFO -   Batch 1700/2527: Loss = 0.0379
2026-01-08 14:11:57,690 - INFO -   Batch 1750/2527: Loss = 0.0505
2026-01-08 14:12:22,655 - INFO -   Batch 1800/2527: Loss = 0.0450
2026-01-08 14:12:47,508 - INFO -   Batch 1850/2527: Loss = 0.0333
2026-01-08 14:13:12,356 - INFO -   Batch 1900/2527: Loss = 0.0277
2026-01-08 14:13:37,075 - INFO -   Batch 1950/2527: Loss = 0.0526
2026-01-08 14:14:01,901 - INFO -   Batch 2000/2527: Loss = 0.0267
2026-01-08 14:14:27,011 - INFO -   Batch 2050/2527: Loss = 0.0376
2026-01-08 14:14:51,994 - INFO -   Batch 2100/2527: Loss = 0.0338
2026-01-08 14:15:16,849 - INFO -   Batch 2150/2527: Loss = 0.0280
2026-01-08 14:15:41,699 - INFO -   Batch 2200/2527: Loss = 0.0445
2026-01-08 14:16:06,698 - INFO -   Batch 2250/2527: Loss = 0.0364
2026-01-08 14:16:31,916 - INFO -   Batch 2300/2527: Loss = 0.0443
2026-01-08 14:16:56,764 - INFO -   Batch 2350/2527: Loss = 0.0262
2026-01-08 14:17:21,883 - INFO -   Batch 2400/2527: Loss = 0.0254
2026-01-08 14:17:46,860 - INFO -   Batch 2450/2527: Loss = 0.0323
2026-01-08 14:18:11,689 - INFO -   Batch 2500/2527: Loss = 0.0269
2026-01-08 14:22:49,141 - INFO - Epoch 149/160: Train Loss: 0.0374, Val Loss: 1.9459, Val mIoU: 39.72%, Val PixelAcc: 79.30%
2026-01-08 14:23:15,401 - INFO -   Batch 50/2527: Loss = 0.0392
2026-01-08 14:23:40,375 - INFO -   Batch 100/2527: Loss = 0.0314
2026-01-08 14:24:05,172 - INFO -   Batch 150/2527: Loss = 0.0441
2026-01-08 14:24:30,356 - INFO -   Batch 200/2527: Loss = 0.0324
2026-01-08 14:24:55,462 - INFO -   Batch 250/2527: Loss = 0.0332
2026-01-08 14:25:20,435 - INFO -   Batch 300/2527: Loss = 0.0306
2026-01-08 14:25:45,286 - INFO -   Batch 350/2527: Loss = 0.0460
2026-01-08 14:26:10,133 - INFO -   Batch 400/2527: Loss = 0.0326
2026-01-08 14:26:35,278 - INFO -   Batch 450/2527: Loss = 0.0348
2026-01-08 14:27:00,151 - INFO -   Batch 500/2527: Loss = 0.0337
2026-01-08 14:27:25,282 - INFO -   Batch 550/2527: Loss = 0.0308
2026-01-08 14:27:50,286 - INFO -   Batch 600/2527: Loss = 0.0293
2026-01-08 14:28:15,105 - INFO -   Batch 650/2527: Loss = 0.0253
2026-01-08 14:28:39,959 - INFO -   Batch 700/2527: Loss = 0.0367
2026-01-08 14:29:04,975 - INFO -   Batch 750/2527: Loss = 0.0650
2026-01-08 14:29:29,824 - INFO -   Batch 800/2527: Loss = 0.0312
2026-01-08 14:29:54,676 - INFO -   Batch 850/2527: Loss = 0.0312
2026-01-08 14:30:19,803 - INFO -   Batch 900/2527: Loss = 0.0222
2026-01-08 14:30:44,788 - INFO -   Batch 950/2527: Loss = 0.0431
2026-01-08 14:31:09,624 - INFO -   Batch 1000/2527: Loss = 0.0377
2026-01-08 14:31:34,485 - INFO -   Batch 1050/2527: Loss = 0.0538
2026-01-08 14:31:59,193 - INFO -   Batch 1100/2527: Loss = 0.0176
2026-01-08 14:32:24,039 - INFO -   Batch 1150/2527: Loss = 0.0270
2026-01-08 14:32:49,512 - INFO -   Batch 1200/2527: Loss = 0.0473
2026-01-08 14:33:14,471 - INFO -   Batch 1250/2527: Loss = 0.0353
2026-01-08 14:33:39,309 - INFO -   Batch 1300/2527: Loss = 0.0336
2026-01-08 14:34:04,125 - INFO -   Batch 1350/2527: Loss = 0.0298
2026-01-08 14:34:29,168 - INFO -   Batch 1400/2527: Loss = 0.0388
2026-01-08 14:34:54,112 - INFO -   Batch 1450/2527: Loss = 0.0407
2026-01-08 14:35:19,291 - INFO -   Batch 1500/2527: Loss = 0.0331
2026-01-08 14:35:44,766 - INFO -   Batch 1550/2527: Loss = 0.0256
2026-01-08 14:36:10,350 - INFO -   Batch 1600/2527: Loss = 0.0333
2026-01-08 14:36:35,558 - INFO -   Batch 1650/2527: Loss = 0.0397
2026-01-08 14:37:00,636 - INFO -   Batch 1700/2527: Loss = 0.0268
2026-01-08 14:37:25,673 - INFO -   Batch 1750/2527: Loss = 0.0221
2026-01-08 14:37:50,803 - INFO -   Batch 1800/2527: Loss = 0.0267
2026-01-08 14:38:16,126 - INFO -   Batch 1850/2527: Loss = 0.0378
2026-01-08 14:38:41,556 - INFO -   Batch 1900/2527: Loss = 0.0388
2026-01-08 14:39:06,676 - INFO -   Batch 1950/2527: Loss = 0.0285
2026-01-08 14:39:31,727 - INFO -   Batch 2000/2527: Loss = 0.0570
2026-01-08 14:39:56,928 - INFO -   Batch 2050/2527: Loss = 0.0237
2026-01-08 14:40:22,079 - INFO -   Batch 2100/2527: Loss = 0.0448
2026-01-08 14:40:47,406 - INFO -   Batch 2150/2527: Loss = 0.0379
2026-01-08 14:41:12,805 - INFO -   Batch 2200/2527: Loss = 0.0379
2026-01-08 14:41:37,832 - INFO -   Batch 2250/2527: Loss = 0.0281
2026-01-08 14:42:02,728 - INFO -   Batch 2300/2527: Loss = 0.0343
2026-01-08 14:42:27,644 - INFO -   Batch 2350/2527: Loss = 0.0273
2026-01-08 14:42:52,428 - INFO -   Batch 2400/2527: Loss = 0.0378
2026-01-08 14:43:17,434 - INFO -   Batch 2450/2527: Loss = 0.0476
2026-01-08 14:43:42,730 - INFO -   Batch 2500/2527: Loss = 0.0340
2026-01-08 14:48:42,641 - INFO - Epoch 150/160: Train Loss: 0.0373, Val Loss: 1.9520, Val mIoU: 39.66%, Val PixelAcc: 79.26%
2026-01-08 14:48:44,736 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_150.pth
2026-01-08 14:48:44,738 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_150.pth
2026-01-08 14:49:09,706 - INFO -   Batch 50/2527: Loss = 0.0246
2026-01-08 14:49:32,928 - INFO -   Batch 100/2527: Loss = 0.0368
2026-01-08 14:49:55,968 - INFO -   Batch 150/2527: Loss = 0.0294
2026-01-08 14:50:18,905 - INFO -   Batch 200/2527: Loss = 0.0398
2026-01-08 14:50:41,778 - INFO -   Batch 250/2527: Loss = 0.0465
2026-01-08 14:51:04,714 - INFO -   Batch 300/2527: Loss = 0.0412
2026-01-08 14:51:27,582 - INFO -   Batch 350/2527: Loss = 0.0501
2026-01-08 14:51:50,451 - INFO -   Batch 400/2527: Loss = 0.0189
2026-01-08 14:52:13,343 - INFO -   Batch 450/2527: Loss = 0.0339
2026-01-08 14:52:36,241 - INFO -   Batch 500/2527: Loss = 0.0350
2026-01-08 14:52:59,492 - INFO -   Batch 550/2527: Loss = 0.0417
2026-01-08 14:53:22,366 - INFO -   Batch 600/2527: Loss = 0.0311
2026-01-08 14:53:45,416 - INFO -   Batch 650/2527: Loss = 0.0378
2026-01-08 14:54:08,375 - INFO -   Batch 700/2527: Loss = 0.0376
2026-01-08 14:54:31,236 - INFO -   Batch 750/2527: Loss = 0.0351
2026-01-08 14:54:54,136 - INFO -   Batch 800/2527: Loss = 0.0515
2026-01-08 14:55:17,473 - INFO -   Batch 850/2527: Loss = 0.0327
2026-01-08 14:55:40,339 - INFO -   Batch 900/2527: Loss = 0.0383
2026-01-08 14:56:03,226 - INFO -   Batch 950/2527: Loss = 0.0359
2026-01-08 14:56:26,113 - INFO -   Batch 1000/2527: Loss = 0.0374
2026-01-08 14:56:49,029 - INFO -   Batch 1050/2527: Loss = 0.0338
2026-01-08 14:57:11,927 - INFO -   Batch 1100/2527: Loss = 0.0366
2026-01-08 14:57:34,877 - INFO -   Batch 1150/2527: Loss = 0.0251
2026-01-08 14:57:57,772 - INFO -   Batch 1200/2527: Loss = 0.0481
2026-01-08 14:58:20,742 - INFO -   Batch 1250/2527: Loss = 0.0363
2026-01-08 14:58:43,612 - INFO -   Batch 1300/2527: Loss = 0.0276
2026-01-08 14:59:06,522 - INFO -   Batch 1350/2527: Loss = 0.0413
2026-01-08 14:59:29,393 - INFO -   Batch 1400/2527: Loss = 0.0389
2026-01-08 14:59:52,294 - INFO -   Batch 1450/2527: Loss = 0.0408
2026-01-08 15:00:15,217 - INFO -   Batch 1500/2527: Loss = 0.0297
2026-01-08 15:00:38,500 - INFO -   Batch 1550/2527: Loss = 0.0356
2026-01-08 15:01:01,412 - INFO -   Batch 1600/2527: Loss = 0.0381
2026-01-08 15:01:24,346 - INFO -   Batch 1650/2527: Loss = 0.0380
2026-01-08 15:01:47,267 - INFO -   Batch 1700/2527: Loss = 0.0294
2026-01-08 15:02:10,147 - INFO -   Batch 1750/2527: Loss = 0.0428
2026-01-08 15:02:33,058 - INFO -   Batch 1800/2527: Loss = 0.0303
2026-01-08 15:02:56,186 - INFO -   Batch 1850/2527: Loss = 0.0315
2026-01-08 15:03:19,120 - INFO -   Batch 1900/2527: Loss = 0.0331
2026-01-08 15:03:42,030 - INFO -   Batch 1950/2527: Loss = 0.0358
2026-01-08 15:04:05,335 - INFO -   Batch 2000/2527: Loss = 0.0295
2026-01-08 15:04:28,228 - INFO -   Batch 2050/2527: Loss = 0.0456
2026-01-08 15:04:51,115 - INFO -   Batch 2100/2527: Loss = 0.0610
2026-01-08 15:05:14,086 - INFO -   Batch 2150/2527: Loss = 0.0382
2026-01-08 15:05:36,972 - INFO -   Batch 2200/2527: Loss = 0.0354
2026-01-08 15:06:00,256 - INFO -   Batch 2250/2527: Loss = 0.0377
2026-01-08 15:06:23,145 - INFO -   Batch 2300/2527: Loss = 0.0391
2026-01-08 15:06:46,017 - INFO -   Batch 2350/2527: Loss = 0.0347
2026-01-08 15:07:08,944 - INFO -   Batch 2400/2527: Loss = 0.0351
2026-01-08 15:07:31,906 - INFO -   Batch 2450/2527: Loss = 0.0325
2026-01-08 15:07:54,791 - INFO -   Batch 2500/2527: Loss = 0.0563
2026-01-08 15:12:31,709 - INFO - Epoch 151/160: Train Loss: 0.0373, Val Loss: 1.9442, Val mIoU: 39.67%, Val PixelAcc: 79.30%
2026-01-08 15:12:57,115 - INFO -   Batch 50/2527: Loss = 0.0430
2026-01-08 15:13:20,705 - INFO -   Batch 100/2527: Loss = 0.0340
2026-01-08 15:13:44,643 - INFO -   Batch 150/2527: Loss = 0.0336
2026-01-08 15:14:08,203 - INFO -   Batch 200/2527: Loss = 0.0457
2026-01-08 15:14:31,948 - INFO -   Batch 250/2527: Loss = 0.0330
2026-01-08 15:14:55,539 - INFO -   Batch 300/2527: Loss = 0.0289
2026-01-08 15:15:19,362 - INFO -   Batch 350/2527: Loss = 0.0401
2026-01-08 15:15:43,216 - INFO -   Batch 400/2527: Loss = 0.0276
2026-01-08 15:16:07,032 - INFO -   Batch 450/2527: Loss = 0.0354
2026-01-08 15:16:30,981 - INFO -   Batch 500/2527: Loss = 0.0240
2026-01-08 15:16:54,598 - INFO -   Batch 550/2527: Loss = 0.0309
2026-01-08 15:17:18,540 - INFO -   Batch 600/2527: Loss = 0.0252
2026-01-08 15:17:42,116 - INFO -   Batch 650/2527: Loss = 0.0347
2026-01-08 15:18:05,921 - INFO -   Batch 700/2527: Loss = 0.0297
2026-01-08 15:18:29,555 - INFO -   Batch 750/2527: Loss = 0.0317
2026-01-08 15:18:53,383 - INFO -   Batch 800/2527: Loss = 0.0351
2026-01-08 15:19:17,272 - INFO -   Batch 850/2527: Loss = 0.0330
2026-01-08 15:19:41,432 - INFO -   Batch 900/2527: Loss = 0.0608
2026-01-08 15:20:05,074 - INFO -   Batch 950/2527: Loss = 0.0337
2026-01-08 15:20:28,689 - INFO -   Batch 1000/2527: Loss = 0.0372
2026-01-08 15:20:52,655 - INFO -   Batch 1050/2527: Loss = 0.0427
2026-01-08 15:21:16,235 - INFO -   Batch 1100/2527: Loss = 0.0500
2026-01-08 15:21:40,002 - INFO -   Batch 1150/2527: Loss = 0.0367
2026-01-08 15:22:04,010 - INFO -   Batch 1200/2527: Loss = 0.0275
2026-01-08 15:22:27,836 - INFO -   Batch 1250/2527: Loss = 0.0326
2026-01-08 15:22:51,689 - INFO -   Batch 1300/2527: Loss = 0.0366
2026-01-08 15:23:15,499 - INFO -   Batch 1350/2527: Loss = 0.0537
2026-01-08 15:23:39,103 - INFO -   Batch 1400/2527: Loss = 0.0372
2026-01-08 15:24:02,734 - INFO -   Batch 1450/2527: Loss = 0.0302
2026-01-08 15:24:26,686 - INFO -   Batch 1500/2527: Loss = 0.0532
2026-01-08 15:24:50,278 - INFO -   Batch 1550/2527: Loss = 0.0393
2026-01-08 15:25:14,063 - INFO -   Batch 1600/2527: Loss = 0.0338
2026-01-08 15:25:37,725 - INFO -   Batch 1650/2527: Loss = 0.0341
2026-01-08 15:26:01,580 - INFO -   Batch 1700/2527: Loss = 0.0437
2026-01-08 15:26:25,453 - INFO -   Batch 1750/2527: Loss = 0.0294
2026-01-08 15:26:49,243 - INFO -   Batch 1800/2527: Loss = 0.0254
2026-01-08 15:27:12,850 - INFO -   Batch 1850/2527: Loss = 0.0368
2026-01-08 15:27:36,496 - INFO -   Batch 1900/2527: Loss = 0.0430
2026-01-08 15:28:00,824 - INFO -   Batch 1950/2527: Loss = 0.0282
2026-01-08 15:28:24,411 - INFO -   Batch 2000/2527: Loss = 0.0596
2026-01-08 15:28:48,170 - INFO -   Batch 2050/2527: Loss = 0.0360
2026-01-08 15:29:11,809 - INFO -   Batch 2100/2527: Loss = 0.0496
2026-01-08 15:29:35,628 - INFO -   Batch 2150/2527: Loss = 0.0309
2026-01-08 15:29:59,484 - INFO -   Batch 2200/2527: Loss = 0.0506
2026-01-08 15:30:23,276 - INFO -   Batch 2250/2527: Loss = 0.0388
2026-01-08 15:30:46,877 - INFO -   Batch 2300/2527: Loss = 0.0534
2026-01-08 15:31:10,864 - INFO -   Batch 2350/2527: Loss = 0.0382
2026-01-08 15:31:34,835 - INFO -   Batch 2400/2527: Loss = 0.0222
2026-01-08 15:31:58,410 - INFO -   Batch 2450/2527: Loss = 0.0371
2026-01-08 15:32:22,173 - INFO -   Batch 2500/2527: Loss = 0.0438
2026-01-08 15:37:12,207 - INFO - Epoch 152/160: Train Loss: 0.0372, Val Loss: 1.9424, Val mIoU: 39.63%, Val PixelAcc: 79.26%
2026-01-08 15:37:38,309 - INFO -   Batch 50/2527: Loss = 0.0311
2026-01-08 15:38:03,744 - INFO -   Batch 100/2527: Loss = 0.0269
2026-01-08 15:38:28,673 - INFO -   Batch 150/2527: Loss = 0.0342
2026-01-08 15:38:53,478 - INFO -   Batch 200/2527: Loss = 0.0473
2026-01-08 15:39:18,322 - INFO -   Batch 250/2527: Loss = 0.0412
2026-01-08 15:39:43,329 - INFO -   Batch 300/2527: Loss = 0.0456
2026-01-08 15:40:08,157 - INFO -   Batch 350/2527: Loss = 0.0439
2026-01-08 15:40:32,992 - INFO -   Batch 400/2527: Loss = 0.0336
2026-01-08 15:40:58,115 - INFO -   Batch 450/2527: Loss = 0.0293
2026-01-08 15:41:23,071 - INFO -   Batch 500/2527: Loss = 0.0394
2026-01-08 15:41:47,882 - INFO -   Batch 550/2527: Loss = 0.0455
2026-01-08 15:42:12,722 - INFO -   Batch 600/2527: Loss = 0.0296
2026-01-08 15:42:37,462 - INFO -   Batch 650/2527: Loss = 0.0335
2026-01-08 15:43:02,302 - INFO -   Batch 700/2527: Loss = 0.0414
2026-01-08 15:43:27,434 - INFO -   Batch 750/2527: Loss = 0.0334
2026-01-08 15:43:52,405 - INFO -   Batch 800/2527: Loss = 0.0366
2026-01-08 15:44:17,604 - INFO -   Batch 850/2527: Loss = 0.0573
2026-01-08 15:44:42,466 - INFO -   Batch 900/2527: Loss = 0.0353
2026-01-08 15:45:07,496 - INFO -   Batch 950/2527: Loss = 0.0386
2026-01-08 15:45:32,314 - INFO -   Batch 1000/2527: Loss = 0.0303
2026-01-08 15:45:57,139 - INFO -   Batch 1050/2527: Loss = 0.0462
2026-01-08 15:46:22,278 - INFO -   Batch 1100/2527: Loss = 0.0383
2026-01-08 15:46:47,263 - INFO -   Batch 1150/2527: Loss = 0.0314
2026-01-08 15:47:12,087 - INFO -   Batch 1200/2527: Loss = 0.0364
2026-01-08 15:47:37,313 - INFO -   Batch 1250/2527: Loss = 0.0387
2026-01-08 15:48:02,057 - INFO -   Batch 1300/2527: Loss = 0.0193
2026-01-08 15:48:26,905 - INFO -   Batch 1350/2527: Loss = 0.0370
2026-01-08 15:48:52,020 - INFO -   Batch 1400/2527: Loss = 0.0370
2026-01-08 15:49:16,990 - INFO -   Batch 1450/2527: Loss = 0.0338
2026-01-08 15:49:41,824 - INFO -   Batch 1500/2527: Loss = 0.0448
2026-01-08 15:50:07,045 - INFO -   Batch 1550/2527: Loss = 0.0404
2026-01-08 15:50:32,058 - INFO -   Batch 1600/2527: Loss = 0.0396
2026-01-08 15:50:56,892 - INFO -   Batch 1650/2527: Loss = 0.0525
2026-01-08 15:51:21,728 - INFO -   Batch 1700/2527: Loss = 0.0235
2026-01-08 15:51:46,848 - INFO -   Batch 1750/2527: Loss = 0.0419
2026-01-08 15:52:11,842 - INFO -   Batch 1800/2527: Loss = 0.0311
2026-01-08 15:52:36,666 - INFO -   Batch 1850/2527: Loss = 0.0458
2026-01-08 15:53:01,523 - INFO -   Batch 1900/2527: Loss = 0.0293
2026-01-08 15:53:26,240 - INFO -   Batch 1950/2527: Loss = 0.0331
2026-01-08 15:53:51,084 - INFO -   Batch 2000/2527: Loss = 0.0342
2026-01-08 15:54:16,194 - INFO -   Batch 2050/2527: Loss = 0.0370
2026-01-08 15:54:41,172 - INFO -   Batch 2100/2527: Loss = 0.0277
2026-01-08 15:55:05,988 - INFO -   Batch 2150/2527: Loss = 0.0417
2026-01-08 15:55:30,834 - INFO -   Batch 2200/2527: Loss = 0.0363
2026-01-08 15:55:55,866 - INFO -   Batch 2250/2527: Loss = 0.0334
2026-01-08 15:56:21,063 - INFO -   Batch 2300/2527: Loss = 0.0347
2026-01-08 15:56:45,901 - INFO -   Batch 2350/2527: Loss = 0.0342
2026-01-08 15:57:11,016 - INFO -   Batch 2400/2527: Loss = 0.0579
2026-01-08 15:57:35,990 - INFO -   Batch 2450/2527: Loss = 0.0379
2026-01-08 15:58:00,816 - INFO -   Batch 2500/2527: Loss = 0.0336
2026-01-08 16:02:38,300 - INFO - Epoch 153/160: Train Loss: 0.0372, Val Loss: 1.9361, Val mIoU: 39.66%, Val PixelAcc: 79.28%
2026-01-08 16:03:04,715 - INFO -   Batch 50/2527: Loss = 0.0394
2026-01-08 16:03:29,695 - INFO -   Batch 100/2527: Loss = 0.0429
2026-01-08 16:03:54,476 - INFO -   Batch 150/2527: Loss = 0.0520
2026-01-08 16:04:19,651 - INFO -   Batch 200/2527: Loss = 0.0336
2026-01-08 16:04:44,759 - INFO -   Batch 250/2527: Loss = 0.0246
2026-01-08 16:05:09,728 - INFO -   Batch 300/2527: Loss = 0.0339
2026-01-08 16:05:34,577 - INFO -   Batch 350/2527: Loss = 0.0327
2026-01-08 16:05:59,403 - INFO -   Batch 400/2527: Loss = 0.0355
2026-01-08 16:06:24,134 - INFO -   Batch 450/2527: Loss = 0.0587
2026-01-08 16:06:49,350 - INFO -   Batch 500/2527: Loss = 0.0447
2026-01-08 16:07:14,475 - INFO -   Batch 550/2527: Loss = 0.0346
2026-01-08 16:07:39,451 - INFO -   Batch 600/2527: Loss = 0.0408
2026-01-08 16:08:04,279 - INFO -   Batch 650/2527: Loss = 0.0484
2026-01-08 16:08:29,130 - INFO -   Batch 700/2527: Loss = 0.0308
2026-01-08 16:08:54,133 - INFO -   Batch 750/2527: Loss = 0.0190
2026-01-08 16:09:18,972 - INFO -   Batch 800/2527: Loss = 0.0474
2026-01-08 16:09:43,843 - INFO -   Batch 850/2527: Loss = 0.0475
2026-01-08 16:10:08,956 - INFO -   Batch 900/2527: Loss = 0.0395
2026-01-08 16:10:33,928 - INFO -   Batch 950/2527: Loss = 0.0412
2026-01-08 16:10:58,766 - INFO -   Batch 1000/2527: Loss = 0.0318
2026-01-08 16:11:23,607 - INFO -   Batch 1050/2527: Loss = 0.0447
2026-01-08 16:11:48,516 - INFO -   Batch 1100/2527: Loss = 0.0562
2026-01-08 16:12:13,344 - INFO -   Batch 1150/2527: Loss = 0.0224
2026-01-08 16:12:38,498 - INFO -   Batch 1200/2527: Loss = 0.0202
2026-01-08 16:13:03,892 - INFO -   Batch 1250/2527: Loss = 0.0474
2026-01-08 16:13:28,941 - INFO -   Batch 1300/2527: Loss = 0.0292
2026-01-08 16:13:53,783 - INFO -   Batch 1350/2527: Loss = 0.0436
2026-01-08 16:14:18,805 - INFO -   Batch 1400/2527: Loss = 0.0423
2026-01-08 16:14:43,644 - INFO -   Batch 1450/2527: Loss = 0.0362
2026-01-08 16:15:08,499 - INFO -   Batch 1500/2527: Loss = 0.0309
2026-01-08 16:15:33,630 - INFO -   Batch 1550/2527: Loss = 0.0387
2026-01-08 16:15:58,621 - INFO -   Batch 1600/2527: Loss = 0.0433
2026-01-08 16:16:23,817 - INFO -   Batch 1650/2527: Loss = 0.0430
2026-01-08 16:16:48,685 - INFO -   Batch 1700/2527: Loss = 0.0356
2026-01-08 16:17:13,425 - INFO -   Batch 1750/2527: Loss = 0.0318
2026-01-08 16:17:38,279 - INFO -   Batch 1800/2527: Loss = 0.0375
2026-01-08 16:18:03,412 - INFO -   Batch 1850/2527: Loss = 0.0401
2026-01-08 16:18:28,430 - INFO -   Batch 1900/2527: Loss = 0.0359
2026-01-08 16:18:53,614 - INFO -   Batch 1950/2527: Loss = 0.0326
2026-01-08 16:19:18,478 - INFO -   Batch 2000/2527: Loss = 0.0431
2026-01-08 16:19:43,481 - INFO -   Batch 2050/2527: Loss = 0.0380
2026-01-08 16:20:08,340 - INFO -   Batch 2100/2527: Loss = 0.0378
2026-01-08 16:20:33,210 - INFO -   Batch 2150/2527: Loss = 0.0328
2026-01-08 16:20:58,332 - INFO -   Batch 2200/2527: Loss = 0.0314
2026-01-08 16:21:23,303 - INFO -   Batch 2250/2527: Loss = 0.0383
2026-01-08 16:21:48,126 - INFO -   Batch 2300/2527: Loss = 0.0365
2026-01-08 16:22:12,978 - INFO -   Batch 2350/2527: Loss = 0.0511
2026-01-08 16:22:37,710 - INFO -   Batch 2400/2527: Loss = 0.0418
2026-01-08 16:23:02,561 - INFO -   Batch 2450/2527: Loss = 0.0508
2026-01-08 16:23:27,674 - INFO -   Batch 2500/2527: Loss = 0.0423
2026-01-08 16:28:18,229 - INFO - Epoch 154/160: Train Loss: 0.0371, Val Loss: 1.9497, Val mIoU: 39.69%, Val PixelAcc: 79.28%
2026-01-08 16:28:43,247 - INFO -   Batch 50/2527: Loss = 0.0401
2026-01-08 16:29:06,194 - INFO -   Batch 100/2527: Loss = 0.0469
2026-01-08 16:29:29,503 - INFO -   Batch 150/2527: Loss = 0.0219
2026-01-08 16:29:52,448 - INFO -   Batch 200/2527: Loss = 0.0312
2026-01-08 16:30:15,427 - INFO -   Batch 250/2527: Loss = 0.0454
2026-01-08 16:30:38,383 - INFO -   Batch 300/2527: Loss = 0.0311
2026-01-08 16:31:01,344 - INFO -   Batch 350/2527: Loss = 0.0294
2026-01-08 16:31:24,300 - INFO -   Batch 400/2527: Loss = 0.0410
2026-01-08 16:31:47,287 - INFO -   Batch 450/2527: Loss = 0.0483
2026-01-08 16:32:10,251 - INFO -   Batch 500/2527: Loss = 0.0355
2026-01-08 16:32:33,597 - INFO -   Batch 550/2527: Loss = 0.0466
2026-01-08 16:32:56,560 - INFO -   Batch 600/2527: Loss = 0.0349
2026-01-08 16:33:19,532 - INFO -   Batch 650/2527: Loss = 0.0507
2026-01-08 16:33:42,500 - INFO -   Batch 700/2527: Loss = 0.0266
2026-01-08 16:34:05,466 - INFO -   Batch 750/2527: Loss = 0.0312
2026-01-08 16:34:28,434 - INFO -   Batch 800/2527: Loss = 0.0446
2026-01-08 16:34:51,753 - INFO -   Batch 850/2527: Loss = 0.0195
2026-01-08 16:35:14,700 - INFO -   Batch 900/2527: Loss = 0.0574
2026-01-08 16:35:37,650 - INFO -   Batch 950/2527: Loss = 0.0490
2026-01-08 16:36:00,576 - INFO -   Batch 1000/2527: Loss = 0.0350
2026-01-08 16:36:23,511 - INFO -   Batch 1050/2527: Loss = 0.0303
2026-01-08 16:36:46,456 - INFO -   Batch 1100/2527: Loss = 0.0365
2026-01-08 16:37:09,399 - INFO -   Batch 1150/2527: Loss = 0.0372
2026-01-08 16:37:32,352 - INFO -   Batch 1200/2527: Loss = 0.0426
2026-01-08 16:37:55,292 - INFO -   Batch 1250/2527: Loss = 0.0289
2026-01-08 16:38:18,257 - INFO -   Batch 1300/2527: Loss = 0.0232
2026-01-08 16:38:41,195 - INFO -   Batch 1350/2527: Loss = 0.0466
2026-01-08 16:39:04,160 - INFO -   Batch 1400/2527: Loss = 0.0501
2026-01-08 16:39:27,118 - INFO -   Batch 1450/2527: Loss = 0.0269
2026-01-08 16:39:50,105 - INFO -   Batch 1500/2527: Loss = 0.0339
2026-01-08 16:40:13,054 - INFO -   Batch 1550/2527: Loss = 0.0505
2026-01-08 16:40:36,352 - INFO -   Batch 1600/2527: Loss = 0.0379
2026-01-08 16:40:59,306 - INFO -   Batch 1650/2527: Loss = 0.0264
2026-01-08 16:41:22,309 - INFO -   Batch 1700/2527: Loss = 0.0482
2026-01-08 16:41:45,260 - INFO -   Batch 1750/2527: Loss = 0.0602
2026-01-08 16:42:08,261 - INFO -   Batch 1800/2527: Loss = 0.0350
2026-01-08 16:42:31,203 - INFO -   Batch 1850/2527: Loss = 0.0351
2026-01-08 16:42:54,127 - INFO -   Batch 1900/2527: Loss = 0.0266
2026-01-08 16:43:17,069 - INFO -   Batch 1950/2527: Loss = 0.0433
2026-01-08 16:43:40,367 - INFO -   Batch 2000/2527: Loss = 0.0275
2026-01-08 16:44:03,298 - INFO -   Batch 2050/2527: Loss = 0.0307
2026-01-08 16:44:26,245 - INFO -   Batch 2100/2527: Loss = 0.0340
2026-01-08 16:44:49,189 - INFO -   Batch 2150/2527: Loss = 0.0313
2026-01-08 16:45:12,133 - INFO -   Batch 2200/2527: Loss = 0.0615
2026-01-08 16:45:35,061 - INFO -   Batch 2250/2527: Loss = 0.0288
2026-01-08 16:45:58,379 - INFO -   Batch 2300/2527: Loss = 0.0465
2026-01-08 16:46:21,325 - INFO -   Batch 2350/2527: Loss = 0.0245
2026-01-08 16:46:44,267 - INFO -   Batch 2400/2527: Loss = 0.0477
2026-01-08 16:47:07,214 - INFO -   Batch 2450/2527: Loss = 0.0338
2026-01-08 16:47:30,197 - INFO -   Batch 2500/2527: Loss = 0.0309
2026-01-08 16:52:07,340 - INFO - Epoch 155/160: Train Loss: 0.0371, Val Loss: 1.9425, Val mIoU: 39.68%, Val PixelAcc: 79.29%
2026-01-08 16:52:32,596 - INFO -   Batch 50/2527: Loss = 0.0300
2026-01-08 16:52:56,180 - INFO -   Batch 100/2527: Loss = 0.0549
2026-01-08 16:53:20,104 - INFO -   Batch 150/2527: Loss = 0.0460
2026-01-08 16:53:43,689 - INFO -   Batch 200/2527: Loss = 0.0531
2026-01-08 16:54:07,448 - INFO -   Batch 250/2527: Loss = 0.0359
2026-01-08 16:54:31,037 - INFO -   Batch 300/2527: Loss = 0.0305
2026-01-08 16:54:54,858 - INFO -   Batch 350/2527: Loss = 0.0285
2026-01-08 16:55:18,725 - INFO -   Batch 400/2527: Loss = 0.0405
2026-01-08 16:55:42,505 - INFO -   Batch 450/2527: Loss = 0.0383
2026-01-08 16:56:06,477 - INFO -   Batch 500/2527: Loss = 0.0430
2026-01-08 16:56:30,107 - INFO -   Batch 550/2527: Loss = 0.0299
2026-01-08 16:56:54,065 - INFO -   Batch 600/2527: Loss = 0.0445
2026-01-08 16:57:17,644 - INFO -   Batch 650/2527: Loss = 0.0344
2026-01-08 16:57:41,398 - INFO -   Batch 700/2527: Loss = 0.0379
2026-01-08 16:58:05,024 - INFO -   Batch 750/2527: Loss = 0.0347
2026-01-08 16:58:28,832 - INFO -   Batch 800/2527: Loss = 0.0338
2026-01-08 16:58:52,701 - INFO -   Batch 850/2527: Loss = 0.0448
2026-01-08 16:59:16,506 - INFO -   Batch 900/2527: Loss = 0.0577
2026-01-08 16:59:40,477 - INFO -   Batch 950/2527: Loss = 0.0285
2026-01-08 17:00:04,103 - INFO -   Batch 1000/2527: Loss = 0.0211
2026-01-08 17:00:28,055 - INFO -   Batch 1050/2527: Loss = 0.0468
2026-01-08 17:00:51,626 - INFO -   Batch 1100/2527: Loss = 0.0274
2026-01-08 17:01:15,406 - INFO -   Batch 1150/2527: Loss = 0.0347
2026-01-08 17:01:39,386 - INFO -   Batch 1200/2527: Loss = 0.0349
2026-01-08 17:02:03,217 - INFO -   Batch 1250/2527: Loss = 0.0345
2026-01-08 17:02:27,068 - INFO -   Batch 1300/2527: Loss = 0.0431
2026-01-08 17:02:50,858 - INFO -   Batch 1350/2527: Loss = 0.0377
2026-01-08 17:03:14,473 - INFO -   Batch 1400/2527: Loss = 0.0303
2026-01-08 17:03:38,094 - INFO -   Batch 1450/2527: Loss = 0.0280
2026-01-08 17:04:02,087 - INFO -   Batch 1500/2527: Loss = 0.0433
2026-01-08 17:04:25,658 - INFO -   Batch 1550/2527: Loss = 0.0437
2026-01-08 17:04:49,439 - INFO -   Batch 1600/2527: Loss = 0.0515
2026-01-08 17:05:13,048 - INFO -   Batch 1650/2527: Loss = 0.0231
2026-01-08 17:05:36,882 - INFO -   Batch 1700/2527: Loss = 0.0495
2026-01-08 17:06:00,744 - INFO -   Batch 1750/2527: Loss = 0.0209
2026-01-08 17:06:24,529 - INFO -   Batch 1800/2527: Loss = 0.0655
2026-01-08 17:06:48,131 - INFO -   Batch 1850/2527: Loss = 0.0266
2026-01-08 17:07:11,753 - INFO -   Batch 1900/2527: Loss = 0.0360
2026-01-08 17:07:36,085 - INFO -   Batch 1950/2527: Loss = 0.0264
2026-01-08 17:07:59,657 - INFO -   Batch 2000/2527: Loss = 0.0297
2026-01-08 17:08:23,406 - INFO -   Batch 2050/2527: Loss = 0.0343
2026-01-08 17:08:46,995 - INFO -   Batch 2100/2527: Loss = 0.0211
2026-01-08 17:09:10,816 - INFO -   Batch 2150/2527: Loss = 0.0435
2026-01-08 17:09:34,668 - INFO -   Batch 2200/2527: Loss = 0.0394
2026-01-08 17:09:58,489 - INFO -   Batch 2250/2527: Loss = 0.0423
2026-01-08 17:10:22,078 - INFO -   Batch 2300/2527: Loss = 0.0259
2026-01-08 17:10:45,697 - INFO -   Batch 2350/2527: Loss = 0.0359
2026-01-08 17:11:10,043 - INFO -   Batch 2400/2527: Loss = 0.0407
2026-01-08 17:11:33,649 - INFO -   Batch 2450/2527: Loss = 0.0356
2026-01-08 17:11:57,396 - INFO -   Batch 2500/2527: Loss = 0.0402
2026-01-08 17:16:47,725 - INFO - Epoch 156/160: Train Loss: 0.0371, Val Loss: 1.9319, Val mIoU: 39.67%, Val PixelAcc: 79.28%
2026-01-08 17:17:13,762 - INFO -   Batch 50/2527: Loss = 0.0364
2026-01-08 17:17:38,819 - INFO -   Batch 100/2527: Loss = 0.0343
2026-01-08 17:18:04,160 - INFO -   Batch 150/2527: Loss = 0.0206
2026-01-08 17:18:28,964 - INFO -   Batch 200/2527: Loss = 0.0319
2026-01-08 17:18:53,788 - INFO -   Batch 250/2527: Loss = 0.0411
2026-01-08 17:19:18,784 - INFO -   Batch 300/2527: Loss = 0.0408
2026-01-08 17:19:43,611 - INFO -   Batch 350/2527: Loss = 0.0281
2026-01-08 17:20:08,480 - INFO -   Batch 400/2527: Loss = 0.0487
2026-01-08 17:20:33,607 - INFO -   Batch 450/2527: Loss = 0.0258
2026-01-08 17:20:58,588 - INFO -   Batch 500/2527: Loss = 0.0344
2026-01-08 17:21:23,411 - INFO -   Batch 550/2527: Loss = 0.0331
2026-01-08 17:21:48,259 - INFO -   Batch 600/2527: Loss = 0.0175
2026-01-08 17:22:13,016 - INFO -   Batch 650/2527: Loss = 0.0338
2026-01-08 17:22:37,877 - INFO -   Batch 700/2527: Loss = 0.0511
2026-01-08 17:23:03,039 - INFO -   Batch 750/2527: Loss = 0.0497
2026-01-08 17:23:28,001 - INFO -   Batch 800/2527: Loss = 0.0282
2026-01-08 17:23:52,822 - INFO -   Batch 850/2527: Loss = 0.0409
2026-01-08 17:24:18,054 - INFO -   Batch 900/2527: Loss = 0.0357
2026-01-08 17:24:43,115 - INFO -   Batch 950/2527: Loss = 0.0407
2026-01-08 17:25:07,941 - INFO -   Batch 1000/2527: Loss = 0.0394
2026-01-08 17:25:32,776 - INFO -   Batch 1050/2527: Loss = 0.0409
2026-01-08 17:25:57,912 - INFO -   Batch 1100/2527: Loss = 0.0333
2026-01-08 17:26:22,886 - INFO -   Batch 1150/2527: Loss = 0.0281
2026-01-08 17:26:47,703 - INFO -   Batch 1200/2527: Loss = 0.0337
2026-01-08 17:27:12,565 - INFO -   Batch 1250/2527: Loss = 0.0613
2026-01-08 17:27:37,689 - INFO -   Batch 1300/2527: Loss = 0.0375
2026-01-08 17:28:02,556 - INFO -   Batch 1350/2527: Loss = 0.0433
2026-01-08 17:28:27,673 - INFO -   Batch 1400/2527: Loss = 0.0408
2026-01-08 17:28:52,635 - INFO -   Batch 1450/2527: Loss = 0.0265
2026-01-08 17:29:17,477 - INFO -   Batch 1500/2527: Loss = 0.0332
2026-01-08 17:29:42,317 - INFO -   Batch 1550/2527: Loss = 0.0432
2026-01-08 17:30:07,691 - INFO -   Batch 1600/2527: Loss = 0.0362
2026-01-08 17:30:32,566 - INFO -   Batch 1650/2527: Loss = 0.0442
2026-01-08 17:30:57,392 - INFO -   Batch 1700/2527: Loss = 0.0278
2026-01-08 17:31:22,515 - INFO -   Batch 1750/2527: Loss = 0.0285
2026-01-08 17:31:47,514 - INFO -   Batch 1800/2527: Loss = 0.0313
2026-01-08 17:32:12,377 - INFO -   Batch 1850/2527: Loss = 0.0346
2026-01-08 17:32:37,199 - INFO -   Batch 1900/2527: Loss = 0.0315
2026-01-08 17:33:01,944 - INFO -   Batch 1950/2527: Loss = 0.0366
2026-01-08 17:33:26,785 - INFO -   Batch 2000/2527: Loss = 0.0402
2026-01-08 17:33:51,911 - INFO -   Batch 2050/2527: Loss = 0.0326
2026-01-08 17:34:16,987 - INFO -   Batch 2100/2527: Loss = 0.0405
2026-01-08 17:34:41,794 - INFO -   Batch 2150/2527: Loss = 0.0211
2026-01-08 17:35:06,649 - INFO -   Batch 2200/2527: Loss = 0.0396
2026-01-08 17:35:31,747 - INFO -   Batch 2250/2527: Loss = 0.0273
2026-01-08 17:35:56,583 - INFO -   Batch 2300/2527: Loss = 0.0448
2026-01-08 17:36:21,834 - INFO -   Batch 2350/2527: Loss = 0.0500
2026-01-08 17:36:46,985 - INFO -   Batch 2400/2527: Loss = 0.0306
2026-01-08 17:37:11,954 - INFO -   Batch 2450/2527: Loss = 0.0346
2026-01-08 17:37:36,808 - INFO -   Batch 2500/2527: Loss = 0.0157
2026-01-08 17:42:15,536 - INFO - Epoch 157/160: Train Loss: 0.0371, Val Loss: 1.9596, Val mIoU: 39.67%, Val PixelAcc: 79.28%
2026-01-08 17:42:42,029 - INFO -   Batch 50/2527: Loss = 0.0313
2026-01-08 17:43:06,998 - INFO -   Batch 100/2527: Loss = 0.0395
2026-01-08 17:43:31,793 - INFO -   Batch 150/2527: Loss = 0.0569
2026-01-08 17:43:56,980 - INFO -   Batch 200/2527: Loss = 0.0332
2026-01-08 17:44:22,141 - INFO -   Batch 250/2527: Loss = 0.0516
2026-01-08 17:44:47,148 - INFO -   Batch 300/2527: Loss = 0.0316
2026-01-08 17:45:11,984 - INFO -   Batch 350/2527: Loss = 0.0257
2026-01-08 17:45:36,811 - INFO -   Batch 400/2527: Loss = 0.0257
2026-01-08 17:46:01,546 - INFO -   Batch 450/2527: Loss = 0.0334
2026-01-08 17:46:26,752 - INFO -   Batch 500/2527: Loss = 0.0520
2026-01-08 17:46:51,879 - INFO -   Batch 550/2527: Loss = 0.0408
2026-01-08 17:47:16,849 - INFO -   Batch 600/2527: Loss = 0.0462
2026-01-08 17:47:41,677 - INFO -   Batch 650/2527: Loss = 0.0392
2026-01-08 17:48:06,545 - INFO -   Batch 700/2527: Loss = 0.0347
2026-01-08 17:48:31,597 - INFO -   Batch 750/2527: Loss = 0.0503
2026-01-08 17:48:56,443 - INFO -   Batch 800/2527: Loss = 0.0387
2026-01-08 17:49:21,268 - INFO -   Batch 850/2527: Loss = 0.0214
2026-01-08 17:49:46,426 - INFO -   Batch 900/2527: Loss = 0.0370
2026-01-08 17:50:11,494 - INFO -   Batch 950/2527: Loss = 0.0351
2026-01-08 17:50:36,413 - INFO -   Batch 1000/2527: Loss = 0.0434
2026-01-08 17:51:01,409 - INFO -   Batch 1050/2527: Loss = 0.0278
2026-01-08 17:51:26,620 - INFO -   Batch 1100/2527: Loss = 0.0388
2026-01-08 17:51:51,850 - INFO -   Batch 1150/2527: Loss = 0.0228
2026-01-08 17:52:17,296 - INFO -   Batch 1200/2527: Loss = 0.0398
2026-01-08 17:52:42,893 - INFO -   Batch 1250/2527: Loss = 0.0382
2026-01-08 17:53:08,043 - INFO -   Batch 1300/2527: Loss = 0.0425
2026-01-08 17:53:33,137 - INFO -   Batch 1350/2527: Loss = 0.0398
2026-01-08 17:53:58,371 - INFO -   Batch 1400/2527: Loss = 0.0465
2026-01-08 17:54:23,344 - INFO -   Batch 1450/2527: Loss = 0.0353
2026-01-08 17:54:48,348 - INFO -   Batch 1500/2527: Loss = 0.0369
2026-01-08 17:55:13,734 - INFO -   Batch 1550/2527: Loss = 0.0244
2026-01-08 17:55:38,914 - INFO -   Batch 1600/2527: Loss = 0.0387
2026-01-08 17:56:04,232 - INFO -   Batch 1650/2527: Loss = 0.0362
2026-01-08 17:56:29,168 - INFO -   Batch 1700/2527: Loss = 0.0276
2026-01-08 17:56:53,994 - INFO -   Batch 1750/2527: Loss = 0.0328
2026-01-08 17:57:18,925 - INFO -   Batch 1800/2527: Loss = 0.0366
2026-01-08 17:57:44,179 - INFO -   Batch 1850/2527: Loss = 0.0309
2026-01-08 17:58:09,603 - INFO -   Batch 1900/2527: Loss = 0.0288
2026-01-08 17:58:34,902 - INFO -   Batch 1950/2527: Loss = 0.0460
2026-01-08 17:58:59,946 - INFO -   Batch 2000/2527: Loss = 0.0387
2026-01-08 17:59:25,011 - INFO -   Batch 2050/2527: Loss = 0.0463
2026-01-08 17:59:49,942 - INFO -   Batch 2100/2527: Loss = 0.0470
2026-01-08 18:00:14,910 - INFO -   Batch 2150/2527: Loss = 0.0433
2026-01-08 18:00:39,983 - INFO -   Batch 2200/2527: Loss = 0.0480
2026-01-08 18:01:04,983 - INFO -   Batch 2250/2527: Loss = 0.0426
2026-01-08 18:01:29,951 - INFO -   Batch 2300/2527: Loss = 0.0235
2026-01-08 18:01:54,971 - INFO -   Batch 2350/2527: Loss = 0.0244
2026-01-08 18:02:19,859 - INFO -   Batch 2400/2527: Loss = 0.0349
2026-01-08 18:02:44,805 - INFO -   Batch 2450/2527: Loss = 0.0354
2026-01-08 18:03:09,929 - INFO -   Batch 2500/2527: Loss = 0.0255
2026-01-08 18:08:02,134 - INFO - Epoch 158/160: Train Loss: 0.0370, Val Loss: 1.9464, Val mIoU: 39.71%, Val PixelAcc: 79.29%
2026-01-08 18:08:27,061 - INFO -   Batch 50/2527: Loss = 0.0485
2026-01-08 18:08:49,979 - INFO -   Batch 100/2527: Loss = 0.0365
2026-01-08 18:09:13,250 - INFO -   Batch 150/2527: Loss = 0.0364
2026-01-08 18:09:36,217 - INFO -   Batch 200/2527: Loss = 0.0388
2026-01-08 18:09:59,168 - INFO -   Batch 250/2527: Loss = 0.0292
2026-01-08 18:10:22,112 - INFO -   Batch 300/2527: Loss = 0.0440
2026-01-08 18:10:45,064 - INFO -   Batch 350/2527: Loss = 0.0368
2026-01-08 18:11:08,053 - INFO -   Batch 400/2527: Loss = 0.0508
2026-01-08 18:11:31,065 - INFO -   Batch 450/2527: Loss = 0.0269
2026-01-08 18:11:54,019 - INFO -   Batch 500/2527: Loss = 0.0307
2026-01-08 18:12:16,978 - INFO -   Batch 550/2527: Loss = 0.0264
2026-01-08 18:12:40,288 - INFO -   Batch 600/2527: Loss = 0.0301
2026-01-08 18:13:03,248 - INFO -   Batch 650/2527: Loss = 0.0432
2026-01-08 18:13:26,213 - INFO -   Batch 700/2527: Loss = 0.0398
2026-01-08 18:13:49,198 - INFO -   Batch 750/2527: Loss = 0.0394
2026-01-08 18:14:12,163 - INFO -   Batch 800/2527: Loss = 0.0332
2026-01-08 18:14:35,497 - INFO -   Batch 850/2527: Loss = 0.0410
2026-01-08 18:14:58,513 - INFO -   Batch 900/2527: Loss = 0.0410
2026-01-08 18:15:21,466 - INFO -   Batch 950/2527: Loss = 0.0274
2026-01-08 18:15:44,460 - INFO -   Batch 1000/2527: Loss = 0.0334
2026-01-08 18:16:07,419 - INFO -   Batch 1050/2527: Loss = 0.0361
2026-01-08 18:16:30,378 - INFO -   Batch 1100/2527: Loss = 0.0417
2026-01-08 18:16:53,341 - INFO -   Batch 1150/2527: Loss = 0.0365
2026-01-08 18:17:16,332 - INFO -   Batch 1200/2527: Loss = 0.0419
2026-01-08 18:17:39,297 - INFO -   Batch 1250/2527: Loss = 0.0485
2026-01-08 18:18:02,256 - INFO -   Batch 1300/2527: Loss = 0.0228
2026-01-08 18:18:25,221 - INFO -   Batch 1350/2527: Loss = 0.0490
2026-01-08 18:18:48,215 - INFO -   Batch 1400/2527: Loss = 0.0498
2026-01-08 18:19:11,171 - INFO -   Batch 1450/2527: Loss = 0.0340
2026-01-08 18:19:34,132 - INFO -   Batch 1500/2527: Loss = 0.0432
2026-01-08 18:19:57,085 - INFO -   Batch 1550/2527: Loss = 0.0304
2026-01-08 18:20:20,428 - INFO -   Batch 1600/2527: Loss = 0.0321
2026-01-08 18:20:43,377 - INFO -   Batch 1650/2527: Loss = 0.0259
2026-01-08 18:21:06,337 - INFO -   Batch 1700/2527: Loss = 0.0329
2026-01-08 18:21:29,299 - INFO -   Batch 1750/2527: Loss = 0.0489
2026-01-08 18:21:52,246 - INFO -   Batch 1800/2527: Loss = 0.0307
2026-01-08 18:22:15,210 - INFO -   Batch 1850/2527: Loss = 0.0421
2026-01-08 18:22:38,174 - INFO -   Batch 1900/2527: Loss = 0.0267
2026-01-08 18:23:01,126 - INFO -   Batch 1950/2527: Loss = 0.0530
2026-01-08 18:23:24,458 - INFO -   Batch 2000/2527: Loss = 0.0385
2026-01-08 18:23:47,401 - INFO -   Batch 2050/2527: Loss = 0.0482
2026-01-08 18:24:10,373 - INFO -   Batch 2100/2527: Loss = 0.0584
2026-01-08 18:24:33,326 - INFO -   Batch 2150/2527: Loss = 0.0409
2026-01-08 18:24:56,293 - INFO -   Batch 2200/2527: Loss = 0.0290
2026-01-08 18:25:19,243 - INFO -   Batch 2250/2527: Loss = 0.0324
2026-01-08 18:25:42,578 - INFO -   Batch 2300/2527: Loss = 0.0273
2026-01-08 18:26:05,535 - INFO -   Batch 2350/2527: Loss = 0.0557
2026-01-08 18:26:28,491 - INFO -   Batch 2400/2527: Loss = 0.0315
2026-01-08 18:26:51,442 - INFO -   Batch 2450/2527: Loss = 0.0297
2026-01-08 18:27:14,417 - INFO -   Batch 2500/2527: Loss = 0.0443
2026-01-08 18:31:51,700 - INFO - Epoch 159/160: Train Loss: 0.0370, Val Loss: 1.9360, Val mIoU: 39.65%, Val PixelAcc: 79.28%
2026-01-08 18:32:16,962 - INFO -   Batch 50/2527: Loss = 0.0293
2026-01-08 18:32:40,535 - INFO -   Batch 100/2527: Loss = 0.0367
2026-01-08 18:33:04,458 - INFO -   Batch 150/2527: Loss = 0.0399
2026-01-08 18:33:28,023 - INFO -   Batch 200/2527: Loss = 0.0578
2026-01-08 18:33:51,763 - INFO -   Batch 250/2527: Loss = 0.0381
2026-01-08 18:34:15,381 - INFO -   Batch 300/2527: Loss = 0.0385
2026-01-08 18:34:39,200 - INFO -   Batch 350/2527: Loss = 0.0323
2026-01-08 18:35:03,063 - INFO -   Batch 400/2527: Loss = 0.0272
2026-01-08 18:35:26,851 - INFO -   Batch 450/2527: Loss = 0.0329
2026-01-08 18:35:50,757 - INFO -   Batch 500/2527: Loss = 0.0481
2026-01-08 18:36:14,742 - INFO -   Batch 550/2527: Loss = 0.0396
2026-01-08 18:36:38,698 - INFO -   Batch 600/2527: Loss = 0.0455
2026-01-08 18:37:02,305 - INFO -   Batch 650/2527: Loss = 0.0351
2026-01-08 18:37:26,066 - INFO -   Batch 700/2527: Loss = 0.0322
2026-01-08 18:37:49,696 - INFO -   Batch 750/2527: Loss = 0.0376
2026-01-08 18:38:13,528 - INFO -   Batch 800/2527: Loss = 0.0544
2026-01-08 18:38:37,395 - INFO -   Batch 850/2527: Loss = 0.0386
2026-01-08 18:39:01,186 - INFO -   Batch 900/2527: Loss = 0.0322
2026-01-08 18:39:25,159 - INFO -   Batch 950/2527: Loss = 0.0389
2026-01-08 18:39:48,790 - INFO -   Batch 1000/2527: Loss = 0.0299
2026-01-08 18:40:12,782 - INFO -   Batch 1050/2527: Loss = 0.0301
2026-01-08 18:40:36,366 - INFO -   Batch 1100/2527: Loss = 0.0413
2026-01-08 18:41:00,124 - INFO -   Batch 1150/2527: Loss = 0.0390
2026-01-08 18:41:23,728 - INFO -   Batch 1200/2527: Loss = 0.0391
2026-01-08 18:41:47,937 - INFO -   Batch 1250/2527: Loss = 0.0363
2026-01-08 18:42:11,833 - INFO -   Batch 1300/2527: Loss = 0.0513
2026-01-08 18:42:35,609 - INFO -   Batch 1350/2527: Loss = 0.0482
2026-01-08 18:42:59,206 - INFO -   Batch 1400/2527: Loss = 0.0315
2026-01-08 18:43:22,837 - INFO -   Batch 1450/2527: Loss = 0.0326
2026-01-08 18:43:46,800 - INFO -   Batch 1500/2527: Loss = 0.0324
2026-01-08 18:44:10,398 - INFO -   Batch 1550/2527: Loss = 0.0202
2026-01-08 18:44:34,151 - INFO -   Batch 1600/2527: Loss = 0.0348
2026-01-08 18:44:57,756 - INFO -   Batch 1650/2527: Loss = 0.0341
2026-01-08 18:45:21,569 - INFO -   Batch 1700/2527: Loss = 0.0393
2026-01-08 18:45:45,435 - INFO -   Batch 1750/2527: Loss = 0.0361
2026-01-08 18:46:09,221 - INFO -   Batch 1800/2527: Loss = 0.0316
2026-01-08 18:46:32,814 - INFO -   Batch 1850/2527: Loss = 0.0344
2026-01-08 18:46:56,441 - INFO -   Batch 1900/2527: Loss = 0.0477
2026-01-08 18:47:20,804 - INFO -   Batch 1950/2527: Loss = 0.0400
2026-01-08 18:47:44,387 - INFO -   Batch 2000/2527: Loss = 0.0414
2026-01-08 18:48:08,149 - INFO -   Batch 2050/2527: Loss = 0.0234
2026-01-08 18:48:31,759 - INFO -   Batch 2100/2527: Loss = 0.0364
2026-01-08 18:48:55,568 - INFO -   Batch 2150/2527: Loss = 0.0315
2026-01-08 18:49:19,454 - INFO -   Batch 2200/2527: Loss = 0.0314
2026-01-08 18:49:43,279 - INFO -   Batch 2250/2527: Loss = 0.0411
2026-01-08 18:50:06,872 - INFO -   Batch 2300/2527: Loss = 0.0252
2026-01-08 18:50:30,498 - INFO -   Batch 2350/2527: Loss = 0.0349
2026-01-08 18:50:54,840 - INFO -   Batch 2400/2527: Loss = 0.0517
2026-01-08 18:51:18,452 - INFO -   Batch 2450/2527: Loss = 0.0336
2026-01-08 18:51:42,264 - INFO -   Batch 2500/2527: Loss = 0.0259
2026-01-08 18:56:32,305 - INFO - Epoch 160/160: Train Loss: 0.0370, Val Loss: 1.9488, Val mIoU: 39.70%, Val PixelAcc: 79.29%
2026-01-08 18:56:34,538 - INFO - ✅ Checkpoint saved: runs/run_261/checkpoint_epoch_160.pth
2026-01-08 18:56:34,540 - INFO - Checkpoint saved: runs/run_261/checkpoint_epoch_160.pth
2026-01-08 18:56:34,541 - INFO - Training complete! Best mIoU: 41.34%
2026-01-08 18:56:34,542 - INFO - Training complete!
2026-01-08 18:56:34,543 - INFO - Generating reports...
2026-01-08 18:56:34,544 - INFO - Generating segmentation training curves...
2026-01-08 18:56:36,980 - INFO - Segmentation training curves saved with base name 'runs/run_261/segmentation_curves_deit_small_upernet'
2026-01-08 18:56:36,982 - INFO - Performing final evaluation on validation set...
2026-01-08 19:01:15,944 - INFO - Generating per-class IoU plot...
2026-01-08 19:01:17,469 - INFO - Per-class IoU plot saved to 'runs/run_261/iou_per_class_deit_small_upernet.png'
2026-01-08 19:01:17,470 - INFO - Generating LR schedule plot...
2026-01-08 19:01:18,306 - INFO - LR schedule plot saved to 'runs/run_261/lr_schedule_deit_small_upernet.png'
2026-01-08 19:01:18,308 - INFO - 
==================================================
2026-01-08 19:01:18,309 - INFO - Final Segmentation Results (deit_small_upernet)
2026-01-08 19:01:18,310 - INFO - ==================================================
2026-01-08 19:01:18,310 - INFO - Loss: 1.9488
2026-01-08 19:01:18,311 - INFO - mIoU: 39.70%
2026-01-08 19:01:18,312 - INFO - Pixel Accuracy: 79.29%
2026-01-08 19:01:18,313 - INFO - Classes Present: 150/150
2026-01-08 19:01:18,318 - INFO - Final metrics saved to runs/run_261/final_metrics_deit_small_upernet.json
2026-01-08 19:01:18,950 - INFO - ✅ Model weights saved: runs/run_261/final_model_deit_small_upernet_weights.pth
2026-01-08 19:01:18,983 - INFO - Final model saved: runs/run_261/final_model_deit_small_upernet_weights.pth
2026-01-08 19:01:18,985 - INFO - Final model metadata saved: runs/run_261/final_model_deit_small_upernet_metadata.json
2026-01-08 19:01:19,057 - INFO - Experiment completed. Results saved to runs/run_261/results_deit_small_upernet.json
2026-01-08 19:01:19,060 - INFO - Experiment completed. Metadata saved to runs/run_261/metadata_deit_small_upernet.json
2026-01-08 19:01:19,061 - INFO - ============================================================
2026-01-08 19:01:19,062 - INFO - SEGMENTATION PIPELINE COMPLETE
2026-01-08 19:01:19,063 - INFO - Final mIoU: 39.70%
2026-01-08 19:01:19,064 - INFO - Final Pixel Acc: 79.29%
2026-01-08 19:01:19,065 - INFO - ============================================================
2026-01-08 19:01:19,075 - INFO - Segmentation training completed successfully!
2026-01-08 19:01:19,077 - INFO - Final mIoU: 39.70%
