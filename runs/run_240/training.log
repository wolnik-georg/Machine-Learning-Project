2025-12-28 11:15:37,386 - INFO - Logging initialized. Log file: runs/run_240/training.log
2025-12-28 11:15:37,587 - INFO - Experiment directory: runs/run_240
2025-12-28 11:15:37,588 - INFO - Setting random seeds for reproducibility (seed: 42)...
2025-12-28 11:15:37,610 - INFO - ✅ All seeds set to 42 (deterministic=False)
2025-12-28 11:15:38,112 - INFO - GPU memory cleared at startup. Available: 23.6GB
2025-12-28 11:15:38,113 - INFO - Using device: cuda
2025-12-28 11:15:38,114 - INFO - Training configuration: epochs=40, warmup=3, lr=0.0005
2025-12-28 11:15:38,116 - INFO - SWIN configuration: variant=tiny, use_shifted_window=True, use_relative_bias=True, use_absolute_pos_embed=False, use_hierarchical_merge=False, use_gradient_checkpointing=True
2025-12-28 11:15:38,117 - INFO - SWIN details: embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], window_size=7
2025-12-28 11:15:38,118 - INFO - Loading dataset...
2025-12-28 11:15:38,121 - INFO - Starting ImageNet data loading from root: /
2025-12-28 11:15:38,123 - INFO - Resolved root path: /
2025-12-28 11:15:38,124 - INFO - Root path exists: True
2025-12-28 11:15:38,126 - INFO - Root path is dir: True
2025-12-28 11:15:38,154 - INFO - Contents of /: ['/.exec', '/.run', '/.shell', '/.singularity.d', '/.test', '/bin', '/boot', '/dev', '/environment', '/etc', '/home', '/lib', '/lib64', '/media', '/mnt', '/opt', '/proc', '/requirements.txt', '/root', '/run', '/sbin', '/singularity', '/srv', '/sys', '/tmp', '/usr', '/var', '/meta', '/train_set', '/train_set_small', '/val_set', '/val_set_unlabeled']
2025-12-28 11:15:38,155 - INFO - Expected train_dir: /train_set, exists: True
2025-12-28 11:15:38,156 - INFO - Expected val_dir: /val_set, exists: True
2025-12-28 11:15:52,510 - INFO - Loaded ImageNet data from /: train=1281167, val=50000
2025-12-28 11:15:52,851 - INFO - Dataset loaded: train=100000 samples (782 batches), val=50000 samples (391 batches), test=50000 samples (391 batches)
2025-12-28 11:15:58,442 - INFO - Training SWIN from scratch
2025-12-28 11:15:58,473 - INFO - Initializing SWIN model from scratch...
2025-12-28 11:15:58,475 - INFO - Model architecture: SWIN
2025-12-28 11:15:58,476 - INFO - Model config: {'type': 'swin', 'variant': 'tiny', 'patch_size': 4, 'embed_dim': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24], 'window_size': 7, 'mlp_ratio': 4.0, 'dropout': 0.0, 'attention_dropout': 0.0, 'projection_dropout': 0.0, 'drop_path_rate': 0.08, 'use_shifted_window': True, 'use_relative_bias': True, 'use_absolute_pos_embed': False, 'use_hierarchical_merge': False, 'use_gradient_checkpointing': True}
2025-12-28 11:15:59,434 - INFO - Model created with random initialization
2025-12-28 11:15:59,436 - INFO - Total parameters: 28,288,354
2025-12-28 11:15:59,437 - INFO - Trainable parameters: 28,288,354
2025-12-28 11:15:59,516 - INFO - Starting from-scratch training...
2025-12-28 11:15:59,518 - INFO - Optimizer: training all model parameters
2025-12-28 11:15:59,521 - INFO - LR Scheduler: cosine with 3 warmup epochs
2025-12-28 11:15:59,523 - INFO - Starting training...
2025-12-28 12:14:31,483 - INFO - Epoch 1/40: LR: 0.000170
2025-12-28 12:14:31,605 - INFO - Epoch 1/40: Train Loss: 6.8849, Val Loss: 6.7706, Val Acc: 0.46%
2025-12-28 12:59:06,546 - INFO - Epoch 2/40: LR: 0.000335
2025-12-28 12:59:06,551 - INFO - Epoch 2/40: Train Loss: 6.7472, Val Loss: 6.4990, Val Acc: 0.95%
2025-12-28 13:44:18,358 - INFO - Epoch 3/40: LR: 0.000500
2025-12-28 13:44:18,381 - INFO - Epoch 3/40: Train Loss: 6.4594, Val Loss: 6.2091, Val Acc: 1.37%
2025-12-28 14:28:32,424 - INFO - Epoch 4/40: LR: 0.000499
2025-12-28 14:28:35,168 - INFO - Epoch 4/40: Train Loss: 6.2506, Val Loss: 5.9739, Val Acc: 2.18%
2025-12-28 15:13:33,511 - INFO - Epoch 5/40: LR: 0.000497
2025-12-28 15:27:57,588 - INFO - Epoch 5/40: Train Loss: 6.0954, Val Loss: 5.8714, Val Acc: 2.62%, Test Loss: 5.8714, Test Acc: 2.62%
2025-12-28 16:13:27,335 - INFO - Epoch 6/40: LR: 0.000493
2025-12-28 16:13:27,495 - INFO - Epoch 6/40: Train Loss: 5.9566, Val Loss: 5.6738, Val Acc: 3.69%
2025-12-28 16:58:37,338 - INFO - Epoch 7/40: LR: 0.000487
2025-12-28 16:58:37,467 - INFO - Epoch 7/40: Train Loss: 5.8313, Val Loss: 5.4994, Val Acc: 4.76%
2025-12-28 17:44:10,569 - INFO - Epoch 8/40: LR: 0.000480
2025-12-28 17:44:10,657 - INFO - Epoch 8/40: Train Loss: 5.7035, Val Loss: 5.3824, Val Acc: 5.82%
2025-12-28 18:28:55,801 - INFO - Epoch 9/40: LR: 0.000471
2025-12-28 18:28:56,012 - INFO - Epoch 9/40: Train Loss: 5.5874, Val Loss: 5.2135, Val Acc: 6.86%
2025-12-28 19:13:27,573 - INFO - Epoch 10/40: LR: 0.000461
2025-12-28 19:28:01,884 - INFO - Epoch 10/40: Train Loss: 5.4797, Val Loss: 5.1610, Val Acc: 7.71%, Test Loss: 5.1610, Test Acc: 7.71%
2025-12-28 19:28:02,022 - INFO - Saving checkpoint for epoch 10...
2025-12-28 19:28:03,278 - INFO - ✅ Checkpoint saved: runs/run_240/checkpoint_epoch_10.pth
2025-12-28 20:13:18,225 - INFO - Epoch 11/40: LR: 0.000450
2025-12-28 20:13:18,655 - INFO - Epoch 11/40: Train Loss: 5.3736, Val Loss: 4.9735, Val Acc: 9.48%
2025-12-28 20:57:45,517 - INFO - Epoch 12/40: LR: 0.000437
2025-12-28 20:57:45,589 - INFO - Epoch 12/40: Train Loss: 5.2787, Val Loss: 4.9196, Val Acc: 10.12%
2025-12-28 21:42:45,277 - INFO - Epoch 13/40: LR: 0.000424
2025-12-28 21:42:45,371 - INFO - Epoch 13/40: Train Loss: 5.1895, Val Loss: 4.9380, Val Acc: 10.16%
2025-12-28 22:27:34,874 - INFO - Epoch 14/40: LR: 0.000409
2025-12-28 22:27:35,653 - INFO - Epoch 14/40: Train Loss: 5.1020, Val Loss: 4.7351, Val Acc: 12.00%
2025-12-28 23:12:34,849 - INFO - Epoch 15/40: LR: 0.000393
2025-12-28 23:27:05,928 - INFO - Epoch 15/40: Train Loss: 5.0077, Val Loss: 4.6000, Val Acc: 13.51%, Test Loss: 4.6000, Test Acc: 13.51%
2025-12-29 00:11:53,354 - INFO - Epoch 16/40: LR: 0.000376
2025-12-29 00:11:53,798 - INFO - Epoch 16/40: Train Loss: 4.9252, Val Loss: 4.5372, Val Acc: 14.07%
2025-12-29 00:57:14,672 - INFO - Epoch 17/40: LR: 0.000359
2025-12-29 00:57:14,736 - INFO - Epoch 17/40: Train Loss: 4.8443, Val Loss: 4.4590, Val Acc: 14.81%
2025-12-29 01:42:37,026 - INFO - Epoch 18/40: LR: 0.000341
2025-12-29 01:42:37,100 - INFO - Epoch 18/40: Train Loss: 4.7655, Val Loss: 4.4536, Val Acc: 14.97%
2025-12-29 02:27:11,472 - INFO - Epoch 19/40: LR: 0.000322
2025-12-29 02:27:13,093 - INFO - Epoch 19/40: Train Loss: 4.6806, Val Loss: 4.3697, Val Acc: 15.97%
2025-12-29 03:12:22,347 - INFO - Epoch 20/40: LR: 0.000304
2025-12-29 03:26:44,356 - INFO - Epoch 20/40: Train Loss: 4.6041, Val Loss: 4.3024, Val Acc: 16.96%, Test Loss: 4.3024, Test Acc: 16.96%
2025-12-29 03:26:44,382 - INFO - Saving checkpoint for epoch 20...
2025-12-29 03:26:45,868 - INFO - ✅ Checkpoint saved: runs/run_240/checkpoint_epoch_20.pth
2025-12-29 04:11:43,093 - INFO - Epoch 21/40: LR: 0.000285
2025-12-29 04:11:43,153 - INFO - Epoch 21/40: Train Loss: 4.5225, Val Loss: 4.2203, Val Acc: 17.84%
2025-12-29 04:56:01,990 - INFO - Epoch 22/40: LR: 0.000265
2025-12-29 04:56:02,195 - INFO - Epoch 22/40: Train Loss: 4.4446, Val Loss: 4.1556, Val Acc: 18.84%
2025-12-29 05:40:09,753 - INFO - Epoch 23/40: LR: 0.000246
2025-12-29 05:40:09,773 - INFO - Epoch 23/40: Train Loss: 4.3610, Val Loss: 4.1192, Val Acc: 19.47%
2025-12-29 06:24:54,374 - INFO - Epoch 24/40: LR: 0.000228
2025-12-29 06:24:54,399 - INFO - Epoch 24/40: Train Loss: 4.2924, Val Loss: 4.0527, Val Acc: 20.15%
2025-12-29 07:09:24,770 - INFO - Epoch 25/40: LR: 0.000209
2025-12-29 07:24:03,378 - INFO - Epoch 25/40: Train Loss: 4.2281, Val Loss: 4.0172, Val Acc: 20.54%, Test Loss: 4.0172, Test Acc: 20.54%
2025-12-29 08:09:05,973 - INFO - Epoch 26/40: LR: 0.000191
2025-12-29 08:09:06,003 - INFO - Epoch 26/40: Train Loss: 4.1582, Val Loss: 3.9589, Val Acc: 21.52%
2025-12-29 08:53:47,078 - INFO - Epoch 27/40: LR: 0.000174
2025-12-29 08:53:47,166 - INFO - Epoch 27/40: Train Loss: 4.0842, Val Loss: 3.8911, Val Acc: 22.32%
2025-12-29 09:38:55,046 - INFO - Epoch 28/40: LR: 0.000157
2025-12-29 09:38:55,088 - INFO - Epoch 28/40: Train Loss: 4.0169, Val Loss: 3.8792, Val Acc: 22.90%
2025-12-29 10:23:52,745 - INFO - Epoch 29/40: LR: 0.000141
2025-12-29 10:23:52,770 - INFO - Epoch 29/40: Train Loss: 3.9610, Val Loss: 3.8324, Val Acc: 23.52%
2025-12-29 11:07:51,889 - INFO - Epoch 30/40: LR: 0.000126
2025-12-29 11:22:39,520 - INFO - Epoch 30/40: Train Loss: 3.8890, Val Loss: 3.7746, Val Acc: 24.26%, Test Loss: 3.7746, Test Acc: 24.26%
2025-12-29 11:22:39,522 - INFO - Saving checkpoint for epoch 30...
2025-12-29 11:22:41,077 - INFO - ✅ Checkpoint saved: runs/run_240/checkpoint_epoch_30.pth
2025-12-29 12:07:22,295 - INFO - Epoch 31/40: LR: 0.000113
2025-12-29 12:07:22,313 - INFO - Epoch 31/40: Train Loss: 3.8367, Val Loss: 3.7561, Val Acc: 24.59%
2025-12-29 12:51:52,361 - INFO - Epoch 32/40: LR: 0.000100
2025-12-29 12:51:52,389 - INFO - Epoch 32/40: Train Loss: 3.7897, Val Loss: 3.7220, Val Acc: 24.86%
2025-12-29 13:36:45,242 - INFO - Epoch 33/40: LR: 0.000089
2025-12-29 13:36:45,335 - INFO - Epoch 33/40: Train Loss: 3.7313, Val Loss: 3.7059, Val Acc: 25.42%
2025-12-29 14:21:26,837 - INFO - Epoch 34/40: LR: 0.000079
2025-12-29 14:21:27,926 - INFO - Epoch 34/40: Train Loss: 3.7081, Val Loss: 3.6651, Val Acc: 26.04%
2025-12-29 15:06:14,877 - INFO - Epoch 35/40: LR: 0.000070
2025-12-29 15:20:43,737 - INFO - Epoch 35/40: Train Loss: 3.6580, Val Loss: 3.6555, Val Acc: 26.13%, Test Loss: 3.6555, Test Acc: 26.13%
2025-12-29 16:12:18,259 - INFO - Epoch 36/40: LR: 0.000063
2025-12-29 16:12:18,295 - INFO - Epoch 36/40: Train Loss: 3.6156, Val Loss: 3.6494, Val Acc: 26.20%
2025-12-29 16:59:13,759 - INFO - Epoch 37/40: LR: 0.000057
2025-12-29 16:59:14,019 - INFO - Epoch 37/40: Train Loss: 3.5877, Val Loss: 3.6274, Val Acc: 26.34%
2025-12-29 17:47:55,624 - INFO - Epoch 38/40: LR: 0.000053
2025-12-29 17:47:55,810 - INFO - Epoch 38/40: Train Loss: 3.5559, Val Loss: 3.6192, Val Acc: 26.53%
2025-12-29 18:34:54,917 - INFO - Epoch 39/40: LR: 0.000051
2025-12-29 18:34:55,161 - INFO - Epoch 39/40: Train Loss: 3.5437, Val Loss: 3.6018, Val Acc: 26.88%
2025-12-29 19:21:24,677 - INFO - Epoch 40/40: LR: 0.000050
2025-12-29 19:35:58,997 - INFO - Epoch 40/40: Train Loss: 3.5118, Val Loss: 3.6053, Val Acc: 26.81%, Test Loss: 3.6053, Test Acc: 26.81%
2025-12-29 19:35:59,155 - INFO - Saving checkpoint for epoch 40...
2025-12-29 19:38:07,092 - INFO - ✅ Checkpoint saved: runs/run_240/checkpoint_epoch_40.pth
2025-12-29 19:38:07,147 - INFO - Training completed!
2025-12-29 19:38:07,189 - INFO - Training completed!
2025-12-29 19:38:07,321 - INFO - Generating training curves...
2025-12-29 19:38:12,692 - INFO - Too many classes (1000) for per-class plotting, skipping
2025-12-29 19:38:12,720 - INFO - Training curves saved as separate images with base name 'runs/run_240/training_curves_from_scratch'
2025-12-29 19:38:12,762 - INFO - Generating confusion matrix on test set...
2025-12-29 20:42:42,889 - INFO - Performing final evaluation of from_scratch model on test set...
2025-12-29 20:58:46,698 - INFO - Final Test Results of from_scratch model: Loss: 3.6053, Accuracy: 26.81%
2025-12-29 20:58:46,787 - INFO - Generating LR schedule plot of from_scratch model...
2025-12-29 20:58:47,696 - INFO - LR schedule plot saved to 'runs/run_240/lr_schedule_from_scratch.png'
2025-12-29 20:58:47,698 - INFO - 
Final Test Results:
2025-12-29 20:58:47,700 - INFO - Loss: 3.6053
2025-12-29 20:58:47,701 - INFO - Accuracy: 26.81%
2025-12-29 20:58:47,702 - INFO - Precision: 26.38%
2025-12-29 20:58:47,703 - INFO - Recall: 26.81%
2025-12-29 20:58:47,704 - INFO - F1 Score: 25.43%
2025-12-29 20:58:48,447 - INFO - Experiment completed. Results saved to runs/run_240/results_from_scratch.json
2025-12-29 20:58:48,452 - INFO - Experiment completed. Metadata saved to runs/run_240/metadata_from_scratch.json
2025-12-29 20:58:49,037 - INFO - ✅ Model weights saved: runs/run_240/final_model_from_scratch_weights.pth
2025-12-29 20:58:49,088 - INFO - Final model saved: runs/run_240/final_model_from_scratch_weights.pth
2025-12-29 20:58:49,089 - INFO - Final model metadata saved: runs/run_240/final_model_from_scratch_metadata.json
2025-12-29 20:58:49,091 - INFO - === FROM-SCRATCH RESULTS (IMAGENET) ===
2025-12-29 20:58:49,092 - INFO - Final Accuracy: 26.81%
2025-12-29 20:58:49,094 - INFO - Final F1 Score: 25.43%
2025-12-29 20:58:49,104 - INFO - from_scratch experiment completed successfully!
