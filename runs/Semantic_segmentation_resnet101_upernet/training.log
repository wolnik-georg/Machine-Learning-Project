2026-01-01 21:42:46,961 - INFO - Logging initialized. Log file: runs/run_259/training.log
2026-01-01 21:42:46,963 - INFO - Experiment directory: runs/run_259
2026-01-01 21:42:46,965 - INFO - Setting random seeds (seed: 42)...
2026-01-01 21:42:46,985 - INFO - ✅ All seeds set to 42 (deterministic=False)
2026-01-01 21:42:47,295 - INFO - Using GPU: NVIDIA GeForce RTX 4070
2026-01-01 21:42:47,296 - INFO - GPU memory: 11.6GB
2026-01-01 21:42:47,298 - INFO - CuDNN benchmark mode enabled
2026-01-01 21:42:47,299 - INFO - Encoder type: resnet
2026-01-01 21:42:47,300 - INFO - Data config: {'dataset': 'ADE20K', 'use_batch_for_val': False, 'val_batch': 5, 'batch_size': 8, 'num_workers': 8, 'root': './datasets', 'img_size': 512, 'n_train': None, 'n_test': None, 'stratified': False}
2026-01-01 21:42:47,301 - INFO - Encoder config: {'type': 'resnet', 'variant': 'resnet101', 'pretrained': True, 'img_size': 512, 'layers': [3, 4, 23, 3], 'use_gradient_checkpointing': True}
2026-01-01 21:42:47,302 - INFO - Downstream config: {'mode': 'from_scratch', 'head_type': 'upernet', 'num_classes': 150, 'hidden_dim': None, 'freeze_encoder': False, 'use_pretrained': True}
2026-01-01 21:42:47,303 - INFO - Training config: {'learning_rate': 6e-05, 'num_epochs': 160, 'warmup_epochs': 2, 'warmup_start_factor': 0.1, 'weight_decay': 0.01, 'mixed_precision': True, 'compile': False}
2026-01-01 21:42:47,305 - INFO - Loading ADE20K dataset...
2026-01-01 21:42:47,307 - INFO - Using user ADE20K dataset from /home/pml20/datasets/ade20k
2026-01-01 21:42:48,909 - INFO - Loaded ADE20K data from /home/pml20/datasets/ade20k: train=20210, val=2000, test=2000
2026-01-01 21:42:48,911 - INFO - Dataset limits applied
2026-01-01 21:42:48,916 - INFO - Dataset loaded: train=20210 (2527 batches), val=2000 (250 batches)
2026-01-01 21:42:48,917 - INFO - ============================================================
2026-01-01 21:42:48,919 - INFO - Segmentation Pipeline: RESNET101 + UperNet on ADE20K
2026-01-01 21:42:48,920 - INFO - ============================================================
2026-01-01 21:42:48,921 - INFO - Creating RESNET101 + UperNet segmentation model...
2026-01-01 21:42:53,153 - INFO - Model parameters:
2026-01-01 21:42:53,154 - INFO -   Encoder: 44,549,160
2026-01-01 21:42:53,156 - INFO -   Head: 40,590,486
2026-01-01 21:42:53,157 - INFO -   Total: 85,139,646
2026-01-01 21:42:53,159 - INFO -   Trainable: 85,139,646
2026-01-01 21:42:53,342 - INFO - Mixed precision: CUDA bf16 selected (hardware supported)
2026-01-01 21:42:53,344 - INFO - Optimizer: training all model parameters (encoder + head)
2026-01-01 21:42:53,349 - INFO - LR Scheduler: cosine with 2 warmup epochs
2026-01-01 21:42:53,351 - INFO - Learning rate: 6e-05, min_lr: 6.000000000000001e-07
2026-01-01 21:42:53,352 - INFO - Training configuration:
2026-01-01 21:42:53,353 - INFO -   Epochs: 160
2026-01-01 21:42:53,355 - INFO -   Warmup epochs: 2
2026-01-01 21:42:53,356 - INFO -   Learning rate: 6e-05
2026-01-01 21:42:53,357 - INFO -   Weight decay: 0.01
2026-01-01 21:42:53,358 - INFO -   Freeze encoder: False
2026-01-01 21:42:53,359 - INFO -   Num classes: 150
2026-01-01 21:42:53,361 - INFO -   Mixed precision: torch.bfloat16
2026-01-01 21:42:53,362 - INFO - Starting training...
2026-01-01 21:42:53,363 - INFO - Starting segmentation training...
2026-01-01 21:42:53,364 - INFO - Training for 160 epochs, 2527 batches/epoch
2026-01-01 21:45:37,463 - INFO -   Batch 50/2527: Loss = 3.3000
2026-01-01 21:46:06,085 - INFO -   Batch 100/2527: Loss = 2.5677
2026-01-01 21:46:34,807 - INFO -   Batch 150/2527: Loss = 2.9213
2026-01-01 21:47:03,847 - INFO -   Batch 200/2527: Loss = 2.3319
2026-01-01 21:47:32,505 - INFO -   Batch 250/2527: Loss = 2.6735
2026-01-01 21:48:01,298 - INFO -   Batch 300/2527: Loss = 2.4484
2026-01-01 21:48:29,878 - INFO -   Batch 350/2527: Loss = 1.9579
2026-01-01 21:48:58,509 - INFO -   Batch 400/2527: Loss = 1.9470
2026-01-01 21:49:27,340 - INFO -   Batch 450/2527: Loss = 1.9140
2026-01-01 21:49:56,076 - INFO -   Batch 500/2527: Loss = 1.8821
2026-01-01 21:50:24,968 - INFO -   Batch 550/2527: Loss = 2.1814
2026-01-01 21:50:53,818 - INFO -   Batch 600/2527: Loss = 2.2954
2026-01-01 21:51:22,654 - INFO -   Batch 650/2527: Loss = 1.6738
2026-01-01 21:51:51,286 - INFO -   Batch 700/2527: Loss = 1.5455
2026-01-01 21:52:19,761 - INFO -   Batch 750/2527: Loss = 1.7440
2026-01-01 21:52:48,608 - INFO -   Batch 800/2527: Loss = 1.5672
2026-01-01 21:53:17,182 - INFO -   Batch 850/2527: Loss = 1.2615
2026-01-01 21:53:46,151 - INFO -   Batch 900/2527: Loss = 2.0777
2026-01-01 21:54:14,886 - INFO -   Batch 950/2527: Loss = 1.4474
2026-01-01 21:54:43,897 - INFO -   Batch 1000/2527: Loss = 1.7648
2026-01-01 21:55:12,464 - INFO -   Batch 1050/2527: Loss = 1.1021
2026-01-01 21:55:40,993 - INFO -   Batch 1100/2527: Loss = 1.9633
2026-01-01 21:56:09,816 - INFO -   Batch 1150/2527: Loss = 1.3280
2026-01-01 21:56:38,504 - INFO -   Batch 1200/2527: Loss = 1.8457
2026-01-01 21:57:07,368 - INFO -   Batch 1250/2527: Loss = 1.8179
2026-01-01 21:57:35,967 - INFO -   Batch 1300/2527: Loss = 1.3961
2026-01-01 21:58:04,488 - INFO -   Batch 1350/2527: Loss = 2.0161
2026-01-01 21:58:33,552 - INFO -   Batch 1400/2527: Loss = 1.7718
2026-01-01 21:59:02,182 - INFO -   Batch 1450/2527: Loss = 1.9680
2026-01-01 21:59:30,947 - INFO -   Batch 1500/2527: Loss = 1.5331
2026-01-01 21:59:59,570 - INFO -   Batch 1550/2527: Loss = 1.2995
2026-01-01 22:00:28,375 - INFO -   Batch 1600/2527: Loss = 1.1596
2026-01-01 22:00:57,052 - INFO -   Batch 1650/2527: Loss = 1.5994
2026-01-01 22:01:25,644 - INFO -   Batch 1700/2527: Loss = 1.3869
2026-01-01 22:01:54,456 - INFO -   Batch 1750/2527: Loss = 1.4333
2026-01-01 22:02:23,413 - INFO -   Batch 1800/2527: Loss = 1.2696
2026-01-01 22:02:52,282 - INFO -   Batch 1850/2527: Loss = 0.9124
2026-01-01 22:03:20,852 - INFO -   Batch 1900/2527: Loss = 1.5477
2026-01-01 22:03:49,321 - INFO -   Batch 1950/2527: Loss = 1.8443
2026-01-01 22:04:18,249 - INFO -   Batch 2000/2527: Loss = 1.5663
2026-01-01 22:04:46,875 - INFO -   Batch 2050/2527: Loss = 1.8453
2026-01-01 22:05:16,026 - INFO -   Batch 2100/2527: Loss = 1.2729
2026-01-01 22:05:44,675 - INFO -   Batch 2150/2527: Loss = 1.4110
2026-01-01 22:06:13,694 - INFO -   Batch 2200/2527: Loss = 1.0380
2026-01-01 22:06:42,341 - INFO -   Batch 2250/2527: Loss = 1.2685
2026-01-01 22:07:10,739 - INFO -   Batch 2300/2527: Loss = 1.9300
2026-01-01 22:07:39,602 - INFO -   Batch 2350/2527: Loss = 1.3934
2026-01-01 22:08:08,265 - INFO -   Batch 2400/2527: Loss = 1.2571
2026-01-01 22:08:37,119 - INFO -   Batch 2450/2527: Loss = 1.0970
2026-01-01 22:09:05,926 - INFO -   Batch 2500/2527: Loss = 1.5466
2026-01-01 22:15:57,560 - INFO - Epoch 1/160: Train Loss: 1.7532, Val Loss: 1.2323, Val mIoU: 14.56%, Val PixelAcc: 70.31%
2026-01-01 22:15:57,562 - INFO -   → New best mIoU: 14.56%
2026-01-01 22:16:01,154 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-01 22:16:01,157 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-01 22:16:31,826 - INFO -   Batch 50/2527: Loss = 1.9088
2026-01-01 22:17:00,430 - INFO -   Batch 100/2527: Loss = 1.1841
2026-01-01 22:17:29,231 - INFO -   Batch 150/2527: Loss = 1.2646
2026-01-01 22:17:57,818 - INFO -   Batch 200/2527: Loss = 1.5453
2026-01-01 22:18:26,378 - INFO -   Batch 250/2527: Loss = 1.3965
2026-01-01 22:18:55,229 - INFO -   Batch 300/2527: Loss = 1.4807
2026-01-01 22:19:24,006 - INFO -   Batch 350/2527: Loss = 1.5096
2026-01-01 22:19:52,901 - INFO -   Batch 400/2527: Loss = 1.6645
2026-01-01 22:20:21,767 - INFO -   Batch 450/2527: Loss = 1.3582
2026-01-01 22:20:50,628 - INFO -   Batch 500/2527: Loss = 1.4209
2026-01-01 22:21:19,251 - INFO -   Batch 550/2527: Loss = 1.0380
2026-01-01 22:21:47,856 - INFO -   Batch 600/2527: Loss = 1.1719
2026-01-01 22:22:16,706 - INFO -   Batch 650/2527: Loss = 1.3223
2026-01-01 22:22:45,331 - INFO -   Batch 700/2527: Loss = 0.9792
2026-01-01 22:23:14,329 - INFO -   Batch 750/2527: Loss = 1.1607
2026-01-01 22:23:43,036 - INFO -   Batch 800/2527: Loss = 1.0781
2026-01-01 22:24:12,106 - INFO -   Batch 850/2527: Loss = 1.3397
2026-01-01 22:24:40,703 - INFO -   Batch 900/2527: Loss = 1.4505
2026-01-01 22:25:09,218 - INFO -   Batch 950/2527: Loss = 1.0146
2026-01-01 22:25:38,099 - INFO -   Batch 1000/2527: Loss = 0.8777
2026-01-01 22:26:06,744 - INFO -   Batch 1050/2527: Loss = 1.2814
2026-01-01 22:26:35,618 - INFO -   Batch 1100/2527: Loss = 1.9516
2026-01-01 22:27:04,330 - INFO -   Batch 1150/2527: Loss = 1.0343
2026-01-01 22:27:32,953 - INFO -   Batch 1200/2527: Loss = 1.2764
2026-01-01 22:28:01,991 - INFO -   Batch 1250/2527: Loss = 1.2773
2026-01-01 22:28:30,652 - INFO -   Batch 1300/2527: Loss = 1.2696
2026-01-01 22:28:59,416 - INFO -   Batch 1350/2527: Loss = 1.3003
2026-01-01 22:29:28,043 - INFO -   Batch 1400/2527: Loss = 1.3637
2026-01-01 22:29:56,909 - INFO -   Batch 1450/2527: Loss = 0.9606
2026-01-01 22:30:25,591 - INFO -   Batch 1500/2527: Loss = 1.1504
2026-01-01 22:30:54,230 - INFO -   Batch 1550/2527: Loss = 0.8794
2026-01-01 22:31:23,103 - INFO -   Batch 1600/2527: Loss = 0.9546
2026-01-01 22:31:52,117 - INFO -   Batch 1650/2527: Loss = 1.0982
2026-01-01 22:32:21,027 - INFO -   Batch 1700/2527: Loss = 1.2918
2026-01-01 22:32:49,595 - INFO -   Batch 1750/2527: Loss = 1.3022
2026-01-01 22:33:18,107 - INFO -   Batch 1800/2527: Loss = 1.5795
2026-01-01 22:33:46,991 - INFO -   Batch 1850/2527: Loss = 0.8400
2026-01-01 22:34:15,620 - INFO -   Batch 1900/2527: Loss = 1.1182
2026-01-01 22:34:44,606 - INFO -   Batch 1950/2527: Loss = 2.0781
2026-01-01 22:35:13,281 - INFO -   Batch 2000/2527: Loss = 1.6751
2026-01-01 22:35:42,347 - INFO -   Batch 2050/2527: Loss = 1.7003
2026-01-01 22:36:10,986 - INFO -   Batch 2100/2527: Loss = 1.6177
2026-01-01 22:36:39,472 - INFO -   Batch 2150/2527: Loss = 0.9983
2026-01-01 22:37:08,347 - INFO -   Batch 2200/2527: Loss = 0.8281
2026-01-01 22:37:37,070 - INFO -   Batch 2250/2527: Loss = 1.6949
2026-01-01 22:38:05,914 - INFO -   Batch 2300/2527: Loss = 0.9383
2026-01-01 22:38:34,754 - INFO -   Batch 2350/2527: Loss = 1.0501
2026-01-01 22:39:03,631 - INFO -   Batch 2400/2527: Loss = 1.6116
2026-01-01 22:39:32,492 - INFO -   Batch 2450/2527: Loss = 0.8455
2026-01-01 22:40:01,053 - INFO -   Batch 2500/2527: Loss = 1.2118
2026-01-01 22:44:39,999 - INFO - Epoch 2/160: Train Loss: 1.2172, Val Loss: 1.0052, Val mIoU: 23.48%, Val PixelAcc: 72.57%
2026-01-01 22:44:40,001 - INFO -   → New best mIoU: 23.48%
2026-01-01 22:44:43,697 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-01 22:44:43,700 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-01 22:45:14,310 - INFO -   Batch 50/2527: Loss = 1.5534
2026-01-01 22:45:43,044 - INFO -   Batch 100/2527: Loss = 1.0173
2026-01-01 22:46:11,586 - INFO -   Batch 150/2527: Loss = 0.9276
2026-01-01 22:46:40,423 - INFO -   Batch 200/2527: Loss = 1.0109
2026-01-01 22:47:09,143 - INFO -   Batch 250/2527: Loss = 1.2594
2026-01-01 22:47:38,035 - INFO -   Batch 300/2527: Loss = 1.2374
2026-01-01 22:48:06,840 - INFO -   Batch 350/2527: Loss = 1.7325
2026-01-01 22:48:35,747 - INFO -   Batch 400/2527: Loss = 1.2472
2026-01-01 22:49:04,202 - INFO -   Batch 450/2527: Loss = 1.8748
2026-01-01 22:49:32,859 - INFO -   Batch 500/2527: Loss = 0.8948
2026-01-01 22:50:01,755 - INFO -   Batch 550/2527: Loss = 0.9085
2026-01-01 22:50:30,536 - INFO -   Batch 600/2527: Loss = 1.0635
2026-01-01 22:50:59,505 - INFO -   Batch 650/2527: Loss = 1.4698
2026-01-01 22:51:28,189 - INFO -   Batch 700/2527: Loss = 1.4768
2026-01-01 22:51:56,913 - INFO -   Batch 750/2527: Loss = 0.9763
2026-01-01 22:52:25,851 - INFO -   Batch 800/2527: Loss = 1.6742
2026-01-01 22:52:54,411 - INFO -   Batch 850/2527: Loss = 1.1283
2026-01-01 22:53:23,271 - INFO -   Batch 900/2527: Loss = 0.9202
2026-01-01 22:53:51,913 - INFO -   Batch 950/2527: Loss = 0.8143
2026-01-01 22:54:20,759 - INFO -   Batch 1000/2527: Loss = 1.0109
2026-01-01 22:54:49,481 - INFO -   Batch 1050/2527: Loss = 0.9362
2026-01-01 22:55:18,052 - INFO -   Batch 1100/2527: Loss = 1.4022
2026-01-01 22:55:47,057 - INFO -   Batch 1150/2527: Loss = 0.9790
2026-01-01 22:56:15,763 - INFO -   Batch 1200/2527: Loss = 1.1057
2026-01-01 22:56:44,519 - INFO -   Batch 1250/2527: Loss = 1.2841
2026-01-01 22:57:13,200 - INFO -   Batch 1300/2527: Loss = 0.9411
2026-01-01 22:57:41,745 - INFO -   Batch 1350/2527: Loss = 0.9708
2026-01-01 22:58:10,588 - INFO -   Batch 1400/2527: Loss = 1.0500
2026-01-01 22:58:39,395 - INFO -   Batch 1450/2527: Loss = 1.1933
2026-01-01 22:59:08,320 - INFO -   Batch 1500/2527: Loss = 1.2340
2026-01-01 22:59:37,075 - INFO -   Batch 1550/2527: Loss = 0.9011
2026-01-01 23:00:05,983 - INFO -   Batch 1600/2527: Loss = 0.6206
2026-01-01 23:00:34,566 - INFO -   Batch 1650/2527: Loss = 0.9802
2026-01-01 23:01:03,095 - INFO -   Batch 1700/2527: Loss = 1.0096
2026-01-01 23:01:31,994 - INFO -   Batch 1750/2527: Loss = 1.0254
2026-01-01 23:02:00,588 - INFO -   Batch 1800/2527: Loss = 1.1911
2026-01-01 23:02:29,561 - INFO -   Batch 1850/2527: Loss = 0.9806
2026-01-01 23:02:58,273 - INFO -   Batch 1900/2527: Loss = 1.6678
2026-01-01 23:03:27,279 - INFO -   Batch 1950/2527: Loss = 1.7712
2026-01-01 23:03:55,835 - INFO -   Batch 2000/2527: Loss = 1.3127
2026-01-01 23:04:24,367 - INFO -   Batch 2050/2527: Loss = 0.9060
2026-01-01 23:04:53,183 - INFO -   Batch 2100/2527: Loss = 1.8951
2026-01-01 23:05:21,836 - INFO -   Batch 2150/2527: Loss = 0.9775
2026-01-01 23:05:50,662 - INFO -   Batch 2200/2527: Loss = 1.0867
2026-01-01 23:06:19,416 - INFO -   Batch 2250/2527: Loss = 1.1331
2026-01-01 23:06:47,984 - INFO -   Batch 2300/2527: Loss = 1.0787
2026-01-01 23:07:17,010 - INFO -   Batch 2350/2527: Loss = 0.8035
2026-01-01 23:07:45,761 - INFO -   Batch 2400/2527: Loss = 0.7858
2026-01-01 23:08:14,559 - INFO -   Batch 2450/2527: Loss = 0.9571
2026-01-01 23:08:43,365 - INFO -   Batch 2500/2527: Loss = 0.9583
2026-01-01 23:13:21,447 - INFO - Epoch 3/160: Train Loss: 1.0638, Val Loss: 1.0045, Val mIoU: 25.45%, Val PixelAcc: 71.80%
2026-01-01 23:13:21,449 - INFO -   → New best mIoU: 25.45%
2026-01-01 23:13:25,079 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-01 23:13:25,082 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-01 23:13:55,455 - INFO -   Batch 50/2527: Loss = 0.8451
2026-01-01 23:14:24,147 - INFO -   Batch 100/2527: Loss = 0.5791
2026-01-01 23:14:52,748 - INFO -   Batch 150/2527: Loss = 1.0963
2026-01-01 23:15:21,827 - INFO -   Batch 200/2527: Loss = 0.7645
2026-01-01 23:15:50,373 - INFO -   Batch 250/2527: Loss = 1.1350
2026-01-01 23:16:18,992 - INFO -   Batch 300/2527: Loss = 0.7912
2026-01-01 23:16:48,073 - INFO -   Batch 350/2527: Loss = 1.1142
2026-01-01 23:17:16,708 - INFO -   Batch 400/2527: Loss = 1.0029
2026-01-01 23:17:45,534 - INFO -   Batch 450/2527: Loss = 0.8204
2026-01-01 23:18:14,109 - INFO -   Batch 500/2527: Loss = 0.9033
2026-01-01 23:18:42,650 - INFO -   Batch 550/2527: Loss = 1.2182
2026-01-01 23:19:11,729 - INFO -   Batch 600/2527: Loss = 0.8875
2026-01-01 23:19:40,489 - INFO -   Batch 650/2527: Loss = 0.8183
2026-01-01 23:20:09,464 - INFO -   Batch 700/2527: Loss = 0.6420
2026-01-01 23:20:38,350 - INFO -   Batch 750/2527: Loss = 1.0684
2026-01-01 23:21:07,245 - INFO -   Batch 800/2527: Loss = 1.2039
2026-01-01 23:21:35,843 - INFO -   Batch 850/2527: Loss = 1.0668
2026-01-01 23:22:04,294 - INFO -   Batch 900/2527: Loss = 0.9045
2026-01-01 23:22:33,151 - INFO -   Batch 950/2527: Loss = 0.7462
2026-01-01 23:23:02,028 - INFO -   Batch 1000/2527: Loss = 0.5859
2026-01-01 23:23:30,819 - INFO -   Batch 1050/2527: Loss = 0.9333
2026-01-01 23:23:59,527 - INFO -   Batch 1100/2527: Loss = 0.7429
2026-01-01 23:24:28,592 - INFO -   Batch 1150/2527: Loss = 0.7804
2026-01-01 23:24:57,157 - INFO -   Batch 1200/2527: Loss = 0.7318
2026-01-01 23:25:25,779 - INFO -   Batch 1250/2527: Loss = 0.8292
2026-01-01 23:25:54,568 - INFO -   Batch 1300/2527: Loss = 0.7479
2026-01-01 23:26:23,228 - INFO -   Batch 1350/2527: Loss = 0.5959
2026-01-01 23:26:52,280 - INFO -   Batch 1400/2527: Loss = 0.8224
2026-01-01 23:27:20,890 - INFO -   Batch 1450/2527: Loss = 0.9296
2026-01-01 23:27:49,483 - INFO -   Batch 1500/2527: Loss = 0.8375
2026-01-01 23:28:18,592 - INFO -   Batch 1550/2527: Loss = 0.6985
2026-01-01 23:28:47,193 - INFO -   Batch 1600/2527: Loss = 0.9922
2026-01-01 23:29:15,991 - INFO -   Batch 1650/2527: Loss = 1.1510
2026-01-01 23:29:44,572 - INFO -   Batch 1700/2527: Loss = 1.0850
2026-01-01 23:30:13,420 - INFO -   Batch 1750/2527: Loss = 0.8527
2026-01-01 23:30:42,221 - INFO -   Batch 1800/2527: Loss = 0.6038
2026-01-01 23:31:10,704 - INFO -   Batch 1850/2527: Loss = 1.2586
2026-01-01 23:31:39,621 - INFO -   Batch 1900/2527: Loss = 1.0008
2026-01-01 23:32:08,527 - INFO -   Batch 1950/2527: Loss = 1.2897
2026-01-01 23:32:37,408 - INFO -   Batch 2000/2527: Loss = 1.1788
2026-01-01 23:33:06,004 - INFO -   Batch 2050/2527: Loss = 0.8355
2026-01-01 23:33:34,770 - INFO -   Batch 2100/2527: Loss = 0.8645
2026-01-01 23:34:03,289 - INFO -   Batch 2150/2527: Loss = 1.0583
2026-01-01 23:34:32,123 - INFO -   Batch 2200/2527: Loss = 0.5842
2026-01-01 23:35:00,932 - INFO -   Batch 2250/2527: Loss = 1.0218
2026-01-01 23:35:29,640 - INFO -   Batch 2300/2527: Loss = 0.9491
2026-01-01 23:35:58,731 - INFO -   Batch 2350/2527: Loss = 0.6352
2026-01-01 23:36:27,453 - INFO -   Batch 2400/2527: Loss = 0.7100
2026-01-01 23:36:55,897 - INFO -   Batch 2450/2527: Loss = 0.6543
2026-01-01 23:37:24,693 - INFO -   Batch 2500/2527: Loss = 0.9299
2026-01-01 23:42:04,620 - INFO - Epoch 4/160: Train Loss: 0.9206, Val Loss: 0.9251, Val mIoU: 29.04%, Val PixelAcc: 73.60%
2026-01-01 23:42:04,622 - INFO -   → New best mIoU: 29.04%
2026-01-01 23:42:08,378 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-01 23:42:08,380 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-01 23:42:38,664 - INFO -   Batch 50/2527: Loss = 0.6660
2026-01-01 23:43:07,400 - INFO -   Batch 100/2527: Loss = 0.7061
2026-01-01 23:43:36,252 - INFO -   Batch 150/2527: Loss = 0.9962
2026-01-01 23:44:04,806 - INFO -   Batch 200/2527: Loss = 0.6507
2026-01-01 23:44:33,580 - INFO -   Batch 250/2527: Loss = 0.9548
2026-01-01 23:45:02,346 - INFO -   Batch 300/2527: Loss = 0.7044
2026-01-01 23:45:31,418 - INFO -   Batch 350/2527: Loss = 0.4723
2026-01-01 23:46:00,004 - INFO -   Batch 400/2527: Loss = 1.0573
2026-01-01 23:46:28,641 - INFO -   Batch 450/2527: Loss = 0.9090
2026-01-01 23:46:57,740 - INFO -   Batch 500/2527: Loss = 0.6740
2026-01-01 23:47:26,439 - INFO -   Batch 550/2527: Loss = 0.5456
2026-01-01 23:47:55,195 - INFO -   Batch 600/2527: Loss = 0.5856
2026-01-01 23:48:23,776 - INFO -   Batch 650/2527: Loss = 0.9004
2026-01-01 23:48:52,727 - INFO -   Batch 700/2527: Loss = 1.0226
2026-01-01 23:49:21,467 - INFO -   Batch 750/2527: Loss = 0.6775
2026-01-01 23:49:50,049 - INFO -   Batch 800/2527: Loss = 0.7607
2026-01-01 23:50:18,960 - INFO -   Batch 850/2527: Loss = 0.6995
2026-01-01 23:50:47,870 - INFO -   Batch 900/2527: Loss = 1.2261
2026-01-01 23:51:16,768 - INFO -   Batch 950/2527: Loss = 0.8133
2026-01-01 23:51:45,359 - INFO -   Batch 1000/2527: Loss = 0.8387
2026-01-01 23:52:13,831 - INFO -   Batch 1050/2527: Loss = 1.1862
2026-01-01 23:52:42,730 - INFO -   Batch 1100/2527: Loss = 0.9133
2026-01-01 23:53:11,557 - INFO -   Batch 1150/2527: Loss = 1.1408
2026-01-01 23:53:40,393 - INFO -   Batch 1200/2527: Loss = 0.6406
2026-01-01 23:54:09,062 - INFO -   Batch 1250/2527: Loss = 0.5960
2026-01-01 23:54:38,154 - INFO -   Batch 1300/2527: Loss = 0.7673
2026-01-01 23:55:06,849 - INFO -   Batch 1350/2527: Loss = 0.7557
2026-01-01 23:55:35,317 - INFO -   Batch 1400/2527: Loss = 0.7592
2026-01-01 23:56:04,145 - INFO -   Batch 1450/2527: Loss = 0.7376
2026-01-01 23:56:32,721 - INFO -   Batch 1500/2527: Loss = 0.8127
2026-01-01 23:57:01,737 - INFO -   Batch 1550/2527: Loss = 0.9279
2026-01-01 23:57:30,349 - INFO -   Batch 1600/2527: Loss = 1.3387
2026-01-01 23:57:59,257 - INFO -   Batch 1650/2527: Loss = 0.9225
2026-01-01 23:58:27,994 - INFO -   Batch 1700/2527: Loss = 1.1996
2026-01-01 23:58:56,668 - INFO -   Batch 1750/2527: Loss = 0.5363
2026-01-01 23:59:25,427 - INFO -   Batch 1800/2527: Loss = 1.2032
2026-01-01 23:59:53,996 - INFO -   Batch 1850/2527: Loss = 1.0937
2026-01-02 00:00:22,868 - INFO -   Batch 1900/2527: Loss = 1.3518
2026-01-02 00:00:51,672 - INFO -   Batch 1950/2527: Loss = 0.4807
2026-01-02 00:01:20,180 - INFO -   Batch 2000/2527: Loss = 0.9722
2026-01-02 00:01:49,128 - INFO -   Batch 2050/2527: Loss = 1.0475
2026-01-02 00:02:18,014 - INFO -   Batch 2100/2527: Loss = 0.6369
2026-01-02 00:02:46,903 - INFO -   Batch 2150/2527: Loss = 1.0269
2026-01-02 00:03:15,631 - INFO -   Batch 2200/2527: Loss = 1.3713
2026-01-02 00:03:44,428 - INFO -   Batch 2250/2527: Loss = 0.7771
2026-01-02 00:04:13,046 - INFO -   Batch 2300/2527: Loss = 0.8167
2026-01-02 00:04:41,771 - INFO -   Batch 2350/2527: Loss = 0.6140
2026-01-02 00:05:10,593 - INFO -   Batch 2400/2527: Loss = 0.8596
2026-01-02 00:05:39,353 - INFO -   Batch 2450/2527: Loss = 0.8801
2026-01-02 00:06:08,398 - INFO -   Batch 2500/2527: Loss = 0.8139
2026-01-02 00:10:47,821 - INFO - Epoch 5/160: Train Loss: 0.8259, Val Loss: 0.8995, Val mIoU: 31.24%, Val PixelAcc: 74.38%
2026-01-02 00:10:47,822 - INFO -   → New best mIoU: 31.24%
2026-01-02 00:10:51,459 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 00:10:51,462 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 00:11:22,407 - INFO -   Batch 50/2527: Loss = 0.4992
2026-01-02 00:11:51,316 - INFO -   Batch 100/2527: Loss = 0.6794
2026-01-02 00:12:20,218 - INFO -   Batch 150/2527: Loss = 0.8092
2026-01-02 00:12:48,784 - INFO -   Batch 200/2527: Loss = 0.6467
2026-01-02 00:13:17,646 - INFO -   Batch 250/2527: Loss = 0.6093
2026-01-02 00:13:46,263 - INFO -   Batch 300/2527: Loss = 0.9767
2026-01-02 00:14:14,766 - INFO -   Batch 350/2527: Loss = 1.0347
2026-01-02 00:14:43,749 - INFO -   Batch 400/2527: Loss = 1.0986
2026-01-02 00:15:12,438 - INFO -   Batch 450/2527: Loss = 0.5174
2026-01-02 00:15:41,488 - INFO -   Batch 500/2527: Loss = 0.8484
2026-01-02 00:16:10,162 - INFO -   Batch 550/2527: Loss = 0.5853
2026-01-02 00:16:38,928 - INFO -   Batch 600/2527: Loss = 0.5024
2026-01-02 00:17:07,439 - INFO -   Batch 650/2527: Loss = 1.0477
2026-01-02 00:17:36,133 - INFO -   Batch 700/2527: Loss = 0.9802
2026-01-02 00:18:04,987 - INFO -   Batch 750/2527: Loss = 0.5513
2026-01-02 00:18:33,727 - INFO -   Batch 800/2527: Loss = 0.8214
2026-01-02 00:19:02,651 - INFO -   Batch 850/2527: Loss = 0.8095
2026-01-02 00:19:31,468 - INFO -   Batch 900/2527: Loss = 0.6062
2026-01-02 00:20:00,067 - INFO -   Batch 950/2527: Loss = 0.8962
2026-01-02 00:20:28,865 - INFO -   Batch 1000/2527: Loss = 0.6507
2026-01-02 00:20:57,637 - INFO -   Batch 1050/2527: Loss = 0.5111
2026-01-02 00:21:26,541 - INFO -   Batch 1100/2527: Loss = 0.6987
2026-01-02 00:21:55,158 - INFO -   Batch 1150/2527: Loss = 0.6825
2026-01-02 00:22:24,171 - INFO -   Batch 1200/2527: Loss = 0.6921
2026-01-02 00:22:52,892 - INFO -   Batch 1250/2527: Loss = 0.7949
2026-01-02 00:23:21,595 - INFO -   Batch 1300/2527: Loss = 0.8849
2026-01-02 00:23:50,464 - INFO -   Batch 1350/2527: Loss = 0.6955
2026-01-02 00:24:19,049 - INFO -   Batch 1400/2527: Loss = 0.4991
2026-01-02 00:24:47,898 - INFO -   Batch 1450/2527: Loss = 0.7561
2026-01-02 00:25:16,564 - INFO -   Batch 1500/2527: Loss = 0.6673
2026-01-02 00:25:45,082 - INFO -   Batch 1550/2527: Loss = 0.6481
2026-01-02 00:26:14,049 - INFO -   Batch 1600/2527: Loss = 1.2189
2026-01-02 00:26:42,776 - INFO -   Batch 1650/2527: Loss = 0.4556
2026-01-02 00:27:11,842 - INFO -   Batch 1700/2527: Loss = 0.8031
2026-01-02 00:27:40,546 - INFO -   Batch 1750/2527: Loss = 0.7618
2026-01-02 00:28:09,345 - INFO -   Batch 1800/2527: Loss = 0.8449
2026-01-02 00:28:37,963 - INFO -   Batch 1850/2527: Loss = 1.0142
2026-01-02 00:29:06,508 - INFO -   Batch 1900/2527: Loss = 0.7551
2026-01-02 00:29:35,359 - INFO -   Batch 1950/2527: Loss = 0.9060
2026-01-02 00:30:04,055 - INFO -   Batch 2000/2527: Loss = 0.5478
2026-01-02 00:30:33,006 - INFO -   Batch 2050/2527: Loss = 0.6148
2026-01-02 00:31:01,810 - INFO -   Batch 2100/2527: Loss = 0.7352
2026-01-02 00:31:30,723 - INFO -   Batch 2150/2527: Loss = 0.5215
2026-01-02 00:31:59,213 - INFO -   Batch 2200/2527: Loss = 0.6965
2026-01-02 00:32:27,828 - INFO -   Batch 2250/2527: Loss = 0.7598
2026-01-02 00:32:56,685 - INFO -   Batch 2300/2527: Loss = 0.6481
2026-01-02 00:33:25,338 - INFO -   Batch 2350/2527: Loss = 0.7322
2026-01-02 00:33:54,279 - INFO -   Batch 2400/2527: Loss = 0.9791
2026-01-02 00:34:22,943 - INFO -   Batch 2450/2527: Loss = 1.1526
2026-01-02 00:34:51,642 - INFO -   Batch 2500/2527: Loss = 0.8233
2026-01-02 00:39:37,655 - INFO - Epoch 6/160: Train Loss: 0.7587, Val Loss: 0.9335, Val mIoU: 31.56%, Val PixelAcc: 73.64%
2026-01-02 00:39:37,657 - INFO -   → New best mIoU: 31.56%
2026-01-02 00:39:41,327 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 00:39:41,330 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 00:40:11,626 - INFO -   Batch 50/2527: Loss = 0.5266
2026-01-02 00:40:40,316 - INFO -   Batch 100/2527: Loss = 0.5631
2026-01-02 00:41:08,808 - INFO -   Batch 150/2527: Loss = 0.6443
2026-01-02 00:41:37,808 - INFO -   Batch 200/2527: Loss = 0.9033
2026-01-02 00:42:06,456 - INFO -   Batch 250/2527: Loss = 0.5891
2026-01-02 00:42:35,427 - INFO -   Batch 300/2527: Loss = 0.9172
2026-01-02 00:43:04,266 - INFO -   Batch 350/2527: Loss = 0.8335
2026-01-02 00:43:33,168 - INFO -   Batch 400/2527: Loss = 0.5332
2026-01-02 00:44:01,755 - INFO -   Batch 450/2527: Loss = 0.4740
2026-01-02 00:44:30,227 - INFO -   Batch 500/2527: Loss = 0.8781
2026-01-02 00:44:59,084 - INFO -   Batch 550/2527: Loss = 0.5523
2026-01-02 00:45:27,831 - INFO -   Batch 600/2527: Loss = 0.6978
2026-01-02 00:45:56,675 - INFO -   Batch 650/2527: Loss = 1.0582
2026-01-02 00:46:25,342 - INFO -   Batch 700/2527: Loss = 0.6880
2026-01-02 00:46:54,435 - INFO -   Batch 750/2527: Loss = 1.0537
2026-01-02 00:47:23,099 - INFO -   Batch 800/2527: Loss = 0.6918
2026-01-02 00:47:51,563 - INFO -   Batch 850/2527: Loss = 0.5215
2026-01-02 00:48:20,326 - INFO -   Batch 900/2527: Loss = 0.7770
2026-01-02 00:48:48,988 - INFO -   Batch 950/2527: Loss = 0.5684
2026-01-02 00:49:18,012 - INFO -   Batch 1000/2527: Loss = 0.4197
2026-01-02 00:49:46,796 - INFO -   Batch 1050/2527: Loss = 0.5311
2026-01-02 00:50:15,401 - INFO -   Batch 1100/2527: Loss = 0.9352
2026-01-02 00:50:44,440 - INFO -   Batch 1150/2527: Loss = 0.6113
2026-01-02 00:51:13,149 - INFO -   Batch 1200/2527: Loss = 0.6659
2026-01-02 00:51:41,970 - INFO -   Batch 1250/2527: Loss = 0.6554
2026-01-02 00:52:10,553 - INFO -   Batch 1300/2527: Loss = 0.4327
2026-01-02 00:52:39,410 - INFO -   Batch 1350/2527: Loss = 0.4437
2026-01-02 00:53:08,251 - INFO -   Batch 1400/2527: Loss = 0.6457
2026-01-02 00:53:36,725 - INFO -   Batch 1450/2527: Loss = 0.7501
2026-01-02 00:54:05,682 - INFO -   Batch 1500/2527: Loss = 0.8240
2026-01-02 00:54:34,511 - INFO -   Batch 1550/2527: Loss = 0.9178
2026-01-02 00:55:03,425 - INFO -   Batch 1600/2527: Loss = 0.6630
2026-01-02 00:55:32,028 - INFO -   Batch 1650/2527: Loss = 0.4673
2026-01-02 00:56:00,839 - INFO -   Batch 1700/2527: Loss = 0.7686
2026-01-02 00:56:29,382 - INFO -   Batch 1750/2527: Loss = 0.5937
2026-01-02 00:56:58,274 - INFO -   Batch 1800/2527: Loss = 0.5219
2026-01-02 00:57:27,070 - INFO -   Batch 1850/2527: Loss = 0.4504
2026-01-02 00:57:55,827 - INFO -   Batch 1900/2527: Loss = 0.5194
2026-01-02 00:58:24,880 - INFO -   Batch 1950/2527: Loss = 0.8215
2026-01-02 00:58:53,547 - INFO -   Batch 2000/2527: Loss = 0.7491
2026-01-02 00:59:22,006 - INFO -   Batch 2050/2527: Loss = 0.5996
2026-01-02 00:59:50,777 - INFO -   Batch 2100/2527: Loss = 0.6400
2026-01-02 01:00:19,454 - INFO -   Batch 2150/2527: Loss = 0.8899
2026-01-02 01:00:48,513 - INFO -   Batch 2200/2527: Loss = 0.6806
2026-01-02 01:01:17,069 - INFO -   Batch 2250/2527: Loss = 0.6720
2026-01-02 01:01:46,021 - INFO -   Batch 2300/2527: Loss = 1.2018
2026-01-02 01:02:14,877 - INFO -   Batch 2350/2527: Loss = 0.8304
2026-01-02 01:02:43,440 - INFO -   Batch 2400/2527: Loss = 0.5851
2026-01-02 01:03:12,263 - INFO -   Batch 2450/2527: Loss = 0.5604
2026-01-02 01:03:40,766 - INFO -   Batch 2500/2527: Loss = 0.8040
2026-01-02 01:08:19,802 - INFO - Epoch 7/160: Train Loss: 0.6851, Val Loss: 0.8836, Val mIoU: 32.93%, Val PixelAcc: 74.87%
2026-01-02 01:08:19,804 - INFO -   → New best mIoU: 32.93%
2026-01-02 01:08:23,494 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 01:08:23,497 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 01:08:53,809 - INFO -   Batch 50/2527: Loss = 0.7150
2026-01-02 01:09:22,858 - INFO -   Batch 100/2527: Loss = 0.5715
2026-01-02 01:09:51,539 - INFO -   Batch 150/2527: Loss = 0.5176
2026-01-02 01:10:20,345 - INFO -   Batch 200/2527: Loss = 0.4894
2026-01-02 01:10:48,930 - INFO -   Batch 250/2527: Loss = 0.5759
2026-01-02 01:11:17,441 - INFO -   Batch 300/2527: Loss = 0.5621
2026-01-02 01:11:46,519 - INFO -   Batch 350/2527: Loss = 0.6860
2026-01-02 01:12:15,105 - INFO -   Batch 400/2527: Loss = 0.4488
2026-01-02 01:12:44,066 - INFO -   Batch 450/2527: Loss = 0.5622
2026-01-02 01:13:12,934 - INFO -   Batch 500/2527: Loss = 0.5823
2026-01-02 01:13:41,798 - INFO -   Batch 550/2527: Loss = 0.6624
2026-01-02 01:14:10,410 - INFO -   Batch 600/2527: Loss = 0.5008
2026-01-02 01:14:38,876 - INFO -   Batch 650/2527: Loss = 0.7404
2026-01-02 01:15:07,685 - INFO -   Batch 700/2527: Loss = 0.3851
2026-01-02 01:15:36,528 - INFO -   Batch 750/2527: Loss = 0.6378
2026-01-02 01:16:05,289 - INFO -   Batch 800/2527: Loss = 0.8010
2026-01-02 01:16:34,037 - INFO -   Batch 850/2527: Loss = 0.5447
2026-01-02 01:17:03,120 - INFO -   Batch 900/2527: Loss = 0.4530
2026-01-02 01:17:31,688 - INFO -   Batch 950/2527: Loss = 0.8016
2026-01-02 01:18:00,213 - INFO -   Batch 1000/2527: Loss = 0.5696
2026-01-02 01:18:29,035 - INFO -   Batch 1050/2527: Loss = 0.3972
2026-01-02 01:18:57,610 - INFO -   Batch 1100/2527: Loss = 0.4454
2026-01-02 01:19:26,687 - INFO -   Batch 1150/2527: Loss = 0.7344
2026-01-02 01:19:55,232 - INFO -   Batch 1200/2527: Loss = 0.7089
2026-01-02 01:20:23,846 - INFO -   Batch 1250/2527: Loss = 0.6562
2026-01-02 01:20:52,934 - INFO -   Batch 1300/2527: Loss = 0.5452
2026-01-02 01:21:21,656 - INFO -   Batch 1350/2527: Loss = 0.4537
2026-01-02 01:21:50,454 - INFO -   Batch 1400/2527: Loss = 0.7672
2026-01-02 01:22:19,214 - INFO -   Batch 1450/2527: Loss = 0.6588
2026-01-02 01:22:48,076 - INFO -   Batch 1500/2527: Loss = 0.7244
2026-01-02 01:23:16,965 - INFO -   Batch 1550/2527: Loss = 0.7715
2026-01-02 01:23:45,450 - INFO -   Batch 1600/2527: Loss = 0.5444
2026-01-02 01:24:14,365 - INFO -   Batch 1650/2527: Loss = 0.6986
2026-01-02 01:24:43,262 - INFO -   Batch 1700/2527: Loss = 0.4005
2026-01-02 01:25:12,125 - INFO -   Batch 1750/2527: Loss = 0.6927
2026-01-02 01:25:40,734 - INFO -   Batch 1800/2527: Loss = 0.2932
2026-01-02 01:26:09,540 - INFO -   Batch 1850/2527: Loss = 0.5002
2026-01-02 01:26:38,074 - INFO -   Batch 1900/2527: Loss = 0.6861
2026-01-02 01:27:06,873 - INFO -   Batch 1950/2527: Loss = 0.9886
2026-01-02 01:27:35,687 - INFO -   Batch 2000/2527: Loss = 0.5011
2026-01-02 01:28:04,435 - INFO -   Batch 2050/2527: Loss = 0.6353
2026-01-02 01:28:33,544 - INFO -   Batch 2100/2527: Loss = 0.4968
2026-01-02 01:29:02,263 - INFO -   Batch 2150/2527: Loss = 0.5160
2026-01-02 01:29:30,740 - INFO -   Batch 2200/2527: Loss = 0.6685
2026-01-02 01:29:59,608 - INFO -   Batch 2250/2527: Loss = 0.6596
2026-01-02 01:30:28,262 - INFO -   Batch 2300/2527: Loss = 0.7723
2026-01-02 01:30:57,315 - INFO -   Batch 2350/2527: Loss = 0.6572
2026-01-02 01:31:25,884 - INFO -   Batch 2400/2527: Loss = 0.5414
2026-01-02 01:31:54,794 - INFO -   Batch 2450/2527: Loss = 0.3902
2026-01-02 01:32:23,718 - INFO -   Batch 2500/2527: Loss = 0.2932
2026-01-02 01:37:02,406 - INFO - Epoch 8/160: Train Loss: 0.6356, Val Loss: 0.8991, Val mIoU: 33.81%, Val PixelAcc: 74.88%
2026-01-02 01:37:02,421 - INFO -   → New best mIoU: 33.81%
2026-01-02 01:37:06,029 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 01:37:06,032 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 01:37:36,972 - INFO -   Batch 50/2527: Loss = 0.6703
2026-01-02 01:38:05,851 - INFO -   Batch 100/2527: Loss = 0.3871
2026-01-02 01:38:34,637 - INFO -   Batch 150/2527: Loss = 0.4690
2026-01-02 01:39:03,085 - INFO -   Batch 200/2527: Loss = 0.6722
2026-01-02 01:39:31,924 - INFO -   Batch 250/2527: Loss = 0.4190
2026-01-02 01:40:00,542 - INFO -   Batch 300/2527: Loss = 0.5446
2026-01-02 01:40:29,399 - INFO -   Batch 350/2527: Loss = 0.7475
2026-01-02 01:40:58,187 - INFO -   Batch 400/2527: Loss = 0.6182
2026-01-02 01:41:27,085 - INFO -   Batch 450/2527: Loss = 0.5503
2026-01-02 01:41:55,827 - INFO -   Batch 500/2527: Loss = 0.6816
2026-01-02 01:42:24,477 - INFO -   Batch 550/2527: Loss = 0.3802
2026-01-02 01:42:53,254 - INFO -   Batch 600/2527: Loss = 0.5684
2026-01-02 01:43:21,883 - INFO -   Batch 650/2527: Loss = 0.4940
2026-01-02 01:43:50,732 - INFO -   Batch 700/2527: Loss = 0.6836
2026-01-02 01:44:19,363 - INFO -   Batch 750/2527: Loss = 0.3961
2026-01-02 01:44:48,017 - INFO -   Batch 800/2527: Loss = 0.5146
2026-01-02 01:45:16,931 - INFO -   Batch 850/2527: Loss = 0.4843
2026-01-02 01:45:45,802 - INFO -   Batch 900/2527: Loss = 0.6920
2026-01-02 01:46:14,711 - INFO -   Batch 950/2527: Loss = 0.4818
2026-01-02 01:46:43,428 - INFO -   Batch 1000/2527: Loss = 0.4950
2026-01-02 01:47:12,292 - INFO -   Batch 1050/2527: Loss = 0.5264
2026-01-02 01:47:40,955 - INFO -   Batch 1100/2527: Loss = 0.4625
2026-01-02 01:48:09,466 - INFO -   Batch 1150/2527: Loss = 0.5283
2026-01-02 01:48:38,460 - INFO -   Batch 1200/2527: Loss = 0.7960
2026-01-02 01:49:07,141 - INFO -   Batch 1250/2527: Loss = 0.5563
2026-01-02 01:49:36,196 - INFO -   Batch 1300/2527: Loss = 0.5398
2026-01-02 01:50:04,924 - INFO -   Batch 1350/2527: Loss = 0.4734
2026-01-02 01:50:33,373 - INFO -   Batch 1400/2527: Loss = 0.4906
2026-01-02 01:51:02,212 - INFO -   Batch 1450/2527: Loss = 0.6057
2026-01-02 01:51:30,859 - INFO -   Batch 1500/2527: Loss = 0.5924
2026-01-02 01:51:59,684 - INFO -   Batch 1550/2527: Loss = 0.5936
2026-01-02 01:52:28,467 - INFO -   Batch 1600/2527: Loss = 0.3412
2026-01-02 01:52:57,352 - INFO -   Batch 1650/2527: Loss = 0.4598
2026-01-02 01:53:26,255 - INFO -   Batch 1700/2527: Loss = 0.7251
2026-01-02 01:53:54,822 - INFO -   Batch 1750/2527: Loss = 0.6561
2026-01-02 01:54:23,649 - INFO -   Batch 1800/2527: Loss = 0.4752
2026-01-02 01:54:52,273 - INFO -   Batch 1850/2527: Loss = 0.4431
2026-01-02 01:55:21,197 - INFO -   Batch 1900/2527: Loss = 0.7257
2026-01-02 01:55:49,788 - INFO -   Batch 1950/2527: Loss = 0.7272
2026-01-02 01:56:18,817 - INFO -   Batch 2000/2527: Loss = 0.5987
2026-01-02 01:56:47,543 - INFO -   Batch 2050/2527: Loss = 0.7583
2026-01-02 01:57:16,273 - INFO -   Batch 2100/2527: Loss = 0.5169
2026-01-02 01:57:45,205 - INFO -   Batch 2150/2527: Loss = 0.5615
2026-01-02 01:58:13,741 - INFO -   Batch 2200/2527: Loss = 0.8795
2026-01-02 01:58:42,586 - INFO -   Batch 2250/2527: Loss = 0.6266
2026-01-02 01:59:11,281 - INFO -   Batch 2300/2527: Loss = 0.3960
2026-01-02 01:59:39,826 - INFO -   Batch 2350/2527: Loss = 0.7860
2026-01-02 02:00:08,768 - INFO -   Batch 2400/2527: Loss = 0.6221
2026-01-02 02:00:37,437 - INFO -   Batch 2450/2527: Loss = 0.5960
2026-01-02 02:01:06,477 - INFO -   Batch 2500/2527: Loss = 0.4756
2026-01-02 02:05:52,014 - INFO - Epoch 9/160: Train Loss: 0.5917, Val Loss: 0.8865, Val mIoU: 34.22%, Val PixelAcc: 74.96%
2026-01-02 02:05:52,016 - INFO -   → New best mIoU: 34.22%
2026-01-02 02:05:55,624 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 02:05:55,627 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 02:06:26,091 - INFO -   Batch 50/2527: Loss = 0.7087
2026-01-02 02:06:54,499 - INFO -   Batch 100/2527: Loss = 0.4615
2026-01-02 02:07:23,286 - INFO -   Batch 150/2527: Loss = 0.5133
2026-01-02 02:07:52,176 - INFO -   Batch 200/2527: Loss = 0.4146
2026-01-02 02:08:20,975 - INFO -   Batch 250/2527: Loss = 0.3825
2026-01-02 02:08:49,884 - INFO -   Batch 300/2527: Loss = 0.5920
2026-01-02 02:09:18,640 - INFO -   Batch 350/2527: Loss = 0.5093
2026-01-02 02:09:47,510 - INFO -   Batch 400/2527: Loss = 0.8331
2026-01-02 02:10:16,111 - INFO -   Batch 450/2527: Loss = 0.4428
2026-01-02 02:10:44,967 - INFO -   Batch 500/2527: Loss = 0.4744
2026-01-02 02:11:13,612 - INFO -   Batch 550/2527: Loss = 0.4498
2026-01-02 02:11:42,710 - INFO -   Batch 600/2527: Loss = 0.5821
2026-01-02 02:12:11,314 - INFO -   Batch 650/2527: Loss = 0.6445
2026-01-02 02:12:39,950 - INFO -   Batch 700/2527: Loss = 0.4049
2026-01-02 02:13:09,085 - INFO -   Batch 750/2527: Loss = 0.4885
2026-01-02 02:13:37,705 - INFO -   Batch 800/2527: Loss = 0.7025
2026-01-02 02:14:06,534 - INFO -   Batch 850/2527: Loss = 0.5851
2026-01-02 02:14:35,100 - INFO -   Batch 900/2527: Loss = 0.5013
2026-01-02 02:15:03,960 - INFO -   Batch 950/2527: Loss = 0.4323
2026-01-02 02:15:32,863 - INFO -   Batch 1000/2527: Loss = 0.6399
2026-01-02 02:16:01,357 - INFO -   Batch 1050/2527: Loss = 0.6603
2026-01-02 02:16:30,307 - INFO -   Batch 1100/2527: Loss = 0.6738
2026-01-02 02:16:59,227 - INFO -   Batch 1150/2527: Loss = 0.5191
2026-01-02 02:17:28,123 - INFO -   Batch 1200/2527: Loss = 0.6885
2026-01-02 02:17:56,716 - INFO -   Batch 1250/2527: Loss = 0.4113
2026-01-02 02:18:25,172 - INFO -   Batch 1300/2527: Loss = 0.8076
2026-01-02 02:18:53,979 - INFO -   Batch 1350/2527: Loss = 0.4233
2026-01-02 02:19:22,879 - INFO -   Batch 1400/2527: Loss = 0.3483
2026-01-02 02:19:51,720 - INFO -   Batch 1450/2527: Loss = 0.4505
2026-01-02 02:20:20,391 - INFO -   Batch 1500/2527: Loss = 0.3970
2026-01-02 02:20:49,503 - INFO -   Batch 1550/2527: Loss = 0.8684
2026-01-02 02:21:18,181 - INFO -   Batch 1600/2527: Loss = 0.6725
2026-01-02 02:21:46,663 - INFO -   Batch 1650/2527: Loss = 0.6853
2026-01-02 02:22:15,494 - INFO -   Batch 1700/2527: Loss = 0.5020
2026-01-02 02:22:44,078 - INFO -   Batch 1750/2527: Loss = 0.6010
2026-01-02 02:23:13,156 - INFO -   Batch 1800/2527: Loss = 0.2966
2026-01-02 02:23:41,732 - INFO -   Batch 1850/2527: Loss = 0.6308
2026-01-02 02:24:10,682 - INFO -   Batch 1900/2527: Loss = 0.6026
2026-01-02 02:24:39,442 - INFO -   Batch 1950/2527: Loss = 0.4829
2026-01-02 02:25:08,121 - INFO -   Batch 2000/2527: Loss = 0.6110
2026-01-02 02:25:36,918 - INFO -   Batch 2050/2527: Loss = 0.3624
2026-01-02 02:26:05,520 - INFO -   Batch 2100/2527: Loss = 0.4215
2026-01-02 02:26:34,338 - INFO -   Batch 2150/2527: Loss = 0.4891
2026-01-02 02:27:03,418 - INFO -   Batch 2200/2527: Loss = 0.4548
2026-01-02 02:27:31,918 - INFO -   Batch 2250/2527: Loss = 0.5689
2026-01-02 02:28:00,837 - INFO -   Batch 2300/2527: Loss = 0.5749
2026-01-02 02:28:29,787 - INFO -   Batch 2350/2527: Loss = 0.5768
2026-01-02 02:28:58,721 - INFO -   Batch 2400/2527: Loss = 0.5050
2026-01-02 02:29:27,315 - INFO -   Batch 2450/2527: Loss = 0.4404
2026-01-02 02:29:56,151 - INFO -   Batch 2500/2527: Loss = 0.4425
2026-01-02 02:34:35,501 - INFO - Epoch 10/160: Train Loss: 0.5459, Val Loss: 0.8701, Val mIoU: 35.05%, Val PixelAcc: 75.95%
2026-01-02 02:34:35,503 - INFO -   → New best mIoU: 35.05%
2026-01-02 02:34:39,080 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 02:34:39,083 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 02:34:42,553 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_10.pth
2026-01-02 02:34:42,555 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_10.pth
2026-01-02 02:35:12,966 - INFO -   Batch 50/2527: Loss = 0.3666
2026-01-02 02:35:42,018 - INFO -   Batch 100/2527: Loss = 0.3815
2026-01-02 02:36:10,590 - INFO -   Batch 150/2527: Loss = 0.4724
2026-01-02 02:36:39,296 - INFO -   Batch 200/2527: Loss = 0.5861
2026-01-02 02:37:07,748 - INFO -   Batch 250/2527: Loss = 0.6342
2026-01-02 02:37:36,572 - INFO -   Batch 300/2527: Loss = 0.5540
2026-01-02 02:38:05,442 - INFO -   Batch 350/2527: Loss = 0.5324
2026-01-02 02:38:34,237 - INFO -   Batch 400/2527: Loss = 0.4669
2026-01-02 02:39:03,000 - INFO -   Batch 450/2527: Loss = 0.4534
2026-01-02 02:39:32,098 - INFO -   Batch 500/2527: Loss = 0.3789
2026-01-02 02:40:00,919 - INFO -   Batch 550/2527: Loss = 0.4322
2026-01-02 02:40:29,397 - INFO -   Batch 600/2527: Loss = 0.3819
2026-01-02 02:40:58,184 - INFO -   Batch 650/2527: Loss = 0.7242
2026-01-02 02:41:26,815 - INFO -   Batch 700/2527: Loss = 0.6070
2026-01-02 02:41:55,901 - INFO -   Batch 750/2527: Loss = 0.5481
2026-01-02 02:42:24,484 - INFO -   Batch 800/2527: Loss = 0.4219
2026-01-02 02:42:53,125 - INFO -   Batch 850/2527: Loss = 0.3879
2026-01-02 02:43:22,231 - INFO -   Batch 900/2527: Loss = 0.5297
2026-01-02 02:43:50,889 - INFO -   Batch 950/2527: Loss = 0.4709
2026-01-02 02:44:19,695 - INFO -   Batch 1000/2527: Loss = 0.4837
2026-01-02 02:44:48,258 - INFO -   Batch 1050/2527: Loss = 0.5218
2026-01-02 02:45:17,083 - INFO -   Batch 1100/2527: Loss = 0.4278
2026-01-02 02:45:45,967 - INFO -   Batch 1150/2527: Loss = 0.5537
2026-01-02 02:46:14,445 - INFO -   Batch 1200/2527: Loss = 0.4293
2026-01-02 02:46:43,407 - INFO -   Batch 1250/2527: Loss = 0.4516
2026-01-02 02:47:12,358 - INFO -   Batch 1300/2527: Loss = 0.5066
2026-01-02 02:47:41,223 - INFO -   Batch 1350/2527: Loss = 0.6229
2026-01-02 02:48:09,864 - INFO -   Batch 1400/2527: Loss = 0.6304
2026-01-02 02:48:38,674 - INFO -   Batch 1450/2527: Loss = 0.4893
2026-01-02 02:49:07,255 - INFO -   Batch 1500/2527: Loss = 0.4218
2026-01-02 02:49:35,981 - INFO -   Batch 1550/2527: Loss = 0.4397
2026-01-02 02:50:04,824 - INFO -   Batch 1600/2527: Loss = 0.4843
2026-01-02 02:50:33,583 - INFO -   Batch 1650/2527: Loss = 0.4916
2026-01-02 02:51:02,678 - INFO -   Batch 1700/2527: Loss = 0.6128
2026-01-02 02:51:31,292 - INFO -   Batch 1750/2527: Loss = 0.4801
2026-01-02 02:51:59,786 - INFO -   Batch 1800/2527: Loss = 0.3647
2026-01-02 02:52:28,614 - INFO -   Batch 1850/2527: Loss = 0.5893
2026-01-02 02:52:57,274 - INFO -   Batch 1900/2527: Loss = 0.3747
2026-01-02 02:53:26,358 - INFO -   Batch 1950/2527: Loss = 0.5038
2026-01-02 02:53:54,934 - INFO -   Batch 2000/2527: Loss = 0.6610
2026-01-02 02:54:23,842 - INFO -   Batch 2050/2527: Loss = 0.4084
2026-01-02 02:54:52,708 - INFO -   Batch 2100/2527: Loss = 0.5302
2026-01-02 02:55:21,272 - INFO -   Batch 2150/2527: Loss = 0.3096
2026-01-02 02:55:50,051 - INFO -   Batch 2200/2527: Loss = 0.6007
2026-01-02 02:56:18,643 - INFO -   Batch 2250/2527: Loss = 0.3758
2026-01-02 02:56:47,481 - INFO -   Batch 2300/2527: Loss = 0.4490
2026-01-02 02:57:16,365 - INFO -   Batch 2350/2527: Loss = 0.4450
2026-01-02 02:57:45,189 - INFO -   Batch 2400/2527: Loss = 0.3836
2026-01-02 02:58:13,801 - INFO -   Batch 2450/2527: Loss = 0.5142
2026-01-02 02:58:42,868 - INFO -   Batch 2500/2527: Loss = 0.7034
2026-01-02 03:03:21,763 - INFO - Epoch 11/160: Train Loss: 0.5143, Val Loss: 0.8526, Val mIoU: 35.37%, Val PixelAcc: 76.42%
2026-01-02 03:03:21,766 - INFO -   → New best mIoU: 35.37%
2026-01-02 03:03:25,416 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 03:03:25,419 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 03:03:55,879 - INFO -   Batch 50/2527: Loss = 0.3464
2026-01-02 03:04:24,887 - INFO -   Batch 100/2527: Loss = 0.3935
2026-01-02 03:04:53,509 - INFO -   Batch 150/2527: Loss = 0.3461
2026-01-02 03:05:22,288 - INFO -   Batch 200/2527: Loss = 0.3911
2026-01-02 03:05:50,927 - INFO -   Batch 250/2527: Loss = 0.3484
2026-01-02 03:06:19,799 - INFO -   Batch 300/2527: Loss = 0.3616
2026-01-02 03:06:48,453 - INFO -   Batch 350/2527: Loss = 0.5172
2026-01-02 03:07:17,086 - INFO -   Batch 400/2527: Loss = 0.3990
2026-01-02 03:07:46,006 - INFO -   Batch 450/2527: Loss = 0.4171
2026-01-02 03:08:14,806 - INFO -   Batch 500/2527: Loss = 0.3926
2026-01-02 03:08:43,706 - INFO -   Batch 550/2527: Loss = 0.4737
2026-01-02 03:09:12,250 - INFO -   Batch 600/2527: Loss = 0.3257
2026-01-02 03:09:41,113 - INFO -   Batch 650/2527: Loss = 0.5115
2026-01-02 03:10:09,816 - INFO -   Batch 700/2527: Loss = 0.6998
2026-01-02 03:10:38,344 - INFO -   Batch 750/2527: Loss = 0.4383
2026-01-02 03:11:07,357 - INFO -   Batch 800/2527: Loss = 0.3983
2026-01-02 03:11:36,218 - INFO -   Batch 850/2527: Loss = 0.4763
2026-01-02 03:12:05,289 - INFO -   Batch 900/2527: Loss = 0.4084
2026-01-02 03:12:34,006 - INFO -   Batch 950/2527: Loss = 0.3605
2026-01-02 03:13:02,470 - INFO -   Batch 1000/2527: Loss = 0.4119
2026-01-02 03:13:31,335 - INFO -   Batch 1050/2527: Loss = 0.4341
2026-01-02 03:14:00,055 - INFO -   Batch 1100/2527: Loss = 0.4970
2026-01-02 03:14:28,909 - INFO -   Batch 1150/2527: Loss = 0.4176
2026-01-02 03:14:57,679 - INFO -   Batch 1200/2527: Loss = 0.8845
2026-01-02 03:15:26,551 - INFO -   Batch 1250/2527: Loss = 0.7366
2026-01-02 03:15:55,369 - INFO -   Batch 1300/2527: Loss = 0.5379
2026-01-02 03:16:23,934 - INFO -   Batch 1350/2527: Loss = 0.5428
2026-01-02 03:16:52,735 - INFO -   Batch 1400/2527: Loss = 0.4589
2026-01-02 03:17:21,388 - INFO -   Batch 1450/2527: Loss = 0.4390
2026-01-02 03:17:50,293 - INFO -   Batch 1500/2527: Loss = 0.4197
2026-01-02 03:18:18,968 - INFO -   Batch 1550/2527: Loss = 0.2661
2026-01-02 03:18:47,974 - INFO -   Batch 1600/2527: Loss = 0.4590
2026-01-02 03:19:16,562 - INFO -   Batch 1650/2527: Loss = 0.4071
2026-01-02 03:19:45,397 - INFO -   Batch 1700/2527: Loss = 0.4671
2026-01-02 03:20:14,303 - INFO -   Batch 1750/2527: Loss = 0.7800
2026-01-02 03:20:42,840 - INFO -   Batch 1800/2527: Loss = 0.4445
2026-01-02 03:21:11,714 - INFO -   Batch 1850/2527: Loss = 0.4091
2026-01-02 03:21:40,399 - INFO -   Batch 1900/2527: Loss = 0.4306
2026-01-02 03:22:08,910 - INFO -   Batch 1950/2527: Loss = 0.4310
2026-01-02 03:22:37,882 - INFO -   Batch 2000/2527: Loss = 0.4958
2026-01-02 03:23:06,610 - INFO -   Batch 2050/2527: Loss = 0.4606
2026-01-02 03:23:35,634 - INFO -   Batch 2100/2527: Loss = 0.3391
2026-01-02 03:24:04,365 - INFO -   Batch 2150/2527: Loss = 0.3586
2026-01-02 03:24:33,146 - INFO -   Batch 2200/2527: Loss = 0.6873
2026-01-02 03:25:01,752 - INFO -   Batch 2250/2527: Loss = 0.3665
2026-01-02 03:25:30,316 - INFO -   Batch 2300/2527: Loss = 0.6493
2026-01-02 03:25:59,186 - INFO -   Batch 2350/2527: Loss = 0.6262
2026-01-02 03:26:27,929 - INFO -   Batch 2400/2527: Loss = 0.4575
2026-01-02 03:26:56,890 - INFO -   Batch 2450/2527: Loss = 0.4392
2026-01-02 03:27:25,704 - INFO -   Batch 2500/2527: Loss = 0.5676
2026-01-02 03:32:10,663 - INFO - Epoch 12/160: Train Loss: 0.4833, Val Loss: 0.8857, Val mIoU: 35.51%, Val PixelAcc: 75.72%
2026-01-02 03:32:10,665 - INFO -   → New best mIoU: 35.51%
2026-01-02 03:32:14,451 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 03:32:14,453 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 03:32:45,049 - INFO -   Batch 50/2527: Loss = 0.3707
2026-01-02 03:33:13,799 - INFO -   Batch 100/2527: Loss = 0.3404
2026-01-02 03:33:42,364 - INFO -   Batch 150/2527: Loss = 0.5217
2026-01-02 03:34:11,434 - INFO -   Batch 200/2527: Loss = 0.5259
2026-01-02 03:34:39,921 - INFO -   Batch 250/2527: Loss = 0.3866
2026-01-02 03:35:08,645 - INFO -   Batch 300/2527: Loss = 0.3038
2026-01-02 03:35:37,730 - INFO -   Batch 350/2527: Loss = 0.6634
2026-01-02 03:36:06,349 - INFO -   Batch 400/2527: Loss = 0.3123
2026-01-02 03:36:35,165 - INFO -   Batch 450/2527: Loss = 0.2915
2026-01-02 03:37:03,687 - INFO -   Batch 500/2527: Loss = 0.3774
2026-01-02 03:37:32,200 - INFO -   Batch 550/2527: Loss = 0.5664
2026-01-02 03:38:01,286 - INFO -   Batch 600/2527: Loss = 0.3398
2026-01-02 03:38:29,848 - INFO -   Batch 650/2527: Loss = 0.4560
2026-01-02 03:38:58,804 - INFO -   Batch 700/2527: Loss = 0.4791
2026-01-02 03:39:27,718 - INFO -   Batch 750/2527: Loss = 0.4862
2026-01-02 03:39:56,615 - INFO -   Batch 800/2527: Loss = 0.3447
2026-01-02 03:40:25,374 - INFO -   Batch 850/2527: Loss = 0.3487
2026-01-02 03:40:53,827 - INFO -   Batch 900/2527: Loss = 0.3832
2026-01-02 03:41:22,667 - INFO -   Batch 950/2527: Loss = 0.3115
2026-01-02 03:41:51,562 - INFO -   Batch 1000/2527: Loss = 0.4201
2026-01-02 03:42:20,353 - INFO -   Batch 1050/2527: Loss = 0.6059
2026-01-02 03:42:49,137 - INFO -   Batch 1100/2527: Loss = 0.5541
2026-01-02 03:43:18,227 - INFO -   Batch 1150/2527: Loss = 0.5567
2026-01-02 03:43:46,792 - INFO -   Batch 1200/2527: Loss = 0.5768
2026-01-02 03:44:15,362 - INFO -   Batch 1250/2527: Loss = 0.5788
2026-01-02 03:44:44,166 - INFO -   Batch 1300/2527: Loss = 0.4956
2026-01-02 03:45:12,808 - INFO -   Batch 1350/2527: Loss = 0.3641
2026-01-02 03:45:41,877 - INFO -   Batch 1400/2527: Loss = 0.3802
2026-01-02 03:46:10,490 - INFO -   Batch 1450/2527: Loss = 0.4171
2026-01-02 03:46:39,084 - INFO -   Batch 1500/2527: Loss = 0.3746
2026-01-02 03:47:08,201 - INFO -   Batch 1550/2527: Loss = 0.4702
2026-01-02 03:47:36,915 - INFO -   Batch 1600/2527: Loss = 0.2677
2026-01-02 03:48:05,747 - INFO -   Batch 1650/2527: Loss = 0.2782
2026-01-02 03:48:34,364 - INFO -   Batch 1700/2527: Loss = 0.4103
2026-01-02 03:49:03,194 - INFO -   Batch 1750/2527: Loss = 0.4955
2026-01-02 03:49:32,035 - INFO -   Batch 1800/2527: Loss = 0.5883
2026-01-02 03:50:00,534 - INFO -   Batch 1850/2527: Loss = 0.3090
2026-01-02 03:50:29,456 - INFO -   Batch 1900/2527: Loss = 0.5185
2026-01-02 03:50:58,388 - INFO -   Batch 1950/2527: Loss = 0.5478
2026-01-02 03:51:27,255 - INFO -   Batch 2000/2527: Loss = 0.3835
2026-01-02 03:51:55,802 - INFO -   Batch 2050/2527: Loss = 0.3603
2026-01-02 03:52:24,240 - INFO -   Batch 2100/2527: Loss = 0.4803
2026-01-02 03:52:53,062 - INFO -   Batch 2150/2527: Loss = 0.7990
2026-01-02 03:53:21,900 - INFO -   Batch 2200/2527: Loss = 0.4890
2026-01-02 03:53:50,759 - INFO -   Batch 2250/2527: Loss = 0.6281
2026-01-02 03:54:19,489 - INFO -   Batch 2300/2527: Loss = 0.5364
2026-01-02 03:54:48,613 - INFO -   Batch 2350/2527: Loss = 0.3144
2026-01-02 03:55:17,266 - INFO -   Batch 2400/2527: Loss = 0.4684
2026-01-02 03:55:45,873 - INFO -   Batch 2450/2527: Loss = 0.3696
2026-01-02 03:56:14,700 - INFO -   Batch 2500/2527: Loss = 0.4883
2026-01-02 04:00:54,708 - INFO - Epoch 13/160: Train Loss: 0.4511, Val Loss: 0.8826, Val mIoU: 36.62%, Val PixelAcc: 76.17%
2026-01-02 04:00:54,710 - INFO -   → New best mIoU: 36.62%
2026-01-02 04:00:58,407 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 04:00:58,410 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 04:01:28,842 - INFO -   Batch 50/2527: Loss = 0.4731
2026-01-02 04:01:57,609 - INFO -   Batch 100/2527: Loss = 0.2686
2026-01-02 04:02:26,526 - INFO -   Batch 150/2527: Loss = 0.3733
2026-01-02 04:02:55,104 - INFO -   Batch 200/2527: Loss = 0.2697
2026-01-02 04:03:23,973 - INFO -   Batch 250/2527: Loss = 0.3265
2026-01-02 04:03:52,623 - INFO -   Batch 300/2527: Loss = 0.3024
2026-01-02 04:04:21,772 - INFO -   Batch 350/2527: Loss = 0.3114
2026-01-02 04:04:50,400 - INFO -   Batch 400/2527: Loss = 0.3706
2026-01-02 04:05:19,051 - INFO -   Batch 450/2527: Loss = 0.3639
2026-01-02 04:05:48,198 - INFO -   Batch 500/2527: Loss = 0.3628
2026-01-02 04:06:17,040 - INFO -   Batch 550/2527: Loss = 0.3564
2026-01-02 04:06:45,910 - INFO -   Batch 600/2527: Loss = 0.2991
2026-01-02 04:07:14,565 - INFO -   Batch 650/2527: Loss = 0.4488
2026-01-02 04:07:43,437 - INFO -   Batch 700/2527: Loss = 0.4158
2026-01-02 04:08:12,224 - INFO -   Batch 750/2527: Loss = 0.4536
2026-01-02 04:08:40,924 - INFO -   Batch 800/2527: Loss = 0.3903
2026-01-02 04:09:09,921 - INFO -   Batch 850/2527: Loss = 0.4086
2026-01-02 04:09:38,884 - INFO -   Batch 900/2527: Loss = 0.2817
2026-01-02 04:10:07,784 - INFO -   Batch 950/2527: Loss = 0.3568
2026-01-02 04:10:36,402 - INFO -   Batch 1000/2527: Loss = 0.4439
2026-01-02 04:11:04,908 - INFO -   Batch 1050/2527: Loss = 0.4800
2026-01-02 04:11:33,810 - INFO -   Batch 1100/2527: Loss = 0.3090
2026-01-02 04:12:02,732 - INFO -   Batch 1150/2527: Loss = 0.3013
2026-01-02 04:12:31,581 - INFO -   Batch 1200/2527: Loss = 0.4188
2026-01-02 04:13:00,384 - INFO -   Batch 1250/2527: Loss = 0.5510
2026-01-02 04:13:29,567 - INFO -   Batch 1300/2527: Loss = 0.3819
2026-01-02 04:13:58,275 - INFO -   Batch 1350/2527: Loss = 0.2329
2026-01-02 04:14:26,822 - INFO -   Batch 1400/2527: Loss = 0.3740
2026-01-02 04:14:55,638 - INFO -   Batch 1450/2527: Loss = 0.3764
2026-01-02 04:15:24,273 - INFO -   Batch 1500/2527: Loss = 0.3801
2026-01-02 04:15:53,378 - INFO -   Batch 1550/2527: Loss = 0.4296
2026-01-02 04:16:22,102 - INFO -   Batch 1600/2527: Loss = 0.5526
2026-01-02 04:16:50,779 - INFO -   Batch 1650/2527: Loss = 0.3005
2026-01-02 04:17:19,862 - INFO -   Batch 1700/2527: Loss = 0.4538
2026-01-02 04:17:48,570 - INFO -   Batch 1750/2527: Loss = 0.5056
2026-01-02 04:18:17,383 - INFO -   Batch 1800/2527: Loss = 0.2342
2026-01-02 04:18:46,049 - INFO -   Batch 1850/2527: Loss = 0.5650
2026-01-02 04:19:14,958 - INFO -   Batch 1900/2527: Loss = 0.4280
2026-01-02 04:19:43,759 - INFO -   Batch 1950/2527: Loss = 0.4539
2026-01-02 04:20:12,262 - INFO -   Batch 2000/2527: Loss = 0.3484
2026-01-02 04:20:41,239 - INFO -   Batch 2050/2527: Loss = 0.4351
2026-01-02 04:21:10,133 - INFO -   Batch 2100/2527: Loss = 0.5002
2026-01-02 04:21:39,067 - INFO -   Batch 2150/2527: Loss = 0.4168
2026-01-02 04:22:07,641 - INFO -   Batch 2200/2527: Loss = 0.3996
2026-01-02 04:22:36,480 - INFO -   Batch 2250/2527: Loss = 0.3990
2026-01-02 04:23:05,072 - INFO -   Batch 2300/2527: Loss = 0.4577
2026-01-02 04:23:33,959 - INFO -   Batch 2350/2527: Loss = 0.4653
2026-01-02 04:24:02,833 - INFO -   Batch 2400/2527: Loss = 0.4847
2026-01-02 04:24:31,822 - INFO -   Batch 2450/2527: Loss = 0.6023
2026-01-02 04:25:00,940 - INFO -   Batch 2500/2527: Loss = 0.4196
2026-01-02 04:29:40,140 - INFO - Epoch 14/160: Train Loss: 0.4289, Val Loss: 0.8835, Val mIoU: 36.69%, Val PixelAcc: 76.43%
2026-01-02 04:29:40,142 - INFO -   → New best mIoU: 36.69%
2026-01-02 04:29:43,785 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 04:29:43,788 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 04:30:14,719 - INFO -   Batch 50/2527: Loss = 0.4066
2026-01-02 04:30:43,595 - INFO -   Batch 100/2527: Loss = 0.4624
2026-01-02 04:31:12,492 - INFO -   Batch 150/2527: Loss = 0.3820
2026-01-02 04:31:41,057 - INFO -   Batch 200/2527: Loss = 0.3539
2026-01-02 04:32:09,933 - INFO -   Batch 250/2527: Loss = 0.3632
2026-01-02 04:32:38,579 - INFO -   Batch 300/2527: Loss = 0.2879
2026-01-02 04:33:07,119 - INFO -   Batch 350/2527: Loss = 0.4051
2026-01-02 04:33:36,094 - INFO -   Batch 400/2527: Loss = 0.4093
2026-01-02 04:34:04,859 - INFO -   Batch 450/2527: Loss = 0.4504
2026-01-02 04:34:33,894 - INFO -   Batch 500/2527: Loss = 0.5887
2026-01-02 04:35:02,569 - INFO -   Batch 550/2527: Loss = 0.2825
2026-01-02 04:35:31,049 - INFO -   Batch 600/2527: Loss = 0.3195
2026-01-02 04:35:59,868 - INFO -   Batch 650/2527: Loss = 0.3521
2026-01-02 04:36:28,571 - INFO -   Batch 700/2527: Loss = 0.3396
2026-01-02 04:36:57,439 - INFO -   Batch 750/2527: Loss = 0.3651
2026-01-02 04:37:26,148 - INFO -   Batch 800/2527: Loss = 0.2415
2026-01-02 04:37:55,075 - INFO -   Batch 850/2527: Loss = 0.3610
2026-01-02 04:38:23,891 - INFO -   Batch 900/2527: Loss = 0.2857
2026-01-02 04:38:52,471 - INFO -   Batch 950/2527: Loss = 0.3956
2026-01-02 04:39:21,325 - INFO -   Batch 1000/2527: Loss = 0.4823
2026-01-02 04:39:49,902 - INFO -   Batch 1050/2527: Loss = 0.3109
2026-01-02 04:40:18,817 - INFO -   Batch 1100/2527: Loss = 0.5497
2026-01-02 04:40:47,447 - INFO -   Batch 1150/2527: Loss = 0.2548
2026-01-02 04:41:16,438 - INFO -   Batch 1200/2527: Loss = 0.5330
2026-01-02 04:41:45,046 - INFO -   Batch 1250/2527: Loss = 0.4418
2026-01-02 04:42:14,074 - INFO -   Batch 1300/2527: Loss = 0.3497
2026-01-02 04:42:42,972 - INFO -   Batch 1350/2527: Loss = 0.4027
2026-01-02 04:43:11,576 - INFO -   Batch 1400/2527: Loss = 0.6563
2026-01-02 04:43:40,413 - INFO -   Batch 1450/2527: Loss = 0.4477
2026-01-02 04:44:09,134 - INFO -   Batch 1500/2527: Loss = 0.4934
2026-01-02 04:44:37,671 - INFO -   Batch 1550/2527: Loss = 0.5404
2026-01-02 04:45:06,621 - INFO -   Batch 1600/2527: Loss = 0.4592
2026-01-02 04:45:35,375 - INFO -   Batch 1650/2527: Loss = 0.3837
2026-01-02 04:46:04,439 - INFO -   Batch 1700/2527: Loss = 0.3345
2026-01-02 04:46:33,084 - INFO -   Batch 1750/2527: Loss = 0.3918
2026-01-02 04:47:01,895 - INFO -   Batch 1800/2527: Loss = 0.3309
2026-01-02 04:47:30,561 - INFO -   Batch 1850/2527: Loss = 0.5602
2026-01-02 04:47:59,145 - INFO -   Batch 1900/2527: Loss = 0.6431
2026-01-02 04:48:28,029 - INFO -   Batch 1950/2527: Loss = 0.3719
2026-01-02 04:48:56,800 - INFO -   Batch 2000/2527: Loss = 0.3612
2026-01-02 04:49:25,746 - INFO -   Batch 2050/2527: Loss = 0.5229
2026-01-02 04:49:54,561 - INFO -   Batch 2100/2527: Loss = 0.4180
2026-01-02 04:50:23,487 - INFO -   Batch 2150/2527: Loss = 0.4049
2026-01-02 04:50:52,000 - INFO -   Batch 2200/2527: Loss = 0.4881
2026-01-02 04:51:20,608 - INFO -   Batch 2250/2527: Loss = 0.2985
2026-01-02 04:51:49,467 - INFO -   Batch 2300/2527: Loss = 0.3064
2026-01-02 04:52:18,137 - INFO -   Batch 2350/2527: Loss = 0.3836
2026-01-02 04:52:47,084 - INFO -   Batch 2400/2527: Loss = 0.3124
2026-01-02 04:53:15,781 - INFO -   Batch 2450/2527: Loss = 0.4987
2026-01-02 04:53:44,505 - INFO -   Batch 2500/2527: Loss = 0.4688
2026-01-02 04:58:30,617 - INFO - Epoch 15/160: Train Loss: 0.4063, Val Loss: 0.8928, Val mIoU: 36.41%, Val PixelAcc: 76.33%
2026-01-02 04:59:00,936 - INFO -   Batch 50/2527: Loss = 0.3109
2026-01-02 04:59:29,550 - INFO -   Batch 100/2527: Loss = 0.3583
2026-01-02 04:59:58,031 - INFO -   Batch 150/2527: Loss = 0.4297
2026-01-02 05:00:27,084 - INFO -   Batch 200/2527: Loss = 0.3664
2026-01-02 05:00:55,712 - INFO -   Batch 250/2527: Loss = 0.3350
2026-01-02 05:01:24,618 - INFO -   Batch 300/2527: Loss = 0.3363
2026-01-02 05:01:53,484 - INFO -   Batch 350/2527: Loss = 0.3335
2026-01-02 05:02:22,353 - INFO -   Batch 400/2527: Loss = 0.3708
2026-01-02 05:02:50,957 - INFO -   Batch 450/2527: Loss = 0.2604
2026-01-02 05:03:19,417 - INFO -   Batch 500/2527: Loss = 0.3690
2026-01-02 05:03:48,275 - INFO -   Batch 550/2527: Loss = 0.3505
2026-01-02 05:04:17,202 - INFO -   Batch 600/2527: Loss = 0.3114
2026-01-02 05:04:46,010 - INFO -   Batch 650/2527: Loss = 0.2576
2026-01-02 05:05:14,807 - INFO -   Batch 700/2527: Loss = 0.4141
2026-01-02 05:05:43,567 - INFO -   Batch 750/2527: Loss = 0.3840
2026-01-02 05:06:12,454 - INFO -   Batch 800/2527: Loss = 0.3616
2026-01-02 05:06:40,976 - INFO -   Batch 850/2527: Loss = 0.3572
2026-01-02 05:07:09,775 - INFO -   Batch 900/2527: Loss = 0.3279
2026-01-02 05:07:38,397 - INFO -   Batch 950/2527: Loss = 0.2744
2026-01-02 05:08:07,477 - INFO -   Batch 1000/2527: Loss = 0.4442
2026-01-02 05:08:36,003 - INFO -   Batch 1050/2527: Loss = 0.3997
2026-01-02 05:09:04,611 - INFO -   Batch 1100/2527: Loss = 0.5016
2026-01-02 05:09:33,749 - INFO -   Batch 1150/2527: Loss = 0.4437
2026-01-02 05:10:02,445 - INFO -   Batch 1200/2527: Loss = 0.4591
2026-01-02 05:10:31,232 - INFO -   Batch 1250/2527: Loss = 0.3576
2026-01-02 05:10:59,814 - INFO -   Batch 1300/2527: Loss = 0.4905
2026-01-02 05:11:28,623 - INFO -   Batch 1350/2527: Loss = 0.3938
2026-01-02 05:11:57,380 - INFO -   Batch 1400/2527: Loss = 0.4263
2026-01-02 05:12:25,944 - INFO -   Batch 1450/2527: Loss = 0.3463
2026-01-02 05:12:54,858 - INFO -   Batch 1500/2527: Loss = 0.2764
2026-01-02 05:13:23,718 - INFO -   Batch 1550/2527: Loss = 0.3420
2026-01-02 05:13:52,575 - INFO -   Batch 1600/2527: Loss = 0.3704
2026-01-02 05:14:21,234 - INFO -   Batch 1650/2527: Loss = 0.3319
2026-01-02 05:14:49,725 - INFO -   Batch 1700/2527: Loss = 0.3388
2026-01-02 05:15:18,602 - INFO -   Batch 1750/2527: Loss = 0.5572
2026-01-02 05:15:47,584 - INFO -   Batch 1800/2527: Loss = 0.3593
2026-01-02 05:16:16,391 - INFO -   Batch 1850/2527: Loss = 0.3196
2026-01-02 05:16:45,134 - INFO -   Batch 1900/2527: Loss = 0.4332
2026-01-02 05:17:14,251 - INFO -   Batch 1950/2527: Loss = 0.3722
2026-01-02 05:17:42,906 - INFO -   Batch 2000/2527: Loss = 0.4263
2026-01-02 05:18:11,373 - INFO -   Batch 2050/2527: Loss = 0.3905
2026-01-02 05:18:40,179 - INFO -   Batch 2100/2527: Loss = 0.3062
2026-01-02 05:19:08,850 - INFO -   Batch 2150/2527: Loss = 0.4172
2026-01-02 05:19:37,902 - INFO -   Batch 2200/2527: Loss = 0.4888
2026-01-02 05:20:06,483 - INFO -   Batch 2250/2527: Loss = 0.3319
2026-01-02 05:20:35,391 - INFO -   Batch 2300/2527: Loss = 0.3453
2026-01-02 05:21:04,147 - INFO -   Batch 2350/2527: Loss = 0.3343
2026-01-02 05:21:32,862 - INFO -   Batch 2400/2527: Loss = 0.3285
2026-01-02 05:22:01,655 - INFO -   Batch 2450/2527: Loss = 0.5896
2026-01-02 05:22:30,304 - INFO -   Batch 2500/2527: Loss = 0.2668
2026-01-02 05:27:10,513 - INFO - Epoch 16/160: Train Loss: 0.3844, Val Loss: 0.9325, Val mIoU: 35.28%, Val PixelAcc: 76.03%
2026-01-02 05:27:41,108 - INFO -   Batch 50/2527: Loss = 0.2701
2026-01-02 05:28:09,927 - INFO -   Batch 100/2527: Loss = 0.2149
2026-01-02 05:28:38,826 - INFO -   Batch 150/2527: Loss = 0.3648
2026-01-02 05:29:07,460 - INFO -   Batch 200/2527: Loss = 0.3164
2026-01-02 05:29:36,269 - INFO -   Batch 250/2527: Loss = 0.3222
2026-01-02 05:30:04,777 - INFO -   Batch 300/2527: Loss = 0.3589
2026-01-02 05:30:33,639 - INFO -   Batch 350/2527: Loss = 0.2674
2026-01-02 05:31:02,485 - INFO -   Batch 400/2527: Loss = 0.3731
2026-01-02 05:31:31,214 - INFO -   Batch 450/2527: Loss = 0.3604
2026-01-02 05:32:00,305 - INFO -   Batch 500/2527: Loss = 0.2870
2026-01-02 05:32:28,944 - INFO -   Batch 550/2527: Loss = 0.4580
2026-01-02 05:32:57,420 - INFO -   Batch 600/2527: Loss = 0.3949
2026-01-02 05:33:26,216 - INFO -   Batch 650/2527: Loss = 0.4707
2026-01-02 05:33:54,840 - INFO -   Batch 700/2527: Loss = 0.6778
2026-01-02 05:34:23,900 - INFO -   Batch 750/2527: Loss = 0.2392
2026-01-02 05:34:52,465 - INFO -   Batch 800/2527: Loss = 0.3285
2026-01-02 05:35:21,403 - INFO -   Batch 850/2527: Loss = 0.2145
2026-01-02 05:35:50,294 - INFO -   Batch 900/2527: Loss = 0.3071
2026-01-02 05:36:18,875 - INFO -   Batch 950/2527: Loss = 0.3000
2026-01-02 05:36:47,662 - INFO -   Batch 1000/2527: Loss = 0.3608
2026-01-02 05:37:16,222 - INFO -   Batch 1050/2527: Loss = 0.2842
2026-01-02 05:37:45,050 - INFO -   Batch 1100/2527: Loss = 0.3093
2026-01-02 05:38:13,888 - INFO -   Batch 1150/2527: Loss = 0.3008
2026-01-02 05:38:42,688 - INFO -   Batch 1200/2527: Loss = 0.4439
2026-01-02 05:39:11,296 - INFO -   Batch 1250/2527: Loss = 0.3764
2026-01-02 05:39:40,193 - INFO -   Batch 1300/2527: Loss = 0.3588
2026-01-02 05:40:09,094 - INFO -   Batch 1350/2527: Loss = 0.3836
2026-01-02 05:40:37,697 - INFO -   Batch 1400/2527: Loss = 0.5531
2026-01-02 05:41:06,494 - INFO -   Batch 1450/2527: Loss = 0.4774
2026-01-02 05:41:35,279 - INFO -   Batch 1500/2527: Loss = 0.2914
2026-01-02 05:42:03,998 - INFO -   Batch 1550/2527: Loss = 0.3861
2026-01-02 05:42:32,822 - INFO -   Batch 1600/2527: Loss = 0.3945
2026-01-02 05:43:01,546 - INFO -   Batch 1650/2527: Loss = 0.5428
2026-01-02 05:43:30,648 - INFO -   Batch 1700/2527: Loss = 0.3554
2026-01-02 05:43:59,362 - INFO -   Batch 1750/2527: Loss = 0.3293
2026-01-02 05:44:28,160 - INFO -   Batch 1800/2527: Loss = 0.4713
2026-01-02 05:44:56,753 - INFO -   Batch 1850/2527: Loss = 0.2704
2026-01-02 05:45:25,240 - INFO -   Batch 1900/2527: Loss = 0.4507
2026-01-02 05:45:54,283 - INFO -   Batch 1950/2527: Loss = 0.3131
2026-01-02 05:46:22,874 - INFO -   Batch 2000/2527: Loss = 0.4299
2026-01-02 05:46:51,825 - INFO -   Batch 2050/2527: Loss = 0.3822
2026-01-02 05:47:20,733 - INFO -   Batch 2100/2527: Loss = 0.3960
2026-01-02 05:47:49,307 - INFO -   Batch 2150/2527: Loss = 0.2427
2026-01-02 05:48:18,129 - INFO -   Batch 2200/2527: Loss = 0.2747
2026-01-02 05:48:46,774 - INFO -   Batch 2250/2527: Loss = 0.2503
2026-01-02 05:49:15,626 - INFO -   Batch 2300/2527: Loss = 0.3546
2026-01-02 05:49:44,470 - INFO -   Batch 2350/2527: Loss = 0.5040
2026-01-02 05:50:13,293 - INFO -   Batch 2400/2527: Loss = 0.4127
2026-01-02 05:50:41,972 - INFO -   Batch 2450/2527: Loss = 0.3332
2026-01-02 05:51:10,727 - INFO -   Batch 2500/2527: Loss = 0.3534
2026-01-02 05:55:49,614 - INFO - Epoch 17/160: Train Loss: 0.3694, Val Loss: 2.6096, Val mIoU: 34.33%, Val PixelAcc: 70.95%
2026-01-02 05:56:20,321 - INFO -   Batch 50/2527: Loss = 0.2713
2026-01-02 05:56:48,979 - INFO -   Batch 100/2527: Loss = 0.3092
2026-01-02 05:57:17,816 - INFO -   Batch 150/2527: Loss = 0.3655
2026-01-02 05:57:46,366 - INFO -   Batch 200/2527: Loss = 0.3865
2026-01-02 05:58:15,188 - INFO -   Batch 250/2527: Loss = 0.4053
2026-01-02 05:58:43,883 - INFO -   Batch 300/2527: Loss = 0.3617
2026-01-02 05:59:12,692 - INFO -   Batch 350/2527: Loss = 0.2355
2026-01-02 05:59:41,402 - INFO -   Batch 400/2527: Loss = 0.3906
2026-01-02 06:00:09,980 - INFO -   Batch 450/2527: Loss = 0.3107
2026-01-02 06:00:39,000 - INFO -   Batch 500/2527: Loss = 0.3059
2026-01-02 06:01:07,713 - INFO -   Batch 550/2527: Loss = 0.3155
2026-01-02 06:01:36,524 - INFO -   Batch 600/2527: Loss = 0.2695
2026-01-02 06:02:05,097 - INFO -   Batch 650/2527: Loss = 0.2544
2026-01-02 06:02:33,990 - INFO -   Batch 700/2527: Loss = 0.2929
2026-01-02 06:03:02,608 - INFO -   Batch 750/2527: Loss = 0.2809
2026-01-02 06:03:31,282 - INFO -   Batch 800/2527: Loss = 0.1871
2026-01-02 06:04:00,236 - INFO -   Batch 850/2527: Loss = 0.3672
2026-01-02 06:04:29,016 - INFO -   Batch 900/2527: Loss = 0.3665
2026-01-02 06:04:57,932 - INFO -   Batch 950/2527: Loss = 0.3010
2026-01-02 06:05:26,543 - INFO -   Batch 1000/2527: Loss = 0.4256
2026-01-02 06:05:55,046 - INFO -   Batch 1050/2527: Loss = 0.3638
2026-01-02 06:06:23,917 - INFO -   Batch 1100/2527: Loss = 0.3405
2026-01-02 06:06:52,732 - INFO -   Batch 1150/2527: Loss = 0.3830
2026-01-02 06:07:21,721 - INFO -   Batch 1200/2527: Loss = 0.4138
2026-01-02 06:07:50,454 - INFO -   Batch 1250/2527: Loss = 0.3258
2026-01-02 06:08:19,482 - INFO -   Batch 1300/2527: Loss = 0.2244
2026-01-02 06:08:48,206 - INFO -   Batch 1350/2527: Loss = 0.3011
2026-01-02 06:09:16,657 - INFO -   Batch 1400/2527: Loss = 0.3729
2026-01-02 06:09:45,505 - INFO -   Batch 1450/2527: Loss = 0.1653
2026-01-02 06:10:14,191 - INFO -   Batch 1500/2527: Loss = 0.3371
2026-01-02 06:10:43,088 - INFO -   Batch 1550/2527: Loss = 0.2642
2026-01-02 06:11:11,810 - INFO -   Batch 1600/2527: Loss = 0.2958
2026-01-02 06:11:40,750 - INFO -   Batch 1650/2527: Loss = 0.2912
2026-01-02 06:12:09,505 - INFO -   Batch 1700/2527: Loss = 0.3093
2026-01-02 06:12:38,178 - INFO -   Batch 1750/2527: Loss = 0.4413
2026-01-02 06:13:07,027 - INFO -   Batch 1800/2527: Loss = 0.5473
2026-01-02 06:13:35,680 - INFO -   Batch 1850/2527: Loss = 0.2434
2026-01-02 06:14:04,604 - INFO -   Batch 1900/2527: Loss = 0.3297
2026-01-02 06:14:33,283 - INFO -   Batch 1950/2527: Loss = 0.3787
2026-01-02 06:15:01,959 - INFO -   Batch 2000/2527: Loss = 0.3649
2026-01-02 06:15:30,846 - INFO -   Batch 2050/2527: Loss = 0.2502
2026-01-02 06:15:59,686 - INFO -   Batch 2100/2527: Loss = 0.3439
2026-01-02 06:16:28,565 - INFO -   Batch 2150/2527: Loss = 0.7226
2026-01-02 06:16:57,179 - INFO -   Batch 2200/2527: Loss = 0.3749
2026-01-02 06:17:26,007 - INFO -   Batch 2250/2527: Loss = 0.4048
2026-01-02 06:17:54,651 - INFO -   Batch 2300/2527: Loss = 0.3785
2026-01-02 06:18:23,174 - INFO -   Batch 2350/2527: Loss = 0.3639
2026-01-02 06:18:52,147 - INFO -   Batch 2400/2527: Loss = 0.3644
2026-01-02 06:19:20,850 - INFO -   Batch 2450/2527: Loss = 0.4887
2026-01-02 06:19:49,904 - INFO -   Batch 2500/2527: Loss = 0.2964
2026-01-02 06:24:35,924 - INFO - Epoch 18/160: Train Loss: 0.3524, Val Loss: 0.9594, Val mIoU: 37.22%, Val PixelAcc: 76.27%
2026-01-02 06:24:35,926 - INFO -   → New best mIoU: 37.22%
2026-01-02 06:24:39,427 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 06:24:39,430 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 06:25:10,149 - INFO -   Batch 50/2527: Loss = 0.4705
2026-01-02 06:25:38,942 - INFO -   Batch 100/2527: Loss = 0.2906
2026-01-02 06:26:07,600 - INFO -   Batch 150/2527: Loss = 0.2688
2026-01-02 06:26:36,576 - INFO -   Batch 200/2527: Loss = 0.3327
2026-01-02 06:27:05,380 - INFO -   Batch 250/2527: Loss = 0.2013
2026-01-02 06:27:34,109 - INFO -   Batch 300/2527: Loss = 0.4625
2026-01-02 06:28:02,859 - INFO -   Batch 350/2527: Loss = 0.2857
2026-01-02 06:28:31,765 - INFO -   Batch 400/2527: Loss = 0.3468
2026-01-02 06:29:00,499 - INFO -   Batch 450/2527: Loss = 0.1964
2026-01-02 06:29:29,322 - INFO -   Batch 500/2527: Loss = 0.2947
2026-01-02 06:29:57,944 - INFO -   Batch 550/2527: Loss = 0.3708
2026-01-02 06:30:27,010 - INFO -   Batch 600/2527: Loss = 0.2714
2026-01-02 06:30:55,490 - INFO -   Batch 650/2527: Loss = 0.4001
2026-01-02 06:31:24,231 - INFO -   Batch 700/2527: Loss = 0.3495
2026-01-02 06:31:53,321 - INFO -   Batch 750/2527: Loss = 0.3251
2026-01-02 06:32:22,011 - INFO -   Batch 800/2527: Loss = 0.2707
2026-01-02 06:32:50,822 - INFO -   Batch 850/2527: Loss = 0.2554
2026-01-02 06:33:19,422 - INFO -   Batch 900/2527: Loss = 0.1521
2026-01-02 06:33:47,967 - INFO -   Batch 950/2527: Loss = 0.2917
2026-01-02 06:34:17,011 - INFO -   Batch 1000/2527: Loss = 0.3143
2026-01-02 06:34:45,657 - INFO -   Batch 1050/2527: Loss = 0.4380
2026-01-02 06:35:14,614 - INFO -   Batch 1100/2527: Loss = 0.3143
2026-01-02 06:35:43,541 - INFO -   Batch 1150/2527: Loss = 0.2838
2026-01-02 06:36:12,449 - INFO -   Batch 1200/2527: Loss = 0.3261
2026-01-02 06:36:41,028 - INFO -   Batch 1250/2527: Loss = 0.3556
2026-01-02 06:37:09,505 - INFO -   Batch 1300/2527: Loss = 0.2893
2026-01-02 06:37:38,362 - INFO -   Batch 1350/2527: Loss = 0.2039
2026-01-02 06:38:07,139 - INFO -   Batch 1400/2527: Loss = 0.3814
2026-01-02 06:38:35,966 - INFO -   Batch 1450/2527: Loss = 0.4252
2026-01-02 06:39:04,685 - INFO -   Batch 1500/2527: Loss = 0.3111
2026-01-02 06:39:33,779 - INFO -   Batch 1550/2527: Loss = 0.4380
2026-01-02 06:40:02,348 - INFO -   Batch 1600/2527: Loss = 0.5294
2026-01-02 06:40:30,896 - INFO -   Batch 1650/2527: Loss = 0.3590
2026-01-02 06:40:59,684 - INFO -   Batch 1700/2527: Loss = 0.2907
2026-01-02 06:41:28,354 - INFO -   Batch 1750/2527: Loss = 0.6190
2026-01-02 06:41:57,429 - INFO -   Batch 1800/2527: Loss = 0.3589
2026-01-02 06:42:26,004 - INFO -   Batch 1850/2527: Loss = 0.2604
2026-01-02 06:42:54,605 - INFO -   Batch 1900/2527: Loss = 0.2285
2026-01-02 06:43:23,669 - INFO -   Batch 1950/2527: Loss = 0.4007
2026-01-02 06:43:52,321 - INFO -   Batch 2000/2527: Loss = 0.3514
2026-01-02 06:44:21,124 - INFO -   Batch 2050/2527: Loss = 0.2797
2026-01-02 06:44:49,666 - INFO -   Batch 2100/2527: Loss = 0.3769
2026-01-02 06:45:18,522 - INFO -   Batch 2150/2527: Loss = 0.2908
2026-01-02 06:45:47,345 - INFO -   Batch 2200/2527: Loss = 0.2319
2026-01-02 06:46:15,847 - INFO -   Batch 2250/2527: Loss = 0.4152
2026-01-02 06:46:44,795 - INFO -   Batch 2300/2527: Loss = 0.3057
2026-01-02 06:47:13,826 - INFO -   Batch 2350/2527: Loss = 0.3383
2026-01-02 06:47:42,751 - INFO -   Batch 2400/2527: Loss = 0.3441
2026-01-02 06:48:11,340 - INFO -   Batch 2450/2527: Loss = 0.2433
2026-01-02 06:48:39,822 - INFO -   Batch 2500/2527: Loss = 0.4499
2026-01-02 06:53:18,842 - INFO - Epoch 19/160: Train Loss: 0.3400, Val Loss: 0.8990, Val mIoU: 37.69%, Val PixelAcc: 77.11%
2026-01-02 06:53:18,843 - INFO -   → New best mIoU: 37.69%
2026-01-02 06:53:22,483 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 06:53:22,486 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 06:53:52,892 - INFO -   Batch 50/2527: Loss = 0.3876
2026-01-02 06:54:21,866 - INFO -   Batch 100/2527: Loss = 0.4379
2026-01-02 06:54:50,778 - INFO -   Batch 150/2527: Loss = 0.2900
2026-01-02 06:55:19,256 - INFO -   Batch 200/2527: Loss = 0.2950
2026-01-02 06:55:47,835 - INFO -   Batch 250/2527: Loss = 0.2478
2026-01-02 06:56:16,691 - INFO -   Batch 300/2527: Loss = 0.2598
2026-01-02 06:56:45,550 - INFO -   Batch 350/2527: Loss = 0.2651
2026-01-02 06:57:14,382 - INFO -   Batch 400/2527: Loss = 0.2220
2026-01-02 06:57:43,135 - INFO -   Batch 450/2527: Loss = 0.4197
2026-01-02 06:58:11,900 - INFO -   Batch 500/2527: Loss = 0.2988
2026-01-02 06:58:40,782 - INFO -   Batch 550/2527: Loss = 0.1908
2026-01-02 06:59:09,347 - INFO -   Batch 600/2527: Loss = 0.1750
2026-01-02 06:59:38,158 - INFO -   Batch 650/2527: Loss = 0.3525
2026-01-02 07:00:06,832 - INFO -   Batch 700/2527: Loss = 0.3384
2026-01-02 07:00:35,902 - INFO -   Batch 750/2527: Loss = 0.3041
2026-01-02 07:01:04,588 - INFO -   Batch 800/2527: Loss = 0.2132
2026-01-02 07:01:33,221 - INFO -   Batch 850/2527: Loss = 0.4122
2026-01-02 07:02:02,334 - INFO -   Batch 900/2527: Loss = 0.3231
2026-01-02 07:02:31,028 - INFO -   Batch 950/2527: Loss = 0.2340
2026-01-02 07:02:59,831 - INFO -   Batch 1000/2527: Loss = 0.3633
2026-01-02 07:03:28,367 - INFO -   Batch 1050/2527: Loss = 0.3539
2026-01-02 07:03:56,890 - INFO -   Batch 1100/2527: Loss = 0.3497
2026-01-02 07:04:25,963 - INFO -   Batch 1150/2527: Loss = 0.2352
2026-01-02 07:04:54,560 - INFO -   Batch 1200/2527: Loss = 0.6077
2026-01-02 07:05:23,508 - INFO -   Batch 1250/2527: Loss = 0.3142
2026-01-02 07:05:52,324 - INFO -   Batch 1300/2527: Loss = 0.3660
2026-01-02 07:06:21,252 - INFO -   Batch 1350/2527: Loss = 0.2298
2026-01-02 07:06:49,876 - INFO -   Batch 1400/2527: Loss = 0.3674
2026-01-02 07:07:18,345 - INFO -   Batch 1450/2527: Loss = 0.5505
2026-01-02 07:07:47,209 - INFO -   Batch 1500/2527: Loss = 0.3853
2026-01-02 07:08:16,055 - INFO -   Batch 1550/2527: Loss = 0.2563
2026-01-02 07:08:44,859 - INFO -   Batch 1600/2527: Loss = 0.3345
2026-01-02 07:09:13,569 - INFO -   Batch 1650/2527: Loss = 0.3082
2026-01-02 07:09:42,612 - INFO -   Batch 1700/2527: Loss = 0.4481
2026-01-02 07:10:11,314 - INFO -   Batch 1750/2527: Loss = 0.2690
2026-01-02 07:10:39,768 - INFO -   Batch 1800/2527: Loss = 0.5047
2026-01-02 07:11:08,565 - INFO -   Batch 1850/2527: Loss = 0.2953
2026-01-02 07:11:37,195 - INFO -   Batch 1900/2527: Loss = 0.4241
2026-01-02 07:12:06,242 - INFO -   Batch 1950/2527: Loss = 0.2517
2026-01-02 07:12:34,980 - INFO -   Batch 2000/2527: Loss = 0.3801
2026-01-02 07:13:03,631 - INFO -   Batch 2050/2527: Loss = 0.3191
2026-01-02 07:13:32,742 - INFO -   Batch 2100/2527: Loss = 0.3149
2026-01-02 07:14:01,408 - INFO -   Batch 2150/2527: Loss = 0.3981
2026-01-02 07:14:30,232 - INFO -   Batch 2200/2527: Loss = 0.2446
2026-01-02 07:14:58,809 - INFO -   Batch 2250/2527: Loss = 0.2122
2026-01-02 07:15:27,666 - INFO -   Batch 2300/2527: Loss = 0.2710
2026-01-02 07:15:56,521 - INFO -   Batch 2350/2527: Loss = 0.2365
2026-01-02 07:16:24,999 - INFO -   Batch 2400/2527: Loss = 0.2778
2026-01-02 07:16:53,954 - INFO -   Batch 2450/2527: Loss = 0.4061
2026-01-02 07:17:22,823 - INFO -   Batch 2500/2527: Loss = 0.2189
2026-01-02 07:22:01,876 - INFO - Epoch 20/160: Train Loss: 0.3344, Val Loss: 0.9208, Val mIoU: 37.69%, Val PixelAcc: 77.10%
2026-01-02 07:22:01,878 - INFO -   → New best mIoU: 37.69%
2026-01-02 07:22:05,507 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 07:22:05,509 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 07:22:09,031 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_20.pth
2026-01-02 07:22:09,034 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_20.pth
2026-01-02 07:22:39,656 - INFO -   Batch 50/2527: Loss = 0.4504
2026-01-02 07:23:08,691 - INFO -   Batch 100/2527: Loss = 0.4084
2026-01-02 07:23:37,377 - INFO -   Batch 150/2527: Loss = 0.3992
2026-01-02 07:24:06,159 - INFO -   Batch 200/2527: Loss = 0.2887
2026-01-02 07:24:34,782 - INFO -   Batch 250/2527: Loss = 0.5134
2026-01-02 07:25:03,336 - INFO -   Batch 300/2527: Loss = 0.3865
2026-01-02 07:25:32,163 - INFO -   Batch 350/2527: Loss = 0.3462
2026-01-02 07:26:00,958 - INFO -   Batch 400/2527: Loss = 0.4053
2026-01-02 07:26:29,903 - INFO -   Batch 450/2527: Loss = 0.3321
2026-01-02 07:26:58,806 - INFO -   Batch 500/2527: Loss = 0.3365
2026-01-02 07:27:27,743 - INFO -   Batch 550/2527: Loss = 0.3013
2026-01-02 07:27:56,342 - INFO -   Batch 600/2527: Loss = 0.3895
2026-01-02 07:28:24,862 - INFO -   Batch 650/2527: Loss = 0.2672
2026-01-02 07:28:53,746 - INFO -   Batch 700/2527: Loss = 0.3943
2026-01-02 07:29:22,338 - INFO -   Batch 750/2527: Loss = 0.2678
2026-01-02 07:29:51,340 - INFO -   Batch 800/2527: Loss = 0.2517
2026-01-02 07:30:20,046 - INFO -   Batch 850/2527: Loss = 0.1641
2026-01-02 07:30:49,136 - INFO -   Batch 900/2527: Loss = 0.2663
2026-01-02 07:31:17,734 - INFO -   Batch 950/2527: Loss = 0.3466
2026-01-02 07:31:46,271 - INFO -   Batch 1000/2527: Loss = 0.3489
2026-01-02 07:32:15,125 - INFO -   Batch 1050/2527: Loss = 0.2964
2026-01-02 07:32:43,826 - INFO -   Batch 1100/2527: Loss = 0.3573
2026-01-02 07:33:12,663 - INFO -   Batch 1150/2527: Loss = 0.3293
2026-01-02 07:33:41,396 - INFO -   Batch 1200/2527: Loss = 0.3090
2026-01-02 07:34:09,966 - INFO -   Batch 1250/2527: Loss = 0.3339
2026-01-02 07:34:39,012 - INFO -   Batch 1300/2527: Loss = 0.2737
2026-01-02 07:35:07,690 - INFO -   Batch 1350/2527: Loss = 0.3827
2026-01-02 07:35:36,525 - INFO -   Batch 1400/2527: Loss = 0.2917
2026-01-02 07:36:05,122 - INFO -   Batch 1450/2527: Loss = 0.3480
2026-01-02 07:36:34,019 - INFO -   Batch 1500/2527: Loss = 0.3394
2026-01-02 07:37:02,670 - INFO -   Batch 1550/2527: Loss = 0.3296
2026-01-02 07:37:31,305 - INFO -   Batch 1600/2527: Loss = 0.3566
2026-01-02 07:38:00,266 - INFO -   Batch 1650/2527: Loss = 0.3497
2026-01-02 07:38:29,249 - INFO -   Batch 1700/2527: Loss = 0.3151
2026-01-02 07:38:58,180 - INFO -   Batch 1750/2527: Loss = 0.4593
2026-01-02 07:39:26,752 - INFO -   Batch 1800/2527: Loss = 0.2518
2026-01-02 07:39:55,280 - INFO -   Batch 1850/2527: Loss = 0.1799
2026-01-02 07:40:24,204 - INFO -   Batch 1900/2527: Loss = 0.3519
2026-01-02 07:40:52,850 - INFO -   Batch 1950/2527: Loss = 0.1905
2026-01-02 07:41:21,884 - INFO -   Batch 2000/2527: Loss = 0.2785
2026-01-02 07:41:50,593 - INFO -   Batch 2050/2527: Loss = 0.2649
2026-01-02 07:42:19,624 - INFO -   Batch 2100/2527: Loss = 0.2552
2026-01-02 07:42:48,336 - INFO -   Batch 2150/2527: Loss = 0.1583
2026-01-02 07:43:16,843 - INFO -   Batch 2200/2527: Loss = 0.2981
2026-01-02 07:43:45,677 - INFO -   Batch 2250/2527: Loss = 0.3131
2026-01-02 07:44:14,389 - INFO -   Batch 2300/2527: Loss = 0.3172
2026-01-02 07:44:43,262 - INFO -   Batch 2350/2527: Loss = 0.2086
2026-01-02 07:45:12,075 - INFO -   Batch 2400/2527: Loss = 0.2271
2026-01-02 07:45:41,023 - INFO -   Batch 2450/2527: Loss = 0.2559
2026-01-02 07:46:09,885 - INFO -   Batch 2500/2527: Loss = 0.2231
2026-01-02 07:50:56,000 - INFO - Epoch 21/160: Train Loss: 0.3145, Val Loss: 0.9425, Val mIoU: 37.08%, Val PixelAcc: 76.96%
2026-01-02 07:51:26,696 - INFO -   Batch 50/2527: Loss = 0.1750
2026-01-02 07:51:55,227 - INFO -   Batch 100/2527: Loss = 0.3301
2026-01-02 07:52:23,746 - INFO -   Batch 150/2527: Loss = 0.2288
2026-01-02 07:52:52,798 - INFO -   Batch 200/2527: Loss = 0.3409
2026-01-02 07:53:21,480 - INFO -   Batch 250/2527: Loss = 0.3825
2026-01-02 07:53:50,417 - INFO -   Batch 300/2527: Loss = 0.3984
2026-01-02 07:54:19,337 - INFO -   Batch 350/2527: Loss = 0.3236
2026-01-02 07:54:48,186 - INFO -   Batch 400/2527: Loss = 0.2149
2026-01-02 07:55:16,809 - INFO -   Batch 450/2527: Loss = 0.3179
2026-01-02 07:55:45,277 - INFO -   Batch 500/2527: Loss = 0.3178
2026-01-02 07:56:14,103 - INFO -   Batch 550/2527: Loss = 0.3504
2026-01-02 07:56:43,025 - INFO -   Batch 600/2527: Loss = 0.2429
2026-01-02 07:57:11,843 - INFO -   Batch 650/2527: Loss = 0.1491
2026-01-02 07:57:40,590 - INFO -   Batch 700/2527: Loss = 0.4524
2026-01-02 07:58:09,360 - INFO -   Batch 750/2527: Loss = 0.2392
2026-01-02 07:58:38,260 - INFO -   Batch 800/2527: Loss = 0.2085
2026-01-02 07:59:06,816 - INFO -   Batch 850/2527: Loss = 0.3998
2026-01-02 07:59:35,613 - INFO -   Batch 900/2527: Loss = 0.2427
2026-01-02 08:00:04,272 - INFO -   Batch 950/2527: Loss = 0.1473
2026-01-02 08:00:33,327 - INFO -   Batch 1000/2527: Loss = 0.3008
2026-01-02 08:01:01,882 - INFO -   Batch 1050/2527: Loss = 0.3049
2026-01-02 08:01:30,498 - INFO -   Batch 1100/2527: Loss = 0.3789
2026-01-02 08:01:59,601 - INFO -   Batch 1150/2527: Loss = 0.3405
2026-01-02 08:02:28,311 - INFO -   Batch 1200/2527: Loss = 0.2140
2026-01-02 08:02:57,080 - INFO -   Batch 1250/2527: Loss = 0.2961
2026-01-02 08:03:25,634 - INFO -   Batch 1300/2527: Loss = 0.1577
2026-01-02 08:03:54,457 - INFO -   Batch 1350/2527: Loss = 0.4325
2026-01-02 08:04:23,177 - INFO -   Batch 1400/2527: Loss = 0.3096
2026-01-02 08:04:51,778 - INFO -   Batch 1450/2527: Loss = 0.3905
2026-01-02 08:05:20,708 - INFO -   Batch 1500/2527: Loss = 0.3323
2026-01-02 08:05:49,590 - INFO -   Batch 1550/2527: Loss = 0.2986
2026-01-02 08:06:18,488 - INFO -   Batch 1600/2527: Loss = 0.1977
2026-01-02 08:06:47,089 - INFO -   Batch 1650/2527: Loss = 0.2069
2026-01-02 08:07:15,563 - INFO -   Batch 1700/2527: Loss = 0.3347
2026-01-02 08:07:44,455 - INFO -   Batch 1750/2527: Loss = 0.4874
2026-01-02 08:08:13,433 - INFO -   Batch 1800/2527: Loss = 0.3696
2026-01-02 08:08:42,257 - INFO -   Batch 1850/2527: Loss = 0.3966
2026-01-02 08:09:11,012 - INFO -   Batch 1900/2527: Loss = 0.3147
2026-01-02 08:09:40,153 - INFO -   Batch 1950/2527: Loss = 0.3277
2026-01-02 08:10:08,822 - INFO -   Batch 2000/2527: Loss = 0.2262
2026-01-02 08:10:37,291 - INFO -   Batch 2050/2527: Loss = 0.1870
2026-01-02 08:11:06,116 - INFO -   Batch 2100/2527: Loss = 0.2946
2026-01-02 08:11:34,768 - INFO -   Batch 2150/2527: Loss = 0.2688
2026-01-02 08:12:03,839 - INFO -   Batch 2200/2527: Loss = 0.3170
2026-01-02 08:12:32,463 - INFO -   Batch 2250/2527: Loss = 0.4143
2026-01-02 08:13:01,370 - INFO -   Batch 2300/2527: Loss = 0.3244
2026-01-02 08:13:30,144 - INFO -   Batch 2350/2527: Loss = 0.2574
2026-01-02 08:13:58,870 - INFO -   Batch 2400/2527: Loss = 0.3945
2026-01-02 08:14:27,664 - INFO -   Batch 2450/2527: Loss = 0.3074
2026-01-02 08:14:56,330 - INFO -   Batch 2500/2527: Loss = 0.3097
2026-01-02 08:19:37,127 - INFO - Epoch 22/160: Train Loss: 0.3023, Val Loss: 0.9438, Val mIoU: 37.31%, Val PixelAcc: 76.89%
2026-01-02 08:20:07,520 - INFO -   Batch 50/2527: Loss = 0.3823
2026-01-02 08:20:36,356 - INFO -   Batch 100/2527: Loss = 0.3597
2026-01-02 08:21:05,214 - INFO -   Batch 150/2527: Loss = 0.3818
2026-01-02 08:21:33,822 - INFO -   Batch 200/2527: Loss = 0.2610
2026-01-02 08:22:02,595 - INFO -   Batch 250/2527: Loss = 0.1553
2026-01-02 08:22:31,108 - INFO -   Batch 300/2527: Loss = 0.3346
2026-01-02 08:22:59,996 - INFO -   Batch 350/2527: Loss = 0.2985
2026-01-02 08:23:28,804 - INFO -   Batch 400/2527: Loss = 0.2737
2026-01-02 08:23:57,565 - INFO -   Batch 450/2527: Loss = 0.1444
2026-01-02 08:24:26,652 - INFO -   Batch 500/2527: Loss = 0.2496
2026-01-02 08:24:55,292 - INFO -   Batch 550/2527: Loss = 0.2947
2026-01-02 08:25:23,777 - INFO -   Batch 600/2527: Loss = 0.2056
2026-01-02 08:25:52,584 - INFO -   Batch 650/2527: Loss = 0.3806
2026-01-02 08:26:21,229 - INFO -   Batch 700/2527: Loss = 0.4213
2026-01-02 08:26:50,307 - INFO -   Batch 750/2527: Loss = 0.2913
2026-01-02 08:27:18,860 - INFO -   Batch 800/2527: Loss = 0.2350
2026-01-02 08:27:47,832 - INFO -   Batch 850/2527: Loss = 0.3551
2026-01-02 08:28:16,706 - INFO -   Batch 900/2527: Loss = 0.4035
2026-01-02 08:28:45,293 - INFO -   Batch 950/2527: Loss = 0.3295
2026-01-02 08:29:14,120 - INFO -   Batch 1000/2527: Loss = 0.3185
2026-01-02 08:29:42,669 - INFO -   Batch 1050/2527: Loss = 0.3373
2026-01-02 08:30:11,505 - INFO -   Batch 1100/2527: Loss = 0.1632
2026-01-02 08:30:40,348 - INFO -   Batch 1150/2527: Loss = 0.2849
2026-01-02 08:31:09,131 - INFO -   Batch 1200/2527: Loss = 0.3144
2026-01-02 08:31:37,789 - INFO -   Batch 1250/2527: Loss = 0.3843
2026-01-02 08:32:06,697 - INFO -   Batch 1300/2527: Loss = 0.2215
2026-01-02 08:32:35,595 - INFO -   Batch 1350/2527: Loss = 0.2917
2026-01-02 08:33:04,250 - INFO -   Batch 1400/2527: Loss = 0.1786
2026-01-02 08:33:33,066 - INFO -   Batch 1450/2527: Loss = 0.2095
2026-01-02 08:34:01,854 - INFO -   Batch 1500/2527: Loss = 0.2667
2026-01-02 08:34:30,603 - INFO -   Batch 1550/2527: Loss = 0.3715
2026-01-02 08:34:59,433 - INFO -   Batch 1600/2527: Loss = 0.1646
2026-01-02 08:35:28,160 - INFO -   Batch 1650/2527: Loss = 0.3166
2026-01-02 08:35:57,284 - INFO -   Batch 1700/2527: Loss = 0.2862
2026-01-02 08:36:25,960 - INFO -   Batch 1750/2527: Loss = 0.3019
2026-01-02 08:36:54,779 - INFO -   Batch 1800/2527: Loss = 0.3266
2026-01-02 08:37:23,350 - INFO -   Batch 1850/2527: Loss = 0.2521
2026-01-02 08:37:51,881 - INFO -   Batch 1900/2527: Loss = 0.3066
2026-01-02 08:38:20,956 - INFO -   Batch 1950/2527: Loss = 0.3052
2026-01-02 08:38:49,575 - INFO -   Batch 2000/2527: Loss = 0.2230
2026-01-02 08:39:18,500 - INFO -   Batch 2050/2527: Loss = 0.2297
2026-01-02 08:39:47,404 - INFO -   Batch 2100/2527: Loss = 0.2720
2026-01-02 08:40:15,985 - INFO -   Batch 2150/2527: Loss = 0.2701
2026-01-02 08:40:44,778 - INFO -   Batch 2200/2527: Loss = 0.3015
2026-01-02 08:41:13,447 - INFO -   Batch 2250/2527: Loss = 0.2719
2026-01-02 08:41:42,269 - INFO -   Batch 2300/2527: Loss = 0.1762
2026-01-02 08:42:11,143 - INFO -   Batch 2350/2527: Loss = 0.2573
2026-01-02 08:42:39,958 - INFO -   Batch 2400/2527: Loss = 0.2721
2026-01-02 08:43:08,651 - INFO -   Batch 2450/2527: Loss = 0.3026
2026-01-02 08:43:37,437 - INFO -   Batch 2500/2527: Loss = 0.3988
2026-01-02 08:48:17,187 - INFO - Epoch 23/160: Train Loss: 0.2924, Val Loss: 0.9549, Val mIoU: 37.08%, Val PixelAcc: 76.79%
2026-01-02 08:48:47,716 - INFO -   Batch 50/2527: Loss = 0.3801
2026-01-02 08:49:16,402 - INFO -   Batch 100/2527: Loss = 0.2330
2026-01-02 08:49:45,293 - INFO -   Batch 150/2527: Loss = 0.3057
2026-01-02 08:50:13,807 - INFO -   Batch 200/2527: Loss = 0.2473
2026-01-02 08:50:42,632 - INFO -   Batch 250/2527: Loss = 0.2442
2026-01-02 08:51:11,322 - INFO -   Batch 300/2527: Loss = 0.3537
2026-01-02 08:51:40,217 - INFO -   Batch 350/2527: Loss = 0.3050
2026-01-02 08:52:08,905 - INFO -   Batch 400/2527: Loss = 0.3353
2026-01-02 08:52:37,507 - INFO -   Batch 450/2527: Loss = 0.3132
2026-01-02 08:53:06,572 - INFO -   Batch 500/2527: Loss = 0.2865
2026-01-02 08:53:35,323 - INFO -   Batch 550/2527: Loss = 0.2963
2026-01-02 08:54:04,091 - INFO -   Batch 600/2527: Loss = 0.3340
2026-01-02 08:54:32,707 - INFO -   Batch 650/2527: Loss = 0.4688
2026-01-02 08:55:01,598 - INFO -   Batch 700/2527: Loss = 0.2024
2026-01-02 08:55:30,250 - INFO -   Batch 750/2527: Loss = 0.2762
2026-01-02 08:55:58,891 - INFO -   Batch 800/2527: Loss = 0.2547
2026-01-02 08:56:27,782 - INFO -   Batch 850/2527: Loss = 0.2783
2026-01-02 08:56:56,613 - INFO -   Batch 900/2527: Loss = 0.2423
2026-01-02 08:57:25,541 - INFO -   Batch 950/2527: Loss = 0.3174
2026-01-02 08:57:54,142 - INFO -   Batch 1000/2527: Loss = 0.3992
2026-01-02 08:58:22,682 - INFO -   Batch 1050/2527: Loss = 0.3461
2026-01-02 08:58:51,615 - INFO -   Batch 1100/2527: Loss = 0.2855
2026-01-02 08:59:20,435 - INFO -   Batch 1150/2527: Loss = 0.2791
2026-01-02 08:59:49,426 - INFO -   Batch 1200/2527: Loss = 0.3850
2026-01-02 09:00:18,134 - INFO -   Batch 1250/2527: Loss = 0.2943
2026-01-02 09:00:47,202 - INFO -   Batch 1300/2527: Loss = 0.2147
2026-01-02 09:01:15,912 - INFO -   Batch 1350/2527: Loss = 0.3099
2026-01-02 09:01:44,374 - INFO -   Batch 1400/2527: Loss = 0.2370
2026-01-02 09:02:13,244 - INFO -   Batch 1450/2527: Loss = 0.2147
2026-01-02 09:02:41,931 - INFO -   Batch 1500/2527: Loss = 0.4963
2026-01-02 09:03:10,784 - INFO -   Batch 1550/2527: Loss = 0.2169
2026-01-02 09:03:39,528 - INFO -   Batch 1600/2527: Loss = 0.1985
2026-01-02 09:04:08,421 - INFO -   Batch 1650/2527: Loss = 0.3035
2026-01-02 09:04:37,132 - INFO -   Batch 1700/2527: Loss = 0.3467
2026-01-02 09:05:05,901 - INFO -   Batch 1750/2527: Loss = 0.3143
2026-01-02 09:05:34,672 - INFO -   Batch 1800/2527: Loss = 0.3163
2026-01-02 09:06:03,373 - INFO -   Batch 1850/2527: Loss = 0.2396
2026-01-02 09:06:32,282 - INFO -   Batch 1900/2527: Loss = 0.3864
2026-01-02 09:07:00,939 - INFO -   Batch 1950/2527: Loss = 0.2686
2026-01-02 09:07:29,581 - INFO -   Batch 2000/2527: Loss = 0.2417
2026-01-02 09:07:58,522 - INFO -   Batch 2050/2527: Loss = 0.3999
2026-01-02 09:08:27,318 - INFO -   Batch 2100/2527: Loss = 0.2206
2026-01-02 09:08:56,259 - INFO -   Batch 2150/2527: Loss = 0.3481
2026-01-02 09:09:24,845 - INFO -   Batch 2200/2527: Loss = 0.2975
2026-01-02 09:09:53,715 - INFO -   Batch 2250/2527: Loss = 0.2997
2026-01-02 09:10:22,325 - INFO -   Batch 2300/2527: Loss = 0.2174
2026-01-02 09:10:50,838 - INFO -   Batch 2350/2527: Loss = 0.1758
2026-01-02 09:11:19,854 - INFO -   Batch 2400/2527: Loss = 0.2612
2026-01-02 09:11:48,602 - INFO -   Batch 2450/2527: Loss = 0.3231
2026-01-02 09:12:17,629 - INFO -   Batch 2500/2527: Loss = 0.3022
2026-01-02 09:17:03,013 - INFO - Epoch 24/160: Train Loss: 0.2844, Val Loss: 0.9363, Val mIoU: 37.31%, Val PixelAcc: 77.28%
2026-01-02 09:17:33,430 - INFO -   Batch 50/2527: Loss = 0.3141
2026-01-02 09:18:02,190 - INFO -   Batch 100/2527: Loss = 0.2500
2026-01-02 09:18:30,810 - INFO -   Batch 150/2527: Loss = 0.2432
2026-01-02 09:18:59,849 - INFO -   Batch 200/2527: Loss = 0.2160
2026-01-02 09:19:28,391 - INFO -   Batch 250/2527: Loss = 0.1834
2026-01-02 09:19:57,010 - INFO -   Batch 300/2527: Loss = 0.2602
2026-01-02 09:20:26,097 - INFO -   Batch 350/2527: Loss = 0.2351
2026-01-02 09:20:54,771 - INFO -   Batch 400/2527: Loss = 0.2157
2026-01-02 09:21:23,594 - INFO -   Batch 450/2527: Loss = 0.5202
2026-01-02 09:21:52,126 - INFO -   Batch 500/2527: Loss = 0.2975
2026-01-02 09:22:21,006 - INFO -   Batch 550/2527: Loss = 0.2739
2026-01-02 09:22:49,749 - INFO -   Batch 600/2527: Loss = 0.2755
2026-01-02 09:23:18,366 - INFO -   Batch 650/2527: Loss = 0.2025
2026-01-02 09:23:47,306 - INFO -   Batch 700/2527: Loss = 0.2858
2026-01-02 09:24:16,187 - INFO -   Batch 750/2527: Loss = 0.1596
2026-01-02 09:24:45,054 - INFO -   Batch 800/2527: Loss = 0.3152
2026-01-02 09:25:13,644 - INFO -   Batch 850/2527: Loss = 0.2502
2026-01-02 09:25:42,142 - INFO -   Batch 900/2527: Loss = 0.2154
2026-01-02 09:26:10,960 - INFO -   Batch 950/2527: Loss = 0.2247
2026-01-02 09:26:39,837 - INFO -   Batch 1000/2527: Loss = 0.1863
2026-01-02 09:27:08,697 - INFO -   Batch 1050/2527: Loss = 0.2676
2026-01-02 09:27:37,420 - INFO -   Batch 1100/2527: Loss = 0.3334
2026-01-02 09:28:06,519 - INFO -   Batch 1150/2527: Loss = 0.3887
2026-01-02 09:28:35,352 - INFO -   Batch 1200/2527: Loss = 0.1766
2026-01-02 09:29:03,821 - INFO -   Batch 1250/2527: Loss = 0.2547
2026-01-02 09:29:32,645 - INFO -   Batch 1300/2527: Loss = 0.2219
2026-01-02 09:30:01,297 - INFO -   Batch 1350/2527: Loss = 0.2699
2026-01-02 09:30:30,365 - INFO -   Batch 1400/2527: Loss = 0.2936
2026-01-02 09:30:58,939 - INFO -   Batch 1450/2527: Loss = 0.2094
2026-01-02 09:31:27,554 - INFO -   Batch 1500/2527: Loss = 0.2167
2026-01-02 09:31:56,668 - INFO -   Batch 1550/2527: Loss = 0.3274
2026-01-02 09:32:25,357 - INFO -   Batch 1600/2527: Loss = 0.2243
2026-01-02 09:32:54,149 - INFO -   Batch 1650/2527: Loss = 0.2126
2026-01-02 09:33:22,727 - INFO -   Batch 1700/2527: Loss = 0.2363
2026-01-02 09:33:51,541 - INFO -   Batch 1750/2527: Loss = 0.2713
2026-01-02 09:34:20,462 - INFO -   Batch 1800/2527: Loss = 0.1510
2026-01-02 09:34:48,943 - INFO -   Batch 1850/2527: Loss = 0.3035
2026-01-02 09:35:17,897 - INFO -   Batch 1900/2527: Loss = 0.2030
2026-01-02 09:35:46,836 - INFO -   Batch 1950/2527: Loss = 0.2055
2026-01-02 09:36:15,747 - INFO -   Batch 2000/2527: Loss = 0.1700
2026-01-02 09:36:44,331 - INFO -   Batch 2050/2527: Loss = 0.1940
2026-01-02 09:37:13,131 - INFO -   Batch 2100/2527: Loss = 0.3450
2026-01-02 09:37:41,735 - INFO -   Batch 2150/2527: Loss = 0.2023
2026-01-02 09:38:10,489 - INFO -   Batch 2200/2527: Loss = 0.2091
2026-01-02 09:38:39,312 - INFO -   Batch 2250/2527: Loss = 0.2193
2026-01-02 09:39:08,028 - INFO -   Batch 2300/2527: Loss = 0.2974
2026-01-02 09:39:37,147 - INFO -   Batch 2350/2527: Loss = 0.3014
2026-01-02 09:40:05,776 - INFO -   Batch 2400/2527: Loss = 0.1696
2026-01-02 09:40:34,262 - INFO -   Batch 2450/2527: Loss = 0.1979
2026-01-02 09:41:03,077 - INFO -   Batch 2500/2527: Loss = 0.1805
2026-01-02 09:45:43,167 - INFO - Epoch 25/160: Train Loss: 0.2750, Val Loss: 0.9439, Val mIoU: 38.47%, Val PixelAcc: 77.19%
2026-01-02 09:45:43,169 - INFO -   → New best mIoU: 38.47%
2026-01-02 09:45:47,152 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 09:45:47,154 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 09:46:17,562 - INFO -   Batch 50/2527: Loss = 0.1849
2026-01-02 09:46:46,539 - INFO -   Batch 100/2527: Loss = 0.3016
2026-01-02 09:47:15,082 - INFO -   Batch 150/2527: Loss = 0.3467
2026-01-02 09:47:43,867 - INFO -   Batch 200/2527: Loss = 0.1619
2026-01-02 09:48:12,489 - INFO -   Batch 250/2527: Loss = 0.2638
2026-01-02 09:48:41,337 - INFO -   Batch 300/2527: Loss = 0.2704
2026-01-02 09:49:10,185 - INFO -   Batch 350/2527: Loss = 0.2819
2026-01-02 09:49:38,987 - INFO -   Batch 400/2527: Loss = 0.1787
2026-01-02 09:50:07,633 - INFO -   Batch 450/2527: Loss = 0.3035
2026-01-02 09:50:36,546 - INFO -   Batch 500/2527: Loss = 0.4952
2026-01-02 09:51:05,440 - INFO -   Batch 550/2527: Loss = 0.1891
2026-01-02 09:51:33,956 - INFO -   Batch 600/2527: Loss = 0.3020
2026-01-02 09:52:02,760 - INFO -   Batch 650/2527: Loss = 0.2437
2026-01-02 09:52:31,418 - INFO -   Batch 700/2527: Loss = 0.2290
2026-01-02 09:53:00,194 - INFO -   Batch 750/2527: Loss = 0.2396
2026-01-02 09:53:29,042 - INFO -   Batch 800/2527: Loss = 0.3304
2026-01-02 09:53:57,700 - INFO -   Batch 850/2527: Loss = 0.3393
2026-01-02 09:54:26,821 - INFO -   Batch 900/2527: Loss = 0.3174
2026-01-02 09:54:55,435 - INFO -   Batch 950/2527: Loss = 0.2882
2026-01-02 09:55:24,234 - INFO -   Batch 1000/2527: Loss = 0.2531
2026-01-02 09:55:52,806 - INFO -   Batch 1050/2527: Loss = 0.2862
2026-01-02 09:56:21,316 - INFO -   Batch 1100/2527: Loss = 0.3589
2026-01-02 09:56:50,354 - INFO -   Batch 1150/2527: Loss = 0.3791
2026-01-02 09:57:19,019 - INFO -   Batch 1200/2527: Loss = 0.2222
2026-01-02 09:57:47,951 - INFO -   Batch 1250/2527: Loss = 0.2170
2026-01-02 09:58:17,026 - INFO -   Batch 1300/2527: Loss = 0.5991
2026-01-02 09:58:45,883 - INFO -   Batch 1350/2527: Loss = 0.2470
2026-01-02 09:59:14,397 - INFO -   Batch 1400/2527: Loss = 0.2757
2026-01-02 09:59:43,025 - INFO -   Batch 1450/2527: Loss = 0.2400
2026-01-02 10:00:11,887 - INFO -   Batch 1500/2527: Loss = 0.4547
2026-01-02 10:00:40,749 - INFO -   Batch 1550/2527: Loss = 0.1987
2026-01-02 10:01:09,566 - INFO -   Batch 1600/2527: Loss = 0.3528
2026-01-02 10:01:38,296 - INFO -   Batch 1650/2527: Loss = 0.1596
2026-01-02 10:02:07,048 - INFO -   Batch 1700/2527: Loss = 0.2168
2026-01-02 10:02:35,982 - INFO -   Batch 1750/2527: Loss = 0.3183
2026-01-02 10:03:04,506 - INFO -   Batch 1800/2527: Loss = 0.2603
2026-01-02 10:03:33,324 - INFO -   Batch 1850/2527: Loss = 0.2419
2026-01-02 10:04:01,953 - INFO -   Batch 1900/2527: Loss = 0.4877
2026-01-02 10:04:31,063 - INFO -   Batch 1950/2527: Loss = 0.2317
2026-01-02 10:04:59,756 - INFO -   Batch 2000/2527: Loss = 0.2377
2026-01-02 10:05:28,374 - INFO -   Batch 2050/2527: Loss = 0.2231
2026-01-02 10:05:57,490 - INFO -   Batch 2100/2527: Loss = 0.1830
2026-01-02 10:06:26,220 - INFO -   Batch 2150/2527: Loss = 0.2570
2026-01-02 10:06:55,001 - INFO -   Batch 2200/2527: Loss = 0.2612
2026-01-02 10:07:23,594 - INFO -   Batch 2250/2527: Loss = 0.1949
2026-01-02 10:07:52,110 - INFO -   Batch 2300/2527: Loss = 0.2297
2026-01-02 10:08:21,139 - INFO -   Batch 2350/2527: Loss = 0.2510
2026-01-02 10:08:49,783 - INFO -   Batch 2400/2527: Loss = 0.3540
2026-01-02 10:09:18,729 - INFO -   Batch 2450/2527: Loss = 0.2000
2026-01-02 10:09:47,579 - INFO -   Batch 2500/2527: Loss = 0.3011
2026-01-02 10:14:27,473 - INFO - Epoch 26/160: Train Loss: 0.2646, Val Loss: 0.9524, Val mIoU: 37.67%, Val PixelAcc: 77.15%
2026-01-02 10:14:58,432 - INFO -   Batch 50/2527: Loss = 0.2293
2026-01-02 10:15:27,227 - INFO -   Batch 100/2527: Loss = 0.2455
2026-01-02 10:15:56,113 - INFO -   Batch 150/2527: Loss = 0.2842
2026-01-02 10:16:24,616 - INFO -   Batch 200/2527: Loss = 0.2431
2026-01-02 10:16:53,491 - INFO -   Batch 250/2527: Loss = 0.2524
2026-01-02 10:17:22,039 - INFO -   Batch 300/2527: Loss = 0.3115
2026-01-02 10:17:50,723 - INFO -   Batch 350/2527: Loss = 0.2879
2026-01-02 10:18:19,673 - INFO -   Batch 400/2527: Loss = 0.2871
2026-01-02 10:18:48,351 - INFO -   Batch 450/2527: Loss = 0.2550
2026-01-02 10:19:17,415 - INFO -   Batch 500/2527: Loss = 0.2716
2026-01-02 10:19:46,120 - INFO -   Batch 550/2527: Loss = 0.2530
2026-01-02 10:20:14,605 - INFO -   Batch 600/2527: Loss = 0.2318
2026-01-02 10:20:43,458 - INFO -   Batch 650/2527: Loss = 0.2378
2026-01-02 10:21:12,092 - INFO -   Batch 700/2527: Loss = 0.2711
2026-01-02 10:21:40,981 - INFO -   Batch 750/2527: Loss = 0.2569
2026-01-02 10:22:09,738 - INFO -   Batch 800/2527: Loss = 0.3564
2026-01-02 10:22:38,700 - INFO -   Batch 850/2527: Loss = 0.1827
2026-01-02 10:23:07,686 - INFO -   Batch 900/2527: Loss = 0.2962
2026-01-02 10:23:36,305 - INFO -   Batch 950/2527: Loss = 0.2963
2026-01-02 10:24:05,132 - INFO -   Batch 1000/2527: Loss = 0.2878
2026-01-02 10:24:33,796 - INFO -   Batch 1050/2527: Loss = 0.3639
2026-01-02 10:25:02,744 - INFO -   Batch 1100/2527: Loss = 0.2092
2026-01-02 10:25:31,396 - INFO -   Batch 1150/2527: Loss = 0.2784
2026-01-02 10:26:00,035 - INFO -   Batch 1200/2527: Loss = 0.2986
2026-01-02 10:26:28,974 - INFO -   Batch 1250/2527: Loss = 0.3834
2026-01-02 10:26:57,833 - INFO -   Batch 1300/2527: Loss = 0.2968
2026-01-02 10:27:26,764 - INFO -   Batch 1350/2527: Loss = 0.1785
2026-01-02 10:27:55,339 - INFO -   Batch 1400/2527: Loss = 0.1751
2026-01-02 10:28:24,160 - INFO -   Batch 1450/2527: Loss = 0.2234
2026-01-02 10:28:52,885 - INFO -   Batch 1500/2527: Loss = 0.2548
2026-01-02 10:29:21,411 - INFO -   Batch 1550/2527: Loss = 0.1961
2026-01-02 10:29:50,420 - INFO -   Batch 1600/2527: Loss = 0.3232
2026-01-02 10:30:19,150 - INFO -   Batch 1650/2527: Loss = 0.3240
2026-01-02 10:30:48,221 - INFO -   Batch 1700/2527: Loss = 0.4859
2026-01-02 10:31:16,907 - INFO -   Batch 1750/2527: Loss = 0.3167
2026-01-02 10:31:45,707 - INFO -   Batch 1800/2527: Loss = 0.2537
2026-01-02 10:32:14,240 - INFO -   Batch 1850/2527: Loss = 0.3076
2026-01-02 10:32:42,875 - INFO -   Batch 1900/2527: Loss = 0.2495
2026-01-02 10:33:11,759 - INFO -   Batch 1950/2527: Loss = 0.3213
2026-01-02 10:33:40,523 - INFO -   Batch 2000/2527: Loss = 0.2503
2026-01-02 10:34:09,458 - INFO -   Batch 2050/2527: Loss = 0.1834
2026-01-02 10:34:38,250 - INFO -   Batch 2100/2527: Loss = 0.2451
2026-01-02 10:35:06,833 - INFO -   Batch 2150/2527: Loss = 0.2144
2026-01-02 10:35:35,634 - INFO -   Batch 2200/2527: Loss = 0.3343
2026-01-02 10:36:04,282 - INFO -   Batch 2250/2527: Loss = 0.2387
2026-01-02 10:36:33,187 - INFO -   Batch 2300/2527: Loss = 0.2432
2026-01-02 10:37:01,829 - INFO -   Batch 2350/2527: Loss = 0.2682
2026-01-02 10:37:30,782 - INFO -   Batch 2400/2527: Loss = 0.3322
2026-01-02 10:37:59,516 - INFO -   Batch 2450/2527: Loss = 0.3474
2026-01-02 10:38:28,254 - INFO -   Batch 2500/2527: Loss = 0.2062
2026-01-02 10:43:14,125 - INFO - Epoch 27/160: Train Loss: 0.2662, Val Loss: 0.9980, Val mIoU: 36.85%, Val PixelAcc: 76.84%
2026-01-02 10:43:44,429 - INFO -   Batch 50/2527: Loss = 0.3002
2026-01-02 10:44:12,848 - INFO -   Batch 100/2527: Loss = 0.2840
2026-01-02 10:44:41,721 - INFO -   Batch 150/2527: Loss = 0.2876
2026-01-02 10:45:10,575 - INFO -   Batch 200/2527: Loss = 0.1698
2026-01-02 10:45:39,397 - INFO -   Batch 250/2527: Loss = 0.1743
2026-01-02 10:46:08,054 - INFO -   Batch 300/2527: Loss = 0.1611
2026-01-02 10:46:37,164 - INFO -   Batch 350/2527: Loss = 0.2398
2026-01-02 10:47:05,886 - INFO -   Batch 400/2527: Loss = 0.3005
2026-01-02 10:47:34,364 - INFO -   Batch 450/2527: Loss = 0.2698
2026-01-02 10:48:03,181 - INFO -   Batch 500/2527: Loss = 0.2382
2026-01-02 10:48:31,814 - INFO -   Batch 550/2527: Loss = 0.2465
2026-01-02 10:49:00,861 - INFO -   Batch 600/2527: Loss = 0.2688
2026-01-02 10:49:29,438 - INFO -   Batch 650/2527: Loss = 0.3544
2026-01-02 10:49:58,359 - INFO -   Batch 700/2527: Loss = 0.2411
2026-01-02 10:50:27,141 - INFO -   Batch 750/2527: Loss = 0.2510
2026-01-02 10:50:55,823 - INFO -   Batch 800/2527: Loss = 0.2110
2026-01-02 10:51:24,623 - INFO -   Batch 850/2527: Loss = 0.2453
2026-01-02 10:51:53,213 - INFO -   Batch 900/2527: Loss = 0.2015
2026-01-02 10:52:22,086 - INFO -   Batch 950/2527: Loss = 0.2278
2026-01-02 10:52:51,116 - INFO -   Batch 1000/2527: Loss = 0.2408
2026-01-02 10:53:19,637 - INFO -   Batch 1050/2527: Loss = 0.4285
2026-01-02 10:53:48,595 - INFO -   Batch 1100/2527: Loss = 0.2366
2026-01-02 10:54:17,497 - INFO -   Batch 1150/2527: Loss = 0.3324
2026-01-02 10:54:46,422 - INFO -   Batch 1200/2527: Loss = 0.2524
2026-01-02 10:55:15,026 - INFO -   Batch 1250/2527: Loss = 0.2269
2026-01-02 10:55:43,845 - INFO -   Batch 1300/2527: Loss = 0.3347
2026-01-02 10:56:12,488 - INFO -   Batch 1350/2527: Loss = 0.2568
2026-01-02 10:56:41,213 - INFO -   Batch 1400/2527: Loss = 0.3184
2026-01-02 10:57:10,000 - INFO -   Batch 1450/2527: Loss = 0.2249
2026-01-02 10:57:38,757 - INFO -   Batch 1500/2527: Loss = 0.2668
2026-01-02 10:58:07,828 - INFO -   Batch 1550/2527: Loss = 0.2897
2026-01-02 10:58:36,559 - INFO -   Batch 1600/2527: Loss = 0.2981
2026-01-02 10:59:05,329 - INFO -   Batch 1650/2527: Loss = 0.2009
2026-01-02 10:59:33,821 - INFO -   Batch 1700/2527: Loss = 0.1890
2026-01-02 11:00:02,549 - INFO -   Batch 1750/2527: Loss = 0.2277
2026-01-02 11:00:31,636 - INFO -   Batch 1800/2527: Loss = 0.2470
2026-01-02 11:01:00,244 - INFO -   Batch 1850/2527: Loss = 0.3429
2026-01-02 11:01:29,201 - INFO -   Batch 1900/2527: Loss = 0.2560
2026-01-02 11:01:58,033 - INFO -   Batch 1950/2527: Loss = 0.2403
2026-01-02 11:02:26,634 - INFO -   Batch 2000/2527: Loss = 0.2592
2026-01-02 11:02:55,432 - INFO -   Batch 2050/2527: Loss = 0.1255
2026-01-02 11:03:24,017 - INFO -   Batch 2100/2527: Loss = 0.2128
2026-01-02 11:03:52,921 - INFO -   Batch 2150/2527: Loss = 0.1932
2026-01-02 11:04:21,713 - INFO -   Batch 2200/2527: Loss = 0.2537
2026-01-02 11:04:50,550 - INFO -   Batch 2250/2527: Loss = 0.2002
2026-01-02 11:05:19,319 - INFO -   Batch 2300/2527: Loss = 0.2401
2026-01-02 11:05:48,107 - INFO -   Batch 2350/2527: Loss = 0.2320
2026-01-02 11:06:16,978 - INFO -   Batch 2400/2527: Loss = 0.1839
2026-01-02 11:06:45,534 - INFO -   Batch 2450/2527: Loss = 0.2145
2026-01-02 11:07:14,329 - INFO -   Batch 2500/2527: Loss = 0.2406
2026-01-02 11:11:54,441 - INFO - Epoch 28/160: Train Loss: 0.2521, Val Loss: 0.9785, Val mIoU: 37.30%, Val PixelAcc: 77.43%
2026-01-02 11:12:24,744 - INFO -   Batch 50/2527: Loss = 0.2661
2026-01-02 11:12:53,754 - INFO -   Batch 100/2527: Loss = 0.2582
2026-01-02 11:13:22,437 - INFO -   Batch 150/2527: Loss = 0.3065
2026-01-02 11:13:51,185 - INFO -   Batch 200/2527: Loss = 0.3356
2026-01-02 11:14:19,761 - INFO -   Batch 250/2527: Loss = 0.2219
2026-01-02 11:14:48,270 - INFO -   Batch 300/2527: Loss = 0.3243
2026-01-02 11:15:17,329 - INFO -   Batch 350/2527: Loss = 0.1847
2026-01-02 11:15:45,901 - INFO -   Batch 400/2527: Loss = 0.1557
2026-01-02 11:16:14,891 - INFO -   Batch 450/2527: Loss = 0.1761
2026-01-02 11:16:43,758 - INFO -   Batch 500/2527: Loss = 0.1971
2026-01-02 11:17:12,348 - INFO -   Batch 550/2527: Loss = 0.2030
2026-01-02 11:17:41,185 - INFO -   Batch 600/2527: Loss = 0.2524
2026-01-02 11:18:09,751 - INFO -   Batch 650/2527: Loss = 0.3079
2026-01-02 11:18:38,646 - INFO -   Batch 700/2527: Loss = 0.2248
2026-01-02 11:19:07,664 - INFO -   Batch 750/2527: Loss = 0.1630
2026-01-02 11:19:36,480 - INFO -   Batch 800/2527: Loss = 0.2984
2026-01-02 11:20:05,224 - INFO -   Batch 850/2527: Loss = 0.2489
2026-01-02 11:20:33,964 - INFO -   Batch 900/2527: Loss = 0.1386
2026-01-02 11:21:02,837 - INFO -   Batch 950/2527: Loss = 0.1730
2026-01-02 11:21:31,452 - INFO -   Batch 1000/2527: Loss = 0.1419
2026-01-02 11:22:00,226 - INFO -   Batch 1050/2527: Loss = 0.1713
2026-01-02 11:22:28,908 - INFO -   Batch 1100/2527: Loss = 0.1391
2026-01-02 11:22:57,930 - INFO -   Batch 1150/2527: Loss = 0.2683
2026-01-02 11:23:26,514 - INFO -   Batch 1200/2527: Loss = 0.1795
2026-01-02 11:23:55,116 - INFO -   Batch 1250/2527: Loss = 0.1834
2026-01-02 11:24:24,218 - INFO -   Batch 1300/2527: Loss = 0.2603
2026-01-02 11:24:52,888 - INFO -   Batch 1350/2527: Loss = 0.1775
2026-01-02 11:25:21,733 - INFO -   Batch 1400/2527: Loss = 0.1899
2026-01-02 11:25:50,342 - INFO -   Batch 1450/2527: Loss = 0.1875
2026-01-02 11:26:18,863 - INFO -   Batch 1500/2527: Loss = 0.3270
2026-01-02 11:26:47,945 - INFO -   Batch 1550/2527: Loss = 0.2398
2026-01-02 11:27:16,525 - INFO -   Batch 1600/2527: Loss = 0.2790
2026-01-02 11:27:45,515 - INFO -   Batch 1650/2527: Loss = 0.1988
2026-01-02 11:28:14,354 - INFO -   Batch 1700/2527: Loss = 0.3213
2026-01-02 11:28:43,250 - INFO -   Batch 1750/2527: Loss = 0.3036
2026-01-02 11:29:11,857 - INFO -   Batch 1800/2527: Loss = 0.1712
2026-01-02 11:29:40,311 - INFO -   Batch 1850/2527: Loss = 0.2230
2026-01-02 11:30:09,139 - INFO -   Batch 1900/2527: Loss = 0.2569
2026-01-02 11:30:37,939 - INFO -   Batch 1950/2527: Loss = 0.2926
2026-01-02 11:31:06,733 - INFO -   Batch 2000/2527: Loss = 0.1607
2026-01-02 11:31:35,524 - INFO -   Batch 2050/2527: Loss = 0.2731
2026-01-02 11:32:04,587 - INFO -   Batch 2100/2527: Loss = 0.3757
2026-01-02 11:32:33,182 - INFO -   Batch 2150/2527: Loss = 0.2372
2026-01-02 11:33:01,736 - INFO -   Batch 2200/2527: Loss = 0.2621
2026-01-02 11:33:30,523 - INFO -   Batch 2250/2527: Loss = 0.2204
2026-01-02 11:33:59,110 - INFO -   Batch 2300/2527: Loss = 0.2390
2026-01-02 11:34:28,169 - INFO -   Batch 2350/2527: Loss = 0.2357
2026-01-02 11:34:56,765 - INFO -   Batch 2400/2527: Loss = 0.1787
2026-01-02 11:35:25,380 - INFO -   Batch 2450/2527: Loss = 0.2198
2026-01-02 11:35:54,493 - INFO -   Batch 2500/2527: Loss = 0.2717
2026-01-02 11:40:32,391 - INFO - Epoch 29/160: Train Loss: 0.2405, Val Loss: 0.9771, Val mIoU: 37.84%, Val PixelAcc: 77.20%
2026-01-02 11:41:03,171 - INFO -   Batch 50/2527: Loss = 0.2771
2026-01-02 11:41:31,842 - INFO -   Batch 100/2527: Loss = 0.2064
2026-01-02 11:42:00,490 - INFO -   Batch 150/2527: Loss = 0.2912
2026-01-02 11:42:29,290 - INFO -   Batch 200/2527: Loss = 0.3648
2026-01-02 11:42:57,958 - INFO -   Batch 250/2527: Loss = 0.1573
2026-01-02 11:43:26,876 - INFO -   Batch 300/2527: Loss = 0.2390
2026-01-02 11:43:55,728 - INFO -   Batch 350/2527: Loss = 0.2906
2026-01-02 11:44:24,354 - INFO -   Batch 400/2527: Loss = 0.2805
2026-01-02 11:44:53,244 - INFO -   Batch 450/2527: Loss = 0.2012
2026-01-02 11:45:22,125 - INFO -   Batch 500/2527: Loss = 0.2069
2026-01-02 11:45:50,995 - INFO -   Batch 550/2527: Loss = 0.2258
2026-01-02 11:46:19,603 - INFO -   Batch 600/2527: Loss = 0.2031
2026-01-02 11:46:48,463 - INFO -   Batch 650/2527: Loss = 0.1950
2026-01-02 11:47:17,134 - INFO -   Batch 700/2527: Loss = 0.1619
2026-01-02 11:47:45,661 - INFO -   Batch 750/2527: Loss = 0.3069
2026-01-02 11:48:14,678 - INFO -   Batch 800/2527: Loss = 0.3440
2026-01-02 11:48:43,371 - INFO -   Batch 850/2527: Loss = 0.1719
2026-01-02 11:49:12,438 - INFO -   Batch 900/2527: Loss = 0.2504
2026-01-02 11:49:41,122 - INFO -   Batch 950/2527: Loss = 0.3014
2026-01-02 11:50:09,945 - INFO -   Batch 1000/2527: Loss = 0.2528
2026-01-02 11:50:38,477 - INFO -   Batch 1050/2527: Loss = 0.2687
2026-01-02 11:51:07,220 - INFO -   Batch 1100/2527: Loss = 0.2349
2026-01-02 11:51:36,088 - INFO -   Batch 1150/2527: Loss = 0.2697
2026-01-02 11:52:04,918 - INFO -   Batch 1200/2527: Loss = 0.2010
2026-01-02 11:52:33,845 - INFO -   Batch 1250/2527: Loss = 0.2225
2026-01-02 11:53:02,692 - INFO -   Batch 1300/2527: Loss = 0.2592
2026-01-02 11:53:31,271 - INFO -   Batch 1350/2527: Loss = 0.2688
2026-01-02 11:54:00,038 - INFO -   Batch 1400/2527: Loss = 0.1908
2026-01-02 11:54:28,713 - INFO -   Batch 1450/2527: Loss = 0.2395
2026-01-02 11:54:57,596 - INFO -   Batch 1500/2527: Loss = 0.2110
2026-01-02 11:55:26,194 - INFO -   Batch 1550/2527: Loss = 0.3504
2026-01-02 11:55:55,176 - INFO -   Batch 1600/2527: Loss = 0.2374
2026-01-02 11:56:23,875 - INFO -   Batch 1650/2527: Loss = 0.2551
2026-01-02 11:56:52,627 - INFO -   Batch 1700/2527: Loss = 0.2159
2026-01-02 11:57:21,581 - INFO -   Batch 1750/2527: Loss = 0.2962
2026-01-02 11:57:50,090 - INFO -   Batch 1800/2527: Loss = 0.3063
2026-01-02 11:58:18,964 - INFO -   Batch 1850/2527: Loss = 0.1742
2026-01-02 11:58:47,588 - INFO -   Batch 1900/2527: Loss = 0.3039
2026-01-02 11:59:16,142 - INFO -   Batch 1950/2527: Loss = 0.1804
2026-01-02 11:59:45,113 - INFO -   Batch 2000/2527: Loss = 0.1982
2026-01-02 12:00:13,801 - INFO -   Batch 2050/2527: Loss = 0.1835
2026-01-02 12:00:42,850 - INFO -   Batch 2100/2527: Loss = 0.2157
2026-01-02 12:01:11,601 - INFO -   Batch 2150/2527: Loss = 0.2461
2026-01-02 12:01:40,416 - INFO -   Batch 2200/2527: Loss = 0.2146
2026-01-02 12:02:09,240 - INFO -   Batch 2250/2527: Loss = 0.2367
2026-01-02 12:02:37,788 - INFO -   Batch 2300/2527: Loss = 0.2951
2026-01-02 12:03:06,604 - INFO -   Batch 2350/2527: Loss = 0.1375
2026-01-02 12:03:35,419 - INFO -   Batch 2400/2527: Loss = 0.1916
2026-01-02 12:04:04,352 - INFO -   Batch 2450/2527: Loss = 0.2687
2026-01-02 12:04:33,178 - INFO -   Batch 2500/2527: Loss = 0.2197
2026-01-02 12:09:17,203 - INFO - Epoch 30/160: Train Loss: 0.2370, Val Loss: 0.9901, Val mIoU: 38.22%, Val PixelAcc: 77.43%
2026-01-02 12:09:20,694 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_30.pth
2026-01-02 12:09:20,696 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_30.pth
2026-01-02 12:09:50,963 - INFO -   Batch 50/2527: Loss = 0.2162
2026-01-02 12:10:19,714 - INFO -   Batch 100/2527: Loss = 0.2396
2026-01-02 12:10:48,255 - INFO -   Batch 150/2527: Loss = 0.2039
2026-01-02 12:11:17,337 - INFO -   Batch 200/2527: Loss = 0.1545
2026-01-02 12:11:45,829 - INFO -   Batch 250/2527: Loss = 0.3321
2026-01-02 12:12:14,579 - INFO -   Batch 300/2527: Loss = 0.2158
2026-01-02 12:12:43,697 - INFO -   Batch 350/2527: Loss = 0.2776
2026-01-02 12:13:12,347 - INFO -   Batch 400/2527: Loss = 0.2055
2026-01-02 12:13:41,176 - INFO -   Batch 450/2527: Loss = 0.2059
2026-01-02 12:14:09,787 - INFO -   Batch 500/2527: Loss = 0.2791
2026-01-02 12:14:38,300 - INFO -   Batch 550/2527: Loss = 0.2872
2026-01-02 12:15:07,342 - INFO -   Batch 600/2527: Loss = 0.1992
2026-01-02 12:15:35,938 - INFO -   Batch 650/2527: Loss = 0.3946
2026-01-02 12:16:04,850 - INFO -   Batch 700/2527: Loss = 0.1886
2026-01-02 12:16:33,764 - INFO -   Batch 750/2527: Loss = 0.1462
2026-01-02 12:17:02,631 - INFO -   Batch 800/2527: Loss = 0.1794
2026-01-02 12:17:31,216 - INFO -   Batch 850/2527: Loss = 0.1896
2026-01-02 12:17:59,686 - INFO -   Batch 900/2527: Loss = 0.2178
2026-01-02 12:18:28,558 - INFO -   Batch 950/2527: Loss = 0.2375
2026-01-02 12:18:57,441 - INFO -   Batch 1000/2527: Loss = 0.2046
2026-01-02 12:19:26,285 - INFO -   Batch 1050/2527: Loss = 0.1872
2026-01-02 12:19:55,176 - INFO -   Batch 1100/2527: Loss = 0.1939
2026-01-02 12:20:23,938 - INFO -   Batch 1150/2527: Loss = 0.1471
2026-01-02 12:20:52,845 - INFO -   Batch 1200/2527: Loss = 0.1347
2026-01-02 12:21:21,421 - INFO -   Batch 1250/2527: Loss = 0.2392
2026-01-02 12:21:50,236 - INFO -   Batch 1300/2527: Loss = 0.1979
2026-01-02 12:22:18,851 - INFO -   Batch 1350/2527: Loss = 0.2729
2026-01-02 12:22:47,909 - INFO -   Batch 1400/2527: Loss = 0.1795
2026-01-02 12:23:16,572 - INFO -   Batch 1450/2527: Loss = 0.2776
2026-01-02 12:23:45,230 - INFO -   Batch 1500/2527: Loss = 0.2098
2026-01-02 12:24:14,318 - INFO -   Batch 1550/2527: Loss = 0.2534
2026-01-02 12:24:42,995 - INFO -   Batch 1600/2527: Loss = 0.2117
2026-01-02 12:25:11,762 - INFO -   Batch 1650/2527: Loss = 0.2276
2026-01-02 12:25:40,386 - INFO -   Batch 1700/2527: Loss = 0.2828
2026-01-02 12:26:09,218 - INFO -   Batch 1750/2527: Loss = 0.1991
2026-01-02 12:26:38,131 - INFO -   Batch 1800/2527: Loss = 0.2335
2026-01-02 12:27:06,647 - INFO -   Batch 1850/2527: Loss = 0.2598
2026-01-02 12:27:35,632 - INFO -   Batch 1900/2527: Loss = 0.2877
2026-01-02 12:28:04,541 - INFO -   Batch 1950/2527: Loss = 0.2319
2026-01-02 12:28:33,451 - INFO -   Batch 2000/2527: Loss = 0.2044
2026-01-02 12:29:02,044 - INFO -   Batch 2050/2527: Loss = 0.2067
2026-01-02 12:29:30,523 - INFO -   Batch 2100/2527: Loss = 0.3028
2026-01-02 12:29:59,419 - INFO -   Batch 2150/2527: Loss = 0.2373
2026-01-02 12:30:28,256 - INFO -   Batch 2200/2527: Loss = 0.2128
2026-01-02 12:30:57,079 - INFO -   Batch 2250/2527: Loss = 0.2449
2026-01-02 12:31:25,771 - INFO -   Batch 2300/2527: Loss = 0.2748
2026-01-02 12:31:54,855 - INFO -   Batch 2350/2527: Loss = 0.2677
2026-01-02 12:32:23,553 - INFO -   Batch 2400/2527: Loss = 0.1638
2026-01-02 12:32:52,037 - INFO -   Batch 2450/2527: Loss = 0.2009
2026-01-02 12:33:20,817 - INFO -   Batch 2500/2527: Loss = 0.2034
2026-01-02 12:38:00,737 - INFO - Epoch 31/160: Train Loss: 0.2289, Val Loss: 1.0042, Val mIoU: 36.94%, Val PixelAcc: 77.17%
2026-01-02 12:38:30,786 - INFO -   Batch 50/2527: Loss = 0.2804
2026-01-02 12:38:59,864 - INFO -   Batch 100/2527: Loss = 0.2587
2026-01-02 12:39:28,481 - INFO -   Batch 150/2527: Loss = 0.3555
2026-01-02 12:39:57,298 - INFO -   Batch 200/2527: Loss = 0.2489
2026-01-02 12:40:25,874 - INFO -   Batch 250/2527: Loss = 0.2337
2026-01-02 12:40:54,712 - INFO -   Batch 300/2527: Loss = 0.1600
2026-01-02 12:41:23,492 - INFO -   Batch 350/2527: Loss = 0.1784
2026-01-02 12:41:51,973 - INFO -   Batch 400/2527: Loss = 0.2010
2026-01-02 12:42:20,916 - INFO -   Batch 450/2527: Loss = 0.2238
2026-01-02 12:42:49,751 - INFO -   Batch 500/2527: Loss = 0.1105
2026-01-02 12:43:18,656 - INFO -   Batch 550/2527: Loss = 0.2143
2026-01-02 12:43:47,186 - INFO -   Batch 600/2527: Loss = 0.2269
2026-01-02 12:44:15,987 - INFO -   Batch 650/2527: Loss = 0.1390
2026-01-02 12:44:44,528 - INFO -   Batch 700/2527: Loss = 0.2418
2026-01-02 12:45:13,399 - INFO -   Batch 750/2527: Loss = 0.2236
2026-01-02 12:45:42,228 - INFO -   Batch 800/2527: Loss = 0.2363
2026-01-02 12:46:11,141 - INFO -   Batch 850/2527: Loss = 0.2354
2026-01-02 12:46:40,202 - INFO -   Batch 900/2527: Loss = 0.1891
2026-01-02 12:47:08,927 - INFO -   Batch 950/2527: Loss = 0.2753
2026-01-02 12:47:37,389 - INFO -   Batch 1000/2527: Loss = 0.2125
2026-01-02 12:48:06,165 - INFO -   Batch 1050/2527: Loss = 0.2318
2026-01-02 12:48:34,858 - INFO -   Batch 1100/2527: Loss = 0.2013
2026-01-02 12:49:03,922 - INFO -   Batch 1150/2527: Loss = 0.2590
2026-01-02 12:49:32,507 - INFO -   Batch 1200/2527: Loss = 0.2155
2026-01-02 12:50:01,460 - INFO -   Batch 1250/2527: Loss = 0.2619
2026-01-02 12:50:30,292 - INFO -   Batch 1300/2527: Loss = 0.1678
2026-01-02 12:50:58,871 - INFO -   Batch 1350/2527: Loss = 0.2270
2026-01-02 12:51:27,711 - INFO -   Batch 1400/2527: Loss = 0.2714
2026-01-02 12:51:56,296 - INFO -   Batch 1450/2527: Loss = 0.1579
2026-01-02 12:52:25,169 - INFO -   Batch 1500/2527: Loss = 0.2738
2026-01-02 12:52:54,063 - INFO -   Batch 1550/2527: Loss = 0.2423
2026-01-02 12:53:22,919 - INFO -   Batch 1600/2527: Loss = 0.1928
2026-01-02 12:53:51,561 - INFO -   Batch 1650/2527: Loss = 0.1617
2026-01-02 12:54:20,474 - INFO -   Batch 1700/2527: Loss = 0.2342
2026-01-02 12:54:49,335 - INFO -   Batch 1750/2527: Loss = 0.2272
2026-01-02 12:55:17,892 - INFO -   Batch 1800/2527: Loss = 0.2314
2026-01-02 12:55:46,650 - INFO -   Batch 1850/2527: Loss = 0.1520
2026-01-02 12:56:15,331 - INFO -   Batch 1900/2527: Loss = 0.2291
2026-01-02 12:56:44,052 - INFO -   Batch 1950/2527: Loss = 0.2085
2026-01-02 12:57:12,846 - INFO -   Batch 2000/2527: Loss = 0.1887
2026-01-02 12:57:41,565 - INFO -   Batch 2050/2527: Loss = 0.2233
2026-01-02 12:58:10,670 - INFO -   Batch 2100/2527: Loss = 0.1646
2026-01-02 12:58:39,350 - INFO -   Batch 2150/2527: Loss = 0.2107
2026-01-02 12:59:08,158 - INFO -   Batch 2200/2527: Loss = 0.2741
2026-01-02 12:59:36,676 - INFO -   Batch 2250/2527: Loss = 0.1542
2026-01-02 13:00:05,222 - INFO -   Batch 2300/2527: Loss = 0.2495
2026-01-02 13:00:34,297 - INFO -   Batch 2350/2527: Loss = 0.2127
2026-01-02 13:01:02,827 - INFO -   Batch 2400/2527: Loss = 0.2421
2026-01-02 13:01:31,817 - INFO -   Batch 2450/2527: Loss = 0.1801
2026-01-02 13:02:00,643 - INFO -   Batch 2500/2527: Loss = 0.1978
2026-01-02 13:06:40,781 - INFO - Epoch 32/160: Train Loss: 0.2252, Val Loss: 1.0224, Val mIoU: 38.09%, Val PixelAcc: 77.33%
2026-01-02 13:07:11,413 - INFO -   Batch 50/2527: Loss = 0.3647
2026-01-02 13:07:40,179 - INFO -   Batch 100/2527: Loss = 0.2331
2026-01-02 13:08:09,035 - INFO -   Batch 150/2527: Loss = 0.1679
2026-01-02 13:08:37,630 - INFO -   Batch 200/2527: Loss = 0.3187
2026-01-02 13:09:06,128 - INFO -   Batch 250/2527: Loss = 0.2653
2026-01-02 13:09:35,000 - INFO -   Batch 300/2527: Loss = 0.1841
2026-01-02 13:10:03,691 - INFO -   Batch 350/2527: Loss = 0.1883
2026-01-02 13:10:32,707 - INFO -   Batch 400/2527: Loss = 0.3081
2026-01-02 13:11:01,569 - INFO -   Batch 450/2527: Loss = 0.2047
2026-01-02 13:11:30,629 - INFO -   Batch 500/2527: Loss = 0.1711
2026-01-02 13:11:59,227 - INFO -   Batch 550/2527: Loss = 0.2157
2026-01-02 13:12:27,835 - INFO -   Batch 600/2527: Loss = 0.2003
2026-01-02 13:12:56,738 - INFO -   Batch 650/2527: Loss = 0.2199
2026-01-02 13:13:25,396 - INFO -   Batch 700/2527: Loss = 0.2459
2026-01-02 13:13:54,258 - INFO -   Batch 750/2527: Loss = 0.1619
2026-01-02 13:14:22,996 - INFO -   Batch 800/2527: Loss = 0.1891
2026-01-02 13:14:51,578 - INFO -   Batch 850/2527: Loss = 0.1612
2026-01-02 13:15:20,650 - INFO -   Batch 900/2527: Loss = 0.2829
2026-01-02 13:15:49,335 - INFO -   Batch 950/2527: Loss = 0.1693
2026-01-02 13:16:18,109 - INFO -   Batch 1000/2527: Loss = 0.2892
2026-01-02 13:16:46,808 - INFO -   Batch 1050/2527: Loss = 0.2048
2026-01-02 13:17:15,696 - INFO -   Batch 1100/2527: Loss = 0.1601
2026-01-02 13:17:44,421 - INFO -   Batch 1150/2527: Loss = 0.2568
2026-01-02 13:18:13,068 - INFO -   Batch 1200/2527: Loss = 0.2316
2026-01-02 13:18:41,973 - INFO -   Batch 1250/2527: Loss = 0.2021
2026-01-02 13:19:10,873 - INFO -   Batch 1300/2527: Loss = 0.4615
2026-01-02 13:19:39,761 - INFO -   Batch 1350/2527: Loss = 0.2328
2026-01-02 13:20:08,322 - INFO -   Batch 1400/2527: Loss = 0.2396
2026-01-02 13:20:36,906 - INFO -   Batch 1450/2527: Loss = 0.1958
2026-01-02 13:21:05,815 - INFO -   Batch 1500/2527: Loss = 0.2768
2026-01-02 13:21:34,458 - INFO -   Batch 1550/2527: Loss = 0.2285
2026-01-02 13:22:03,460 - INFO -   Batch 1600/2527: Loss = 0.2538
2026-01-02 13:22:32,109 - INFO -   Batch 1650/2527: Loss = 0.1884
2026-01-02 13:23:01,208 - INFO -   Batch 1700/2527: Loss = 0.3133
2026-01-02 13:23:30,055 - INFO -   Batch 1750/2527: Loss = 0.2298
2026-01-02 13:23:58,573 - INFO -   Batch 1800/2527: Loss = 0.1583
2026-01-02 13:24:27,449 - INFO -   Batch 1850/2527: Loss = 0.2681
2026-01-02 13:24:56,112 - INFO -   Batch 1900/2527: Loss = 0.1844
2026-01-02 13:25:25,007 - INFO -   Batch 1950/2527: Loss = 0.2052
2026-01-02 13:25:53,850 - INFO -   Batch 2000/2527: Loss = 0.2604
2026-01-02 13:26:22,960 - INFO -   Batch 2050/2527: Loss = 0.1915
2026-01-02 13:26:52,099 - INFO -   Batch 2100/2527: Loss = 0.2113
2026-01-02 13:27:20,859 - INFO -   Batch 2150/2527: Loss = 0.2117
2026-01-02 13:27:49,693 - INFO -   Batch 2200/2527: Loss = 0.1362
2026-01-02 13:28:18,444 - INFO -   Batch 2250/2527: Loss = 0.3323
2026-01-02 13:28:47,543 - INFO -   Batch 2300/2527: Loss = 0.1863
2026-01-02 13:29:16,579 - INFO -   Batch 2350/2527: Loss = 0.1897
2026-01-02 13:29:45,294 - INFO -   Batch 2400/2527: Loss = 0.2129
2026-01-02 13:30:14,292 - INFO -   Batch 2450/2527: Loss = 0.2049
2026-01-02 13:30:43,286 - INFO -   Batch 2500/2527: Loss = 0.2369
2026-01-02 13:35:30,276 - INFO - Epoch 33/160: Train Loss: 0.2225, Val Loss: 1.0191, Val mIoU: 37.47%, Val PixelAcc: 77.30%
2026-01-02 13:36:00,512 - INFO -   Batch 50/2527: Loss = 0.1768
2026-01-02 13:36:28,970 - INFO -   Batch 100/2527: Loss = 0.1921
2026-01-02 13:36:57,816 - INFO -   Batch 150/2527: Loss = 0.1505
2026-01-02 13:37:26,648 - INFO -   Batch 200/2527: Loss = 0.1606
2026-01-02 13:37:55,466 - INFO -   Batch 250/2527: Loss = 0.1782
2026-01-02 13:38:24,274 - INFO -   Batch 300/2527: Loss = 0.2214
2026-01-02 13:38:53,048 - INFO -   Batch 350/2527: Loss = 0.2764
2026-01-02 13:39:21,957 - INFO -   Batch 400/2527: Loss = 0.1726
2026-01-02 13:39:50,615 - INFO -   Batch 450/2527: Loss = 0.1955
2026-01-02 13:40:19,508 - INFO -   Batch 500/2527: Loss = 0.1961
2026-01-02 13:40:48,419 - INFO -   Batch 550/2527: Loss = 0.2485
2026-01-02 13:41:17,483 - INFO -   Batch 600/2527: Loss = 0.1864
2026-01-02 13:41:46,088 - INFO -   Batch 650/2527: Loss = 0.2642
2026-01-02 13:42:14,745 - INFO -   Batch 700/2527: Loss = 0.1678
2026-01-02 13:42:43,896 - INFO -   Batch 750/2527: Loss = 0.1499
2026-01-02 13:43:12,578 - INFO -   Batch 800/2527: Loss = 0.1516
2026-01-02 13:43:41,420 - INFO -   Batch 850/2527: Loss = 0.2081
2026-01-02 13:44:10,089 - INFO -   Batch 900/2527: Loss = 0.2096
2026-01-02 13:44:38,939 - INFO -   Batch 950/2527: Loss = 0.2449
2026-01-02 13:45:07,708 - INFO -   Batch 1000/2527: Loss = 0.1801
2026-01-02 13:45:36,318 - INFO -   Batch 1050/2527: Loss = 0.2191
2026-01-02 13:46:05,264 - INFO -   Batch 1100/2527: Loss = 0.1949
2026-01-02 13:46:34,245 - INFO -   Batch 1150/2527: Loss = 0.2094
2026-01-02 13:47:03,176 - INFO -   Batch 1200/2527: Loss = 0.2773
2026-01-02 13:47:31,866 - INFO -   Batch 1250/2527: Loss = 0.1852
2026-01-02 13:48:00,355 - INFO -   Batch 1300/2527: Loss = 0.2566
2026-01-02 13:48:29,243 - INFO -   Batch 1350/2527: Loss = 0.1858
2026-01-02 13:48:58,103 - INFO -   Batch 1400/2527: Loss = 0.1982
2026-01-02 13:49:26,936 - INFO -   Batch 1450/2527: Loss = 0.2641
2026-01-02 13:49:55,662 - INFO -   Batch 1500/2527: Loss = 0.2160
2026-01-02 13:50:24,781 - INFO -   Batch 1550/2527: Loss = 0.1597
2026-01-02 13:50:53,548 - INFO -   Batch 1600/2527: Loss = 0.2867
2026-01-02 13:51:22,049 - INFO -   Batch 1650/2527: Loss = 0.2394
2026-01-02 13:51:50,900 - INFO -   Batch 1700/2527: Loss = 0.2405
2026-01-02 13:52:19,504 - INFO -   Batch 1750/2527: Loss = 0.2485
2026-01-02 13:52:48,654 - INFO -   Batch 1800/2527: Loss = 0.2216
2026-01-02 13:53:17,305 - INFO -   Batch 1850/2527: Loss = 0.1551
2026-01-02 13:53:46,334 - INFO -   Batch 1900/2527: Loss = 0.2235
2026-01-02 13:54:15,201 - INFO -   Batch 1950/2527: Loss = 0.1790
2026-01-02 13:54:43,914 - INFO -   Batch 2000/2527: Loss = 0.2167
2026-01-02 13:55:12,749 - INFO -   Batch 2050/2527: Loss = 0.1665
2026-01-02 13:55:41,385 - INFO -   Batch 2100/2527: Loss = 0.1834
2026-01-02 13:56:10,268 - INFO -   Batch 2150/2527: Loss = 0.1938
2026-01-02 13:56:39,188 - INFO -   Batch 2200/2527: Loss = 0.2236
2026-01-02 13:57:07,748 - INFO -   Batch 2250/2527: Loss = 0.2077
2026-01-02 13:57:36,780 - INFO -   Batch 2300/2527: Loss = 0.2065
2026-01-02 13:58:05,730 - INFO -   Batch 2350/2527: Loss = 0.2103
2026-01-02 13:58:34,692 - INFO -   Batch 2400/2527: Loss = 0.1905
2026-01-02 13:59:03,473 - INFO -   Batch 2450/2527: Loss = 0.2930
2026-01-02 13:59:32,344 - INFO -   Batch 2500/2527: Loss = 0.2181
2026-01-02 14:04:12,425 - INFO - Epoch 34/160: Train Loss: 0.2125, Val Loss: 0.9769, Val mIoU: 38.65%, Val PixelAcc: 77.76%
2026-01-02 14:04:12,427 - INFO -   → New best mIoU: 38.65%
2026-01-02 14:04:16,220 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 14:04:16,223 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 14:04:46,553 - INFO -   Batch 50/2527: Loss = 0.3319
2026-01-02 14:05:15,464 - INFO -   Batch 100/2527: Loss = 0.2996
2026-01-02 14:05:44,351 - INFO -   Batch 150/2527: Loss = 0.2509
2026-01-02 14:06:13,104 - INFO -   Batch 200/2527: Loss = 0.2481
2026-01-02 14:06:41,596 - INFO -   Batch 250/2527: Loss = 0.1359
2026-01-02 14:07:10,452 - INFO -   Batch 300/2527: Loss = 0.1939
2026-01-02 14:07:39,368 - INFO -   Batch 350/2527: Loss = 0.2109
2026-01-02 14:08:08,161 - INFO -   Batch 400/2527: Loss = 0.1897
2026-01-02 14:08:36,932 - INFO -   Batch 450/2527: Loss = 0.1235
2026-01-02 14:09:06,025 - INFO -   Batch 500/2527: Loss = 0.1828
2026-01-02 14:09:34,596 - INFO -   Batch 550/2527: Loss = 0.3143
2026-01-02 14:10:03,194 - INFO -   Batch 600/2527: Loss = 0.2539
2026-01-02 14:10:32,002 - INFO -   Batch 650/2527: Loss = 0.2208
2026-01-02 14:11:00,637 - INFO -   Batch 700/2527: Loss = 0.1804
2026-01-02 14:11:29,728 - INFO -   Batch 750/2527: Loss = 0.1850
2026-01-02 14:11:58,360 - INFO -   Batch 800/2527: Loss = 0.2867
2026-01-02 14:12:26,987 - INFO -   Batch 850/2527: Loss = 0.1903
2026-01-02 14:12:56,105 - INFO -   Batch 900/2527: Loss = 0.1832
2026-01-02 14:13:24,799 - INFO -   Batch 950/2527: Loss = 0.2379
2026-01-02 14:13:53,628 - INFO -   Batch 1000/2527: Loss = 0.2136
2026-01-02 14:14:22,259 - INFO -   Batch 1050/2527: Loss = 0.1740
2026-01-02 14:14:51,119 - INFO -   Batch 1100/2527: Loss = 0.1602
2026-01-02 14:15:19,953 - INFO -   Batch 1150/2527: Loss = 0.1904
2026-01-02 14:15:48,462 - INFO -   Batch 1200/2527: Loss = 0.1588
2026-01-02 14:16:17,394 - INFO -   Batch 1250/2527: Loss = 0.1776
2026-01-02 14:16:46,308 - INFO -   Batch 1300/2527: Loss = 0.1801
2026-01-02 14:17:15,202 - INFO -   Batch 1350/2527: Loss = 0.1940
2026-01-02 14:17:43,771 - INFO -   Batch 1400/2527: Loss = 0.1779
2026-01-02 14:18:12,260 - INFO -   Batch 1450/2527: Loss = 0.1866
2026-01-02 14:18:41,123 - INFO -   Batch 1500/2527: Loss = 0.2423
2026-01-02 14:19:09,959 - INFO -   Batch 1550/2527: Loss = 0.1978
2026-01-02 14:19:38,787 - INFO -   Batch 1600/2527: Loss = 0.1853
2026-01-02 14:20:07,478 - INFO -   Batch 1650/2527: Loss = 0.1721
2026-01-02 14:20:36,602 - INFO -   Batch 1700/2527: Loss = 0.1355
2026-01-02 14:21:05,235 - INFO -   Batch 1750/2527: Loss = 0.3538
2026-01-02 14:21:33,703 - INFO -   Batch 1800/2527: Loss = 0.2728
2026-01-02 14:22:02,527 - INFO -   Batch 1850/2527: Loss = 0.1433
2026-01-02 14:22:31,131 - INFO -   Batch 1900/2527: Loss = 0.2654
2026-01-02 14:23:00,257 - INFO -   Batch 1950/2527: Loss = 0.1828
2026-01-02 14:23:28,932 - INFO -   Batch 2000/2527: Loss = 0.1680
2026-01-02 14:23:57,909 - INFO -   Batch 2050/2527: Loss = 0.1061
2026-01-02 14:24:26,975 - INFO -   Batch 2100/2527: Loss = 0.2985
2026-01-02 14:24:55,560 - INFO -   Batch 2150/2527: Loss = 0.1478
2026-01-02 14:25:24,352 - INFO -   Batch 2200/2527: Loss = 0.1818
2026-01-02 14:25:52,974 - INFO -   Batch 2250/2527: Loss = 0.2419
2026-01-02 14:26:21,795 - INFO -   Batch 2300/2527: Loss = 0.2390
2026-01-02 14:26:50,675 - INFO -   Batch 2350/2527: Loss = 0.1683
2026-01-02 14:27:19,176 - INFO -   Batch 2400/2527: Loss = 0.2035
2026-01-02 14:27:48,143 - INFO -   Batch 2450/2527: Loss = 0.3084
2026-01-02 14:28:16,999 - INFO -   Batch 2500/2527: Loss = 0.1712
2026-01-02 14:32:55,094 - INFO - Epoch 35/160: Train Loss: 0.2058, Val Loss: 1.0498, Val mIoU: 38.02%, Val PixelAcc: 76.64%
2026-01-02 14:33:25,711 - INFO -   Batch 50/2527: Loss = 0.1763
2026-01-02 14:33:54,382 - INFO -   Batch 100/2527: Loss = 0.2769
2026-01-02 14:34:23,273 - INFO -   Batch 150/2527: Loss = 0.1515
2026-01-02 14:34:51,822 - INFO -   Batch 200/2527: Loss = 0.1823
2026-01-02 14:35:20,700 - INFO -   Batch 250/2527: Loss = 0.2188
2026-01-02 14:35:49,342 - INFO -   Batch 300/2527: Loss = 0.2519
2026-01-02 14:36:17,870 - INFO -   Batch 350/2527: Loss = 0.1973
2026-01-02 14:36:46,855 - INFO -   Batch 400/2527: Loss = 0.1855
2026-01-02 14:37:15,551 - INFO -   Batch 450/2527: Loss = 0.1974
2026-01-02 14:37:44,644 - INFO -   Batch 500/2527: Loss = 0.1362
2026-01-02 14:38:13,388 - INFO -   Batch 550/2527: Loss = 0.1446
2026-01-02 14:38:42,192 - INFO -   Batch 600/2527: Loss = 0.2791
2026-01-02 14:39:10,865 - INFO -   Batch 650/2527: Loss = 0.1842
2026-01-02 14:39:39,426 - INFO -   Batch 700/2527: Loss = 0.2044
2026-01-02 14:40:08,270 - INFO -   Batch 750/2527: Loss = 0.1523
2026-01-02 14:40:37,001 - INFO -   Batch 800/2527: Loss = 0.1466
2026-01-02 14:41:05,937 - INFO -   Batch 850/2527: Loss = 0.1915
2026-01-02 14:41:34,840 - INFO -   Batch 900/2527: Loss = 0.1559
2026-01-02 14:42:03,724 - INFO -   Batch 950/2527: Loss = 0.2294
2026-01-02 14:42:32,227 - INFO -   Batch 1000/2527: Loss = 0.1801
2026-01-02 14:43:00,853 - INFO -   Batch 1050/2527: Loss = 0.2375
2026-01-02 14:43:29,753 - INFO -   Batch 1100/2527: Loss = 0.2537
2026-01-02 14:43:58,393 - INFO -   Batch 1150/2527: Loss = 0.1679
2026-01-02 14:44:27,388 - INFO -   Batch 1200/2527: Loss = 0.1814
2026-01-02 14:44:56,055 - INFO -   Batch 1250/2527: Loss = 0.1385
2026-01-02 14:45:24,767 - INFO -   Batch 1300/2527: Loss = 0.2451
2026-01-02 14:45:53,685 - INFO -   Batch 1350/2527: Loss = 0.2243
2026-01-02 14:46:22,220 - INFO -   Batch 1400/2527: Loss = 0.2407
2026-01-02 14:46:51,071 - INFO -   Batch 1450/2527: Loss = 0.1481
2026-01-02 14:47:19,770 - INFO -   Batch 1500/2527: Loss = 0.1294
2026-01-02 14:47:48,628 - INFO -   Batch 1550/2527: Loss = 0.1791
2026-01-02 14:48:17,428 - INFO -   Batch 1600/2527: Loss = 0.1847
2026-01-02 14:48:46,000 - INFO -   Batch 1650/2527: Loss = 0.2007
2026-01-02 14:49:15,038 - INFO -   Batch 1700/2527: Loss = 0.2569
2026-01-02 14:49:43,932 - INFO -   Batch 1750/2527: Loss = 0.1861
2026-01-02 14:50:12,699 - INFO -   Batch 1800/2527: Loss = 0.1536
2026-01-02 14:50:41,417 - INFO -   Batch 1850/2527: Loss = 0.2249
2026-01-02 14:51:10,352 - INFO -   Batch 1900/2527: Loss = 0.1937
2026-01-02 14:51:38,885 - INFO -   Batch 1950/2527: Loss = 0.2284
2026-01-02 14:52:07,651 - INFO -   Batch 2000/2527: Loss = 0.1478
2026-01-02 14:52:36,617 - INFO -   Batch 2050/2527: Loss = 0.2703
2026-01-02 14:53:05,479 - INFO -   Batch 2100/2527: Loss = 0.2249
2026-01-02 14:53:34,446 - INFO -   Batch 2150/2527: Loss = 0.1881
2026-01-02 14:54:02,990 - INFO -   Batch 2200/2527: Loss = 0.1117
2026-01-02 14:54:31,504 - INFO -   Batch 2250/2527: Loss = 0.1869
2026-01-02 14:55:00,430 - INFO -   Batch 2300/2527: Loss = 0.1769
2026-01-02 14:55:29,117 - INFO -   Batch 2350/2527: Loss = 0.1668
2026-01-02 14:55:58,106 - INFO -   Batch 2400/2527: Loss = 0.1840
2026-01-02 14:56:26,899 - INFO -   Batch 2450/2527: Loss = 0.1902
2026-01-02 14:56:55,965 - INFO -   Batch 2500/2527: Loss = 0.1497
2026-01-02 15:01:41,620 - INFO - Epoch 36/160: Train Loss: 0.2037, Val Loss: 1.0520, Val mIoU: 38.32%, Val PixelAcc: 76.90%
2026-01-02 15:02:12,084 - INFO -   Batch 50/2527: Loss = 0.1706
2026-01-02 15:02:40,881 - INFO -   Batch 100/2527: Loss = 0.1648
2026-01-02 15:03:09,490 - INFO -   Batch 150/2527: Loss = 0.2455
2026-01-02 15:03:38,200 - INFO -   Batch 200/2527: Loss = 0.2316
2026-01-02 15:04:07,034 - INFO -   Batch 250/2527: Loss = 0.1642
2026-01-02 15:04:35,769 - INFO -   Batch 300/2527: Loss = 0.1914
2026-01-02 15:05:04,867 - INFO -   Batch 350/2527: Loss = 0.1910
2026-01-02 15:05:33,528 - INFO -   Batch 400/2527: Loss = 0.1810
2026-01-02 15:06:02,323 - INFO -   Batch 450/2527: Loss = 0.1436
2026-01-02 15:06:30,986 - INFO -   Batch 500/2527: Loss = 0.1744
2026-01-02 15:06:59,504 - INFO -   Batch 550/2527: Loss = 0.1873
2026-01-02 15:07:28,575 - INFO -   Batch 600/2527: Loss = 0.2374
2026-01-02 15:07:57,218 - INFO -   Batch 650/2527: Loss = 0.2702
2026-01-02 15:08:26,197 - INFO -   Batch 700/2527: Loss = 0.2114
2026-01-02 15:08:55,090 - INFO -   Batch 750/2527: Loss = 0.1510
2026-01-02 15:09:24,014 - INFO -   Batch 800/2527: Loss = 0.1920
2026-01-02 15:09:52,508 - INFO -   Batch 850/2527: Loss = 0.2395
2026-01-02 15:10:21,044 - INFO -   Batch 900/2527: Loss = 0.1496
2026-01-02 15:10:49,939 - INFO -   Batch 950/2527: Loss = 0.1940
2026-01-02 15:11:18,793 - INFO -   Batch 1000/2527: Loss = 0.2244
2026-01-02 15:11:47,617 - INFO -   Batch 1050/2527: Loss = 0.2078
2026-01-02 15:12:16,303 - INFO -   Batch 1100/2527: Loss = 0.1742
2026-01-02 15:12:45,065 - INFO -   Batch 1150/2527: Loss = 0.2857
2026-01-02 15:13:14,006 - INFO -   Batch 1200/2527: Loss = 0.2674
2026-01-02 15:13:42,608 - INFO -   Batch 1250/2527: Loss = 0.1832
2026-01-02 15:14:11,385 - INFO -   Batch 1300/2527: Loss = 0.1425
2026-01-02 15:14:39,989 - INFO -   Batch 1350/2527: Loss = 0.1891
2026-01-02 15:15:09,047 - INFO -   Batch 1400/2527: Loss = 0.1683
2026-01-02 15:15:37,681 - INFO -   Batch 1450/2527: Loss = 0.1109
2026-01-02 15:16:06,294 - INFO -   Batch 1500/2527: Loss = 0.1690
2026-01-02 15:16:35,358 - INFO -   Batch 1550/2527: Loss = 0.1996
2026-01-02 15:17:04,031 - INFO -   Batch 1600/2527: Loss = 0.1860
2026-01-02 15:17:32,847 - INFO -   Batch 1650/2527: Loss = 0.1919
2026-01-02 15:18:01,476 - INFO -   Batch 1700/2527: Loss = 0.1391
2026-01-02 15:18:30,009 - INFO -   Batch 1750/2527: Loss = 0.1443
2026-01-02 15:18:59,168 - INFO -   Batch 1800/2527: Loss = 0.1767
2026-01-02 15:19:27,912 - INFO -   Batch 1850/2527: Loss = 0.1960
2026-01-02 15:19:56,912 - INFO -   Batch 1900/2527: Loss = 0.1824
2026-01-02 15:20:25,865 - INFO -   Batch 1950/2527: Loss = 0.2141
2026-01-02 15:20:54,733 - INFO -   Batch 2000/2527: Loss = 0.2987
2026-01-02 15:21:23,423 - INFO -   Batch 2050/2527: Loss = 0.2585
2026-01-02 15:21:51,910 - INFO -   Batch 2100/2527: Loss = 0.1258
2026-01-02 15:22:20,757 - INFO -   Batch 2150/2527: Loss = 0.2152
2026-01-02 15:22:49,665 - INFO -   Batch 2200/2527: Loss = 0.2224
2026-01-02 15:23:18,414 - INFO -   Batch 2250/2527: Loss = 0.2172
2026-01-02 15:23:47,214 - INFO -   Batch 2300/2527: Loss = 0.1929
2026-01-02 15:24:16,316 - INFO -   Batch 2350/2527: Loss = 0.1423
2026-01-02 15:24:44,932 - INFO -   Batch 2400/2527: Loss = 0.2114
2026-01-02 15:25:13,567 - INFO -   Batch 2450/2527: Loss = 0.2797
2026-01-02 15:25:42,413 - INFO -   Batch 2500/2527: Loss = 0.1375
2026-01-02 15:30:22,636 - INFO - Epoch 37/160: Train Loss: 0.1988, Val Loss: 1.0380, Val mIoU: 38.15%, Val PixelAcc: 77.54%
2026-01-02 15:30:52,920 - INFO -   Batch 50/2527: Loss = 0.1449
2026-01-02 15:31:22,007 - INFO -   Batch 100/2527: Loss = 0.2272
2026-01-02 15:31:50,618 - INFO -   Batch 150/2527: Loss = 0.1745
2026-01-02 15:32:19,411 - INFO -   Batch 200/2527: Loss = 0.2122
2026-01-02 15:32:48,016 - INFO -   Batch 250/2527: Loss = 0.1524
2026-01-02 15:33:16,902 - INFO -   Batch 300/2527: Loss = 0.1455
2026-01-02 15:33:45,835 - INFO -   Batch 350/2527: Loss = 0.1992
2026-01-02 15:34:14,309 - INFO -   Batch 400/2527: Loss = 0.1859
2026-01-02 15:34:43,264 - INFO -   Batch 450/2527: Loss = 0.1792
2026-01-02 15:35:12,217 - INFO -   Batch 500/2527: Loss = 0.2037
2026-01-02 15:35:41,118 - INFO -   Batch 550/2527: Loss = 0.1458
2026-01-02 15:36:09,718 - INFO -   Batch 600/2527: Loss = 0.1547
2026-01-02 15:36:38,214 - INFO -   Batch 650/2527: Loss = 0.1918
2026-01-02 15:37:07,070 - INFO -   Batch 700/2527: Loss = 0.1462
2026-01-02 15:37:35,943 - INFO -   Batch 750/2527: Loss = 0.1357
2026-01-02 15:38:04,760 - INFO -   Batch 800/2527: Loss = 0.1490
2026-01-02 15:38:33,454 - INFO -   Batch 850/2527: Loss = 0.1739
2026-01-02 15:39:02,578 - INFO -   Batch 900/2527: Loss = 0.2083
2026-01-02 15:39:31,276 - INFO -   Batch 950/2527: Loss = 0.2213
2026-01-02 15:39:59,771 - INFO -   Batch 1000/2527: Loss = 0.1123
2026-01-02 15:40:28,618 - INFO -   Batch 1050/2527: Loss = 0.1454
2026-01-02 15:40:57,234 - INFO -   Batch 1100/2527: Loss = 0.1131
2026-01-02 15:41:26,327 - INFO -   Batch 1150/2527: Loss = 0.2900
2026-01-02 15:41:54,900 - INFO -   Batch 1200/2527: Loss = 0.2349
2026-01-02 15:42:23,853 - INFO -   Batch 1250/2527: Loss = 0.2589
2026-01-02 15:42:52,625 - INFO -   Batch 1300/2527: Loss = 0.2044
2026-01-02 15:43:21,367 - INFO -   Batch 1350/2527: Loss = 0.1824
2026-01-02 15:43:50,231 - INFO -   Batch 1400/2527: Loss = 0.1224
2026-01-02 15:44:18,854 - INFO -   Batch 1450/2527: Loss = 0.1901
2026-01-02 15:44:47,678 - INFO -   Batch 1500/2527: Loss = 0.1287
2026-01-02 15:45:16,746 - INFO -   Batch 1550/2527: Loss = 0.1607
2026-01-02 15:45:45,265 - INFO -   Batch 1600/2527: Loss = 0.1736
2026-01-02 15:46:14,171 - INFO -   Batch 1650/2527: Loss = 0.1726
2026-01-02 15:46:43,086 - INFO -   Batch 1700/2527: Loss = 0.5732
2026-01-02 15:47:12,000 - INFO -   Batch 1750/2527: Loss = 0.1461
2026-01-02 15:47:40,629 - INFO -   Batch 1800/2527: Loss = 0.2387
2026-01-02 15:48:09,430 - INFO -   Batch 1850/2527: Loss = 0.1604
2026-01-02 15:48:38,087 - INFO -   Batch 1900/2527: Loss = 0.1882
2026-01-02 15:49:06,825 - INFO -   Batch 1950/2527: Loss = 0.1875
2026-01-02 15:49:35,697 - INFO -   Batch 2000/2527: Loss = 0.2716
2026-01-02 15:50:04,393 - INFO -   Batch 2050/2527: Loss = 0.2228
2026-01-02 15:50:33,526 - INFO -   Batch 2100/2527: Loss = 0.1430
2026-01-02 15:51:02,217 - INFO -   Batch 2150/2527: Loss = 0.1937
2026-01-02 15:51:31,048 - INFO -   Batch 2200/2527: Loss = 0.2215
2026-01-02 15:51:59,544 - INFO -   Batch 2250/2527: Loss = 0.1812
2026-01-02 15:52:28,266 - INFO -   Batch 2300/2527: Loss = 0.2143
2026-01-02 15:52:57,356 - INFO -   Batch 2350/2527: Loss = 0.1382
2026-01-02 15:53:26,033 - INFO -   Batch 2400/2527: Loss = 0.1576
2026-01-02 15:53:55,003 - INFO -   Batch 2450/2527: Loss = 0.2146
2026-01-02 15:54:23,936 - INFO -   Batch 2500/2527: Loss = 0.1669
2026-01-02 15:59:03,735 - INFO - Epoch 38/160: Train Loss: 0.1885, Val Loss: 1.0351, Val mIoU: 38.13%, Val PixelAcc: 77.60%
2026-01-02 15:59:34,644 - INFO -   Batch 50/2527: Loss = 0.1446
2026-01-02 16:00:03,426 - INFO -   Batch 100/2527: Loss = 0.1405
2026-01-02 16:00:32,326 - INFO -   Batch 150/2527: Loss = 0.1291
2026-01-02 16:01:00,780 - INFO -   Batch 200/2527: Loss = 0.2051
2026-01-02 16:01:29,364 - INFO -   Batch 250/2527: Loss = 0.1568
2026-01-02 16:01:58,248 - INFO -   Batch 300/2527: Loss = 0.1691
2026-01-02 16:02:26,951 - INFO -   Batch 350/2527: Loss = 0.1856
2026-01-02 16:02:55,936 - INFO -   Batch 400/2527: Loss = 0.2317
2026-01-02 16:03:24,602 - INFO -   Batch 450/2527: Loss = 0.1963
2026-01-02 16:03:53,338 - INFO -   Batch 500/2527: Loss = 0.2292
2026-01-02 16:04:22,214 - INFO -   Batch 550/2527: Loss = 0.1539
2026-01-02 16:04:50,819 - INFO -   Batch 600/2527: Loss = 0.2594
2026-01-02 16:05:19,661 - INFO -   Batch 650/2527: Loss = 0.3425
2026-01-02 16:05:48,305 - INFO -   Batch 700/2527: Loss = 0.1889
2026-01-02 16:06:17,179 - INFO -   Batch 750/2527: Loss = 0.2248
2026-01-02 16:06:45,912 - INFO -   Batch 800/2527: Loss = 0.1804
2026-01-02 16:07:14,546 - INFO -   Batch 850/2527: Loss = 0.1541
2026-01-02 16:07:43,613 - INFO -   Batch 900/2527: Loss = 0.2070
2026-01-02 16:08:12,265 - INFO -   Batch 950/2527: Loss = 0.1722
2026-01-02 16:08:41,087 - INFO -   Batch 1000/2527: Loss = 0.1908
2026-01-02 16:09:09,725 - INFO -   Batch 1050/2527: Loss = 0.1878
2026-01-02 16:09:38,284 - INFO -   Batch 1100/2527: Loss = 0.2060
2026-01-02 16:10:07,197 - INFO -   Batch 1150/2527: Loss = 0.2192
2026-01-02 16:10:36,114 - INFO -   Batch 1200/2527: Loss = 0.2055
2026-01-02 16:11:05,065 - INFO -   Batch 1250/2527: Loss = 0.1603
2026-01-02 16:11:33,926 - INFO -   Batch 1300/2527: Loss = 0.1379
2026-01-02 16:12:02,822 - INFO -   Batch 1350/2527: Loss = 0.2627
2026-01-02 16:12:31,444 - INFO -   Batch 1400/2527: Loss = 0.1973
2026-01-02 16:12:59,972 - INFO -   Batch 1450/2527: Loss = 0.1596
2026-01-02 16:13:28,876 - INFO -   Batch 1500/2527: Loss = 0.2445
2026-01-02 16:13:57,563 - INFO -   Batch 1550/2527: Loss = 0.1907
2026-01-02 16:14:26,527 - INFO -   Batch 1600/2527: Loss = 0.1706
2026-01-02 16:14:55,238 - INFO -   Batch 1650/2527: Loss = 0.1264
2026-01-02 16:15:24,279 - INFO -   Batch 1700/2527: Loss = 0.1644
2026-01-02 16:15:53,043 - INFO -   Batch 1750/2527: Loss = 0.1492
2026-01-02 16:16:21,537 - INFO -   Batch 1800/2527: Loss = 0.1679
2026-01-02 16:16:50,422 - INFO -   Batch 1850/2527: Loss = 0.1893
2026-01-02 16:17:19,205 - INFO -   Batch 1900/2527: Loss = 0.2375
2026-01-02 16:17:48,122 - INFO -   Batch 1950/2527: Loss = 0.1110
2026-01-02 16:18:16,912 - INFO -   Batch 2000/2527: Loss = 0.2215
2026-01-02 16:18:45,546 - INFO -   Batch 2050/2527: Loss = 0.2251
2026-01-02 16:19:14,688 - INFO -   Batch 2100/2527: Loss = 0.2019
2026-01-02 16:19:43,325 - INFO -   Batch 2150/2527: Loss = 0.1896
2026-01-02 16:20:12,178 - INFO -   Batch 2200/2527: Loss = 0.2029
2026-01-02 16:20:40,894 - INFO -   Batch 2250/2527: Loss = 0.2080
2026-01-02 16:21:09,821 - INFO -   Batch 2300/2527: Loss = 0.1675
2026-01-02 16:21:38,485 - INFO -   Batch 2350/2527: Loss = 0.2047
2026-01-02 16:22:07,175 - INFO -   Batch 2400/2527: Loss = 0.1732
2026-01-02 16:22:36,130 - INFO -   Batch 2450/2527: Loss = 0.1881
2026-01-02 16:23:05,045 - INFO -   Batch 2500/2527: Loss = 0.2186
2026-01-02 16:27:50,510 - INFO - Epoch 39/160: Train Loss: 0.1953, Val Loss: 1.0408, Val mIoU: 38.12%, Val PixelAcc: 77.40%
2026-01-02 16:28:21,058 - INFO -   Batch 50/2527: Loss = 0.1718
2026-01-02 16:28:49,643 - INFO -   Batch 100/2527: Loss = 0.1820
2026-01-02 16:29:18,518 - INFO -   Batch 150/2527: Loss = 0.1671
2026-01-02 16:29:47,384 - INFO -   Batch 200/2527: Loss = 0.1567
2026-01-02 16:30:16,211 - INFO -   Batch 250/2527: Loss = 0.1887
2026-01-02 16:30:44,911 - INFO -   Batch 300/2527: Loss = 0.1698
2026-01-02 16:31:13,699 - INFO -   Batch 350/2527: Loss = 0.1539
2026-01-02 16:31:42,619 - INFO -   Batch 400/2527: Loss = 0.1593
2026-01-02 16:32:11,270 - INFO -   Batch 450/2527: Loss = 0.1426
2026-01-02 16:32:40,049 - INFO -   Batch 500/2527: Loss = 0.2083
2026-01-02 16:33:08,676 - INFO -   Batch 550/2527: Loss = 0.1878
2026-01-02 16:33:37,740 - INFO -   Batch 600/2527: Loss = 0.1766
2026-01-02 16:34:06,264 - INFO -   Batch 650/2527: Loss = 0.1474
2026-01-02 16:34:35,041 - INFO -   Batch 700/2527: Loss = 0.2666
2026-01-02 16:35:04,187 - INFO -   Batch 750/2527: Loss = 0.2379
2026-01-02 16:35:32,866 - INFO -   Batch 800/2527: Loss = 0.1582
2026-01-02 16:36:01,700 - INFO -   Batch 850/2527: Loss = 0.1301
2026-01-02 16:36:30,234 - INFO -   Batch 900/2527: Loss = 0.1608
2026-01-02 16:36:58,777 - INFO -   Batch 950/2527: Loss = 0.2405
2026-01-02 16:37:27,867 - INFO -   Batch 1000/2527: Loss = 0.1437
2026-01-02 16:37:56,432 - INFO -   Batch 1050/2527: Loss = 0.1527
2026-01-02 16:38:25,449 - INFO -   Batch 1100/2527: Loss = 0.1432
2026-01-02 16:38:54,373 - INFO -   Batch 1150/2527: Loss = 0.1048
2026-01-02 16:39:23,279 - INFO -   Batch 1200/2527: Loss = 0.2218
2026-01-02 16:39:52,086 - INFO -   Batch 1250/2527: Loss = 0.1858
2026-01-02 16:40:20,588 - INFO -   Batch 1300/2527: Loss = 0.1572
2026-01-02 16:40:49,501 - INFO -   Batch 1350/2527: Loss = 0.2249
2026-01-02 16:41:18,373 - INFO -   Batch 1400/2527: Loss = 0.1788
2026-01-02 16:41:47,234 - INFO -   Batch 1450/2527: Loss = 0.1246
2026-01-02 16:42:16,078 - INFO -   Batch 1500/2527: Loss = 0.2214
2026-01-02 16:42:45,157 - INFO -   Batch 1550/2527: Loss = 0.1474
2026-01-02 16:43:13,783 - INFO -   Batch 1600/2527: Loss = 0.1459
2026-01-02 16:43:42,436 - INFO -   Batch 1650/2527: Loss = 0.2013
2026-01-02 16:44:11,293 - INFO -   Batch 1700/2527: Loss = 0.1674
2026-01-02 16:44:39,940 - INFO -   Batch 1750/2527: Loss = 0.1774
2026-01-02 16:45:09,083 - INFO -   Batch 1800/2527: Loss = 0.1958
2026-01-02 16:45:37,801 - INFO -   Batch 1850/2527: Loss = 0.2396
2026-01-02 16:46:06,456 - INFO -   Batch 1900/2527: Loss = 0.1803
2026-01-02 16:46:35,720 - INFO -   Batch 1950/2527: Loss = 0.2560
2026-01-02 16:47:04,455 - INFO -   Batch 2000/2527: Loss = 0.1361
2026-01-02 16:47:33,313 - INFO -   Batch 2050/2527: Loss = 0.1922
2026-01-02 16:48:01,933 - INFO -   Batch 2100/2527: Loss = 0.2369
2026-01-02 16:48:30,841 - INFO -   Batch 2150/2527: Loss = 0.1353
2026-01-02 16:48:59,735 - INFO -   Batch 2200/2527: Loss = 0.1559
2026-01-02 16:49:28,194 - INFO -   Batch 2250/2527: Loss = 0.1887
2026-01-02 16:49:57,061 - INFO -   Batch 2300/2527: Loss = 0.2170
2026-01-02 16:50:26,055 - INFO -   Batch 2350/2527: Loss = 0.1821
2026-01-02 16:50:54,940 - INFO -   Batch 2400/2527: Loss = 0.2574
2026-01-02 16:51:23,544 - INFO -   Batch 2450/2527: Loss = 0.2075
2026-01-02 16:51:52,048 - INFO -   Batch 2500/2527: Loss = 0.1637
2026-01-02 16:56:33,514 - INFO - Epoch 40/160: Train Loss: 0.1825, Val Loss: 1.0673, Val mIoU: 37.95%, Val PixelAcc: 77.92%
2026-01-02 16:56:39,535 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_40.pth
2026-01-02 16:56:39,538 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_40.pth
2026-01-02 16:57:09,772 - INFO -   Batch 50/2527: Loss = 0.2321
2026-01-02 16:57:38,826 - INFO -   Batch 100/2527: Loss = 0.1943
2026-01-02 16:58:07,734 - INFO -   Batch 150/2527: Loss = 0.1845
2026-01-02 16:58:36,202 - INFO -   Batch 200/2527: Loss = 0.2990
2026-01-02 16:59:04,748 - INFO -   Batch 250/2527: Loss = 0.1996
2026-01-02 16:59:33,623 - INFO -   Batch 300/2527: Loss = 0.2869
2026-01-02 17:00:02,486 - INFO -   Batch 350/2527: Loss = 0.1800
2026-01-02 17:00:31,352 - INFO -   Batch 400/2527: Loss = 0.2261
2026-01-02 17:01:00,016 - INFO -   Batch 450/2527: Loss = 0.1507
2026-01-02 17:01:28,835 - INFO -   Batch 500/2527: Loss = 0.1749
2026-01-02 17:01:57,769 - INFO -   Batch 550/2527: Loss = 0.1343
2026-01-02 17:02:26,363 - INFO -   Batch 600/2527: Loss = 0.1693
2026-01-02 17:02:55,191 - INFO -   Batch 650/2527: Loss = 0.1795
2026-01-02 17:03:23,864 - INFO -   Batch 700/2527: Loss = 0.1931
2026-01-02 17:03:52,908 - INFO -   Batch 750/2527: Loss = 0.1343
2026-01-02 17:04:21,573 - INFO -   Batch 800/2527: Loss = 0.1909
2026-01-02 17:04:50,229 - INFO -   Batch 850/2527: Loss = 0.1136
2026-01-02 17:05:19,352 - INFO -   Batch 900/2527: Loss = 0.1794
2026-01-02 17:05:48,224 - INFO -   Batch 950/2527: Loss = 0.1654
2026-01-02 17:06:17,028 - INFO -   Batch 1000/2527: Loss = 0.1284
2026-01-02 17:06:45,714 - INFO -   Batch 1050/2527: Loss = 0.2023
2026-01-02 17:07:14,621 - INFO -   Batch 1100/2527: Loss = 0.1392
2026-01-02 17:07:43,378 - INFO -   Batch 1150/2527: Loss = 0.1351
2026-01-02 17:08:12,030 - INFO -   Batch 1200/2527: Loss = 0.1865
2026-01-02 17:08:41,014 - INFO -   Batch 1250/2527: Loss = 0.1979
2026-01-02 17:09:09,940 - INFO -   Batch 1300/2527: Loss = 0.1712
2026-01-02 17:09:38,930 - INFO -   Batch 1350/2527: Loss = 0.3345
2026-01-02 17:10:07,510 - INFO -   Batch 1400/2527: Loss = 0.2130
2026-01-02 17:10:36,009 - INFO -   Batch 1450/2527: Loss = 0.1482
2026-01-02 17:11:04,912 - INFO -   Batch 1500/2527: Loss = 0.1049
2026-01-02 17:11:33,846 - INFO -   Batch 1550/2527: Loss = 0.2256
2026-01-02 17:12:02,678 - INFO -   Batch 1600/2527: Loss = 0.1343
2026-01-02 17:12:31,511 - INFO -   Batch 1650/2527: Loss = 0.1565
2026-01-02 17:13:00,630 - INFO -   Batch 1700/2527: Loss = 0.1314
2026-01-02 17:13:29,406 - INFO -   Batch 1750/2527: Loss = 0.1890
2026-01-02 17:13:57,940 - INFO -   Batch 1800/2527: Loss = 0.1670
2026-01-02 17:14:26,746 - INFO -   Batch 1850/2527: Loss = 0.1584
2026-01-02 17:14:55,355 - INFO -   Batch 1900/2527: Loss = 0.2434
2026-01-02 17:15:24,392 - INFO -   Batch 1950/2527: Loss = 0.1721
2026-01-02 17:15:52,994 - INFO -   Batch 2000/2527: Loss = 0.2000
2026-01-02 17:16:21,636 - INFO -   Batch 2050/2527: Loss = 0.1511
2026-01-02 17:16:50,758 - INFO -   Batch 2100/2527: Loss = 0.2794
2026-01-02 17:17:19,462 - INFO -   Batch 2150/2527: Loss = 0.1826
2026-01-02 17:17:48,394 - INFO -   Batch 2200/2527: Loss = 0.1337
2026-01-02 17:18:17,040 - INFO -   Batch 2250/2527: Loss = 0.1345
2026-01-02 17:18:45,943 - INFO -   Batch 2300/2527: Loss = 0.1781
2026-01-02 17:19:14,862 - INFO -   Batch 2350/2527: Loss = 0.2211
2026-01-02 17:19:43,386 - INFO -   Batch 2400/2527: Loss = 0.1397
2026-01-02 17:20:12,360 - INFO -   Batch 2450/2527: Loss = 0.1422
2026-01-02 17:20:41,348 - INFO -   Batch 2500/2527: Loss = 0.1478
2026-01-02 17:25:21,006 - INFO - Epoch 41/160: Train Loss: 0.1799, Val Loss: 1.0496, Val mIoU: 39.26%, Val PixelAcc: 77.76%
2026-01-02 17:25:21,008 - INFO -   → New best mIoU: 39.26%
2026-01-02 17:25:24,972 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 17:25:24,975 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 17:25:55,408 - INFO -   Batch 50/2527: Loss = 0.2123
2026-01-02 17:26:24,242 - INFO -   Batch 100/2527: Loss = 0.1663
2026-01-02 17:26:53,037 - INFO -   Batch 150/2527: Loss = 0.2180
2026-01-02 17:27:21,867 - INFO -   Batch 200/2527: Loss = 0.1274
2026-01-02 17:27:50,523 - INFO -   Batch 250/2527: Loss = 0.1863
2026-01-02 17:28:19,405 - INFO -   Batch 300/2527: Loss = 0.1748
2026-01-02 17:28:47,972 - INFO -   Batch 350/2527: Loss = 0.1378
2026-01-02 17:29:16,780 - INFO -   Batch 400/2527: Loss = 0.1951
2026-01-02 17:29:45,709 - INFO -   Batch 450/2527: Loss = 0.1985
2026-01-02 17:30:14,568 - INFO -   Batch 500/2527: Loss = 0.1727
2026-01-02 17:30:43,468 - INFO -   Batch 550/2527: Loss = 0.1243
2026-01-02 17:31:12,115 - INFO -   Batch 600/2527: Loss = 0.1533
2026-01-02 17:31:40,666 - INFO -   Batch 650/2527: Loss = 0.1897
2026-01-02 17:32:09,626 - INFO -   Batch 700/2527: Loss = 0.1624
2026-01-02 17:32:38,232 - INFO -   Batch 750/2527: Loss = 0.1500
2026-01-02 17:33:07,271 - INFO -   Batch 800/2527: Loss = 0.2118
2026-01-02 17:33:35,987 - INFO -   Batch 850/2527: Loss = 0.1687
2026-01-02 17:34:05,116 - INFO -   Batch 900/2527: Loss = 0.1415
2026-01-02 17:34:33,835 - INFO -   Batch 950/2527: Loss = 0.1455
2026-01-02 17:35:02,336 - INFO -   Batch 1000/2527: Loss = 0.1666
2026-01-02 17:35:31,238 - INFO -   Batch 1050/2527: Loss = 0.2733
2026-01-02 17:36:00,002 - INFO -   Batch 1100/2527: Loss = 0.1804
2026-01-02 17:36:28,916 - INFO -   Batch 1150/2527: Loss = 0.2068
2026-01-02 17:36:57,912 - INFO -   Batch 1200/2527: Loss = 0.2350
2026-01-02 17:37:26,518 - INFO -   Batch 1250/2527: Loss = 0.1681
2026-01-02 17:37:55,608 - INFO -   Batch 1300/2527: Loss = 0.1731
2026-01-02 17:38:24,454 - INFO -   Batch 1350/2527: Loss = 0.1895
2026-01-02 17:38:53,263 - INFO -   Batch 1400/2527: Loss = 0.3217
2026-01-02 17:39:21,955 - INFO -   Batch 1450/2527: Loss = 0.1088
2026-01-02 17:39:50,827 - INFO -   Batch 1500/2527: Loss = 0.1602
2026-01-02 17:40:19,511 - INFO -   Batch 1550/2527: Loss = 0.1969
2026-01-02 17:40:48,131 - INFO -   Batch 1600/2527: Loss = 0.1998
2026-01-02 17:41:17,077 - INFO -   Batch 1650/2527: Loss = 0.1457
2026-01-02 17:41:45,901 - INFO -   Batch 1700/2527: Loss = 0.1795
2026-01-02 17:42:14,810 - INFO -   Batch 1750/2527: Loss = 0.2025
2026-01-02 17:42:43,384 - INFO -   Batch 1800/2527: Loss = 0.1704
2026-01-02 17:43:12,272 - INFO -   Batch 1850/2527: Loss = 0.2081
2026-01-02 17:43:40,946 - INFO -   Batch 1900/2527: Loss = 0.2706
2026-01-02 17:44:09,824 - INFO -   Batch 1950/2527: Loss = 0.1570
2026-01-02 17:44:38,876 - INFO -   Batch 2000/2527: Loss = 0.1706
2026-01-02 17:45:07,676 - INFO -   Batch 2050/2527: Loss = 0.1456
2026-01-02 17:45:36,700 - INFO -   Batch 2100/2527: Loss = 0.1415
2026-01-02 17:46:05,367 - INFO -   Batch 2150/2527: Loss = 0.1319
2026-01-02 17:46:33,880 - INFO -   Batch 2200/2527: Loss = 0.2053
2026-01-02 17:47:02,691 - INFO -   Batch 2250/2527: Loss = 0.2124
2026-01-02 17:47:31,521 - INFO -   Batch 2300/2527: Loss = 0.1447
2026-01-02 17:48:00,374 - INFO -   Batch 2350/2527: Loss = 0.0851
2026-01-02 17:48:29,134 - INFO -   Batch 2400/2527: Loss = 0.1817
2026-01-02 17:48:58,109 - INFO -   Batch 2450/2527: Loss = 0.1448
2026-01-02 17:49:26,941 - INFO -   Batch 2500/2527: Loss = 0.1610
2026-01-02 17:54:19,703 - INFO - Epoch 42/160: Train Loss: 0.1760, Val Loss: 1.0507, Val mIoU: 39.05%, Val PixelAcc: 77.85%
2026-01-02 17:54:50,299 - INFO -   Batch 50/2527: Loss = 0.1587
2026-01-02 17:55:18,872 - INFO -   Batch 100/2527: Loss = 0.1453
2026-01-02 17:55:47,378 - INFO -   Batch 150/2527: Loss = 0.3081
2026-01-02 17:56:16,462 - INFO -   Batch 200/2527: Loss = 0.1358
2026-01-02 17:56:45,131 - INFO -   Batch 250/2527: Loss = 0.1256
2026-01-02 17:57:14,091 - INFO -   Batch 300/2527: Loss = 0.1638
2026-01-02 17:57:42,988 - INFO -   Batch 350/2527: Loss = 0.2399
2026-01-02 17:58:11,865 - INFO -   Batch 400/2527: Loss = 0.1480
2026-01-02 17:58:40,529 - INFO -   Batch 450/2527: Loss = 0.1528
2026-01-02 17:59:09,014 - INFO -   Batch 500/2527: Loss = 0.1264
2026-01-02 17:59:37,899 - INFO -   Batch 550/2527: Loss = 0.2656
2026-01-02 18:00:06,765 - INFO -   Batch 600/2527: Loss = 0.1809
2026-01-02 18:00:35,622 - INFO -   Batch 650/2527: Loss = 0.1036
2026-01-02 18:01:04,424 - INFO -   Batch 700/2527: Loss = 0.1860
2026-01-02 18:01:33,601 - INFO -   Batch 750/2527: Loss = 0.2468
2026-01-02 18:02:02,231 - INFO -   Batch 800/2527: Loss = 0.1582
2026-01-02 18:02:30,791 - INFO -   Batch 850/2527: Loss = 0.1153
2026-01-02 18:02:59,675 - INFO -   Batch 900/2527: Loss = 0.1337
2026-01-02 18:03:28,331 - INFO -   Batch 950/2527: Loss = 0.1675
2026-01-02 18:03:57,457 - INFO -   Batch 1000/2527: Loss = 0.1649
2026-01-02 18:04:26,116 - INFO -   Batch 1050/2527: Loss = 0.1386
2026-01-02 18:04:54,784 - INFO -   Batch 1100/2527: Loss = 0.1718
2026-01-02 18:05:23,895 - INFO -   Batch 1150/2527: Loss = 0.1300
2026-01-02 18:05:52,677 - INFO -   Batch 1200/2527: Loss = 0.1589
2026-01-02 18:06:21,498 - INFO -   Batch 1250/2527: Loss = 0.1474
2026-01-02 18:06:50,316 - INFO -   Batch 1300/2527: Loss = 0.1448
2026-01-02 18:07:19,161 - INFO -   Batch 1350/2527: Loss = 0.2194
2026-01-02 18:07:48,107 - INFO -   Batch 1400/2527: Loss = 0.2262
2026-01-02 18:08:16,636 - INFO -   Batch 1450/2527: Loss = 0.1925
2026-01-02 18:08:45,618 - INFO -   Batch 1500/2527: Loss = 0.2307
2026-01-02 18:09:14,547 - INFO -   Batch 1550/2527: Loss = 0.2105
2026-01-02 18:09:43,475 - INFO -   Batch 1600/2527: Loss = 0.1414
2026-01-02 18:10:12,089 - INFO -   Batch 1650/2527: Loss = 0.1064
2026-01-02 18:10:41,001 - INFO -   Batch 1700/2527: Loss = 0.2350
2026-01-02 18:11:09,574 - INFO -   Batch 1750/2527: Loss = 0.1533
2026-01-02 18:11:38,430 - INFO -   Batch 1800/2527: Loss = 0.2287
2026-01-02 18:12:07,277 - INFO -   Batch 1850/2527: Loss = 0.1486
2026-01-02 18:12:36,071 - INFO -   Batch 1900/2527: Loss = 0.2066
2026-01-02 18:13:05,195 - INFO -   Batch 1950/2527: Loss = 0.1394
2026-01-02 18:13:34,190 - INFO -   Batch 2000/2527: Loss = 0.2826
2026-01-02 18:14:02,659 - INFO -   Batch 2050/2527: Loss = 0.2378
2026-01-02 18:14:31,438 - INFO -   Batch 2100/2527: Loss = 0.3512
2026-01-02 18:15:00,314 - INFO -   Batch 2150/2527: Loss = 0.1724
2026-01-02 18:15:29,398 - INFO -   Batch 2200/2527: Loss = 0.1954
2026-01-02 18:15:58,279 - INFO -   Batch 2250/2527: Loss = 0.1513
2026-01-02 18:16:27,084 - INFO -   Batch 2300/2527: Loss = 0.1665
2026-01-02 18:16:56,211 - INFO -   Batch 2350/2527: Loss = 0.1719
2026-01-02 18:17:24,909 - INFO -   Batch 2400/2527: Loss = 0.1583
2026-01-02 18:17:53,689 - INFO -   Batch 2450/2527: Loss = 0.3447
2026-01-02 18:18:22,284 - INFO -   Batch 2500/2527: Loss = 0.3288
2026-01-02 18:23:03,247 - INFO - Epoch 43/160: Train Loss: 0.1690, Val Loss: 1.1337, Val mIoU: 37.81%, Val PixelAcc: 76.77%
2026-01-02 18:23:33,920 - INFO -   Batch 50/2527: Loss = 0.0950
2026-01-02 18:24:02,808 - INFO -   Batch 100/2527: Loss = 0.2137
2026-01-02 18:24:31,691 - INFO -   Batch 150/2527: Loss = 0.1763
2026-01-02 18:25:00,351 - INFO -   Batch 200/2527: Loss = 0.1037
2026-01-02 18:25:29,151 - INFO -   Batch 250/2527: Loss = 0.2124
2026-01-02 18:25:57,807 - INFO -   Batch 300/2527: Loss = 0.1944
2026-01-02 18:26:26,558 - INFO -   Batch 350/2527: Loss = 0.2174
2026-01-02 18:26:55,348 - INFO -   Batch 400/2527: Loss = 0.1925
2026-01-02 18:27:24,086 - INFO -   Batch 450/2527: Loss = 0.2723
2026-01-02 18:27:53,234 - INFO -   Batch 500/2527: Loss = 0.2629
2026-01-02 18:28:21,931 - INFO -   Batch 550/2527: Loss = 0.1596
2026-01-02 18:28:50,790 - INFO -   Batch 600/2527: Loss = 0.1651
2026-01-02 18:29:19,302 - INFO -   Batch 650/2527: Loss = 0.1933
2026-01-02 18:29:48,014 - INFO -   Batch 700/2527: Loss = 0.1730
2026-01-02 18:30:17,156 - INFO -   Batch 750/2527: Loss = 0.1420
2026-01-02 18:30:45,786 - INFO -   Batch 800/2527: Loss = 0.1532
2026-01-02 18:31:14,817 - INFO -   Batch 850/2527: Loss = 0.1940
2026-01-02 18:31:43,690 - INFO -   Batch 900/2527: Loss = 0.1913
2026-01-02 18:32:12,290 - INFO -   Batch 950/2527: Loss = 0.1221
2026-01-02 18:32:41,106 - INFO -   Batch 1000/2527: Loss = 0.1972
2026-01-02 18:33:09,744 - INFO -   Batch 1050/2527: Loss = 0.1673
2026-01-02 18:33:38,594 - INFO -   Batch 1100/2527: Loss = 0.1496
2026-01-02 18:34:07,430 - INFO -   Batch 1150/2527: Loss = 0.2304
2026-01-02 18:34:36,255 - INFO -   Batch 1200/2527: Loss = 0.1418
2026-01-02 18:35:05,040 - INFO -   Batch 1250/2527: Loss = 0.1738
2026-01-02 18:35:33,832 - INFO -   Batch 1300/2527: Loss = 0.1256
2026-01-02 18:36:02,690 - INFO -   Batch 1350/2527: Loss = 0.2037
2026-01-02 18:36:31,293 - INFO -   Batch 1400/2527: Loss = 0.1100
2026-01-02 18:37:00,168 - INFO -   Batch 1450/2527: Loss = 0.1188
2026-01-02 18:37:28,815 - INFO -   Batch 1500/2527: Loss = 0.1918
2026-01-02 18:37:57,553 - INFO -   Batch 1550/2527: Loss = 0.1183
2026-01-02 18:38:26,395 - INFO -   Batch 1600/2527: Loss = 0.1215
2026-01-02 18:38:55,107 - INFO -   Batch 1650/2527: Loss = 0.1211
2026-01-02 18:39:24,260 - INFO -   Batch 1700/2527: Loss = 0.1154
2026-01-02 18:39:52,944 - INFO -   Batch 1750/2527: Loss = 0.1475
2026-01-02 18:40:21,783 - INFO -   Batch 1800/2527: Loss = 0.1549
2026-01-02 18:40:50,564 - INFO -   Batch 1850/2527: Loss = 0.1597
2026-01-02 18:41:19,127 - INFO -   Batch 1900/2527: Loss = 0.1818
2026-01-02 18:41:48,183 - INFO -   Batch 1950/2527: Loss = 0.1699
2026-01-02 18:42:16,878 - INFO -   Batch 2000/2527: Loss = 0.2018
2026-01-02 18:42:45,860 - INFO -   Batch 2050/2527: Loss = 0.1137
2026-01-02 18:43:14,844 - INFO -   Batch 2100/2527: Loss = 0.1753
2026-01-02 18:43:43,751 - INFO -   Batch 2150/2527: Loss = 0.1569
2026-01-02 18:44:12,275 - INFO -   Batch 2200/2527: Loss = 0.1501
2026-01-02 18:44:40,908 - INFO -   Batch 2250/2527: Loss = 0.1679
2026-01-02 18:45:09,845 - INFO -   Batch 2300/2527: Loss = 0.1473
2026-01-02 18:45:38,715 - INFO -   Batch 2350/2527: Loss = 0.1240
2026-01-02 18:46:07,707 - INFO -   Batch 2400/2527: Loss = 0.1682
2026-01-02 18:46:36,531 - INFO -   Batch 2450/2527: Loss = 0.1728
2026-01-02 18:47:05,316 - INFO -   Batch 2500/2527: Loss = 0.1101
2026-01-02 18:51:47,087 - INFO - Epoch 44/160: Train Loss: 0.1726, Val Loss: 1.1226, Val mIoU: 38.37%, Val PixelAcc: 77.22%
2026-01-02 18:52:17,599 - INFO -   Batch 50/2527: Loss = 0.1673
2026-01-02 18:52:46,678 - INFO -   Batch 100/2527: Loss = 0.1766
2026-01-02 18:53:15,433 - INFO -   Batch 150/2527: Loss = 0.2701
2026-01-02 18:53:43,958 - INFO -   Batch 200/2527: Loss = 0.1427
2026-01-02 18:54:12,842 - INFO -   Batch 250/2527: Loss = 0.1415
2026-01-02 18:54:41,614 - INFO -   Batch 300/2527: Loss = 0.1300
2026-01-02 18:55:10,499 - INFO -   Batch 350/2527: Loss = 0.1740
2026-01-02 18:55:39,349 - INFO -   Batch 400/2527: Loss = 0.0839
2026-01-02 18:56:07,968 - INFO -   Batch 450/2527: Loss = 0.1001
2026-01-02 18:56:37,061 - INFO -   Batch 500/2527: Loss = 0.1046
2026-01-02 18:57:05,725 - INFO -   Batch 550/2527: Loss = 0.1573
2026-01-02 18:57:34,524 - INFO -   Batch 600/2527: Loss = 0.1636
2026-01-02 18:58:03,192 - INFO -   Batch 650/2527: Loss = 0.1230
2026-01-02 18:58:32,113 - INFO -   Batch 700/2527: Loss = 0.1545
2026-01-02 18:59:00,702 - INFO -   Batch 750/2527: Loss = 0.1631
2026-01-02 18:59:29,340 - INFO -   Batch 800/2527: Loss = 0.1623
2026-01-02 18:59:58,312 - INFO -   Batch 850/2527: Loss = 0.1193
2026-01-02 19:00:27,210 - INFO -   Batch 900/2527: Loss = 0.2137
2026-01-02 19:00:56,167 - INFO -   Batch 950/2527: Loss = 0.2101
2026-01-02 19:01:24,815 - INFO -   Batch 1000/2527: Loss = 0.1112
2026-01-02 19:01:53,771 - INFO -   Batch 1050/2527: Loss = 0.1704
2026-01-02 19:02:22,374 - INFO -   Batch 1100/2527: Loss = 0.1489
2026-01-02 19:02:51,030 - INFO -   Batch 1150/2527: Loss = 0.2064
2026-01-02 19:03:20,063 - INFO -   Batch 1200/2527: Loss = 0.2455
2026-01-02 19:03:48,890 - INFO -   Batch 1250/2527: Loss = 0.1105
2026-01-02 19:04:17,953 - INFO -   Batch 1300/2527: Loss = 0.2087
2026-01-02 19:04:46,728 - INFO -   Batch 1350/2527: Loss = 0.1632
2026-01-02 19:05:15,235 - INFO -   Batch 1400/2527: Loss = 0.2620
2026-01-02 19:05:44,048 - INFO -   Batch 1450/2527: Loss = 0.1107
2026-01-02 19:06:12,899 - INFO -   Batch 1500/2527: Loss = 0.1523
2026-01-02 19:06:41,814 - INFO -   Batch 1550/2527: Loss = 0.1131
2026-01-02 19:07:10,630 - INFO -   Batch 1600/2527: Loss = 0.1081
2026-01-02 19:07:39,572 - INFO -   Batch 1650/2527: Loss = 0.1967
2026-01-02 19:08:08,555 - INFO -   Batch 1700/2527: Loss = 0.1441
2026-01-02 19:08:37,314 - INFO -   Batch 1750/2527: Loss = 0.1426
2026-01-02 19:09:06,806 - INFO -   Batch 1800/2527: Loss = 0.2174
2026-01-02 19:09:35,662 - INFO -   Batch 1850/2527: Loss = 0.1201
2026-01-02 19:10:04,642 - INFO -   Batch 1900/2527: Loss = 0.0952
2026-01-02 19:10:33,333 - INFO -   Batch 1950/2527: Loss = 0.1638
2026-01-02 19:11:02,588 - INFO -   Batch 2000/2527: Loss = 0.2201
2026-01-02 19:11:31,176 - INFO -   Batch 2050/2527: Loss = 0.1467
2026-01-02 19:12:00,319 - INFO -   Batch 2100/2527: Loss = 0.1295
2026-01-02 19:12:29,364 - INFO -   Batch 2150/2527: Loss = 0.1100
2026-01-02 19:12:58,265 - INFO -   Batch 2200/2527: Loss = 0.2025
2026-01-02 19:13:27,067 - INFO -   Batch 2250/2527: Loss = 0.2016
2026-01-02 19:13:55,818 - INFO -   Batch 2300/2527: Loss = 0.2387
2026-01-02 19:14:24,422 - INFO -   Batch 2350/2527: Loss = 0.1973
2026-01-02 19:14:53,526 - INFO -   Batch 2400/2527: Loss = 0.1292
2026-01-02 19:15:22,348 - INFO -   Batch 2450/2527: Loss = 0.1704
2026-01-02 19:15:51,451 - INFO -   Batch 2500/2527: Loss = 0.1528
2026-01-02 19:20:44,753 - INFO - Epoch 45/160: Train Loss: 0.1612, Val Loss: 1.0682, Val mIoU: 38.71%, Val PixelAcc: 77.86%
2026-01-02 19:21:15,140 - INFO -   Batch 50/2527: Loss = 0.1128
2026-01-02 19:21:43,914 - INFO -   Batch 100/2527: Loss = 0.1560
2026-01-02 19:22:12,554 - INFO -   Batch 150/2527: Loss = 0.1707
2026-01-02 19:22:41,639 - INFO -   Batch 200/2527: Loss = 0.2129
2026-01-02 19:23:10,378 - INFO -   Batch 250/2527: Loss = 0.1571
2026-01-02 19:23:39,023 - INFO -   Batch 300/2527: Loss = 0.1877
2026-01-02 19:24:08,214 - INFO -   Batch 350/2527: Loss = 0.1344
2026-01-02 19:24:36,949 - INFO -   Batch 400/2527: Loss = 0.1503
2026-01-02 19:25:05,797 - INFO -   Batch 450/2527: Loss = 0.1581
2026-01-02 19:25:34,492 - INFO -   Batch 500/2527: Loss = 0.2159
2026-01-02 19:26:03,386 - INFO -   Batch 550/2527: Loss = 0.1593
2026-01-02 19:26:32,229 - INFO -   Batch 600/2527: Loss = 0.1024
2026-01-02 19:27:00,732 - INFO -   Batch 650/2527: Loss = 0.1509
2026-01-02 19:27:29,769 - INFO -   Batch 700/2527: Loss = 0.1415
2026-01-02 19:27:58,690 - INFO -   Batch 750/2527: Loss = 0.1278
2026-01-02 19:28:27,619 - INFO -   Batch 800/2527: Loss = 0.1216
2026-01-02 19:28:56,206 - INFO -   Batch 850/2527: Loss = 0.1494
2026-01-02 19:29:24,779 - INFO -   Batch 900/2527: Loss = 0.1054
2026-01-02 19:29:53,663 - INFO -   Batch 950/2527: Loss = 0.1372
2026-01-02 19:30:22,527 - INFO -   Batch 1000/2527: Loss = 0.1545
2026-01-02 19:30:51,361 - INFO -   Batch 1050/2527: Loss = 0.0904
2026-01-02 19:31:20,090 - INFO -   Batch 1100/2527: Loss = 0.2108
2026-01-02 19:31:49,316 - INFO -   Batch 1150/2527: Loss = 0.1733
2026-01-02 19:32:18,009 - INFO -   Batch 1200/2527: Loss = 0.1444
2026-01-02 19:32:46,479 - INFO -   Batch 1250/2527: Loss = 0.1380
2026-01-02 19:33:15,315 - INFO -   Batch 1300/2527: Loss = 0.1174
2026-01-02 19:33:43,943 - INFO -   Batch 1350/2527: Loss = 0.1584
2026-01-02 19:34:13,027 - INFO -   Batch 1400/2527: Loss = 0.1985
2026-01-02 19:34:41,725 - INFO -   Batch 1450/2527: Loss = 0.1581
2026-01-02 19:35:10,706 - INFO -   Batch 1500/2527: Loss = 0.0865
2026-01-02 19:35:39,718 - INFO -   Batch 1550/2527: Loss = 0.2271
2026-01-02 19:36:08,280 - INFO -   Batch 1600/2527: Loss = 0.1764
2026-01-02 19:36:37,016 - INFO -   Batch 1650/2527: Loss = 0.1207
2026-01-02 19:37:05,690 - INFO -   Batch 1700/2527: Loss = 0.2360
2026-01-02 19:37:34,476 - INFO -   Batch 1750/2527: Loss = 0.1479
2026-01-02 19:38:03,561 - INFO -   Batch 1800/2527: Loss = 0.1508
2026-01-02 19:38:32,170 - INFO -   Batch 1850/2527: Loss = 0.1803
2026-01-02 19:39:00,817 - INFO -   Batch 1900/2527: Loss = 0.2068
2026-01-02 19:39:29,960 - INFO -   Batch 1950/2527: Loss = 0.1107
2026-01-02 19:39:58,681 - INFO -   Batch 2000/2527: Loss = 0.1421
2026-01-02 19:40:27,719 - INFO -   Batch 2050/2527: Loss = 0.2315
2026-01-02 19:40:56,371 - INFO -   Batch 2100/2527: Loss = 0.1147
2026-01-02 19:41:25,243 - INFO -   Batch 2150/2527: Loss = 0.2045
2026-01-02 19:41:54,193 - INFO -   Batch 2200/2527: Loss = 0.1945
2026-01-02 19:42:22,729 - INFO -   Batch 2250/2527: Loss = 0.1992
2026-01-02 19:42:51,639 - INFO -   Batch 2300/2527: Loss = 0.1098
2026-01-02 19:43:20,595 - INFO -   Batch 2350/2527: Loss = 0.1268
2026-01-02 19:43:49,547 - INFO -   Batch 2400/2527: Loss = 0.2769
2026-01-02 19:44:18,324 - INFO -   Batch 2450/2527: Loss = 0.1273
2026-01-02 19:44:47,160 - INFO -   Batch 2500/2527: Loss = 0.1409
2026-01-02 19:49:27,893 - INFO - Epoch 46/160: Train Loss: 0.1607, Val Loss: 1.0880, Val mIoU: 38.51%, Val PixelAcc: 77.71%
2026-01-02 19:49:58,522 - INFO -   Batch 50/2527: Loss = 0.1230
2026-01-02 19:50:27,376 - INFO -   Batch 100/2527: Loss = 0.2332
2026-01-02 19:50:56,254 - INFO -   Batch 150/2527: Loss = 0.1543
2026-01-02 19:51:24,838 - INFO -   Batch 200/2527: Loss = 0.1653
2026-01-02 19:51:53,323 - INFO -   Batch 250/2527: Loss = 0.1076
2026-01-02 19:52:22,193 - INFO -   Batch 300/2527: Loss = 0.1568
2026-01-02 19:52:51,032 - INFO -   Batch 350/2527: Loss = 0.1858
2026-01-02 19:53:19,888 - INFO -   Batch 400/2527: Loss = 0.1454
2026-01-02 19:53:48,594 - INFO -   Batch 450/2527: Loss = 0.1675
2026-01-02 19:54:17,376 - INFO -   Batch 500/2527: Loss = 0.1378
2026-01-02 19:54:46,283 - INFO -   Batch 550/2527: Loss = 0.1840
2026-01-02 19:55:14,878 - INFO -   Batch 600/2527: Loss = 0.1543
2026-01-02 19:55:43,753 - INFO -   Batch 650/2527: Loss = 0.1152
2026-01-02 19:56:12,554 - INFO -   Batch 700/2527: Loss = 0.1737
2026-01-02 19:56:41,625 - INFO -   Batch 750/2527: Loss = 0.2194
2026-01-02 19:57:10,276 - INFO -   Batch 800/2527: Loss = 0.1823
2026-01-02 19:57:38,908 - INFO -   Batch 850/2527: Loss = 0.2568
2026-01-02 19:58:08,043 - INFO -   Batch 900/2527: Loss = 0.1171
2026-01-02 19:58:36,765 - INFO -   Batch 950/2527: Loss = 0.1002
2026-01-02 19:59:05,545 - INFO -   Batch 1000/2527: Loss = 0.1347
2026-01-02 19:59:34,185 - INFO -   Batch 1050/2527: Loss = 0.1122
2026-01-02 20:00:03,031 - INFO -   Batch 1100/2527: Loss = 0.2132
2026-01-02 20:00:31,781 - INFO -   Batch 1150/2527: Loss = 0.2749
2026-01-02 20:01:00,400 - INFO -   Batch 1200/2527: Loss = 0.1617
2026-01-02 20:01:29,352 - INFO -   Batch 1250/2527: Loss = 0.1626
2026-01-02 20:01:58,282 - INFO -   Batch 1300/2527: Loss = 0.1480
2026-01-02 20:02:27,199 - INFO -   Batch 1350/2527: Loss = 0.1575
2026-01-02 20:02:55,841 - INFO -   Batch 1400/2527: Loss = 0.1222
2026-01-02 20:03:24,312 - INFO -   Batch 1450/2527: Loss = 0.1769
2026-01-02 20:03:53,174 - INFO -   Batch 1500/2527: Loss = 0.1110
2026-01-02 20:04:22,036 - INFO -   Batch 1550/2527: Loss = 0.1903
2026-01-02 20:04:50,888 - INFO -   Batch 1600/2527: Loss = 0.1254
2026-01-02 20:05:19,568 - INFO -   Batch 1650/2527: Loss = 0.1279
2026-01-02 20:05:48,704 - INFO -   Batch 1700/2527: Loss = 0.1478
2026-01-02 20:06:17,416 - INFO -   Batch 1750/2527: Loss = 0.2309
2026-01-02 20:06:45,925 - INFO -   Batch 1800/2527: Loss = 0.1434
2026-01-02 20:07:14,770 - INFO -   Batch 1850/2527: Loss = 0.2101
2026-01-02 20:07:43,430 - INFO -   Batch 1900/2527: Loss = 0.1708
2026-01-02 20:08:12,480 - INFO -   Batch 1950/2527: Loss = 0.2021
2026-01-02 20:08:41,137 - INFO -   Batch 2000/2527: Loss = 0.1319
2026-01-02 20:09:10,091 - INFO -   Batch 2050/2527: Loss = 0.1347
2026-01-02 20:09:38,975 - INFO -   Batch 2100/2527: Loss = 0.2023
2026-01-02 20:10:07,653 - INFO -   Batch 2150/2527: Loss = 0.1735
2026-01-02 20:10:36,462 - INFO -   Batch 2200/2527: Loss = 0.2374
2026-01-02 20:11:05,032 - INFO -   Batch 2250/2527: Loss = 0.1716
2026-01-02 20:11:33,912 - INFO -   Batch 2300/2527: Loss = 0.1501
2026-01-02 20:12:02,740 - INFO -   Batch 2350/2527: Loss = 0.1675
2026-01-02 20:12:31,265 - INFO -   Batch 2400/2527: Loss = 0.1494
2026-01-02 20:13:00,220 - INFO -   Batch 2450/2527: Loss = 0.1895
2026-01-02 20:13:29,140 - INFO -   Batch 2500/2527: Loss = 0.1479
2026-01-02 20:18:09,495 - INFO - Epoch 47/160: Train Loss: 0.1598, Val Loss: 1.1527, Val mIoU: 38.65%, Val PixelAcc: 77.34%
2026-01-02 20:18:39,938 - INFO -   Batch 50/2527: Loss = 0.1927
2026-01-02 20:19:08,719 - INFO -   Batch 100/2527: Loss = 0.1235
2026-01-02 20:19:37,570 - INFO -   Batch 150/2527: Loss = 0.1121
2026-01-02 20:20:06,159 - INFO -   Batch 200/2527: Loss = 0.1839
2026-01-02 20:20:34,989 - INFO -   Batch 250/2527: Loss = 0.1599
2026-01-02 20:21:03,840 - INFO -   Batch 300/2527: Loss = 0.1844
2026-01-02 20:21:32,375 - INFO -   Batch 350/2527: Loss = 0.1388
2026-01-02 20:22:01,356 - INFO -   Batch 400/2527: Loss = 0.1270
2026-01-02 20:22:30,084 - INFO -   Batch 450/2527: Loss = 0.1627
2026-01-02 20:22:59,092 - INFO -   Batch 500/2527: Loss = 0.1322
2026-01-02 20:23:27,814 - INFO -   Batch 550/2527: Loss = 0.1952
2026-01-02 20:23:56,675 - INFO -   Batch 600/2527: Loss = 0.2103
2026-01-02 20:24:25,294 - INFO -   Batch 650/2527: Loss = 0.1616
2026-01-02 20:24:53,857 - INFO -   Batch 700/2527: Loss = 0.1597
2026-01-02 20:25:22,731 - INFO -   Batch 750/2527: Loss = 0.1630
2026-01-02 20:25:51,481 - INFO -   Batch 800/2527: Loss = 0.1526
2026-01-02 20:26:20,407 - INFO -   Batch 850/2527: Loss = 0.1383
2026-01-02 20:26:49,226 - INFO -   Batch 900/2527: Loss = 0.1072
2026-01-02 20:27:17,812 - INFO -   Batch 950/2527: Loss = 0.1728
2026-01-02 20:27:46,651 - INFO -   Batch 1000/2527: Loss = 0.1203
2026-01-02 20:28:15,328 - INFO -   Batch 1050/2527: Loss = 0.1206
2026-01-02 20:28:44,235 - INFO -   Batch 1100/2527: Loss = 0.2992
2026-01-02 20:29:12,942 - INFO -   Batch 1150/2527: Loss = 0.1593
2026-01-02 20:29:41,876 - INFO -   Batch 1200/2527: Loss = 0.1315
2026-01-02 20:30:10,587 - INFO -   Batch 1250/2527: Loss = 0.1592
2026-01-02 20:30:39,291 - INFO -   Batch 1300/2527: Loss = 0.1773
2026-01-02 20:31:08,187 - INFO -   Batch 1350/2527: Loss = 0.2093
2026-01-02 20:31:36,764 - INFO -   Batch 1400/2527: Loss = 0.1515
2026-01-02 20:32:05,605 - INFO -   Batch 1450/2527: Loss = 0.1269
2026-01-02 20:32:34,242 - INFO -   Batch 1500/2527: Loss = 0.1165
2026-01-02 20:33:03,110 - INFO -   Batch 1550/2527: Loss = 0.1454
2026-01-02 20:33:31,895 - INFO -   Batch 1600/2527: Loss = 0.1779
2026-01-02 20:34:00,493 - INFO -   Batch 1650/2527: Loss = 0.1325
2026-01-02 20:34:29,560 - INFO -   Batch 1700/2527: Loss = 0.1507
2026-01-02 20:34:58,195 - INFO -   Batch 1750/2527: Loss = 0.1688
2026-01-02 20:35:26,984 - INFO -   Batch 1800/2527: Loss = 0.1762
2026-01-02 20:35:55,623 - INFO -   Batch 1850/2527: Loss = 0.1448
2026-01-02 20:36:24,177 - INFO -   Batch 1900/2527: Loss = 0.1541
2026-01-02 20:36:53,044 - INFO -   Batch 1950/2527: Loss = 0.1036
2026-01-02 20:37:21,779 - INFO -   Batch 2000/2527: Loss = 0.2113
2026-01-02 20:37:50,680 - INFO -   Batch 2050/2527: Loss = 0.0953
2026-01-02 20:38:19,568 - INFO -   Batch 2100/2527: Loss = 0.1715
2026-01-02 20:38:48,466 - INFO -   Batch 2150/2527: Loss = 0.1977
2026-01-02 20:39:17,255 - INFO -   Batch 2200/2527: Loss = 0.1127
2026-01-02 20:39:45,762 - INFO -   Batch 2250/2527: Loss = 0.1943
2026-01-02 20:40:14,663 - INFO -   Batch 2300/2527: Loss = 0.1777
2026-01-02 20:40:43,316 - INFO -   Batch 2350/2527: Loss = 0.1596
2026-01-02 20:41:12,325 - INFO -   Batch 2400/2527: Loss = 0.1462
2026-01-02 20:41:41,038 - INFO -   Batch 2450/2527: Loss = 0.1691
2026-01-02 20:42:10,108 - INFO -   Batch 2500/2527: Loss = 0.1432
2026-01-02 20:46:55,630 - INFO - Epoch 48/160: Train Loss: 0.1545, Val Loss: 1.1467, Val mIoU: 38.71%, Val PixelAcc: 77.33%
2026-01-02 20:47:25,957 - INFO -   Batch 50/2527: Loss = 0.3066
2026-01-02 20:47:54,670 - INFO -   Batch 100/2527: Loss = 0.2056
2026-01-02 20:48:23,279 - INFO -   Batch 150/2527: Loss = 0.1173
2026-01-02 20:48:52,007 - INFO -   Batch 200/2527: Loss = 0.1351
2026-01-02 20:49:20,802 - INFO -   Batch 250/2527: Loss = 0.1189
2026-01-02 20:49:49,558 - INFO -   Batch 300/2527: Loss = 0.1849
2026-01-02 20:50:18,639 - INFO -   Batch 350/2527: Loss = 0.0914
2026-01-02 20:50:47,523 - INFO -   Batch 400/2527: Loss = 0.1448
2026-01-02 20:51:16,340 - INFO -   Batch 450/2527: Loss = 0.1359
2026-01-02 20:51:44,801 - INFO -   Batch 500/2527: Loss = 0.1812
2026-01-02 20:52:13,468 - INFO -   Batch 550/2527: Loss = 0.1646
2026-01-02 20:52:42,548 - INFO -   Batch 600/2527: Loss = 0.1295
2026-01-02 20:53:11,174 - INFO -   Batch 650/2527: Loss = 0.1625
2026-01-02 20:53:40,133 - INFO -   Batch 700/2527: Loss = 0.1438
2026-01-02 20:54:08,969 - INFO -   Batch 750/2527: Loss = 0.1292
2026-01-02 20:54:37,533 - INFO -   Batch 800/2527: Loss = 0.1429
2026-01-02 20:55:06,369 - INFO -   Batch 850/2527: Loss = 0.1028
2026-01-02 20:55:34,914 - INFO -   Batch 900/2527: Loss = 0.1049
2026-01-02 20:56:03,764 - INFO -   Batch 950/2527: Loss = 0.1341
2026-01-02 20:56:32,638 - INFO -   Batch 1000/2527: Loss = 0.1192
2026-01-02 20:57:01,442 - INFO -   Batch 1050/2527: Loss = 0.1281
2026-01-02 20:57:30,217 - INFO -   Batch 1100/2527: Loss = 0.1877
2026-01-02 20:57:58,982 - INFO -   Batch 1150/2527: Loss = 0.1409
2026-01-02 20:58:27,852 - INFO -   Batch 1200/2527: Loss = 0.1495
2026-01-02 20:58:56,486 - INFO -   Batch 1250/2527: Loss = 0.1395
2026-01-02 20:59:25,228 - INFO -   Batch 1300/2527: Loss = 0.1913
2026-01-02 20:59:53,848 - INFO -   Batch 1350/2527: Loss = 0.1239
2026-01-02 21:00:22,910 - INFO -   Batch 1400/2527: Loss = 0.1791
2026-01-02 21:00:51,393 - INFO -   Batch 1450/2527: Loss = 0.1388
2026-01-02 21:01:20,132 - INFO -   Batch 1500/2527: Loss = 0.1150
2026-01-02 21:01:49,244 - INFO -   Batch 1550/2527: Loss = 0.1606
2026-01-02 21:02:17,873 - INFO -   Batch 1600/2527: Loss = 0.1312
2026-01-02 21:02:46,684 - INFO -   Batch 1650/2527: Loss = 0.1604
2026-01-02 21:03:15,249 - INFO -   Batch 1700/2527: Loss = 0.1491
2026-01-02 21:03:43,751 - INFO -   Batch 1750/2527: Loss = 0.1417
2026-01-02 21:04:12,811 - INFO -   Batch 1800/2527: Loss = 0.1446
2026-01-02 21:04:41,364 - INFO -   Batch 1850/2527: Loss = 0.1160
2026-01-02 21:05:10,322 - INFO -   Batch 1900/2527: Loss = 0.1911
2026-01-02 21:05:39,202 - INFO -   Batch 1950/2527: Loss = 0.2077
2026-01-02 21:06:08,094 - INFO -   Batch 2000/2527: Loss = 0.1464
2026-01-02 21:06:36,710 - INFO -   Batch 2050/2527: Loss = 0.1238
2026-01-02 21:07:05,173 - INFO -   Batch 2100/2527: Loss = 0.1501
2026-01-02 21:07:33,984 - INFO -   Batch 2150/2527: Loss = 0.1500
2026-01-02 21:08:02,854 - INFO -   Batch 2200/2527: Loss = 0.1145
2026-01-02 21:08:31,625 - INFO -   Batch 2250/2527: Loss = 0.1527
2026-01-02 21:09:00,558 - INFO -   Batch 2300/2527: Loss = 0.1514
2026-01-02 21:09:29,323 - INFO -   Batch 2350/2527: Loss = 0.2234
2026-01-02 21:09:58,230 - INFO -   Batch 2400/2527: Loss = 0.1825
2026-01-02 21:10:26,824 - INFO -   Batch 2450/2527: Loss = 0.1206
2026-01-02 21:10:55,621 - INFO -   Batch 2500/2527: Loss = 0.1625
2026-01-02 21:15:34,096 - INFO - Epoch 49/160: Train Loss: 0.1536, Val Loss: 1.1111, Val mIoU: 39.13%, Val PixelAcc: 77.80%
2026-01-02 21:16:04,312 - INFO -   Batch 50/2527: Loss = 0.1609
2026-01-02 21:16:33,355 - INFO -   Batch 100/2527: Loss = 0.1525
2026-01-02 21:17:02,181 - INFO -   Batch 150/2527: Loss = 0.1713
2026-01-02 21:17:30,985 - INFO -   Batch 200/2527: Loss = 0.2207
2026-01-02 21:17:59,616 - INFO -   Batch 250/2527: Loss = 0.1955
2026-01-02 21:18:28,439 - INFO -   Batch 300/2527: Loss = 0.1443
2026-01-02 21:18:57,152 - INFO -   Batch 350/2527: Loss = 0.1128
2026-01-02 21:19:25,794 - INFO -   Batch 400/2527: Loss = 0.1201
2026-01-02 21:19:54,731 - INFO -   Batch 450/2527: Loss = 0.1029
2026-01-02 21:20:23,626 - INFO -   Batch 500/2527: Loss = 0.1549
2026-01-02 21:20:52,506 - INFO -   Batch 550/2527: Loss = 0.2356
2026-01-02 21:21:21,092 - INFO -   Batch 600/2527: Loss = 0.1050
2026-01-02 21:21:49,547 - INFO -   Batch 650/2527: Loss = 0.1842
2026-01-02 21:22:18,435 - INFO -   Batch 700/2527: Loss = 0.1605
2026-01-02 21:22:47,277 - INFO -   Batch 750/2527: Loss = 0.1365
2026-01-02 21:23:16,130 - INFO -   Batch 800/2527: Loss = 0.1971
2026-01-02 21:23:44,913 - INFO -   Batch 850/2527: Loss = 0.1085
2026-01-02 21:24:14,062 - INFO -   Batch 900/2527: Loss = 0.2503
2026-01-02 21:24:42,751 - INFO -   Batch 950/2527: Loss = 0.3027
2026-01-02 21:25:11,237 - INFO -   Batch 1000/2527: Loss = 0.1387
2026-01-02 21:25:40,057 - INFO -   Batch 1050/2527: Loss = 0.1741
2026-01-02 21:26:08,620 - INFO -   Batch 1100/2527: Loss = 0.1297
2026-01-02 21:26:37,677 - INFO -   Batch 1150/2527: Loss = 0.1264
2026-01-02 21:27:06,315 - INFO -   Batch 1200/2527: Loss = 0.1149
2026-01-02 21:27:34,925 - INFO -   Batch 1250/2527: Loss = 0.1470
2026-01-02 21:28:03,996 - INFO -   Batch 1300/2527: Loss = 0.1536
2026-01-02 21:28:32,651 - INFO -   Batch 1350/2527: Loss = 0.1360
2026-01-02 21:29:01,454 - INFO -   Batch 1400/2527: Loss = 0.1332
2026-01-02 21:29:30,062 - INFO -   Batch 1450/2527: Loss = 0.0976
2026-01-02 21:29:58,933 - INFO -   Batch 1500/2527: Loss = 0.1420
2026-01-02 21:30:27,719 - INFO -   Batch 1550/2527: Loss = 0.1817
2026-01-02 21:30:56,191 - INFO -   Batch 1600/2527: Loss = 0.1551
2026-01-02 21:31:25,147 - INFO -   Batch 1650/2527: Loss = 0.1658
2026-01-02 21:31:53,989 - INFO -   Batch 1700/2527: Loss = 0.1723
2026-01-02 21:32:22,910 - INFO -   Batch 1750/2527: Loss = 0.1954
2026-01-02 21:32:51,473 - INFO -   Batch 1800/2527: Loss = 0.1528
2026-01-02 21:33:20,316 - INFO -   Batch 1850/2527: Loss = 0.0964
2026-01-02 21:33:48,808 - INFO -   Batch 1900/2527: Loss = 0.1339
2026-01-02 21:34:17,967 - INFO -   Batch 1950/2527: Loss = 0.1431
2026-01-02 21:34:46,997 - INFO -   Batch 2000/2527: Loss = 0.1358
2026-01-02 21:35:16,107 - INFO -   Batch 2050/2527: Loss = 0.1544
2026-01-02 21:35:45,383 - INFO -   Batch 2100/2527: Loss = 0.1108
2026-01-02 21:36:14,401 - INFO -   Batch 2150/2527: Loss = 0.1342
2026-01-02 21:36:43,265 - INFO -   Batch 2200/2527: Loss = 0.1156
2026-01-02 21:37:12,286 - INFO -   Batch 2250/2527: Loss = 0.1155
2026-01-02 21:37:41,296 - INFO -   Batch 2300/2527: Loss = 0.1561
2026-01-02 21:38:10,665 - INFO -   Batch 2350/2527: Loss = 0.1223
2026-01-02 21:38:39,626 - INFO -   Batch 2400/2527: Loss = 0.1187
2026-01-02 21:39:08,928 - INFO -   Batch 2450/2527: Loss = 0.1921
2026-01-02 21:39:38,029 - INFO -   Batch 2500/2527: Loss = 0.1524
2026-01-02 21:44:27,014 - INFO - Epoch 50/160: Train Loss: 0.1486, Val Loss: 1.1043, Val mIoU: 38.17%, Val PixelAcc: 77.81%
2026-01-02 21:44:30,414 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_50.pth
2026-01-02 21:44:30,417 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_50.pth
2026-01-02 21:45:01,971 - INFO -   Batch 50/2527: Loss = 0.1591
2026-01-02 21:45:31,100 - INFO -   Batch 100/2527: Loss = 0.1776
2026-01-02 21:46:00,114 - INFO -   Batch 150/2527: Loss = 0.1624
2026-01-02 21:46:28,824 - INFO -   Batch 200/2527: Loss = 0.1614
2026-01-02 21:46:58,025 - INFO -   Batch 250/2527: Loss = 0.1806
2026-01-02 21:47:27,135 - INFO -   Batch 300/2527: Loss = 0.1620
2026-01-02 21:47:56,429 - INFO -   Batch 350/2527: Loss = 0.1101
2026-01-02 21:48:25,610 - INFO -   Batch 400/2527: Loss = 0.1523
2026-01-02 21:48:54,876 - INFO -   Batch 450/2527: Loss = 0.1525
2026-01-02 21:49:24,221 - INFO -   Batch 500/2527: Loss = 0.2178
2026-01-02 21:49:52,975 - INFO -   Batch 550/2527: Loss = 0.1485
2026-01-02 21:50:21,946 - INFO -   Batch 600/2527: Loss = 0.0899
2026-01-02 21:50:50,857 - INFO -   Batch 650/2527: Loss = 0.1886
2026-01-02 21:51:19,982 - INFO -   Batch 700/2527: Loss = 0.1008
2026-01-02 21:51:48,950 - INFO -   Batch 750/2527: Loss = 0.1475
2026-01-02 21:52:17,847 - INFO -   Batch 800/2527: Loss = 0.1390
2026-01-02 21:52:47,013 - INFO -   Batch 850/2527: Loss = 0.1182
2026-01-02 21:53:16,077 - INFO -   Batch 900/2527: Loss = 0.2214
2026-01-02 21:53:45,366 - INFO -   Batch 950/2527: Loss = 0.1700
2026-01-02 21:54:14,234 - INFO -   Batch 1000/2527: Loss = 0.1622
2026-01-02 21:54:43,191 - INFO -   Batch 1050/2527: Loss = 0.1254
2026-01-02 21:55:12,197 - INFO -   Batch 1100/2527: Loss = 0.1230
2026-01-02 21:55:40,897 - INFO -   Batch 1150/2527: Loss = 0.1314
2026-01-02 21:56:09,503 - INFO -   Batch 1200/2527: Loss = 0.1219
2026-01-02 21:56:38,439 - INFO -   Batch 1250/2527: Loss = 0.1247
2026-01-02 21:57:07,435 - INFO -   Batch 1300/2527: Loss = 0.1731
2026-01-02 21:57:36,321 - INFO -   Batch 1350/2527: Loss = 0.1457
2026-01-02 21:58:04,914 - INFO -   Batch 1400/2527: Loss = 0.1449
2026-01-02 21:58:33,427 - INFO -   Batch 1450/2527: Loss = 0.1473
2026-01-02 21:59:02,338 - INFO -   Batch 1500/2527: Loss = 0.0919
2026-01-02 21:59:31,101 - INFO -   Batch 1550/2527: Loss = 0.1224
2026-01-02 22:00:00,170 - INFO -   Batch 1600/2527: Loss = 0.1051
2026-01-02 22:00:28,907 - INFO -   Batch 1650/2527: Loss = 0.1462
2026-01-02 22:00:58,025 - INFO -   Batch 1700/2527: Loss = 0.1347
2026-01-02 22:01:26,743 - INFO -   Batch 1750/2527: Loss = 0.1084
2026-01-02 22:01:55,179 - INFO -   Batch 1800/2527: Loss = 0.1177
2026-01-02 22:02:24,093 - INFO -   Batch 1850/2527: Loss = 0.1682
2026-01-02 22:02:52,955 - INFO -   Batch 1900/2527: Loss = 0.0881
2026-01-02 22:03:22,128 - INFO -   Batch 1950/2527: Loss = 0.2000
2026-01-02 22:03:51,124 - INFO -   Batch 2000/2527: Loss = 0.1382
2026-01-02 22:04:19,986 - INFO -   Batch 2050/2527: Loss = 0.1310
2026-01-02 22:04:48,905 - INFO -   Batch 2100/2527: Loss = 0.1152
2026-01-02 22:05:17,779 - INFO -   Batch 2150/2527: Loss = 0.2081
2026-01-02 22:05:46,571 - INFO -   Batch 2200/2527: Loss = 0.1913
2026-01-02 22:06:15,690 - INFO -   Batch 2250/2527: Loss = 0.1389
2026-01-02 22:06:44,594 - INFO -   Batch 2300/2527: Loss = 0.1386
2026-01-02 22:07:13,589 - INFO -   Batch 2350/2527: Loss = 0.1442
2026-01-02 22:07:42,728 - INFO -   Batch 2400/2527: Loss = 0.1555
2026-01-02 22:08:11,366 - INFO -   Batch 2450/2527: Loss = 0.1877
2026-01-02 22:08:40,445 - INFO -   Batch 2500/2527: Loss = 0.1736
2026-01-02 22:13:34,473 - INFO - Epoch 51/160: Train Loss: 0.1455, Val Loss: 1.0938, Val mIoU: 39.86%, Val PixelAcc: 78.33%
2026-01-02 22:13:34,475 - INFO -   → New best mIoU: 39.86%
2026-01-02 22:13:38,402 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-02 22:13:38,405 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-02 22:14:09,117 - INFO -   Batch 50/2527: Loss = 0.1660
2026-01-02 22:14:37,808 - INFO -   Batch 100/2527: Loss = 0.1258
2026-01-02 22:15:06,637 - INFO -   Batch 150/2527: Loss = 0.1178
2026-01-02 22:15:35,814 - INFO -   Batch 200/2527: Loss = 0.1312
2026-01-02 22:16:04,538 - INFO -   Batch 250/2527: Loss = 0.0931
2026-01-02 22:16:33,536 - INFO -   Batch 300/2527: Loss = 0.1393
2026-01-02 22:17:02,576 - INFO -   Batch 350/2527: Loss = 0.1084
2026-01-02 22:17:31,345 - INFO -   Batch 400/2527: Loss = 0.1630
2026-01-02 22:18:00,283 - INFO -   Batch 450/2527: Loss = 0.1070
2026-01-02 22:18:29,228 - INFO -   Batch 500/2527: Loss = 0.1131
2026-01-02 22:18:58,343 - INFO -   Batch 550/2527: Loss = 0.0735
2026-01-02 22:19:27,361 - INFO -   Batch 600/2527: Loss = 0.0967
2026-01-02 22:19:56,381 - INFO -   Batch 650/2527: Loss = 0.1251
2026-01-02 22:20:25,309 - INFO -   Batch 700/2527: Loss = 0.1792
2026-01-02 22:20:54,405 - INFO -   Batch 750/2527: Loss = 0.1251
2026-01-02 22:21:23,506 - INFO -   Batch 800/2527: Loss = 0.0980
2026-01-02 22:21:52,238 - INFO -   Batch 850/2527: Loss = 0.0882
2026-01-02 22:22:21,499 - INFO -   Batch 900/2527: Loss = 0.1092
2026-01-02 22:22:50,464 - INFO -   Batch 950/2527: Loss = 0.1681
2026-01-02 22:23:19,358 - INFO -   Batch 1000/2527: Loss = 0.0995
2026-01-02 22:23:48,528 - INFO -   Batch 1050/2527: Loss = 0.1827
2026-01-02 22:24:17,510 - INFO -   Batch 1100/2527: Loss = 0.1312
2026-01-02 22:24:46,760 - INFO -   Batch 1150/2527: Loss = 0.2037
2026-01-02 22:25:15,546 - INFO -   Batch 1200/2527: Loss = 0.1508
2026-01-02 22:25:44,261 - INFO -   Batch 1250/2527: Loss = 0.1352
2026-01-02 22:26:13,161 - INFO -   Batch 1300/2527: Loss = 0.1341
2026-01-02 22:26:41,744 - INFO -   Batch 1350/2527: Loss = 0.0990
2026-01-02 22:27:10,562 - INFO -   Batch 1400/2527: Loss = 0.1556
2026-01-02 22:27:39,330 - INFO -   Batch 1450/2527: Loss = 0.1261
2026-01-02 22:28:08,049 - INFO -   Batch 1500/2527: Loss = 0.1649
2026-01-02 22:28:37,102 - INFO -   Batch 1550/2527: Loss = 0.1502
2026-01-02 22:29:05,767 - INFO -   Batch 1600/2527: Loss = 0.0996
2026-01-02 22:29:34,190 - INFO -   Batch 1650/2527: Loss = 0.1839
2026-01-02 22:30:02,935 - INFO -   Batch 1700/2527: Loss = 0.1462
2026-01-02 22:30:31,782 - INFO -   Batch 1750/2527: Loss = 0.1193
2026-01-02 22:31:00,863 - INFO -   Batch 1800/2527: Loss = 0.0695
2026-01-02 22:31:29,471 - INFO -   Batch 1850/2527: Loss = 0.1167
2026-01-02 22:31:58,426 - INFO -   Batch 1900/2527: Loss = 0.1323
2026-01-02 22:32:27,306 - INFO -   Batch 1950/2527: Loss = 0.1400
2026-01-02 22:32:55,844 - INFO -   Batch 2000/2527: Loss = 0.1461
2026-01-02 22:33:24,596 - INFO -   Batch 2050/2527: Loss = 0.1238
2026-01-02 22:33:53,151 - INFO -   Batch 2100/2527: Loss = 0.1337
2026-01-02 22:34:22,055 - INFO -   Batch 2150/2527: Loss = 0.1307
2026-01-02 22:34:50,902 - INFO -   Batch 2200/2527: Loss = 0.1193
2026-01-02 22:35:19,350 - INFO -   Batch 2250/2527: Loss = 0.1967
2026-01-02 22:35:48,307 - INFO -   Batch 2300/2527: Loss = 0.1271
2026-01-02 22:36:17,353 - INFO -   Batch 2350/2527: Loss = 0.1394
2026-01-02 22:36:46,281 - INFO -   Batch 2400/2527: Loss = 0.1266
2026-01-02 22:37:14,958 - INFO -   Batch 2450/2527: Loss = 0.1312
2026-01-02 22:37:43,710 - INFO -   Batch 2500/2527: Loss = 0.1367
2026-01-02 22:42:30,424 - INFO - Epoch 52/160: Train Loss: 0.1411, Val Loss: 1.1721, Val mIoU: 37.68%, Val PixelAcc: 76.67%
2026-01-02 22:43:00,851 - INFO -   Batch 50/2527: Loss = 0.1443
2026-01-02 22:43:29,944 - INFO -   Batch 100/2527: Loss = 0.0944
2026-01-02 22:43:58,609 - INFO -   Batch 150/2527: Loss = 0.1226
2026-01-02 22:44:27,425 - INFO -   Batch 200/2527: Loss = 0.1987
2026-01-02 22:44:55,901 - INFO -   Batch 250/2527: Loss = 0.1440
2026-01-02 22:45:24,549 - INFO -   Batch 300/2527: Loss = 0.1446
2026-01-02 22:45:53,643 - INFO -   Batch 350/2527: Loss = 0.1598
2026-01-02 22:46:22,265 - INFO -   Batch 400/2527: Loss = 0.2062
2026-01-02 22:46:51,187 - INFO -   Batch 450/2527: Loss = 0.1204
2026-01-02 22:47:20,038 - INFO -   Batch 500/2527: Loss = 0.1527
2026-01-02 22:47:48,638 - INFO -   Batch 550/2527: Loss = 0.1115
2026-01-02 22:48:17,449 - INFO -   Batch 600/2527: Loss = 0.1668
2026-01-02 22:48:46,104 - INFO -   Batch 650/2527: Loss = 0.1752
2026-01-02 22:49:14,928 - INFO -   Batch 700/2527: Loss = 0.1057
2026-01-02 22:49:43,752 - INFO -   Batch 750/2527: Loss = 0.2978
2026-01-02 22:50:12,578 - INFO -   Batch 800/2527: Loss = 0.1390
2026-01-02 22:50:41,322 - INFO -   Batch 850/2527: Loss = 0.1207
2026-01-02 22:51:10,112 - INFO -   Batch 900/2527: Loss = 0.2717
2026-01-02 22:51:39,018 - INFO -   Batch 950/2527: Loss = 0.1220
2026-01-02 22:52:07,554 - INFO -   Batch 1000/2527: Loss = 0.1670
2026-01-02 22:52:36,380 - INFO -   Batch 1050/2527: Loss = 0.1301
2026-01-02 22:53:04,986 - INFO -   Batch 1100/2527: Loss = 0.1070
2026-01-02 22:53:33,725 - INFO -   Batch 1150/2527: Loss = 0.0967
2026-01-02 22:54:02,568 - INFO -   Batch 1200/2527: Loss = 0.1663
2026-01-02 22:54:31,310 - INFO -   Batch 1250/2527: Loss = 0.1601
2026-01-02 22:55:00,388 - INFO -   Batch 1300/2527: Loss = 0.1869
2026-01-02 22:55:29,101 - INFO -   Batch 1350/2527: Loss = 0.1109
2026-01-02 22:55:57,877 - INFO -   Batch 1400/2527: Loss = 0.1500
2026-01-02 22:56:26,664 - INFO -   Batch 1450/2527: Loss = 0.1063
2026-01-02 22:56:55,165 - INFO -   Batch 1500/2527: Loss = 0.2029
2026-01-02 22:57:24,250 - INFO -   Batch 1550/2527: Loss = 0.1601
2026-01-02 22:57:52,919 - INFO -   Batch 1600/2527: Loss = 0.1081
2026-01-02 22:58:21,869 - INFO -   Batch 1650/2527: Loss = 0.1163
2026-01-02 22:58:50,802 - INFO -   Batch 1700/2527: Loss = 0.1565
2026-01-02 22:59:19,731 - INFO -   Batch 1750/2527: Loss = 0.1637
2026-01-02 22:59:48,200 - INFO -   Batch 1800/2527: Loss = 0.1258
2026-01-02 23:00:16,786 - INFO -   Batch 1850/2527: Loss = 0.1686
2026-01-02 23:00:45,649 - INFO -   Batch 1900/2527: Loss = 0.0897
2026-01-02 23:01:14,453 - INFO -   Batch 1950/2527: Loss = 0.1093
2026-01-02 23:01:43,301 - INFO -   Batch 2000/2527: Loss = 0.0893
2026-01-02 23:02:12,042 - INFO -   Batch 2050/2527: Loss = 0.1486
2026-01-02 23:02:40,819 - INFO -   Batch 2100/2527: Loss = 0.1252
2026-01-02 23:03:09,755 - INFO -   Batch 2150/2527: Loss = 0.1239
2026-01-02 23:03:38,401 - INFO -   Batch 2200/2527: Loss = 0.1046
2026-01-02 23:04:07,183 - INFO -   Batch 2250/2527: Loss = 0.1284
2026-01-02 23:04:35,842 - INFO -   Batch 2300/2527: Loss = 0.1546
2026-01-02 23:05:04,872 - INFO -   Batch 2350/2527: Loss = 0.1094
2026-01-02 23:05:33,443 - INFO -   Batch 2400/2527: Loss = 0.1820
2026-01-02 23:06:02,059 - INFO -   Batch 2450/2527: Loss = 0.1504
2026-01-02 23:06:31,125 - INFO -   Batch 2500/2527: Loss = 0.1665
2026-01-02 23:11:10,975 - INFO - Epoch 53/160: Train Loss: 0.1406, Val Loss: 1.1064, Val mIoU: 39.18%, Val PixelAcc: 78.16%
2026-01-02 23:11:41,535 - INFO -   Batch 50/2527: Loss = 0.1739
2026-01-02 23:12:10,549 - INFO -   Batch 100/2527: Loss = 0.1526
2026-01-02 23:12:39,211 - INFO -   Batch 150/2527: Loss = 0.1634
2026-01-02 23:13:07,965 - INFO -   Batch 200/2527: Loss = 0.0916
2026-01-02 23:13:36,620 - INFO -   Batch 250/2527: Loss = 0.1674
2026-01-02 23:14:05,476 - INFO -   Batch 300/2527: Loss = 0.0878
2026-01-02 23:14:34,078 - INFO -   Batch 350/2527: Loss = 0.1227
2026-01-02 23:15:02,747 - INFO -   Batch 400/2527: Loss = 0.1162
2026-01-02 23:15:31,671 - INFO -   Batch 450/2527: Loss = 0.1405
2026-01-02 23:16:00,547 - INFO -   Batch 500/2527: Loss = 0.1067
2026-01-02 23:16:29,479 - INFO -   Batch 550/2527: Loss = 0.1714
2026-01-02 23:16:58,025 - INFO -   Batch 600/2527: Loss = 0.1230
2026-01-02 23:17:26,891 - INFO -   Batch 650/2527: Loss = 0.1426
2026-01-02 23:17:55,438 - INFO -   Batch 700/2527: Loss = 0.1802
2026-01-02 23:18:24,017 - INFO -   Batch 750/2527: Loss = 0.1163
2026-01-02 23:18:53,045 - INFO -   Batch 800/2527: Loss = 0.1382
2026-01-02 23:19:21,725 - INFO -   Batch 850/2527: Loss = 0.1232
2026-01-02 23:19:50,771 - INFO -   Batch 900/2527: Loss = 0.1245
2026-01-02 23:20:19,496 - INFO -   Batch 950/2527: Loss = 0.1613
2026-01-02 23:20:47,987 - INFO -   Batch 1000/2527: Loss = 0.1196
2026-01-02 23:21:16,823 - INFO -   Batch 1050/2527: Loss = 0.0924
2026-01-02 23:21:45,675 - INFO -   Batch 1100/2527: Loss = 0.1352
2026-01-02 23:22:14,501 - INFO -   Batch 1150/2527: Loss = 0.1189
2026-01-02 23:22:43,299 - INFO -   Batch 1200/2527: Loss = 0.1630
2026-01-02 23:23:12,196 - INFO -   Batch 1250/2527: Loss = 0.1233
2026-01-02 23:23:41,073 - INFO -   Batch 1300/2527: Loss = 0.1200
2026-01-02 23:24:09,643 - INFO -   Batch 1350/2527: Loss = 0.1018
2026-01-02 23:24:38,466 - INFO -   Batch 1400/2527: Loss = 0.1614
2026-01-02 23:25:07,087 - INFO -   Batch 1450/2527: Loss = 0.1607
2026-01-02 23:25:36,022 - INFO -   Batch 1500/2527: Loss = 0.2088
2026-01-02 23:26:04,615 - INFO -   Batch 1550/2527: Loss = 0.1196
2026-01-02 23:26:33,616 - INFO -   Batch 1600/2527: Loss = 0.2193
2026-01-02 23:27:02,188 - INFO -   Batch 1650/2527: Loss = 0.1249
2026-01-02 23:27:31,007 - INFO -   Batch 1700/2527: Loss = 0.1247
2026-01-02 23:27:59,966 - INFO -   Batch 1750/2527: Loss = 0.1103
2026-01-02 23:28:28,604 - INFO -   Batch 1800/2527: Loss = 0.1216
2026-01-02 23:28:57,480 - INFO -   Batch 1850/2527: Loss = 0.2029
2026-01-02 23:29:26,187 - INFO -   Batch 1900/2527: Loss = 0.1282
2026-01-02 23:29:54,709 - INFO -   Batch 1950/2527: Loss = 0.1350
2026-01-02 23:30:23,673 - INFO -   Batch 2000/2527: Loss = 0.1700
2026-01-02 23:30:52,369 - INFO -   Batch 2050/2527: Loss = 0.1143
2026-01-02 23:31:21,419 - INFO -   Batch 2100/2527: Loss = 0.1525
2026-01-02 23:31:50,193 - INFO -   Batch 2150/2527: Loss = 0.1324
2026-01-02 23:32:18,970 - INFO -   Batch 2200/2527: Loss = 0.1093
2026-01-02 23:32:47,612 - INFO -   Batch 2250/2527: Loss = 0.1560
2026-01-02 23:33:16,199 - INFO -   Batch 2300/2527: Loss = 0.1285
2026-01-02 23:33:45,076 - INFO -   Batch 2350/2527: Loss = 0.1409
2026-01-02 23:34:13,840 - INFO -   Batch 2400/2527: Loss = 0.1467
2026-01-02 23:34:42,761 - INFO -   Batch 2450/2527: Loss = 0.2679
2026-01-02 23:35:11,538 - INFO -   Batch 2500/2527: Loss = 0.1008
2026-01-02 23:39:57,268 - INFO - Epoch 54/160: Train Loss: 0.1371, Val Loss: 1.1090, Val mIoU: 39.17%, Val PixelAcc: 78.37%
2026-01-02 23:40:27,972 - INFO -   Batch 50/2527: Loss = 0.1157
2026-01-02 23:40:56,522 - INFO -   Batch 100/2527: Loss = 0.1221
2026-01-02 23:41:25,308 - INFO -   Batch 150/2527: Loss = 0.1389
2026-01-02 23:41:54,101 - INFO -   Batch 200/2527: Loss = 0.1392
2026-01-02 23:42:22,570 - INFO -   Batch 250/2527: Loss = 0.1381
2026-01-02 23:42:51,538 - INFO -   Batch 300/2527: Loss = 0.1505
2026-01-02 23:43:20,474 - INFO -   Batch 350/2527: Loss = 0.1434
2026-01-02 23:43:49,328 - INFO -   Batch 400/2527: Loss = 0.1117
2026-01-02 23:44:17,903 - INFO -   Batch 450/2527: Loss = 0.0895
2026-01-02 23:44:46,372 - INFO -   Batch 500/2527: Loss = 0.1516
2026-01-02 23:45:15,188 - INFO -   Batch 550/2527: Loss = 0.1464
2026-01-02 23:45:44,050 - INFO -   Batch 600/2527: Loss = 0.1145
2026-01-02 23:46:12,873 - INFO -   Batch 650/2527: Loss = 0.1171
2026-01-02 23:46:41,592 - INFO -   Batch 700/2527: Loss = 0.1978
2026-01-02 23:47:10,687 - INFO -   Batch 750/2527: Loss = 0.0973
2026-01-02 23:47:39,331 - INFO -   Batch 800/2527: Loss = 0.1922
2026-01-02 23:48:07,798 - INFO -   Batch 850/2527: Loss = 0.1802
2026-01-02 23:48:36,596 - INFO -   Batch 900/2527: Loss = 0.1329
2026-01-02 23:49:05,210 - INFO -   Batch 950/2527: Loss = 0.1054
2026-01-02 23:49:34,281 - INFO -   Batch 1000/2527: Loss = 0.1594
2026-01-02 23:50:02,907 - INFO -   Batch 1050/2527: Loss = 0.1022
2026-01-02 23:50:31,826 - INFO -   Batch 1100/2527: Loss = 0.0861
2026-01-02 23:51:00,900 - INFO -   Batch 1150/2527: Loss = 0.1491
2026-01-02 23:51:29,464 - INFO -   Batch 1200/2527: Loss = 0.1602
2026-01-02 23:51:58,241 - INFO -   Batch 1250/2527: Loss = 0.1150
2026-01-02 23:52:26,846 - INFO -   Batch 1300/2527: Loss = 0.1643
2026-01-02 23:52:55,677 - INFO -   Batch 1350/2527: Loss = 0.1595
2026-01-02 23:53:24,546 - INFO -   Batch 1400/2527: Loss = 0.1909
2026-01-02 23:53:53,020 - INFO -   Batch 1450/2527: Loss = 0.1246
2026-01-02 23:54:21,967 - INFO -   Batch 1500/2527: Loss = 0.0931
2026-01-02 23:54:50,860 - INFO -   Batch 1550/2527: Loss = 0.1099
2026-01-02 23:55:19,793 - INFO -   Batch 1600/2527: Loss = 0.1822
2026-01-02 23:55:48,322 - INFO -   Batch 1650/2527: Loss = 0.1255
2026-01-02 23:56:17,140 - INFO -   Batch 1700/2527: Loss = 0.1589
2026-01-02 23:56:45,784 - INFO -   Batch 1750/2527: Loss = 0.1524
2026-01-02 23:57:14,536 - INFO -   Batch 1800/2527: Loss = 0.1108
2026-01-02 23:57:43,395 - INFO -   Batch 1850/2527: Loss = 0.1366
2026-01-02 23:58:12,153 - INFO -   Batch 1900/2527: Loss = 0.1081
2026-01-02 23:58:41,253 - INFO -   Batch 1950/2527: Loss = 0.0980
2026-01-02 23:59:09,970 - INFO -   Batch 2000/2527: Loss = 0.1101
2026-01-02 23:59:38,754 - INFO -   Batch 2050/2527: Loss = 0.1365
2026-01-03 00:00:07,218 - INFO -   Batch 2100/2527: Loss = 0.1478
2026-01-03 00:00:35,813 - INFO -   Batch 2150/2527: Loss = 0.1294
2026-01-03 00:01:04,882 - INFO -   Batch 2200/2527: Loss = 0.1781
2026-01-03 00:01:33,499 - INFO -   Batch 2250/2527: Loss = 0.0777
2026-01-03 00:02:02,467 - INFO -   Batch 2300/2527: Loss = 0.1519
2026-01-03 00:02:31,339 - INFO -   Batch 2350/2527: Loss = 0.1199
2026-01-03 00:02:59,938 - INFO -   Batch 2400/2527: Loss = 0.1268
2026-01-03 00:03:28,744 - INFO -   Batch 2450/2527: Loss = 0.1813
2026-01-03 00:03:57,355 - INFO -   Batch 2500/2527: Loss = 0.1816
2026-01-03 00:08:36,962 - INFO - Epoch 55/160: Train Loss: 0.1343, Val Loss: 1.1444, Val mIoU: 39.17%, Val PixelAcc: 78.26%
2026-01-03 00:09:07,021 - INFO -   Batch 50/2527: Loss = 0.1102
2026-01-03 00:09:35,882 - INFO -   Batch 100/2527: Loss = 0.1504
2026-01-03 00:10:04,793 - INFO -   Batch 150/2527: Loss = 0.0948
2026-01-03 00:10:33,295 - INFO -   Batch 200/2527: Loss = 0.1455
2026-01-03 00:11:02,092 - INFO -   Batch 250/2527: Loss = 0.4546
2026-01-03 00:11:30,759 - INFO -   Batch 300/2527: Loss = 0.1510
2026-01-03 00:11:59,495 - INFO -   Batch 350/2527: Loss = 0.1373
2026-01-03 00:12:28,284 - INFO -   Batch 400/2527: Loss = 0.0949
2026-01-03 00:12:56,988 - INFO -   Batch 450/2527: Loss = 0.1180
2026-01-03 00:13:26,057 - INFO -   Batch 500/2527: Loss = 0.1947
2026-01-03 00:13:54,736 - INFO -   Batch 550/2527: Loss = 0.0823
2026-01-03 00:14:23,494 - INFO -   Batch 600/2527: Loss = 0.1444
2026-01-03 00:14:52,066 - INFO -   Batch 650/2527: Loss = 0.1358
2026-01-03 00:15:20,575 - INFO -   Batch 700/2527: Loss = 0.1005
2026-01-03 00:15:49,663 - INFO -   Batch 750/2527: Loss = 0.1335
2026-01-03 00:16:18,266 - INFO -   Batch 800/2527: Loss = 0.0954
2026-01-03 00:16:47,265 - INFO -   Batch 850/2527: Loss = 0.1341
2026-01-03 00:17:16,288 - INFO -   Batch 900/2527: Loss = 0.1308
2026-01-03 00:17:45,230 - INFO -   Batch 950/2527: Loss = 0.1064
2026-01-03 00:18:13,675 - INFO -   Batch 1000/2527: Loss = 0.1630
2026-01-03 00:18:42,242 - INFO -   Batch 1050/2527: Loss = 0.1389
2026-01-03 00:19:11,105 - INFO -   Batch 1100/2527: Loss = 0.1737
2026-01-03 00:19:39,938 - INFO -   Batch 1150/2527: Loss = 0.1786
2026-01-03 00:20:08,765 - INFO -   Batch 1200/2527: Loss = 0.1428
2026-01-03 00:20:37,510 - INFO -   Batch 1250/2527: Loss = 0.0724
2026-01-03 00:21:06,272 - INFO -   Batch 1300/2527: Loss = 0.2006
2026-01-03 00:21:35,138 - INFO -   Batch 1350/2527: Loss = 0.1316
2026-01-03 00:22:03,701 - INFO -   Batch 1400/2527: Loss = 0.1108
2026-01-03 00:22:32,491 - INFO -   Batch 1450/2527: Loss = 0.1162
2026-01-03 00:23:01,159 - INFO -   Batch 1500/2527: Loss = 0.0989
2026-01-03 00:23:30,172 - INFO -   Batch 1550/2527: Loss = 0.0920
2026-01-03 00:23:58,841 - INFO -   Batch 1600/2527: Loss = 0.0993
2026-01-03 00:24:27,461 - INFO -   Batch 1650/2527: Loss = 0.1144
2026-01-03 00:24:56,573 - INFO -   Batch 1700/2527: Loss = 0.0905
2026-01-03 00:25:25,261 - INFO -   Batch 1750/2527: Loss = 0.1440
2026-01-03 00:25:54,059 - INFO -   Batch 1800/2527: Loss = 0.1869
2026-01-03 00:26:22,616 - INFO -   Batch 1850/2527: Loss = 0.1294
2026-01-03 00:26:51,130 - INFO -   Batch 1900/2527: Loss = 0.0999
2026-01-03 00:27:20,212 - INFO -   Batch 1950/2527: Loss = 0.1108
2026-01-03 00:27:48,854 - INFO -   Batch 2000/2527: Loss = 0.1368
2026-01-03 00:28:17,813 - INFO -   Batch 2050/2527: Loss = 0.1443
2026-01-03 00:28:46,645 - INFO -   Batch 2100/2527: Loss = 0.1359
2026-01-03 00:29:15,576 - INFO -   Batch 2150/2527: Loss = 0.1134
2026-01-03 00:29:44,180 - INFO -   Batch 2200/2527: Loss = 0.1104
2026-01-03 00:30:12,650 - INFO -   Batch 2250/2527: Loss = 0.1263
2026-01-03 00:30:41,481 - INFO -   Batch 2300/2527: Loss = 0.1351
2026-01-03 00:31:10,306 - INFO -   Batch 2350/2527: Loss = 0.1513
2026-01-03 00:31:39,089 - INFO -   Batch 2400/2527: Loss = 0.1205
2026-01-03 00:32:07,829 - INFO -   Batch 2450/2527: Loss = 0.2470
2026-01-03 00:32:36,861 - INFO -   Batch 2500/2527: Loss = 0.0993
2026-01-03 00:37:16,709 - INFO - Epoch 56/160: Train Loss: 0.1314, Val Loss: 1.1734, Val mIoU: 38.52%, Val PixelAcc: 77.89%
2026-01-03 00:37:47,347 - INFO -   Batch 50/2527: Loss = 0.0967
2026-01-03 00:38:16,337 - INFO -   Batch 100/2527: Loss = 0.1169
2026-01-03 00:38:44,987 - INFO -   Batch 150/2527: Loss = 0.0874
2026-01-03 00:39:13,688 - INFO -   Batch 200/2527: Loss = 0.1334
2026-01-03 00:39:42,215 - INFO -   Batch 250/2527: Loss = 0.1272
2026-01-03 00:40:11,119 - INFO -   Batch 300/2527: Loss = 0.1142
2026-01-03 00:40:39,791 - INFO -   Batch 350/2527: Loss = 0.0821
2026-01-03 00:41:08,785 - INFO -   Batch 400/2527: Loss = 0.0853
2026-01-03 00:41:37,523 - INFO -   Batch 450/2527: Loss = 0.1128
2026-01-03 00:42:06,567 - INFO -   Batch 500/2527: Loss = 0.0874
2026-01-03 00:42:35,139 - INFO -   Batch 550/2527: Loss = 0.1364
2026-01-03 00:43:03,764 - INFO -   Batch 600/2527: Loss = 0.1125
2026-01-03 00:43:32,618 - INFO -   Batch 650/2527: Loss = 0.1487
2026-01-03 00:44:01,298 - INFO -   Batch 700/2527: Loss = 0.1408
2026-01-03 00:44:30,143 - INFO -   Batch 750/2527: Loss = 0.1110
2026-01-03 00:44:58,931 - INFO -   Batch 800/2527: Loss = 0.1336
2026-01-03 00:45:27,537 - INFO -   Batch 850/2527: Loss = 0.0994
2026-01-03 00:45:56,609 - INFO -   Batch 900/2527: Loss = 0.0988
2026-01-03 00:46:25,326 - INFO -   Batch 950/2527: Loss = 0.1295
2026-01-03 00:46:54,126 - INFO -   Batch 1000/2527: Loss = 0.1488
2026-01-03 00:47:22,764 - INFO -   Batch 1050/2527: Loss = 0.1764
2026-01-03 00:47:51,652 - INFO -   Batch 1100/2527: Loss = 0.1023
2026-01-03 00:48:20,278 - INFO -   Batch 1150/2527: Loss = 0.1036
2026-01-03 00:48:48,913 - INFO -   Batch 1200/2527: Loss = 0.0993
2026-01-03 00:49:17,860 - INFO -   Batch 1250/2527: Loss = 0.1604
2026-01-03 00:49:46,710 - INFO -   Batch 1300/2527: Loss = 0.0722
2026-01-03 00:50:15,655 - INFO -   Batch 1350/2527: Loss = 0.1119
2026-01-03 00:50:44,221 - INFO -   Batch 1400/2527: Loss = 0.1516
2026-01-03 00:51:13,064 - INFO -   Batch 1450/2527: Loss = 0.1111
2026-01-03 00:51:41,646 - INFO -   Batch 1500/2527: Loss = 0.0919
2026-01-03 00:52:10,326 - INFO -   Batch 1550/2527: Loss = 0.0684
2026-01-03 00:52:39,305 - INFO -   Batch 1600/2527: Loss = 0.1748
2026-01-03 00:53:07,993 - INFO -   Batch 1650/2527: Loss = 0.1183
2026-01-03 00:53:37,034 - INFO -   Batch 1700/2527: Loss = 0.1473
2026-01-03 00:54:05,716 - INFO -   Batch 1750/2527: Loss = 0.1237
2026-01-03 00:54:34,172 - INFO -   Batch 1800/2527: Loss = 0.1066
2026-01-03 00:55:03,040 - INFO -   Batch 1850/2527: Loss = 0.1232
2026-01-03 00:55:31,663 - INFO -   Batch 1900/2527: Loss = 0.0901
2026-01-03 00:56:00,525 - INFO -   Batch 1950/2527: Loss = 0.0995
2026-01-03 00:56:29,309 - INFO -   Batch 2000/2527: Loss = 0.1073
2026-01-03 00:56:58,220 - INFO -   Batch 2050/2527: Loss = 0.1647
2026-01-03 00:57:27,235 - INFO -   Batch 2100/2527: Loss = 0.1335
2026-01-03 00:57:55,820 - INFO -   Batch 2150/2527: Loss = 0.1475
2026-01-03 00:58:24,630 - INFO -   Batch 2200/2527: Loss = 0.1395
2026-01-03 00:58:53,290 - INFO -   Batch 2250/2527: Loss = 0.1258
2026-01-03 00:59:22,199 - INFO -   Batch 2300/2527: Loss = 0.1570
2026-01-03 00:59:50,909 - INFO -   Batch 2350/2527: Loss = 0.1451
2026-01-03 01:00:19,555 - INFO -   Batch 2400/2527: Loss = 0.2171
2026-01-03 01:00:48,480 - INFO -   Batch 2450/2527: Loss = 0.0943
2026-01-03 01:01:17,333 - INFO -   Batch 2500/2527: Loss = 0.1484
2026-01-03 01:06:02,696 - INFO - Epoch 57/160: Train Loss: 0.1306, Val Loss: 1.1331, Val mIoU: 39.10%, Val PixelAcc: 78.26%
2026-01-03 01:06:33,098 - INFO -   Batch 50/2527: Loss = 0.1617
2026-01-03 01:07:01,512 - INFO -   Batch 100/2527: Loss = 0.1558
2026-01-03 01:07:30,341 - INFO -   Batch 150/2527: Loss = 0.1373
2026-01-03 01:07:59,175 - INFO -   Batch 200/2527: Loss = 0.1305
2026-01-03 01:08:27,994 - INFO -   Batch 250/2527: Loss = 0.1414
2026-01-03 01:08:56,926 - INFO -   Batch 300/2527: Loss = 0.1150
2026-01-03 01:09:25,680 - INFO -   Batch 350/2527: Loss = 0.0992
2026-01-03 01:09:54,557 - INFO -   Batch 400/2527: Loss = 0.1572
2026-01-03 01:10:23,159 - INFO -   Batch 450/2527: Loss = 0.0536
2026-01-03 01:10:51,966 - INFO -   Batch 500/2527: Loss = 0.1361
2026-01-03 01:11:20,622 - INFO -   Batch 550/2527: Loss = 0.1669
2026-01-03 01:11:49,660 - INFO -   Batch 600/2527: Loss = 0.1446
2026-01-03 01:12:18,278 - INFO -   Batch 650/2527: Loss = 0.2378
2026-01-03 01:12:46,924 - INFO -   Batch 700/2527: Loss = 0.1920
2026-01-03 01:13:16,027 - INFO -   Batch 750/2527: Loss = 0.1492
2026-01-03 01:13:44,675 - INFO -   Batch 800/2527: Loss = 0.1321
2026-01-03 01:14:13,466 - INFO -   Batch 850/2527: Loss = 0.1296
2026-01-03 01:14:42,041 - INFO -   Batch 900/2527: Loss = 0.1327
2026-01-03 01:15:10,925 - INFO -   Batch 950/2527: Loss = 0.0971
2026-01-03 01:15:39,787 - INFO -   Batch 1000/2527: Loss = 0.1096
2026-01-03 01:16:08,293 - INFO -   Batch 1050/2527: Loss = 0.1297
2026-01-03 01:16:37,292 - INFO -   Batch 1100/2527: Loss = 0.1313
2026-01-03 01:17:06,177 - INFO -   Batch 1150/2527: Loss = 0.1049
2026-01-03 01:17:35,105 - INFO -   Batch 1200/2527: Loss = 0.1586
2026-01-03 01:18:03,657 - INFO -   Batch 1250/2527: Loss = 0.0938
2026-01-03 01:18:32,132 - INFO -   Batch 1300/2527: Loss = 0.1098
2026-01-03 01:19:00,956 - INFO -   Batch 1350/2527: Loss = 0.1137
2026-01-03 01:19:29,822 - INFO -   Batch 1400/2527: Loss = 0.1293
2026-01-03 01:19:58,605 - INFO -   Batch 1450/2527: Loss = 0.0955
2026-01-03 01:20:27,312 - INFO -   Batch 1500/2527: Loss = 0.1045
2026-01-03 01:20:56,403 - INFO -   Batch 1550/2527: Loss = 0.1551
2026-01-03 01:21:25,100 - INFO -   Batch 1600/2527: Loss = 0.1325
2026-01-03 01:21:53,598 - INFO -   Batch 1650/2527: Loss = 0.1183
2026-01-03 01:22:22,408 - INFO -   Batch 1700/2527: Loss = 0.1423
2026-01-03 01:22:50,985 - INFO -   Batch 1750/2527: Loss = 0.1402
2026-01-03 01:23:20,072 - INFO -   Batch 1800/2527: Loss = 0.1235
2026-01-03 01:23:48,632 - INFO -   Batch 1850/2527: Loss = 0.1557
2026-01-03 01:24:17,629 - INFO -   Batch 1900/2527: Loss = 0.1227
2026-01-03 01:24:46,394 - INFO -   Batch 1950/2527: Loss = 0.1505
2026-01-03 01:25:15,037 - INFO -   Batch 2000/2527: Loss = 0.1725
2026-01-03 01:25:43,853 - INFO -   Batch 2050/2527: Loss = 0.1076
2026-01-03 01:26:12,468 - INFO -   Batch 2100/2527: Loss = 0.0954
2026-01-03 01:26:41,351 - INFO -   Batch 2150/2527: Loss = 0.1395
2026-01-03 01:27:10,394 - INFO -   Batch 2200/2527: Loss = 0.1528
2026-01-03 01:27:38,897 - INFO -   Batch 2250/2527: Loss = 0.1703
2026-01-03 01:28:07,833 - INFO -   Batch 2300/2527: Loss = 0.1459
2026-01-03 01:28:36,783 - INFO -   Batch 2350/2527: Loss = 0.1044
2026-01-03 01:29:05,688 - INFO -   Batch 2400/2527: Loss = 0.0893
2026-01-03 01:29:34,311 - INFO -   Batch 2450/2527: Loss = 0.1578
2026-01-03 01:30:03,111 - INFO -   Batch 2500/2527: Loss = 0.1163
2026-01-03 01:34:40,961 - INFO - Epoch 58/160: Train Loss: 0.1320, Val Loss: 1.1492, Val mIoU: 40.20%, Val PixelAcc: 78.35%
2026-01-03 01:34:40,963 - INFO -   → New best mIoU: 40.20%
2026-01-03 01:34:44,681 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-03 01:34:44,683 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-03 01:35:14,839 - INFO -   Batch 50/2527: Loss = 0.1300
2026-01-03 01:35:43,784 - INFO -   Batch 100/2527: Loss = 0.1136
2026-01-03 01:36:12,725 - INFO -   Batch 150/2527: Loss = 0.1017
2026-01-03 01:36:41,307 - INFO -   Batch 200/2527: Loss = 0.1450
2026-01-03 01:37:09,761 - INFO -   Batch 250/2527: Loss = 0.1601
2026-01-03 01:37:38,648 - INFO -   Batch 300/2527: Loss = 0.0760
2026-01-03 01:38:07,467 - INFO -   Batch 350/2527: Loss = 0.0808
2026-01-03 01:38:36,291 - INFO -   Batch 400/2527: Loss = 0.0896
2026-01-03 01:39:05,016 - INFO -   Batch 450/2527: Loss = 0.1138
2026-01-03 01:39:34,099 - INFO -   Batch 500/2527: Loss = 0.1169
2026-01-03 01:40:02,664 - INFO -   Batch 550/2527: Loss = 0.1148
2026-01-03 01:40:31,306 - INFO -   Batch 600/2527: Loss = 0.1080
2026-01-03 01:41:00,096 - INFO -   Batch 650/2527: Loss = 0.1350
2026-01-03 01:41:28,840 - INFO -   Batch 700/2527: Loss = 0.1116
2026-01-03 01:41:57,893 - INFO -   Batch 750/2527: Loss = 0.1247
2026-01-03 01:42:26,495 - INFO -   Batch 800/2527: Loss = 0.0954
2026-01-03 01:42:55,119 - INFO -   Batch 850/2527: Loss = 0.1246
2026-01-03 01:43:24,237 - INFO -   Batch 900/2527: Loss = 0.1305
2026-01-03 01:43:52,873 - INFO -   Batch 950/2527: Loss = 0.1465
2026-01-03 01:44:21,710 - INFO -   Batch 1000/2527: Loss = 0.0738
2026-01-03 01:44:50,291 - INFO -   Batch 1050/2527: Loss = 0.1316
2026-01-03 01:45:19,152 - INFO -   Batch 1100/2527: Loss = 0.1417
2026-01-03 01:45:47,941 - INFO -   Batch 1150/2527: Loss = 0.1590
2026-01-03 01:46:16,425 - INFO -   Batch 1200/2527: Loss = 0.1478
2026-01-03 01:46:45,390 - INFO -   Batch 1250/2527: Loss = 0.1194
2026-01-03 01:47:14,239 - INFO -   Batch 1300/2527: Loss = 0.1087
2026-01-03 01:47:43,156 - INFO -   Batch 1350/2527: Loss = 0.0852
2026-01-03 01:48:11,712 - INFO -   Batch 1400/2527: Loss = 0.1357
2026-01-03 01:48:40,486 - INFO -   Batch 1450/2527: Loss = 0.1399
2026-01-03 01:49:09,029 - INFO -   Batch 1500/2527: Loss = 0.1517
2026-01-03 01:49:37,855 - INFO -   Batch 1550/2527: Loss = 0.0848
2026-01-03 01:50:06,675 - INFO -   Batch 1600/2527: Loss = 0.1387
2026-01-03 01:50:35,424 - INFO -   Batch 1650/2527: Loss = 0.1424
2026-01-03 01:51:04,507 - INFO -   Batch 1700/2527: Loss = 0.0870
2026-01-03 01:51:33,228 - INFO -   Batch 1750/2527: Loss = 0.1782
2026-01-03 01:52:01,683 - INFO -   Batch 1800/2527: Loss = 0.1110
2026-01-03 01:52:30,495 - INFO -   Batch 1850/2527: Loss = 0.0994
2026-01-03 01:52:59,273 - INFO -   Batch 1900/2527: Loss = 0.1082
2026-01-03 01:53:28,340 - INFO -   Batch 1950/2527: Loss = 0.1553
2026-01-03 01:53:56,946 - INFO -   Batch 2000/2527: Loss = 0.1048
2026-01-03 01:54:25,898 - INFO -   Batch 2050/2527: Loss = 0.1878
2026-01-03 01:54:54,778 - INFO -   Batch 2100/2527: Loss = 0.1383
2026-01-03 01:55:23,339 - INFO -   Batch 2150/2527: Loss = 0.1250
2026-01-03 01:55:52,140 - INFO -   Batch 2200/2527: Loss = 0.1252
2026-01-03 01:56:20,758 - INFO -   Batch 2250/2527: Loss = 0.1222
2026-01-03 01:56:49,579 - INFO -   Batch 2300/2527: Loss = 0.1223
2026-01-03 01:57:18,405 - INFO -   Batch 2350/2527: Loss = 0.1264
2026-01-03 01:57:46,886 - INFO -   Batch 2400/2527: Loss = 0.1001
2026-01-03 01:58:15,828 - INFO -   Batch 2450/2527: Loss = 0.1059
2026-01-03 01:58:44,750 - INFO -   Batch 2500/2527: Loss = 0.1464
2026-01-03 02:03:24,294 - INFO - Epoch 59/160: Train Loss: 0.1240, Val Loss: 1.1543, Val mIoU: 39.01%, Val PixelAcc: 78.40%
2026-01-03 02:03:54,797 - INFO -   Batch 50/2527: Loss = 0.1576
2026-01-03 02:04:23,477 - INFO -   Batch 100/2527: Loss = 0.1370
2026-01-03 02:04:52,376 - INFO -   Batch 150/2527: Loss = 0.1266
2026-01-03 02:05:20,961 - INFO -   Batch 200/2527: Loss = 0.1215
2026-01-03 02:05:49,802 - INFO -   Batch 250/2527: Loss = 0.1020
2026-01-03 02:06:18,518 - INFO -   Batch 300/2527: Loss = 0.1284
2026-01-03 02:06:47,385 - INFO -   Batch 350/2527: Loss = 0.1754
2026-01-03 02:07:16,014 - INFO -   Batch 400/2527: Loss = 0.1415
2026-01-03 02:07:44,727 - INFO -   Batch 450/2527: Loss = 0.1290
2026-01-03 02:08:13,801 - INFO -   Batch 500/2527: Loss = 0.1436
2026-01-03 02:08:42,463 - INFO -   Batch 550/2527: Loss = 0.0726
2026-01-03 02:09:11,234 - INFO -   Batch 600/2527: Loss = 0.1534
2026-01-03 02:09:39,920 - INFO -   Batch 650/2527: Loss = 0.1241
2026-01-03 02:10:08,458 - INFO -   Batch 700/2527: Loss = 0.1455
2026-01-03 02:10:37,299 - INFO -   Batch 750/2527: Loss = 0.1009
2026-01-03 02:11:06,068 - INFO -   Batch 800/2527: Loss = 0.0997
2026-01-03 02:11:34,959 - INFO -   Batch 850/2527: Loss = 0.1689
2026-01-03 02:12:03,822 - INFO -   Batch 900/2527: Loss = 0.1232
2026-01-03 02:12:32,725 - INFO -   Batch 950/2527: Loss = 0.1026
2026-01-03 02:13:01,226 - INFO -   Batch 1000/2527: Loss = 0.1856
2026-01-03 02:13:29,747 - INFO -   Batch 1050/2527: Loss = 0.0912
2026-01-03 02:13:58,648 - INFO -   Batch 1100/2527: Loss = 0.1051
2026-01-03 02:14:27,234 - INFO -   Batch 1150/2527: Loss = 0.1108
2026-01-03 02:14:56,217 - INFO -   Batch 1200/2527: Loss = 0.0873
2026-01-03 02:15:24,868 - INFO -   Batch 1250/2527: Loss = 0.0921
2026-01-03 02:15:53,582 - INFO -   Batch 1300/2527: Loss = 0.1103
2026-01-03 02:16:22,495 - INFO -   Batch 1350/2527: Loss = 0.1313
2026-01-03 02:16:51,088 - INFO -   Batch 1400/2527: Loss = 0.1498
2026-01-03 02:17:19,958 - INFO -   Batch 1450/2527: Loss = 0.1356
2026-01-03 02:17:48,813 - INFO -   Batch 1500/2527: Loss = 0.1048
2026-01-03 02:18:17,641 - INFO -   Batch 1550/2527: Loss = 0.1272
2026-01-03 02:18:46,423 - INFO -   Batch 1600/2527: Loss = 0.0918
2026-01-03 02:19:15,044 - INFO -   Batch 1650/2527: Loss = 0.1861
2026-01-03 02:19:44,094 - INFO -   Batch 1700/2527: Loss = 0.0864
2026-01-03 02:20:12,858 - INFO -   Batch 1750/2527: Loss = 0.1149
2026-01-03 02:20:41,635 - INFO -   Batch 1800/2527: Loss = 0.1399
2026-01-03 02:21:10,289 - INFO -   Batch 1850/2527: Loss = 0.1188
2026-01-03 02:21:39,200 - INFO -   Batch 1900/2527: Loss = 0.0983
2026-01-03 02:22:07,729 - INFO -   Batch 1950/2527: Loss = 0.1015
2026-01-03 02:22:36,450 - INFO -   Batch 2000/2527: Loss = 0.1365
2026-01-03 02:23:05,402 - INFO -   Batch 2050/2527: Loss = 0.1272
2026-01-03 02:23:34,276 - INFO -   Batch 2100/2527: Loss = 0.1427
2026-01-03 02:24:03,177 - INFO -   Batch 2150/2527: Loss = 0.1338
2026-01-03 02:24:31,813 - INFO -   Batch 2200/2527: Loss = 0.0996
2026-01-03 02:25:00,308 - INFO -   Batch 2250/2527: Loss = 0.1324
2026-01-03 02:25:29,206 - INFO -   Batch 2300/2527: Loss = 0.1573
2026-01-03 02:25:57,868 - INFO -   Batch 2350/2527: Loss = 0.1249
2026-01-03 02:26:26,852 - INFO -   Batch 2400/2527: Loss = 0.0904
2026-01-03 02:26:55,516 - INFO -   Batch 2450/2527: Loss = 0.1294
2026-01-03 02:27:24,511 - INFO -   Batch 2500/2527: Loss = 0.1200
2026-01-03 02:32:09,682 - INFO - Epoch 60/160: Train Loss: 0.1258, Val Loss: 1.1535, Val mIoU: 39.68%, Val PixelAcc: 78.60%
2026-01-03 02:32:13,262 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_60.pth
2026-01-03 02:32:13,265 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_60.pth
2026-01-03 02:32:43,917 - INFO -   Batch 50/2527: Loss = 0.1227
2026-01-03 02:33:12,426 - INFO -   Batch 100/2527: Loss = 0.1094
2026-01-03 02:33:41,270 - INFO -   Batch 150/2527: Loss = 0.0878
2026-01-03 02:34:10,119 - INFO -   Batch 200/2527: Loss = 0.1045
2026-01-03 02:34:38,576 - INFO -   Batch 250/2527: Loss = 0.0983
2026-01-03 02:35:07,520 - INFO -   Batch 300/2527: Loss = 0.0837
2026-01-03 02:35:36,420 - INFO -   Batch 350/2527: Loss = 0.0816
2026-01-03 02:36:05,363 - INFO -   Batch 400/2527: Loss = 0.0943
2026-01-03 02:36:33,932 - INFO -   Batch 450/2527: Loss = 0.1193
2026-01-03 02:37:02,753 - INFO -   Batch 500/2527: Loss = 0.1453
2026-01-03 02:37:31,389 - INFO -   Batch 550/2527: Loss = 0.1216
2026-01-03 02:38:00,145 - INFO -   Batch 600/2527: Loss = 0.1518
2026-01-03 02:38:28,935 - INFO -   Batch 650/2527: Loss = 0.0832
2026-01-03 02:38:57,696 - INFO -   Batch 700/2527: Loss = 0.1007
2026-01-03 02:39:26,777 - INFO -   Batch 750/2527: Loss = 0.0966
2026-01-03 02:39:55,424 - INFO -   Batch 800/2527: Loss = 0.1542
2026-01-03 02:40:23,881 - INFO -   Batch 850/2527: Loss = 0.1266
2026-01-03 02:40:52,663 - INFO -   Batch 900/2527: Loss = 0.0996
2026-01-03 02:41:21,327 - INFO -   Batch 950/2527: Loss = 0.1331
2026-01-03 02:41:50,385 - INFO -   Batch 1000/2527: Loss = 0.1219
2026-01-03 02:42:18,956 - INFO -   Batch 1050/2527: Loss = 0.1064
2026-01-03 02:42:47,908 - INFO -   Batch 1100/2527: Loss = 0.1367
2026-01-03 02:43:16,756 - INFO -   Batch 1150/2527: Loss = 0.1012
2026-01-03 02:43:45,333 - INFO -   Batch 1200/2527: Loss = 0.0874
2026-01-03 02:44:14,133 - INFO -   Batch 1250/2527: Loss = 0.1233
2026-01-03 02:44:42,661 - INFO -   Batch 1300/2527: Loss = 0.1422
2026-01-03 02:45:11,531 - INFO -   Batch 1350/2527: Loss = 0.1220
2026-01-03 02:45:40,398 - INFO -   Batch 1400/2527: Loss = 0.1195
2026-01-03 02:46:09,257 - INFO -   Batch 1450/2527: Loss = 0.1341
2026-01-03 02:46:37,897 - INFO -   Batch 1500/2527: Loss = 0.1516
2026-01-03 02:47:06,960 - INFO -   Batch 1550/2527: Loss = 0.1390
2026-01-03 02:47:35,872 - INFO -   Batch 1600/2527: Loss = 0.1139
2026-01-03 02:48:04,495 - INFO -   Batch 1650/2527: Loss = 0.1096
2026-01-03 02:48:33,291 - INFO -   Batch 1700/2527: Loss = 0.1272
2026-01-03 02:49:01,981 - INFO -   Batch 1750/2527: Loss = 0.1363
2026-01-03 02:49:30,725 - INFO -   Batch 1800/2527: Loss = 0.1042
2026-01-03 02:49:59,555 - INFO -   Batch 1850/2527: Loss = 0.0870
2026-01-03 02:50:28,302 - INFO -   Batch 1900/2527: Loss = 0.1100
2026-01-03 02:50:57,414 - INFO -   Batch 1950/2527: Loss = 0.1077
2026-01-03 02:51:26,055 - INFO -   Batch 2000/2527: Loss = 0.1754
2026-01-03 02:51:54,855 - INFO -   Batch 2050/2527: Loss = 0.0975
2026-01-03 02:52:23,437 - INFO -   Batch 2100/2527: Loss = 0.1182
2026-01-03 02:52:51,965 - INFO -   Batch 2150/2527: Loss = 0.1347
2026-01-03 02:53:21,028 - INFO -   Batch 2200/2527: Loss = 0.1431
2026-01-03 02:53:49,695 - INFO -   Batch 2250/2527: Loss = 0.0815
2026-01-03 02:54:18,667 - INFO -   Batch 2300/2527: Loss = 0.1328
2026-01-03 02:54:47,544 - INFO -   Batch 2350/2527: Loss = 0.1279
2026-01-03 02:55:16,428 - INFO -   Batch 2400/2527: Loss = 0.1071
2026-01-03 02:55:44,906 - INFO -   Batch 2450/2527: Loss = 0.1357
2026-01-03 02:56:13,451 - INFO -   Batch 2500/2527: Loss = 0.1512
2026-01-03 03:00:51,747 - INFO - Epoch 61/160: Train Loss: 0.1207, Val Loss: 1.1474, Val mIoU: 39.65%, Val PixelAcc: 78.27%
2026-01-03 03:01:22,079 - INFO -   Batch 50/2527: Loss = 0.1700
2026-01-03 03:01:50,785 - INFO -   Batch 100/2527: Loss = 0.1139
2026-01-03 03:02:19,708 - INFO -   Batch 150/2527: Loss = 0.1143
2026-01-03 03:02:48,300 - INFO -   Batch 200/2527: Loss = 0.1185
2026-01-03 03:03:17,068 - INFO -   Batch 250/2527: Loss = 0.1064
2026-01-03 03:03:45,682 - INFO -   Batch 300/2527: Loss = 0.1134
2026-01-03 03:04:14,708 - INFO -   Batch 350/2527: Loss = 0.1020
2026-01-03 03:04:43,168 - INFO -   Batch 400/2527: Loss = 0.0862
2026-01-03 03:05:11,921 - INFO -   Batch 450/2527: Loss = 0.1516
2026-01-03 03:05:40,990 - INFO -   Batch 500/2527: Loss = 0.1182
2026-01-03 03:06:09,631 - INFO -   Batch 550/2527: Loss = 0.0975
2026-01-03 03:06:38,432 - INFO -   Batch 600/2527: Loss = 0.1148
2026-01-03 03:07:07,032 - INFO -   Batch 650/2527: Loss = 0.1014
2026-01-03 03:07:35,536 - INFO -   Batch 700/2527: Loss = 0.1060
2026-01-03 03:08:04,591 - INFO -   Batch 750/2527: Loss = 0.1073
2026-01-03 03:08:33,125 - INFO -   Batch 800/2527: Loss = 0.0786
2026-01-03 03:09:02,055 - INFO -   Batch 850/2527: Loss = 0.1238
2026-01-03 03:09:30,935 - INFO -   Batch 900/2527: Loss = 0.1050
2026-01-03 03:09:59,855 - INFO -   Batch 950/2527: Loss = 0.1236
2026-01-03 03:10:28,388 - INFO -   Batch 1000/2527: Loss = 0.1262
2026-01-03 03:10:56,860 - INFO -   Batch 1050/2527: Loss = 0.1162
2026-01-03 03:11:25,723 - INFO -   Batch 1100/2527: Loss = 0.0733
2026-01-03 03:11:54,581 - INFO -   Batch 1150/2527: Loss = 0.1297
2026-01-03 03:12:23,402 - INFO -   Batch 1200/2527: Loss = 0.1344
2026-01-03 03:12:52,300 - INFO -   Batch 1250/2527: Loss = 0.0860
2026-01-03 03:13:21,068 - INFO -   Batch 1300/2527: Loss = 0.1252
2026-01-03 03:13:49,934 - INFO -   Batch 1350/2527: Loss = 0.1107
2026-01-03 03:14:18,522 - INFO -   Batch 1400/2527: Loss = 0.1083
2026-01-03 03:14:47,326 - INFO -   Batch 1450/2527: Loss = 0.1476
2026-01-03 03:15:15,982 - INFO -   Batch 1500/2527: Loss = 0.0996
2026-01-03 03:15:45,053 - INFO -   Batch 1550/2527: Loss = 0.1355
2026-01-03 03:16:13,656 - INFO -   Batch 1600/2527: Loss = 0.1344
2026-01-03 03:16:42,280 - INFO -   Batch 1650/2527: Loss = 0.1043
2026-01-03 03:17:11,382 - INFO -   Batch 1700/2527: Loss = 0.1260
2026-01-03 03:17:40,005 - INFO -   Batch 1750/2527: Loss = 0.1247
2026-01-03 03:18:08,842 - INFO -   Batch 1800/2527: Loss = 0.1561
2026-01-03 03:18:37,422 - INFO -   Batch 1850/2527: Loss = 0.1055
2026-01-03 03:19:06,307 - INFO -   Batch 1900/2527: Loss = 0.0868
2026-01-03 03:19:35,173 - INFO -   Batch 1950/2527: Loss = 0.1359
2026-01-03 03:20:03,659 - INFO -   Batch 2000/2527: Loss = 0.0673
2026-01-03 03:20:32,603 - INFO -   Batch 2050/2527: Loss = 0.1432
2026-01-03 03:21:01,508 - INFO -   Batch 2100/2527: Loss = 0.0860
2026-01-03 03:21:30,382 - INFO -   Batch 2150/2527: Loss = 0.1326
2026-01-03 03:21:58,945 - INFO -   Batch 2200/2527: Loss = 0.0811
2026-01-03 03:22:27,414 - INFO -   Batch 2250/2527: Loss = 0.1470
2026-01-03 03:22:56,242 - INFO -   Batch 2300/2527: Loss = 0.1014
2026-01-03 03:23:25,096 - INFO -   Batch 2350/2527: Loss = 0.1048
2026-01-03 03:23:53,898 - INFO -   Batch 2400/2527: Loss = 0.1177
2026-01-03 03:24:22,641 - INFO -   Batch 2450/2527: Loss = 0.1698
2026-01-03 03:24:51,762 - INFO -   Batch 2500/2527: Loss = 0.0982
2026-01-03 03:29:30,306 - INFO - Epoch 62/160: Train Loss: 0.1180, Val Loss: 1.1538, Val mIoU: 39.26%, Val PixelAcc: 78.65%
2026-01-03 03:30:00,815 - INFO -   Batch 50/2527: Loss = 0.1475
2026-01-03 03:30:29,840 - INFO -   Batch 100/2527: Loss = 0.0963
2026-01-03 03:30:58,455 - INFO -   Batch 150/2527: Loss = 0.1029
2026-01-03 03:31:26,912 - INFO -   Batch 200/2527: Loss = 0.0896
2026-01-03 03:31:55,771 - INFO -   Batch 250/2527: Loss = 0.0926
2026-01-03 03:32:24,431 - INFO -   Batch 300/2527: Loss = 0.1759
2026-01-03 03:32:53,283 - INFO -   Batch 350/2527: Loss = 0.1206
2026-01-03 03:33:22,031 - INFO -   Batch 400/2527: Loss = 0.1570
2026-01-03 03:33:50,948 - INFO -   Batch 450/2527: Loss = 0.1967
2026-01-03 03:34:19,776 - INFO -   Batch 500/2527: Loss = 0.1494
2026-01-03 03:34:48,358 - INFO -   Batch 550/2527: Loss = 0.0920
2026-01-03 03:35:17,131 - INFO -   Batch 600/2527: Loss = 0.1842
2026-01-03 03:35:45,760 - INFO -   Batch 650/2527: Loss = 0.1087
2026-01-03 03:36:14,602 - INFO -   Batch 700/2527: Loss = 0.1081
2026-01-03 03:36:43,266 - INFO -   Batch 750/2527: Loss = 0.1303
2026-01-03 03:37:12,252 - INFO -   Batch 800/2527: Loss = 0.0871
2026-01-03 03:37:40,843 - INFO -   Batch 850/2527: Loss = 0.0631
2026-01-03 03:38:09,837 - INFO -   Batch 900/2527: Loss = 0.1442
2026-01-03 03:38:38,750 - INFO -   Batch 950/2527: Loss = 0.1352
2026-01-03 03:39:07,346 - INFO -   Batch 1000/2527: Loss = 0.0760
2026-01-03 03:39:36,224 - INFO -   Batch 1050/2527: Loss = 0.1178
2026-01-03 03:40:04,919 - INFO -   Batch 1100/2527: Loss = 0.1348
2026-01-03 03:40:33,431 - INFO -   Batch 1150/2527: Loss = 0.0910
2026-01-03 03:41:02,426 - INFO -   Batch 1200/2527: Loss = 0.1347
2026-01-03 03:41:31,108 - INFO -   Batch 1250/2527: Loss = 0.1237
2026-01-03 03:42:00,178 - INFO -   Batch 1300/2527: Loss = 0.1272
2026-01-03 03:42:28,863 - INFO -   Batch 1350/2527: Loss = 0.1210
2026-01-03 03:42:57,633 - INFO -   Batch 1400/2527: Loss = 0.1288
2026-01-03 03:43:26,306 - INFO -   Batch 1450/2527: Loss = 0.1363
2026-01-03 03:43:54,847 - INFO -   Batch 1500/2527: Loss = 0.1405
2026-01-03 03:44:23,685 - INFO -   Batch 1550/2527: Loss = 0.0986
2026-01-03 03:44:52,517 - INFO -   Batch 1600/2527: Loss = 0.1167
2026-01-03 03:45:21,416 - INFO -   Batch 1650/2527: Loss = 0.0854
2026-01-03 03:45:50,276 - INFO -   Batch 1700/2527: Loss = 0.0904
2026-01-03 03:46:19,177 - INFO -   Batch 1750/2527: Loss = 0.1302
2026-01-03 03:46:47,650 - INFO -   Batch 1800/2527: Loss = 0.0940
2026-01-03 03:47:16,235 - INFO -   Batch 1850/2527: Loss = 0.0899
2026-01-03 03:47:45,116 - INFO -   Batch 1900/2527: Loss = 0.1776
2026-01-03 03:48:13,746 - INFO -   Batch 1950/2527: Loss = 0.0704
2026-01-03 03:48:42,722 - INFO -   Batch 2000/2527: Loss = 0.1065
2026-01-03 03:49:11,371 - INFO -   Batch 2050/2527: Loss = 0.0931
2026-01-03 03:49:40,063 - INFO -   Batch 2100/2527: Loss = 0.1232
2026-01-03 03:50:08,982 - INFO -   Batch 2150/2527: Loss = 0.1153
2026-01-03 03:50:37,538 - INFO -   Batch 2200/2527: Loss = 0.1064
2026-01-03 03:51:06,380 - INFO -   Batch 2250/2527: Loss = 0.1194
2026-01-03 03:51:35,024 - INFO -   Batch 2300/2527: Loss = 0.0793
2026-01-03 03:52:03,890 - INFO -   Batch 2350/2527: Loss = 0.1062
2026-01-03 03:52:32,657 - INFO -   Batch 2400/2527: Loss = 0.1135
2026-01-03 03:53:01,276 - INFO -   Batch 2450/2527: Loss = 0.0937
2026-01-03 03:53:30,288 - INFO -   Batch 2500/2527: Loss = 0.0844
2026-01-03 03:58:16,338 - INFO - Epoch 63/160: Train Loss: 0.1188, Val Loss: 1.1749, Val mIoU: 39.74%, Val PixelAcc: 78.39%
2026-01-03 03:58:46,504 - INFO -   Batch 50/2527: Loss = 0.1139
2026-01-03 03:59:15,264 - INFO -   Batch 100/2527: Loss = 0.0973
2026-01-03 03:59:43,882 - INFO -   Batch 150/2527: Loss = 0.1924
2026-01-03 04:00:12,924 - INFO -   Batch 200/2527: Loss = 0.0843
2026-01-03 04:00:41,446 - INFO -   Batch 250/2527: Loss = 0.1008
2026-01-03 04:01:10,429 - INFO -   Batch 300/2527: Loss = 0.0978
2026-01-03 04:01:39,287 - INFO -   Batch 350/2527: Loss = 0.1380
2026-01-03 04:02:07,860 - INFO -   Batch 400/2527: Loss = 0.1068
2026-01-03 04:02:36,673 - INFO -   Batch 450/2527: Loss = 0.1116
2026-01-03 04:03:05,210 - INFO -   Batch 500/2527: Loss = 0.1259
2026-01-03 04:03:34,088 - INFO -   Batch 550/2527: Loss = 0.1483
2026-01-03 04:04:02,903 - INFO -   Batch 600/2527: Loss = 0.1214
2026-01-03 04:04:31,697 - INFO -   Batch 650/2527: Loss = 0.0864
2026-01-03 04:05:00,309 - INFO -   Batch 700/2527: Loss = 0.1072
2026-01-03 04:05:29,181 - INFO -   Batch 750/2527: Loss = 0.0977
2026-01-03 04:05:58,080 - INFO -   Batch 800/2527: Loss = 0.1018
2026-01-03 04:06:26,737 - INFO -   Batch 850/2527: Loss = 0.0844
2026-01-03 04:06:55,492 - INFO -   Batch 900/2527: Loss = 0.1143
2026-01-03 04:07:24,305 - INFO -   Batch 950/2527: Loss = 0.1358
2026-01-03 04:07:53,057 - INFO -   Batch 1000/2527: Loss = 0.1147
2026-01-03 04:08:21,883 - INFO -   Batch 1050/2527: Loss = 0.1232
2026-01-03 04:08:50,606 - INFO -   Batch 1100/2527: Loss = 0.1191
2026-01-03 04:09:19,738 - INFO -   Batch 1150/2527: Loss = 0.1123
2026-01-03 04:09:48,433 - INFO -   Batch 1200/2527: Loss = 0.0961
2026-01-03 04:10:17,257 - INFO -   Batch 1250/2527: Loss = 0.1298
2026-01-03 04:10:45,821 - INFO -   Batch 1300/2527: Loss = 0.0985
2026-01-03 04:11:14,365 - INFO -   Batch 1350/2527: Loss = 0.1121
2026-01-03 04:11:43,436 - INFO -   Batch 1400/2527: Loss = 0.1036
2026-01-03 04:12:11,990 - INFO -   Batch 1450/2527: Loss = 0.1050
2026-01-03 04:12:40,940 - INFO -   Batch 1500/2527: Loss = 0.0855
2026-01-03 04:13:09,897 - INFO -   Batch 1550/2527: Loss = 0.1384
2026-01-03 04:13:38,492 - INFO -   Batch 1600/2527: Loss = 0.1170
2026-01-03 04:14:07,288 - INFO -   Batch 1650/2527: Loss = 0.1178
2026-01-03 04:14:35,948 - INFO -   Batch 1700/2527: Loss = 0.1223
2026-01-03 04:15:04,768 - INFO -   Batch 1750/2527: Loss = 0.1196
2026-01-03 04:15:33,654 - INFO -   Batch 1800/2527: Loss = 0.0763
2026-01-03 04:16:02,463 - INFO -   Batch 1850/2527: Loss = 0.1197
2026-01-03 04:16:31,159 - INFO -   Batch 1900/2527: Loss = 0.0965
2026-01-03 04:16:59,951 - INFO -   Batch 1950/2527: Loss = 0.0936
2026-01-03 04:17:28,858 - INFO -   Batch 2000/2527: Loss = 0.0879
2026-01-03 04:17:57,456 - INFO -   Batch 2050/2527: Loss = 0.1180
2026-01-03 04:18:26,252 - INFO -   Batch 2100/2527: Loss = 0.1068
2026-01-03 04:18:54,818 - INFO -   Batch 2150/2527: Loss = 0.1369
2026-01-03 04:19:23,880 - INFO -   Batch 2200/2527: Loss = 0.1126
2026-01-03 04:19:52,365 - INFO -   Batch 2250/2527: Loss = 0.1031
2026-01-03 04:20:21,073 - INFO -   Batch 2300/2527: Loss = 0.0670
2026-01-03 04:20:50,186 - INFO -   Batch 2350/2527: Loss = 0.1396
2026-01-03 04:21:18,805 - INFO -   Batch 2400/2527: Loss = 0.1111
2026-01-03 04:21:47,604 - INFO -   Batch 2450/2527: Loss = 0.1031
2026-01-03 04:22:16,205 - INFO -   Batch 2500/2527: Loss = 0.1127
2026-01-03 04:26:55,303 - INFO - Epoch 64/160: Train Loss: 0.1148, Val Loss: 1.1839, Val mIoU: 40.23%, Val PixelAcc: 78.44%
2026-01-03 04:26:55,314 - INFO -   → New best mIoU: 40.23%
2026-01-03 04:26:58,976 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-03 04:26:58,978 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-03 04:27:29,404 - INFO -   Batch 50/2527: Loss = 0.1718
2026-01-03 04:27:58,478 - INFO -   Batch 100/2527: Loss = 0.1028
2026-01-03 04:28:27,366 - INFO -   Batch 150/2527: Loss = 0.1023
2026-01-03 04:28:55,816 - INFO -   Batch 200/2527: Loss = 0.1361
2026-01-03 04:29:24,313 - INFO -   Batch 250/2527: Loss = 0.1308
2026-01-03 04:29:53,079 - INFO -   Batch 300/2527: Loss = 0.1537
2026-01-03 04:30:22,121 - INFO -   Batch 350/2527: Loss = 0.1160
2026-01-03 04:30:50,723 - INFO -   Batch 400/2527: Loss = 0.1220
2026-01-03 04:31:19,620 - INFO -   Batch 450/2527: Loss = 0.1897
2026-01-03 04:31:48,540 - INFO -   Batch 500/2527: Loss = 0.1590
2026-01-03 04:32:17,094 - INFO -   Batch 550/2527: Loss = 0.0955
2026-01-03 04:32:45,935 - INFO -   Batch 600/2527: Loss = 0.1201
2026-01-03 04:33:14,672 - INFO -   Batch 650/2527: Loss = 0.0790
2026-01-03 04:33:43,550 - INFO -   Batch 700/2527: Loss = 0.1147
2026-01-03 04:34:12,408 - INFO -   Batch 750/2527: Loss = 0.2252
2026-01-03 04:34:41,268 - INFO -   Batch 800/2527: Loss = 0.1164
2026-01-03 04:35:10,030 - INFO -   Batch 850/2527: Loss = 0.1013
2026-01-03 04:35:38,785 - INFO -   Batch 900/2527: Loss = 0.1115
2026-01-03 04:36:07,691 - INFO -   Batch 950/2527: Loss = 0.1256
2026-01-03 04:36:36,259 - INFO -   Batch 1000/2527: Loss = 0.1364
2026-01-03 04:37:05,086 - INFO -   Batch 1050/2527: Loss = 0.1023
2026-01-03 04:37:33,698 - INFO -   Batch 1100/2527: Loss = 0.1415
2026-01-03 04:38:02,410 - INFO -   Batch 1150/2527: Loss = 0.1112
2026-01-03 04:38:31,238 - INFO -   Batch 1200/2527: Loss = 0.1036
2026-01-03 04:38:59,990 - INFO -   Batch 1250/2527: Loss = 0.1535
2026-01-03 04:39:29,084 - INFO -   Batch 1300/2527: Loss = 0.1208
2026-01-03 04:39:57,817 - INFO -   Batch 1350/2527: Loss = 0.1611
2026-01-03 04:40:26,607 - INFO -   Batch 1400/2527: Loss = 0.0876
2026-01-03 04:40:55,246 - INFO -   Batch 1450/2527: Loss = 0.1044
2026-01-03 04:41:23,778 - INFO -   Batch 1500/2527: Loss = 0.1676
2026-01-03 04:41:52,849 - INFO -   Batch 1550/2527: Loss = 0.1262
2026-01-03 04:42:21,433 - INFO -   Batch 1600/2527: Loss = 0.1039
2026-01-03 04:42:50,418 - INFO -   Batch 1650/2527: Loss = 0.0934
2026-01-03 04:43:19,337 - INFO -   Batch 1700/2527: Loss = 0.1357
2026-01-03 04:43:48,233 - INFO -   Batch 1750/2527: Loss = 0.1217
2026-01-03 04:44:16,703 - INFO -   Batch 1800/2527: Loss = 0.1052
2026-01-03 04:44:45,229 - INFO -   Batch 1850/2527: Loss = 0.1566
2026-01-03 04:45:14,102 - INFO -   Batch 1900/2527: Loss = 0.1178
2026-01-03 04:45:42,922 - INFO -   Batch 1950/2527: Loss = 0.0921
2026-01-03 04:46:11,744 - INFO -   Batch 2000/2527: Loss = 0.0970
2026-01-03 04:46:40,457 - INFO -   Batch 2050/2527: Loss = 0.1775
2026-01-03 04:47:09,214 - INFO -   Batch 2100/2527: Loss = 0.1172
2026-01-03 04:47:38,094 - INFO -   Batch 2150/2527: Loss = 0.1356
2026-01-03 04:48:06,682 - INFO -   Batch 2200/2527: Loss = 0.1237
2026-01-03 04:48:35,480 - INFO -   Batch 2250/2527: Loss = 0.1217
2026-01-03 04:49:04,115 - INFO -   Batch 2300/2527: Loss = 0.0916
2026-01-03 04:49:33,148 - INFO -   Batch 2350/2527: Loss = 0.0980
2026-01-03 04:50:01,798 - INFO -   Batch 2400/2527: Loss = 0.1277
2026-01-03 04:50:30,409 - INFO -   Batch 2450/2527: Loss = 0.1074
2026-01-03 04:50:59,520 - INFO -   Batch 2500/2527: Loss = 0.0785
2026-01-03 04:55:39,265 - INFO - Epoch 65/160: Train Loss: 0.1153, Val Loss: 1.1610, Val mIoU: 39.62%, Val PixelAcc: 78.52%
2026-01-03 04:56:09,834 - INFO -   Batch 50/2527: Loss = 0.1087
2026-01-03 04:56:38,855 - INFO -   Batch 100/2527: Loss = 0.1026
2026-01-03 04:57:07,556 - INFO -   Batch 150/2527: Loss = 0.0810
2026-01-03 04:57:36,351 - INFO -   Batch 200/2527: Loss = 0.1127
2026-01-03 04:58:05,133 - INFO -   Batch 250/2527: Loss = 0.1540
2026-01-03 04:58:34,013 - INFO -   Batch 300/2527: Loss = 0.1049
2026-01-03 04:59:02,653 - INFO -   Batch 350/2527: Loss = 0.1374
2026-01-03 04:59:31,281 - INFO -   Batch 400/2527: Loss = 0.1144
2026-01-03 05:00:00,231 - INFO -   Batch 450/2527: Loss = 0.0827
2026-01-03 05:00:29,049 - INFO -   Batch 500/2527: Loss = 0.0981
2026-01-03 05:00:57,985 - INFO -   Batch 550/2527: Loss = 0.0994
2026-01-03 05:01:26,585 - INFO -   Batch 600/2527: Loss = 0.1115
2026-01-03 05:01:55,407 - INFO -   Batch 650/2527: Loss = 0.1141
2026-01-03 05:02:23,978 - INFO -   Batch 700/2527: Loss = 0.1136
2026-01-03 05:02:52,621 - INFO -   Batch 750/2527: Loss = 0.0957
2026-01-03 05:03:21,579 - INFO -   Batch 800/2527: Loss = 0.0940
2026-01-03 05:03:50,308 - INFO -   Batch 850/2527: Loss = 0.0716
2026-01-03 05:04:19,348 - INFO -   Batch 900/2527: Loss = 0.2023
2026-01-03 05:04:48,115 - INFO -   Batch 950/2527: Loss = 0.1332
2026-01-03 05:05:16,593 - INFO -   Batch 1000/2527: Loss = 0.1015
2026-01-03 05:05:45,425 - INFO -   Batch 1050/2527: Loss = 0.1135
2026-01-03 05:06:14,167 - INFO -   Batch 1100/2527: Loss = 0.0924
2026-01-03 05:06:43,065 - INFO -   Batch 1150/2527: Loss = 0.1378
2026-01-03 05:07:11,783 - INFO -   Batch 1200/2527: Loss = 0.0966
2026-01-03 05:07:40,716 - INFO -   Batch 1250/2527: Loss = 0.0848
2026-01-03 05:08:09,562 - INFO -   Batch 1300/2527: Loss = 0.1197
2026-01-03 05:08:38,123 - INFO -   Batch 1350/2527: Loss = 0.1318
2026-01-03 05:09:06,969 - INFO -   Batch 1400/2527: Loss = 0.0765
2026-01-03 05:09:35,565 - INFO -   Batch 1450/2527: Loss = 0.1041
2026-01-03 05:10:04,464 - INFO -   Batch 1500/2527: Loss = 0.1791
2026-01-03 05:10:33,134 - INFO -   Batch 1550/2527: Loss = 0.1246
2026-01-03 05:11:01,795 - INFO -   Batch 1600/2527: Loss = 0.1152
2026-01-03 05:11:30,702 - INFO -   Batch 1650/2527: Loss = 0.1498
2026-01-03 05:11:59,490 - INFO -   Batch 1700/2527: Loss = 0.0700
2026-01-03 05:12:28,361 - INFO -   Batch 1750/2527: Loss = 0.0849
2026-01-03 05:12:56,924 - INFO -   Batch 1800/2527: Loss = 0.0769
2026-01-03 05:13:25,755 - INFO -   Batch 1850/2527: Loss = 0.0886
2026-01-03 05:13:54,433 - INFO -   Batch 1900/2527: Loss = 0.1124
2026-01-03 05:14:22,953 - INFO -   Batch 1950/2527: Loss = 0.0958
2026-01-03 05:14:51,913 - INFO -   Batch 2000/2527: Loss = 0.1138
2026-01-03 05:15:20,642 - INFO -   Batch 2050/2527: Loss = 0.1235
2026-01-03 05:15:49,697 - INFO -   Batch 2100/2527: Loss = 0.1076
2026-01-03 05:16:18,558 - INFO -   Batch 2150/2527: Loss = 0.1255
2026-01-03 05:16:47,380 - INFO -   Batch 2200/2527: Loss = 0.1071
2026-01-03 05:17:16,000 - INFO -   Batch 2250/2527: Loss = 0.1071
2026-01-03 05:17:44,559 - INFO -   Batch 2300/2527: Loss = 0.1386
2026-01-03 05:18:13,483 - INFO -   Batch 2350/2527: Loss = 0.0954
2026-01-03 05:18:42,272 - INFO -   Batch 2400/2527: Loss = 0.0942
2026-01-03 05:19:11,198 - INFO -   Batch 2450/2527: Loss = 0.1242
2026-01-03 05:19:40,026 - INFO -   Batch 2500/2527: Loss = 0.1335
2026-01-03 05:24:25,695 - INFO - Epoch 66/160: Train Loss: 0.1104, Val Loss: 1.1876, Val mIoU: 39.31%, Val PixelAcc: 78.29%
2026-01-03 05:24:56,342 - INFO -   Batch 50/2527: Loss = 0.1392
2026-01-03 05:25:24,822 - INFO -   Batch 100/2527: Loss = 0.0832
2026-01-03 05:25:53,637 - INFO -   Batch 150/2527: Loss = 0.1088
2026-01-03 05:26:22,461 - INFO -   Batch 200/2527: Loss = 0.2292
2026-01-03 05:26:50,923 - INFO -   Batch 250/2527: Loss = 0.1162
2026-01-03 05:27:19,885 - INFO -   Batch 300/2527: Loss = 0.0965
2026-01-03 05:27:48,878 - INFO -   Batch 350/2527: Loss = 0.1008
2026-01-03 05:28:17,799 - INFO -   Batch 400/2527: Loss = 0.0939
2026-01-03 05:28:46,401 - INFO -   Batch 450/2527: Loss = 0.1143
2026-01-03 05:29:14,861 - INFO -   Batch 500/2527: Loss = 0.1001
2026-01-03 05:29:43,727 - INFO -   Batch 550/2527: Loss = 0.1133
2026-01-03 05:30:12,588 - INFO -   Batch 600/2527: Loss = 0.0881
2026-01-03 05:30:41,385 - INFO -   Batch 650/2527: Loss = 0.1322
2026-01-03 05:31:10,140 - INFO -   Batch 700/2527: Loss = 0.1247
2026-01-03 05:31:39,229 - INFO -   Batch 750/2527: Loss = 0.1071
2026-01-03 05:32:07,935 - INFO -   Batch 800/2527: Loss = 0.1333
2026-01-03 05:32:36,424 - INFO -   Batch 850/2527: Loss = 0.1669
2026-01-03 05:33:05,210 - INFO -   Batch 900/2527: Loss = 0.1298
2026-01-03 05:33:33,877 - INFO -   Batch 950/2527: Loss = 0.1152
2026-01-03 05:34:02,946 - INFO -   Batch 1000/2527: Loss = 0.1497
2026-01-03 05:34:31,580 - INFO -   Batch 1050/2527: Loss = 0.0875
2026-01-03 05:35:00,524 - INFO -   Batch 1100/2527: Loss = 0.0942
2026-01-03 05:35:29,309 - INFO -   Batch 1150/2527: Loss = 0.1450
2026-01-03 05:35:57,980 - INFO -   Batch 1200/2527: Loss = 0.1257
2026-01-03 05:36:26,818 - INFO -   Batch 1250/2527: Loss = 0.0874
2026-01-03 05:36:55,316 - INFO -   Batch 1300/2527: Loss = 0.1492
2026-01-03 05:37:24,178 - INFO -   Batch 1350/2527: Loss = 0.0791
2026-01-03 05:37:53,006 - INFO -   Batch 1400/2527: Loss = 0.1215
2026-01-03 05:38:21,498 - INFO -   Batch 1450/2527: Loss = 0.1321
2026-01-03 05:38:50,432 - INFO -   Batch 1500/2527: Loss = 0.0945
2026-01-03 05:39:19,299 - INFO -   Batch 1550/2527: Loss = 0.0972
2026-01-03 05:39:48,199 - INFO -   Batch 1600/2527: Loss = 0.1092
2026-01-03 05:40:16,803 - INFO -   Batch 1650/2527: Loss = 0.0883
2026-01-03 05:40:45,573 - INFO -   Batch 1700/2527: Loss = 0.1424
2026-01-03 05:41:14,173 - INFO -   Batch 1750/2527: Loss = 0.1020
2026-01-03 05:41:42,924 - INFO -   Batch 1800/2527: Loss = 0.0796
2026-01-03 05:42:11,715 - INFO -   Batch 1850/2527: Loss = 0.0919
2026-01-03 05:42:40,437 - INFO -   Batch 1900/2527: Loss = 0.1414
2026-01-03 05:43:09,540 - INFO -   Batch 1950/2527: Loss = 0.0964
2026-01-03 05:43:38,181 - INFO -   Batch 2000/2527: Loss = 0.0977
2026-01-03 05:44:07,007 - INFO -   Batch 2050/2527: Loss = 0.1030
2026-01-03 05:44:35,492 - INFO -   Batch 2100/2527: Loss = 0.1285
2026-01-03 05:45:04,163 - INFO -   Batch 2150/2527: Loss = 0.1118
2026-01-03 05:45:33,247 - INFO -   Batch 2200/2527: Loss = 0.1449
2026-01-03 05:46:01,986 - INFO -   Batch 2250/2527: Loss = 0.1245
2026-01-03 05:46:30,956 - INFO -   Batch 2300/2527: Loss = 0.1056
2026-01-03 05:46:59,849 - INFO -   Batch 2350/2527: Loss = 0.1020
2026-01-03 05:47:28,408 - INFO -   Batch 2400/2527: Loss = 0.1053
2026-01-03 05:47:57,249 - INFO -   Batch 2450/2527: Loss = 0.1466
2026-01-03 05:48:25,849 - INFO -   Batch 2500/2527: Loss = 0.0886
2026-01-03 05:53:06,465 - INFO - Epoch 67/160: Train Loss: 0.1127, Val Loss: 1.1723, Val mIoU: 40.11%, Val PixelAcc: 78.46%
2026-01-03 05:53:36,590 - INFO -   Batch 50/2527: Loss = 0.0977
2026-01-03 05:54:05,605 - INFO -   Batch 100/2527: Loss = 0.1048
2026-01-03 05:54:34,487 - INFO -   Batch 150/2527: Loss = 0.1741
2026-01-03 05:55:03,092 - INFO -   Batch 200/2527: Loss = 0.1216
2026-01-03 05:55:31,894 - INFO -   Batch 250/2527: Loss = 0.1162
2026-01-03 05:56:00,531 - INFO -   Batch 300/2527: Loss = 0.0969
2026-01-03 05:56:29,232 - INFO -   Batch 350/2527: Loss = 0.1162
2026-01-03 05:56:58,057 - INFO -   Batch 400/2527: Loss = 0.1055
2026-01-03 05:57:26,768 - INFO -   Batch 450/2527: Loss = 0.1243
2026-01-03 05:57:55,878 - INFO -   Batch 500/2527: Loss = 0.1263
2026-01-03 05:58:24,519 - INFO -   Batch 550/2527: Loss = 0.1082
2026-01-03 05:58:53,336 - INFO -   Batch 600/2527: Loss = 0.1064
2026-01-03 05:59:21,903 - INFO -   Batch 650/2527: Loss = 0.1284
2026-01-03 05:59:50,416 - INFO -   Batch 700/2527: Loss = 0.1426
2026-01-03 06:00:19,508 - INFO -   Batch 750/2527: Loss = 0.1021
2026-01-03 06:00:48,132 - INFO -   Batch 800/2527: Loss = 0.1048
2026-01-03 06:01:17,107 - INFO -   Batch 850/2527: Loss = 0.1220
2026-01-03 06:01:46,035 - INFO -   Batch 900/2527: Loss = 0.1023
2026-01-03 06:02:14,900 - INFO -   Batch 950/2527: Loss = 0.1087
2026-01-03 06:02:43,395 - INFO -   Batch 1000/2527: Loss = 0.1342
2026-01-03 06:03:11,936 - INFO -   Batch 1050/2527: Loss = 0.0891
2026-01-03 06:03:40,768 - INFO -   Batch 1100/2527: Loss = 0.0836
2026-01-03 06:04:09,643 - INFO -   Batch 1150/2527: Loss = 0.0781
2026-01-03 06:04:38,403 - INFO -   Batch 1200/2527: Loss = 0.1016
2026-01-03 06:05:07,128 - INFO -   Batch 1250/2527: Loss = 0.1188
2026-01-03 06:05:35,885 - INFO -   Batch 1300/2527: Loss = 0.1406
2026-01-03 06:06:04,822 - INFO -   Batch 1350/2527: Loss = 0.0740
2026-01-03 06:06:33,402 - INFO -   Batch 1400/2527: Loss = 0.0852
2026-01-03 06:07:02,203 - INFO -   Batch 1450/2527: Loss = 0.0841
2026-01-03 06:07:30,790 - INFO -   Batch 1500/2527: Loss = 0.0754
2026-01-03 06:07:59,884 - INFO -   Batch 1550/2527: Loss = 0.1379
2026-01-03 06:08:28,408 - INFO -   Batch 1600/2527: Loss = 0.1899
2026-01-03 06:08:57,037 - INFO -   Batch 1650/2527: Loss = 0.1322
2026-01-03 06:09:26,141 - INFO -   Batch 1700/2527: Loss = 0.1511
2026-01-03 06:09:54,761 - INFO -   Batch 1750/2527: Loss = 0.1461
2026-01-03 06:10:23,558 - INFO -   Batch 1800/2527: Loss = 0.0909
2026-01-03 06:10:52,158 - INFO -   Batch 1850/2527: Loss = 0.0694
2026-01-03 06:11:20,678 - INFO -   Batch 1900/2527: Loss = 0.0973
2026-01-03 06:11:49,745 - INFO -   Batch 1950/2527: Loss = 0.0925
2026-01-03 06:12:18,521 - INFO -   Batch 2000/2527: Loss = 0.1100
2026-01-03 06:12:47,425 - INFO -   Batch 2050/2527: Loss = 0.1147
2026-01-03 06:13:16,343 - INFO -   Batch 2100/2527: Loss = 0.1068
2026-01-03 06:13:45,235 - INFO -   Batch 2150/2527: Loss = 0.0950
2026-01-03 06:14:13,849 - INFO -   Batch 2200/2527: Loss = 0.1254
2026-01-03 06:14:42,330 - INFO -   Batch 2250/2527: Loss = 0.0973
2026-01-03 06:15:11,179 - INFO -   Batch 2300/2527: Loss = 0.0734
2026-01-03 06:15:40,024 - INFO -   Batch 2350/2527: Loss = 0.0967
2026-01-03 06:16:08,840 - INFO -   Batch 2400/2527: Loss = 0.0902
2026-01-03 06:16:37,529 - INFO -   Batch 2450/2527: Loss = 0.0960
2026-01-03 06:17:06,638 - INFO -   Batch 2500/2527: Loss = 0.0919
2026-01-03 06:21:44,593 - INFO - Epoch 68/160: Train Loss: 0.1070, Val Loss: 1.1912, Val mIoU: 39.28%, Val PixelAcc: 78.57%
2026-01-03 06:22:15,162 - INFO -   Batch 50/2527: Loss = 0.1239
2026-01-03 06:22:44,155 - INFO -   Batch 100/2527: Loss = 0.0797
2026-01-03 06:23:12,794 - INFO -   Batch 150/2527: Loss = 0.0802
2026-01-03 06:23:41,238 - INFO -   Batch 200/2527: Loss = 0.0781
2026-01-03 06:24:10,059 - INFO -   Batch 250/2527: Loss = 0.0775
2026-01-03 06:24:38,773 - INFO -   Batch 300/2527: Loss = 0.1151
2026-01-03 06:25:07,722 - INFO -   Batch 350/2527: Loss = 0.1311
2026-01-03 06:25:36,580 - INFO -   Batch 400/2527: Loss = 0.1364
2026-01-03 06:26:05,458 - INFO -   Batch 450/2527: Loss = 0.0944
2026-01-03 06:26:34,343 - INFO -   Batch 500/2527: Loss = 0.1434
2026-01-03 06:27:02,912 - INFO -   Batch 550/2527: Loss = 0.1063
2026-01-03 06:27:31,712 - INFO -   Batch 600/2527: Loss = 0.0645
2026-01-03 06:28:00,296 - INFO -   Batch 650/2527: Loss = 0.0953
2026-01-03 06:28:29,191 - INFO -   Batch 700/2527: Loss = 0.1008
2026-01-03 06:28:57,859 - INFO -   Batch 750/2527: Loss = 0.0952
2026-01-03 06:29:26,520 - INFO -   Batch 800/2527: Loss = 0.0989
2026-01-03 06:29:55,452 - INFO -   Batch 850/2527: Loss = 0.1177
2026-01-03 06:30:24,199 - INFO -   Batch 900/2527: Loss = 0.0868
2026-01-03 06:30:53,111 - INFO -   Batch 950/2527: Loss = 0.1024
2026-01-03 06:31:21,692 - INFO -   Batch 1000/2527: Loss = 0.0915
2026-01-03 06:31:50,551 - INFO -   Batch 1050/2527: Loss = 0.0583
2026-01-03 06:32:19,203 - INFO -   Batch 1100/2527: Loss = 0.0804
2026-01-03 06:32:47,705 - INFO -   Batch 1150/2527: Loss = 0.0864
2026-01-03 06:33:16,670 - INFO -   Batch 1200/2527: Loss = 0.1022
2026-01-03 06:33:45,375 - INFO -   Batch 1250/2527: Loss = 0.1052
2026-01-03 06:34:14,394 - INFO -   Batch 1300/2527: Loss = 0.0778
2026-01-03 06:34:43,090 - INFO -   Batch 1350/2527: Loss = 0.1247
2026-01-03 06:35:11,836 - INFO -   Batch 1400/2527: Loss = 0.1791
2026-01-03 06:35:40,358 - INFO -   Batch 1450/2527: Loss = 0.1061
2026-01-03 06:36:09,083 - INFO -   Batch 1500/2527: Loss = 0.1030
2026-01-03 06:36:37,937 - INFO -   Batch 1550/2527: Loss = 0.0949
2026-01-03 06:37:06,839 - INFO -   Batch 1600/2527: Loss = 0.0825
2026-01-03 06:37:35,787 - INFO -   Batch 1650/2527: Loss = 0.1078
2026-01-03 06:38:04,608 - INFO -   Batch 1700/2527: Loss = 0.0987
2026-01-03 06:38:33,190 - INFO -   Batch 1750/2527: Loss = 0.1116
2026-01-03 06:39:02,030 - INFO -   Batch 1800/2527: Loss = 0.0861
2026-01-03 06:39:30,678 - INFO -   Batch 1850/2527: Loss = 0.1059
2026-01-03 06:39:59,608 - INFO -   Batch 1900/2527: Loss = 0.0810
2026-01-03 06:40:28,266 - INFO -   Batch 1950/2527: Loss = 0.1204
2026-01-03 06:40:57,254 - INFO -   Batch 2000/2527: Loss = 0.0462
2026-01-03 06:41:25,940 - INFO -   Batch 2050/2527: Loss = 0.0584
2026-01-03 06:41:54,660 - INFO -   Batch 2100/2527: Loss = 0.0879
2026-01-03 06:42:23,541 - INFO -   Batch 2150/2527: Loss = 0.0645
2026-01-03 06:42:52,168 - INFO -   Batch 2200/2527: Loss = 0.0868
2026-01-03 06:43:21,000 - INFO -   Batch 2250/2527: Loss = 0.1185
2026-01-03 06:43:49,768 - INFO -   Batch 2300/2527: Loss = 0.1076
2026-01-03 06:44:18,616 - INFO -   Batch 2350/2527: Loss = 0.0910
2026-01-03 06:44:47,268 - INFO -   Batch 2400/2527: Loss = 0.0844
2026-01-03 06:45:16,001 - INFO -   Batch 2450/2527: Loss = 0.0830
2026-01-03 06:45:45,069 - INFO -   Batch 2500/2527: Loss = 0.0673
2026-01-03 06:50:30,410 - INFO - Epoch 69/160: Train Loss: 0.1069, Val Loss: 1.2158, Val mIoU: 39.63%, Val PixelAcc: 78.21%
2026-01-03 06:51:00,674 - INFO -   Batch 50/2527: Loss = 0.0759
2026-01-03 06:51:29,350 - INFO -   Batch 100/2527: Loss = 0.0838
2026-01-03 06:51:58,183 - INFO -   Batch 150/2527: Loss = 0.0945
2026-01-03 06:52:27,020 - INFO -   Batch 200/2527: Loss = 0.1001
2026-01-03 06:52:55,804 - INFO -   Batch 250/2527: Loss = 0.0891
2026-01-03 06:53:24,520 - INFO -   Batch 300/2527: Loss = 0.1101
2026-01-03 06:53:53,297 - INFO -   Batch 350/2527: Loss = 0.1239
2026-01-03 06:54:22,144 - INFO -   Batch 400/2527: Loss = 0.1140
2026-01-03 06:54:50,799 - INFO -   Batch 450/2527: Loss = 0.0782
2026-01-03 06:55:19,620 - INFO -   Batch 500/2527: Loss = 0.1057
2026-01-03 06:55:48,426 - INFO -   Batch 550/2527: Loss = 0.1469
2026-01-03 06:56:17,476 - INFO -   Batch 600/2527: Loss = 0.1063
2026-01-03 06:56:46,070 - INFO -   Batch 650/2527: Loss = 0.1217
2026-01-03 06:57:14,673 - INFO -   Batch 700/2527: Loss = 0.1027
2026-01-03 06:57:43,801 - INFO -   Batch 750/2527: Loss = 0.0652
2026-01-03 06:58:12,491 - INFO -   Batch 800/2527: Loss = 0.0751
2026-01-03 06:58:41,290 - INFO -   Batch 850/2527: Loss = 0.0970
2026-01-03 06:59:09,868 - INFO -   Batch 900/2527: Loss = 0.0686
2026-01-03 06:59:38,690 - INFO -   Batch 950/2527: Loss = 0.0787
2026-01-03 07:00:07,412 - INFO -   Batch 1000/2527: Loss = 0.1387
2026-01-03 07:00:36,009 - INFO -   Batch 1050/2527: Loss = 0.1221
2026-01-03 07:01:04,957 - INFO -   Batch 1100/2527: Loss = 0.1104
2026-01-03 07:01:33,870 - INFO -   Batch 1150/2527: Loss = 0.1014
2026-01-03 07:02:02,762 - INFO -   Batch 1200/2527: Loss = 0.1704
2026-01-03 07:02:31,416 - INFO -   Batch 1250/2527: Loss = 0.0767
2026-01-03 07:02:59,876 - INFO -   Batch 1300/2527: Loss = 0.1217
2026-01-03 07:03:28,730 - INFO -   Batch 1350/2527: Loss = 0.0628
2026-01-03 07:03:57,572 - INFO -   Batch 1400/2527: Loss = 0.1064
2026-01-03 07:04:26,384 - INFO -   Batch 1450/2527: Loss = 0.1010
2026-01-03 07:04:55,050 - INFO -   Batch 1500/2527: Loss = 0.0811
2026-01-03 07:05:24,172 - INFO -   Batch 1550/2527: Loss = 0.0841
2026-01-03 07:05:52,844 - INFO -   Batch 1600/2527: Loss = 0.1016
2026-01-03 07:06:21,299 - INFO -   Batch 1650/2527: Loss = 0.1435
2026-01-03 07:06:50,126 - INFO -   Batch 1700/2527: Loss = 0.0731
2026-01-03 07:07:18,687 - INFO -   Batch 1750/2527: Loss = 0.0892
2026-01-03 07:07:47,784 - INFO -   Batch 1800/2527: Loss = 0.1102
2026-01-03 07:08:16,406 - INFO -   Batch 1850/2527: Loss = 0.1047
2026-01-03 07:08:45,321 - INFO -   Batch 1900/2527: Loss = 0.0691
2026-01-03 07:09:14,119 - INFO -   Batch 1950/2527: Loss = 0.1091
2026-01-03 07:09:42,762 - INFO -   Batch 2000/2527: Loss = 0.1056
2026-01-03 07:10:11,531 - INFO -   Batch 2050/2527: Loss = 0.1035
2026-01-03 07:10:40,096 - INFO -   Batch 2100/2527: Loss = 0.1510
2026-01-03 07:11:08,934 - INFO -   Batch 2150/2527: Loss = 0.0919
2026-01-03 07:11:37,811 - INFO -   Batch 2200/2527: Loss = 0.1077
2026-01-03 07:12:06,287 - INFO -   Batch 2250/2527: Loss = 0.1432
2026-01-03 07:12:35,217 - INFO -   Batch 2300/2527: Loss = 0.1450
2026-01-03 07:13:04,087 - INFO -   Batch 2350/2527: Loss = 0.1023
2026-01-03 07:13:32,967 - INFO -   Batch 2400/2527: Loss = 0.1102
2026-01-03 07:14:01,690 - INFO -   Batch 2450/2527: Loss = 0.1031
2026-01-03 07:14:30,514 - INFO -   Batch 2500/2527: Loss = 0.0641
2026-01-03 07:19:10,804 - INFO - Epoch 70/160: Train Loss: 0.1026, Val Loss: 1.2327, Val mIoU: 39.72%, Val PixelAcc: 78.42%
2026-01-03 07:19:14,296 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_70.pth
2026-01-03 07:19:14,299 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_70.pth
2026-01-03 07:19:44,861 - INFO -   Batch 50/2527: Loss = 0.0946
2026-01-03 07:20:13,719 - INFO -   Batch 100/2527: Loss = 0.0885
2026-01-03 07:20:42,596 - INFO -   Batch 150/2527: Loss = 0.0969
2026-01-03 07:21:11,347 - INFO -   Batch 200/2527: Loss = 0.1888
2026-01-03 07:21:39,822 - INFO -   Batch 250/2527: Loss = 0.0861
2026-01-03 07:22:08,658 - INFO -   Batch 300/2527: Loss = 0.0914
2026-01-03 07:22:37,534 - INFO -   Batch 350/2527: Loss = 0.0923
2026-01-03 07:23:06,355 - INFO -   Batch 400/2527: Loss = 0.0788
2026-01-03 07:23:35,102 - INFO -   Batch 450/2527: Loss = 0.1011
2026-01-03 07:24:04,203 - INFO -   Batch 500/2527: Loss = 0.0971
2026-01-03 07:24:32,788 - INFO -   Batch 550/2527: Loss = 0.1107
2026-01-03 07:25:01,379 - INFO -   Batch 600/2527: Loss = 0.0998
2026-01-03 07:25:30,229 - INFO -   Batch 650/2527: Loss = 0.0734
2026-01-03 07:25:58,839 - INFO -   Batch 700/2527: Loss = 0.0721
2026-01-03 07:26:27,908 - INFO -   Batch 750/2527: Loss = 0.0975
2026-01-03 07:26:56,507 - INFO -   Batch 800/2527: Loss = 0.0725
2026-01-03 07:27:25,113 - INFO -   Batch 850/2527: Loss = 0.0873
2026-01-03 07:27:54,252 - INFO -   Batch 900/2527: Loss = 0.1123
2026-01-03 07:28:22,984 - INFO -   Batch 950/2527: Loss = 0.0884
2026-01-03 07:28:51,784 - INFO -   Batch 1000/2527: Loss = 0.0685
2026-01-03 07:29:20,411 - INFO -   Batch 1050/2527: Loss = 0.0890
2026-01-03 07:29:49,239 - INFO -   Batch 1100/2527: Loss = 0.0926
2026-01-03 07:30:18,080 - INFO -   Batch 1150/2527: Loss = 0.1115
2026-01-03 07:30:46,544 - INFO -   Batch 1200/2527: Loss = 0.1108
2026-01-03 07:31:15,464 - INFO -   Batch 1250/2527: Loss = 0.1109
2026-01-03 07:31:44,406 - INFO -   Batch 1300/2527: Loss = 0.1139
2026-01-03 07:32:13,302 - INFO -   Batch 1350/2527: Loss = 0.0865
2026-01-03 07:32:41,819 - INFO -   Batch 1400/2527: Loss = 0.1480
2026-01-03 07:33:10,296 - INFO -   Batch 1450/2527: Loss = 0.0945
2026-01-03 07:33:39,164 - INFO -   Batch 1500/2527: Loss = 0.1185
2026-01-03 07:34:08,003 - INFO -   Batch 1550/2527: Loss = 0.0712
2026-01-03 07:34:36,828 - INFO -   Batch 1600/2527: Loss = 0.0798
2026-01-03 07:35:05,498 - INFO -   Batch 1650/2527: Loss = 0.0904
2026-01-03 07:35:34,617 - INFO -   Batch 1700/2527: Loss = 0.1097
2026-01-03 07:36:03,286 - INFO -   Batch 1750/2527: Loss = 0.0971
2026-01-03 07:36:31,755 - INFO -   Batch 1800/2527: Loss = 0.1037
2026-01-03 07:37:00,569 - INFO -   Batch 1850/2527: Loss = 0.0868
2026-01-03 07:37:29,167 - INFO -   Batch 1900/2527: Loss = 0.1093
2026-01-03 07:37:58,236 - INFO -   Batch 1950/2527: Loss = 0.0879
2026-01-03 07:38:26,846 - INFO -   Batch 2000/2527: Loss = 0.0818
2026-01-03 07:38:55,771 - INFO -   Batch 2050/2527: Loss = 0.0971
2026-01-03 07:39:24,848 - INFO -   Batch 2100/2527: Loss = 0.1101
2026-01-03 07:39:53,419 - INFO -   Batch 2150/2527: Loss = 0.1243
2026-01-03 07:40:22,181 - INFO -   Batch 2200/2527: Loss = 0.1109
2026-01-03 07:40:50,832 - INFO -   Batch 2250/2527: Loss = 0.0975
2026-01-03 07:41:19,664 - INFO -   Batch 2300/2527: Loss = 0.0984
2026-01-03 07:41:48,591 - INFO -   Batch 2350/2527: Loss = 0.1089
2026-01-03 07:42:17,083 - INFO -   Batch 2400/2527: Loss = 0.1373
2026-01-03 07:42:46,041 - INFO -   Batch 2450/2527: Loss = 0.1030
2026-01-03 07:43:14,921 - INFO -   Batch 2500/2527: Loss = 0.0868
2026-01-03 07:47:54,362 - INFO - Epoch 71/160: Train Loss: 0.1053, Val Loss: 1.2381, Val mIoU: 39.95%, Val PixelAcc: 78.46%
2026-01-03 07:48:24,893 - INFO -   Batch 50/2527: Loss = 0.0917
2026-01-03 07:48:53,563 - INFO -   Batch 100/2527: Loss = 0.0944
2026-01-03 07:49:22,456 - INFO -   Batch 150/2527: Loss = 0.1495
2026-01-03 07:49:51,002 - INFO -   Batch 200/2527: Loss = 0.0775
2026-01-03 07:50:19,867 - INFO -   Batch 250/2527: Loss = 0.0925
2026-01-03 07:50:48,495 - INFO -   Batch 300/2527: Loss = 0.1175
2026-01-03 07:51:17,018 - INFO -   Batch 350/2527: Loss = 0.0784
2026-01-03 07:51:46,006 - INFO -   Batch 400/2527: Loss = 0.1125
2026-01-03 07:52:14,751 - INFO -   Batch 450/2527: Loss = 0.0849
2026-01-03 07:52:43,809 - INFO -   Batch 500/2527: Loss = 0.1177
2026-01-03 07:53:12,564 - INFO -   Batch 550/2527: Loss = 0.0834
2026-01-03 07:53:41,361 - INFO -   Batch 600/2527: Loss = 0.1227
2026-01-03 07:54:10,049 - INFO -   Batch 650/2527: Loss = 0.0809
2026-01-03 07:54:38,641 - INFO -   Batch 700/2527: Loss = 0.0823
2026-01-03 07:55:07,451 - INFO -   Batch 750/2527: Loss = 0.0947
2026-01-03 07:55:36,178 - INFO -   Batch 800/2527: Loss = 0.0956
2026-01-03 07:56:05,104 - INFO -   Batch 850/2527: Loss = 0.0831
2026-01-03 07:56:33,930 - INFO -   Batch 900/2527: Loss = 0.0929
2026-01-03 07:57:02,839 - INFO -   Batch 950/2527: Loss = 0.1121
2026-01-03 07:57:31,296 - INFO -   Batch 1000/2527: Loss = 0.0870
2026-01-03 07:57:59,870 - INFO -   Batch 1050/2527: Loss = 0.0879
2026-01-03 07:58:28,754 - INFO -   Batch 1100/2527: Loss = 0.1095
2026-01-03 07:58:57,377 - INFO -   Batch 1150/2527: Loss = 0.1189
2026-01-03 07:59:26,394 - INFO -   Batch 1200/2527: Loss = 0.0849
2026-01-03 07:59:55,026 - INFO -   Batch 1250/2527: Loss = 0.0938
2026-01-03 08:00:23,763 - INFO -   Batch 1300/2527: Loss = 0.0882
2026-01-03 08:00:52,676 - INFO -   Batch 1350/2527: Loss = 0.1198
2026-01-03 08:01:21,249 - INFO -   Batch 1400/2527: Loss = 0.1272
2026-01-03 08:01:50,088 - INFO -   Batch 1450/2527: Loss = 0.1157
2026-01-03 08:02:18,784 - INFO -   Batch 1500/2527: Loss = 0.1021
2026-01-03 08:02:47,626 - INFO -   Batch 1550/2527: Loss = 0.1346
2026-01-03 08:03:16,446 - INFO -   Batch 1600/2527: Loss = 0.0969
2026-01-03 08:03:45,054 - INFO -   Batch 1650/2527: Loss = 0.1100
2026-01-03 08:04:14,071 - INFO -   Batch 1700/2527: Loss = 0.0812
2026-01-03 08:04:42,985 - INFO -   Batch 1750/2527: Loss = 0.0916
2026-01-03 08:05:11,778 - INFO -   Batch 1800/2527: Loss = 0.0937
2026-01-03 08:05:40,420 - INFO -   Batch 1850/2527: Loss = 0.1097
2026-01-03 08:06:09,318 - INFO -   Batch 1900/2527: Loss = 0.1162
2026-01-03 08:06:37,835 - INFO -   Batch 1950/2527: Loss = 0.0921
2026-01-03 08:07:06,594 - INFO -   Batch 2000/2527: Loss = 0.1430
2026-01-03 08:07:35,531 - INFO -   Batch 2050/2527: Loss = 0.1459
2026-01-03 08:08:04,360 - INFO -   Batch 2100/2527: Loss = 0.0993
2026-01-03 08:08:33,285 - INFO -   Batch 2150/2527: Loss = 0.1155
2026-01-03 08:09:01,813 - INFO -   Batch 2200/2527: Loss = 0.1007
2026-01-03 08:09:30,333 - INFO -   Batch 2250/2527: Loss = 0.1336
2026-01-03 08:09:59,237 - INFO -   Batch 2300/2527: Loss = 0.1183
2026-01-03 08:10:27,919 - INFO -   Batch 2350/2527: Loss = 0.1245
2026-01-03 08:10:56,871 - INFO -   Batch 2400/2527: Loss = 0.0844
2026-01-03 08:11:25,675 - INFO -   Batch 2450/2527: Loss = 0.0895
2026-01-03 08:11:54,699 - INFO -   Batch 2500/2527: Loss = 0.1083
2026-01-03 08:16:39,174 - INFO - Epoch 72/160: Train Loss: 0.1003, Val Loss: 1.2731, Val mIoU: 39.43%, Val PixelAcc: 78.14%
2026-01-03 08:17:09,486 - INFO -   Batch 50/2527: Loss = 0.0943
2026-01-03 08:17:38,284 - INFO -   Batch 100/2527: Loss = 0.1076
2026-01-03 08:18:06,887 - INFO -   Batch 150/2527: Loss = 0.1078
2026-01-03 08:18:35,624 - INFO -   Batch 200/2527: Loss = 0.0864
2026-01-03 08:19:04,439 - INFO -   Batch 250/2527: Loss = 0.1262
2026-01-03 08:19:33,138 - INFO -   Batch 300/2527: Loss = 0.1170
2026-01-03 08:20:02,265 - INFO -   Batch 350/2527: Loss = 0.0726
2026-01-03 08:20:30,957 - INFO -   Batch 400/2527: Loss = 0.0823
2026-01-03 08:20:59,734 - INFO -   Batch 450/2527: Loss = 0.1027
2026-01-03 08:21:28,351 - INFO -   Batch 500/2527: Loss = 0.0700
2026-01-03 08:21:56,877 - INFO -   Batch 550/2527: Loss = 0.0937
2026-01-03 08:22:25,934 - INFO -   Batch 600/2527: Loss = 0.1013
2026-01-03 08:22:54,589 - INFO -   Batch 650/2527: Loss = 0.0968
2026-01-03 08:23:23,514 - INFO -   Batch 700/2527: Loss = 0.0786
2026-01-03 08:23:52,424 - INFO -   Batch 750/2527: Loss = 0.1463
2026-01-03 08:24:21,270 - INFO -   Batch 800/2527: Loss = 0.0970
2026-01-03 08:24:49,785 - INFO -   Batch 850/2527: Loss = 0.1215
2026-01-03 08:25:18,344 - INFO -   Batch 900/2527: Loss = 0.0839
2026-01-03 08:25:47,182 - INFO -   Batch 950/2527: Loss = 0.0979
2026-01-03 08:26:16,031 - INFO -   Batch 1000/2527: Loss = 0.0637
2026-01-03 08:26:44,851 - INFO -   Batch 1050/2527: Loss = 0.0833
2026-01-03 08:27:13,504 - INFO -   Batch 1100/2527: Loss = 0.0736
2026-01-03 08:27:42,285 - INFO -   Batch 1150/2527: Loss = 0.0965
2026-01-03 08:28:11,224 - INFO -   Batch 1200/2527: Loss = 0.1069
2026-01-03 08:28:39,789 - INFO -   Batch 1250/2527: Loss = 0.1045
2026-01-03 08:29:08,595 - INFO -   Batch 1300/2527: Loss = 0.1065
2026-01-03 08:29:37,155 - INFO -   Batch 1350/2527: Loss = 0.1148
2026-01-03 08:30:06,203 - INFO -   Batch 1400/2527: Loss = 0.0929
2026-01-03 08:30:34,781 - INFO -   Batch 1450/2527: Loss = 0.1154
2026-01-03 08:31:03,403 - INFO -   Batch 1500/2527: Loss = 0.1669
2026-01-03 08:31:32,464 - INFO -   Batch 1550/2527: Loss = 0.1276
2026-01-03 08:32:01,160 - INFO -   Batch 1600/2527: Loss = 0.0717
2026-01-03 08:32:29,934 - INFO -   Batch 1650/2527: Loss = 0.0928
2026-01-03 08:32:58,551 - INFO -   Batch 1700/2527: Loss = 0.1124
2026-01-03 08:33:27,079 - INFO -   Batch 1750/2527: Loss = 0.1104
2026-01-03 08:33:56,099 - INFO -   Batch 1800/2527: Loss = 0.1015
2026-01-03 08:34:24,889 - INFO -   Batch 1850/2527: Loss = 0.1358
2026-01-03 08:34:53,854 - INFO -   Batch 1900/2527: Loss = 0.0779
2026-01-03 08:35:22,715 - INFO -   Batch 1950/2527: Loss = 0.0798
2026-01-03 08:35:51,622 - INFO -   Batch 2000/2527: Loss = 0.0739
2026-01-03 08:36:20,204 - INFO -   Batch 2050/2527: Loss = 0.0897
2026-01-03 08:36:48,679 - INFO -   Batch 2100/2527: Loss = 0.0644
2026-01-03 08:37:17,534 - INFO -   Batch 2150/2527: Loss = 0.1323
2026-01-03 08:37:46,345 - INFO -   Batch 2200/2527: Loss = 0.1398
2026-01-03 08:38:15,195 - INFO -   Batch 2250/2527: Loss = 0.1017
2026-01-03 08:38:43,859 - INFO -   Batch 2300/2527: Loss = 0.0837
2026-01-03 08:39:12,965 - INFO -   Batch 2350/2527: Loss = 0.0538
2026-01-03 08:39:41,537 - INFO -   Batch 2400/2527: Loss = 0.1177
2026-01-03 08:40:10,147 - INFO -   Batch 2450/2527: Loss = 0.0992
2026-01-03 08:40:38,930 - INFO -   Batch 2500/2527: Loss = 0.1090
2026-01-03 08:45:18,214 - INFO - Epoch 73/160: Train Loss: 0.0994, Val Loss: 1.2496, Val mIoU: 39.86%, Val PixelAcc: 78.57%
2026-01-03 08:45:48,417 - INFO -   Batch 50/2527: Loss = 0.1219
2026-01-03 08:46:17,500 - INFO -   Batch 100/2527: Loss = 0.1132
2026-01-03 08:46:46,129 - INFO -   Batch 150/2527: Loss = 0.0598
2026-01-03 08:47:14,901 - INFO -   Batch 200/2527: Loss = 0.0619
2026-01-03 08:47:43,547 - INFO -   Batch 250/2527: Loss = 0.0954
2026-01-03 08:48:12,360 - INFO -   Batch 300/2527: Loss = 0.0818
2026-01-03 08:48:41,302 - INFO -   Batch 350/2527: Loss = 0.0923
2026-01-03 08:49:09,794 - INFO -   Batch 400/2527: Loss = 0.0847
2026-01-03 08:49:38,768 - INFO -   Batch 450/2527: Loss = 0.0849
2026-01-03 08:50:07,653 - INFO -   Batch 500/2527: Loss = 0.0708
2026-01-03 08:50:36,541 - INFO -   Batch 550/2527: Loss = 0.1342
2026-01-03 08:51:05,070 - INFO -   Batch 600/2527: Loss = 0.1126
2026-01-03 08:51:33,530 - INFO -   Batch 650/2527: Loss = 0.0836
2026-01-03 08:52:02,408 - INFO -   Batch 700/2527: Loss = 0.0772
2026-01-03 08:52:31,242 - INFO -   Batch 750/2527: Loss = 0.1007
2026-01-03 08:53:00,054 - INFO -   Batch 800/2527: Loss = 0.0826
2026-01-03 08:53:28,734 - INFO -   Batch 850/2527: Loss = 0.0976
2026-01-03 08:53:57,837 - INFO -   Batch 900/2527: Loss = 0.0853
2026-01-03 08:54:26,544 - INFO -   Batch 950/2527: Loss = 0.0723
2026-01-03 08:54:55,005 - INFO -   Batch 1000/2527: Loss = 0.1401
2026-01-03 08:55:23,804 - INFO -   Batch 1050/2527: Loss = 0.0818
2026-01-03 08:55:52,388 - INFO -   Batch 1100/2527: Loss = 0.0893
2026-01-03 08:56:21,421 - INFO -   Batch 1150/2527: Loss = 0.0968
2026-01-03 08:56:50,015 - INFO -   Batch 1200/2527: Loss = 0.0891
2026-01-03 08:57:18,950 - INFO -   Batch 1250/2527: Loss = 0.1192
2026-01-03 08:57:47,709 - INFO -   Batch 1300/2527: Loss = 0.1015
2026-01-03 08:58:16,423 - INFO -   Batch 1350/2527: Loss = 0.0713
2026-01-03 08:58:45,235 - INFO -   Batch 1400/2527: Loss = 0.1213
2026-01-03 08:59:13,824 - INFO -   Batch 1450/2527: Loss = 0.1098
2026-01-03 08:59:42,674 - INFO -   Batch 1500/2527: Loss = 0.0564
2026-01-03 09:00:11,656 - INFO -   Batch 1550/2527: Loss = 0.0949
2026-01-03 09:00:40,146 - INFO -   Batch 1600/2527: Loss = 0.1214
2026-01-03 09:01:09,113 - INFO -   Batch 1650/2527: Loss = 0.0858
2026-01-03 09:01:38,012 - INFO -   Batch 1700/2527: Loss = 0.0894
2026-01-03 09:02:06,935 - INFO -   Batch 1750/2527: Loss = 0.1007
2026-01-03 09:02:35,516 - INFO -   Batch 1800/2527: Loss = 0.0888
2026-01-03 09:03:04,332 - INFO -   Batch 1850/2527: Loss = 0.1342
2026-01-03 09:03:32,981 - INFO -   Batch 1900/2527: Loss = 0.0864
2026-01-03 09:04:01,716 - INFO -   Batch 1950/2527: Loss = 0.0910
2026-01-03 09:04:30,534 - INFO -   Batch 2000/2527: Loss = 0.0982
2026-01-03 09:04:59,273 - INFO -   Batch 2050/2527: Loss = 0.0949
2026-01-03 09:05:28,349 - INFO -   Batch 2100/2527: Loss = 0.1012
2026-01-03 09:05:57,069 - INFO -   Batch 2150/2527: Loss = 0.0803
2026-01-03 09:06:25,861 - INFO -   Batch 2200/2527: Loss = 0.0904
2026-01-03 09:06:54,328 - INFO -   Batch 2250/2527: Loss = 0.0912
2026-01-03 09:07:23,015 - INFO -   Batch 2300/2527: Loss = 0.0950
2026-01-03 09:07:52,090 - INFO -   Batch 2350/2527: Loss = 0.1110
2026-01-03 09:08:20,689 - INFO -   Batch 2400/2527: Loss = 0.0682
2026-01-03 09:08:49,626 - INFO -   Batch 2450/2527: Loss = 0.0804
2026-01-03 09:09:18,454 - INFO -   Batch 2500/2527: Loss = 0.0957
2026-01-03 09:13:58,359 - INFO - Epoch 74/160: Train Loss: 0.0975, Val Loss: 1.2708, Val mIoU: 39.36%, Val PixelAcc: 78.32%
2026-01-03 09:14:29,293 - INFO -   Batch 50/2527: Loss = 0.1080
2026-01-03 09:14:58,137 - INFO -   Batch 100/2527: Loss = 0.0580
2026-01-03 09:15:26,976 - INFO -   Batch 150/2527: Loss = 0.1128
2026-01-03 09:15:55,422 - INFO -   Batch 200/2527: Loss = 0.1070
2026-01-03 09:16:24,006 - INFO -   Batch 250/2527: Loss = 0.0852
2026-01-03 09:16:52,871 - INFO -   Batch 300/2527: Loss = 0.1093
2026-01-03 09:17:21,545 - INFO -   Batch 350/2527: Loss = 0.0657
2026-01-03 09:17:50,545 - INFO -   Batch 400/2527: Loss = 0.1048
2026-01-03 09:18:19,185 - INFO -   Batch 450/2527: Loss = 0.1180
2026-01-03 09:18:47,901 - INFO -   Batch 500/2527: Loss = 0.1002
2026-01-03 09:19:16,801 - INFO -   Batch 550/2527: Loss = 0.0750
2026-01-03 09:19:45,368 - INFO -   Batch 600/2527: Loss = 0.1318
2026-01-03 09:20:14,256 - INFO -   Batch 650/2527: Loss = 0.0745
2026-01-03 09:20:42,887 - INFO -   Batch 700/2527: Loss = 0.0913
2026-01-03 09:21:11,774 - INFO -   Batch 750/2527: Loss = 0.1006
2026-01-03 09:21:40,494 - INFO -   Batch 800/2527: Loss = 0.0753
2026-01-03 09:22:09,081 - INFO -   Batch 850/2527: Loss = 0.0497
2026-01-03 09:22:38,143 - INFO -   Batch 900/2527: Loss = 0.1062
2026-01-03 09:23:06,829 - INFO -   Batch 950/2527: Loss = 0.1068
2026-01-03 09:23:35,594 - INFO -   Batch 1000/2527: Loss = 0.0750
2026-01-03 09:24:04,257 - INFO -   Batch 1050/2527: Loss = 0.1071
2026-01-03 09:24:32,815 - INFO -   Batch 1100/2527: Loss = 0.0942
2026-01-03 09:25:01,661 - INFO -   Batch 1150/2527: Loss = 0.1202
2026-01-03 09:25:30,603 - INFO -   Batch 1200/2527: Loss = 0.0722
2026-01-03 09:25:59,490 - INFO -   Batch 1250/2527: Loss = 0.0738
2026-01-03 09:26:28,378 - INFO -   Batch 1300/2527: Loss = 0.0696
2026-01-03 09:26:57,311 - INFO -   Batch 1350/2527: Loss = 0.0676
2026-01-03 09:27:25,886 - INFO -   Batch 1400/2527: Loss = 0.1205
2026-01-03 09:27:54,403 - INFO -   Batch 1450/2527: Loss = 0.1012
2026-01-03 09:28:23,303 - INFO -   Batch 1500/2527: Loss = 0.0889
2026-01-03 09:28:51,950 - INFO -   Batch 1550/2527: Loss = 0.0910
2026-01-03 09:29:20,979 - INFO -   Batch 1600/2527: Loss = 0.0766
2026-01-03 09:29:49,662 - INFO -   Batch 1650/2527: Loss = 0.1090
2026-01-03 09:30:18,720 - INFO -   Batch 1700/2527: Loss = 0.0827
2026-01-03 09:30:47,401 - INFO -   Batch 1750/2527: Loss = 0.0854
2026-01-03 09:31:15,848 - INFO -   Batch 1800/2527: Loss = 0.0853
2026-01-03 09:31:44,730 - INFO -   Batch 1850/2527: Loss = 0.1182
2026-01-03 09:32:13,476 - INFO -   Batch 1900/2527: Loss = 0.0928
2026-01-03 09:32:42,335 - INFO -   Batch 1950/2527: Loss = 0.0977
2026-01-03 09:33:11,161 - INFO -   Batch 2000/2527: Loss = 0.0770
2026-01-03 09:33:39,756 - INFO -   Batch 2050/2527: Loss = 0.0834
2026-01-03 09:34:08,777 - INFO -   Batch 2100/2527: Loss = 0.0910
2026-01-03 09:34:37,465 - INFO -   Batch 2150/2527: Loss = 0.1037
2026-01-03 09:35:06,230 - INFO -   Batch 2200/2527: Loss = 0.1157
2026-01-03 09:35:34,908 - INFO -   Batch 2250/2527: Loss = 0.0841
2026-01-03 09:36:03,791 - INFO -   Batch 2300/2527: Loss = 0.1478
2026-01-03 09:36:32,421 - INFO -   Batch 2350/2527: Loss = 0.1131
2026-01-03 09:37:01,072 - INFO -   Batch 2400/2527: Loss = 0.0938
2026-01-03 09:37:30,005 - INFO -   Batch 2450/2527: Loss = 0.0921
2026-01-03 09:37:58,857 - INFO -   Batch 2500/2527: Loss = 0.1269
2026-01-03 09:42:42,478 - INFO - Epoch 75/160: Train Loss: 0.0985, Val Loss: 1.2374, Val mIoU: 39.04%, Val PixelAcc: 78.25%
2026-01-03 09:43:13,146 - INFO -   Batch 50/2527: Loss = 0.0867
2026-01-03 09:43:41,744 - INFO -   Batch 100/2527: Loss = 0.0856
2026-01-03 09:44:10,580 - INFO -   Batch 150/2527: Loss = 0.0842
2026-01-03 09:44:39,453 - INFO -   Batch 200/2527: Loss = 0.0792
2026-01-03 09:45:08,231 - INFO -   Batch 250/2527: Loss = 0.0833
2026-01-03 09:45:36,937 - INFO -   Batch 300/2527: Loss = 0.0765
2026-01-03 09:46:05,718 - INFO -   Batch 350/2527: Loss = 0.0766
2026-01-03 09:46:34,586 - INFO -   Batch 400/2527: Loss = 0.0883
2026-01-03 09:47:03,238 - INFO -   Batch 450/2527: Loss = 0.0883
2026-01-03 09:47:32,059 - INFO -   Batch 500/2527: Loss = 0.1110
2026-01-03 09:48:00,643 - INFO -   Batch 550/2527: Loss = 0.0917
2026-01-03 09:48:29,706 - INFO -   Batch 600/2527: Loss = 0.1121
2026-01-03 09:48:58,197 - INFO -   Batch 650/2527: Loss = 0.0748
2026-01-03 09:49:26,967 - INFO -   Batch 700/2527: Loss = 0.1074
2026-01-03 09:49:56,080 - INFO -   Batch 750/2527: Loss = 0.1067
2026-01-03 09:50:24,690 - INFO -   Batch 800/2527: Loss = 0.1161
2026-01-03 09:50:53,532 - INFO -   Batch 850/2527: Loss = 0.0848
2026-01-03 09:51:22,067 - INFO -   Batch 900/2527: Loss = 0.0813
2026-01-03 09:51:50,594 - INFO -   Batch 950/2527: Loss = 0.1277
2026-01-03 09:52:19,700 - INFO -   Batch 1000/2527: Loss = 0.1198
2026-01-03 09:52:48,288 - INFO -   Batch 1050/2527: Loss = 0.1198
2026-01-03 09:53:17,220 - INFO -   Batch 1100/2527: Loss = 0.0881
2026-01-03 09:53:46,131 - INFO -   Batch 1150/2527: Loss = 0.0751
2026-01-03 09:54:15,015 - INFO -   Batch 1200/2527: Loss = 0.0727
2026-01-03 09:54:43,802 - INFO -   Batch 1250/2527: Loss = 0.0829
2026-01-03 09:55:12,298 - INFO -   Batch 1300/2527: Loss = 0.2019
2026-01-03 09:55:41,109 - INFO -   Batch 1350/2527: Loss = 0.0743
2026-01-03 09:56:09,980 - INFO -   Batch 1400/2527: Loss = 0.1061
2026-01-03 09:56:38,815 - INFO -   Batch 1450/2527: Loss = 0.0987
2026-01-03 09:57:07,547 - INFO -   Batch 1500/2527: Loss = 0.1105
2026-01-03 09:57:36,657 - INFO -   Batch 1550/2527: Loss = 0.0752
2026-01-03 09:58:05,217 - INFO -   Batch 1600/2527: Loss = 0.0812
2026-01-03 09:58:33,816 - INFO -   Batch 1650/2527: Loss = 0.0771
2026-01-03 09:59:02,659 - INFO -   Batch 1700/2527: Loss = 0.0814
2026-01-03 09:59:31,266 - INFO -   Batch 1750/2527: Loss = 0.1245
2026-01-03 10:00:00,340 - INFO -   Batch 1800/2527: Loss = 0.0831
2026-01-03 10:00:28,927 - INFO -   Batch 1850/2527: Loss = 0.1398
2026-01-03 10:00:57,553 - INFO -   Batch 1900/2527: Loss = 0.0779
2026-01-03 10:01:26,662 - INFO -   Batch 1950/2527: Loss = 0.0919
2026-01-03 10:01:55,396 - INFO -   Batch 2000/2527: Loss = 0.0552
2026-01-03 10:02:24,166 - INFO -   Batch 2050/2527: Loss = 0.1078
2026-01-03 10:02:52,790 - INFO -   Batch 2100/2527: Loss = 0.1127
2026-01-03 10:03:21,595 - INFO -   Batch 2150/2527: Loss = 0.1091
2026-01-03 10:03:50,449 - INFO -   Batch 2200/2527: Loss = 0.0869
2026-01-03 10:04:18,945 - INFO -   Batch 2250/2527: Loss = 0.1452
2026-01-03 10:04:47,904 - INFO -   Batch 2300/2527: Loss = 0.0863
2026-01-03 10:05:16,798 - INFO -   Batch 2350/2527: Loss = 0.0995
2026-01-03 10:05:45,712 - INFO -   Batch 2400/2527: Loss = 0.0648
2026-01-03 10:06:14,244 - INFO -   Batch 2450/2527: Loss = 0.0995
2026-01-03 10:06:42,721 - INFO -   Batch 2500/2527: Loss = 0.1130
2026-01-03 10:11:22,385 - INFO - Epoch 76/160: Train Loss: 0.0944, Val Loss: 1.2647, Val mIoU: 39.38%, Val PixelAcc: 78.55%
2026-01-03 10:11:52,705 - INFO -   Batch 50/2527: Loss = 0.1755
2026-01-03 10:12:21,765 - INFO -   Batch 100/2527: Loss = 0.0950
2026-01-03 10:12:50,485 - INFO -   Batch 150/2527: Loss = 0.0692
2026-01-03 10:13:18,924 - INFO -   Batch 200/2527: Loss = 0.0971
2026-01-03 10:13:47,685 - INFO -   Batch 250/2527: Loss = 0.1016
2026-01-03 10:14:16,278 - INFO -   Batch 300/2527: Loss = 0.1086
2026-01-03 10:14:45,292 - INFO -   Batch 350/2527: Loss = 0.1464
2026-01-03 10:15:13,916 - INFO -   Batch 400/2527: Loss = 0.0973
2026-01-03 10:15:42,874 - INFO -   Batch 450/2527: Loss = 0.0968
2026-01-03 10:16:11,637 - INFO -   Batch 500/2527: Loss = 0.0849
2026-01-03 10:16:40,326 - INFO -   Batch 550/2527: Loss = 0.1044
2026-01-03 10:17:09,136 - INFO -   Batch 600/2527: Loss = 0.0703
2026-01-03 10:17:37,689 - INFO -   Batch 650/2527: Loss = 0.0785
2026-01-03 10:18:06,523 - INFO -   Batch 700/2527: Loss = 0.0895
2026-01-03 10:18:35,313 - INFO -   Batch 750/2527: Loss = 0.0894
2026-01-03 10:19:03,820 - INFO -   Batch 800/2527: Loss = 0.1233
2026-01-03 10:19:32,770 - INFO -   Batch 850/2527: Loss = 0.1131
2026-01-03 10:20:01,671 - INFO -   Batch 900/2527: Loss = 0.1001
2026-01-03 10:20:30,586 - INFO -   Batch 950/2527: Loss = 0.1054
2026-01-03 10:20:59,334 - INFO -   Batch 1000/2527: Loss = 0.0772
2026-01-03 10:21:28,137 - INFO -   Batch 1050/2527: Loss = 0.0904
2026-01-03 10:21:56,784 - INFO -   Batch 1100/2527: Loss = 0.0874
2026-01-03 10:22:25,507 - INFO -   Batch 1150/2527: Loss = 0.1345
2026-01-03 10:22:54,305 - INFO -   Batch 1200/2527: Loss = 0.0856
2026-01-03 10:23:23,068 - INFO -   Batch 1250/2527: Loss = 0.0719
2026-01-03 10:23:52,129 - INFO -   Batch 1300/2527: Loss = 0.0578
2026-01-03 10:24:20,843 - INFO -   Batch 1350/2527: Loss = 0.0980
2026-01-03 10:24:49,352 - INFO -   Batch 1400/2527: Loss = 0.1218
2026-01-03 10:25:18,134 - INFO -   Batch 1450/2527: Loss = 0.1092
2026-01-03 10:25:46,763 - INFO -   Batch 1500/2527: Loss = 0.0830
2026-01-03 10:26:15,815 - INFO -   Batch 1550/2527: Loss = 0.0921
2026-01-03 10:26:44,428 - INFO -   Batch 1600/2527: Loss = 0.1083
2026-01-03 10:27:13,389 - INFO -   Batch 1650/2527: Loss = 0.0652
2026-01-03 10:27:42,320 - INFO -   Batch 1700/2527: Loss = 0.0735
2026-01-03 10:28:10,898 - INFO -   Batch 1750/2527: Loss = 0.0915
2026-01-03 10:28:39,725 - INFO -   Batch 1800/2527: Loss = 0.0686
2026-01-03 10:29:08,337 - INFO -   Batch 1850/2527: Loss = 0.1103
2026-01-03 10:29:37,207 - INFO -   Batch 1900/2527: Loss = 0.0715
2026-01-03 10:30:06,003 - INFO -   Batch 1950/2527: Loss = 0.1035
2026-01-03 10:30:34,827 - INFO -   Batch 2000/2527: Loss = 0.0953
2026-01-03 10:31:03,434 - INFO -   Batch 2050/2527: Loss = 0.0580
2026-01-03 10:31:32,348 - INFO -   Batch 2100/2527: Loss = 0.0971
2026-01-03 10:32:01,235 - INFO -   Batch 2150/2527: Loss = 0.0739
2026-01-03 10:32:29,816 - INFO -   Batch 2200/2527: Loss = 0.1414
2026-01-03 10:32:58,610 - INFO -   Batch 2250/2527: Loss = 0.0802
2026-01-03 10:33:27,290 - INFO -   Batch 2300/2527: Loss = 0.0815
2026-01-03 10:33:56,023 - INFO -   Batch 2350/2527: Loss = 0.0699
2026-01-03 10:34:24,799 - INFO -   Batch 2400/2527: Loss = 0.0821
2026-01-03 10:34:53,508 - INFO -   Batch 2450/2527: Loss = 0.0696
2026-01-03 10:35:22,611 - INFO -   Batch 2500/2527: Loss = 0.0979
2026-01-03 10:40:02,859 - INFO - Epoch 77/160: Train Loss: 0.0945, Val Loss: 1.3113, Val mIoU: 38.79%, Val PixelAcc: 78.06%
2026-01-03 10:40:33,283 - INFO -   Batch 50/2527: Loss = 0.0954
2026-01-03 10:41:02,121 - INFO -   Batch 100/2527: Loss = 0.1266
2026-01-03 10:41:31,032 - INFO -   Batch 150/2527: Loss = 0.1066
2026-01-03 10:41:59,637 - INFO -   Batch 200/2527: Loss = 0.0615
2026-01-03 10:42:28,500 - INFO -   Batch 250/2527: Loss = 0.0898
2026-01-03 10:42:57,187 - INFO -   Batch 300/2527: Loss = 0.0744
2026-01-03 10:43:25,719 - INFO -   Batch 350/2527: Loss = 0.0960
2026-01-03 10:43:54,704 - INFO -   Batch 400/2527: Loss = 0.0870
2026-01-03 10:44:23,380 - INFO -   Batch 450/2527: Loss = 0.0993
2026-01-03 10:44:52,450 - INFO -   Batch 500/2527: Loss = 0.0832
2026-01-03 10:45:21,167 - INFO -   Batch 550/2527: Loss = 0.1035
2026-01-03 10:45:49,974 - INFO -   Batch 600/2527: Loss = 0.0762
2026-01-03 10:46:18,575 - INFO -   Batch 650/2527: Loss = 0.1136
2026-01-03 10:46:47,156 - INFO -   Batch 700/2527: Loss = 0.0757
2026-01-03 10:47:16,021 - INFO -   Batch 750/2527: Loss = 0.0724
2026-01-03 10:47:44,833 - INFO -   Batch 800/2527: Loss = 0.1252
2026-01-03 10:48:13,742 - INFO -   Batch 850/2527: Loss = 0.1049
2026-01-03 10:48:42,569 - INFO -   Batch 900/2527: Loss = 0.0699
2026-01-03 10:49:11,138 - INFO -   Batch 950/2527: Loss = 0.1632
2026-01-03 10:49:39,933 - INFO -   Batch 1000/2527: Loss = 0.1175
2026-01-03 10:50:08,574 - INFO -   Batch 1050/2527: Loss = 0.1250
2026-01-03 10:50:37,431 - INFO -   Batch 1100/2527: Loss = 0.0965
2026-01-03 10:51:06,098 - INFO -   Batch 1150/2527: Loss = 0.0843
2026-01-03 10:51:35,133 - INFO -   Batch 1200/2527: Loss = 0.0976
2026-01-03 10:52:03,849 - INFO -   Batch 1250/2527: Loss = 0.0956
2026-01-03 10:52:32,594 - INFO -   Batch 1300/2527: Loss = 0.0842
2026-01-03 10:53:01,522 - INFO -   Batch 1350/2527: Loss = 0.0689
2026-01-03 10:53:30,286 - INFO -   Batch 1400/2527: Loss = 0.0731
2026-01-03 10:53:59,162 - INFO -   Batch 1450/2527: Loss = 0.0809
2026-01-03 10:54:27,851 - INFO -   Batch 1500/2527: Loss = 0.0699
2026-01-03 10:54:56,765 - INFO -   Batch 1550/2527: Loss = 0.0826
2026-01-03 10:55:25,403 - INFO -   Batch 1600/2527: Loss = 0.0870
2026-01-03 10:55:54,128 - INFO -   Batch 1650/2527: Loss = 0.0957
2026-01-03 10:56:23,189 - INFO -   Batch 1700/2527: Loss = 0.0892
2026-01-03 10:56:51,895 - INFO -   Batch 1750/2527: Loss = 0.0935
2026-01-03 10:57:20,692 - INFO -   Batch 1800/2527: Loss = 0.1001
2026-01-03 10:57:49,315 - INFO -   Batch 1850/2527: Loss = 0.0820
2026-01-03 10:58:17,892 - INFO -   Batch 1900/2527: Loss = 0.0921
2026-01-03 10:58:46,753 - INFO -   Batch 1950/2527: Loss = 0.1251
2026-01-03 10:59:15,587 - INFO -   Batch 2000/2527: Loss = 0.0794
2026-01-03 10:59:44,501 - INFO -   Batch 2050/2527: Loss = 0.1015
2026-01-03 11:00:13,392 - INFO -   Batch 2100/2527: Loss = 0.1203
2026-01-03 11:00:42,318 - INFO -   Batch 2150/2527: Loss = 0.0697
2026-01-03 11:01:10,897 - INFO -   Batch 2200/2527: Loss = 0.1080
2026-01-03 11:01:39,438 - INFO -   Batch 2250/2527: Loss = 0.0760
2026-01-03 11:02:08,340 - INFO -   Batch 2300/2527: Loss = 0.0697
2026-01-03 11:02:36,984 - INFO -   Batch 2350/2527: Loss = 0.0900
2026-01-03 11:03:05,998 - INFO -   Batch 2400/2527: Loss = 0.0784
2026-01-03 11:03:34,744 - INFO -   Batch 2450/2527: Loss = 0.1051
2026-01-03 11:04:03,818 - INFO -   Batch 2500/2527: Loss = 0.0601
2026-01-03 11:08:47,901 - INFO - Epoch 78/160: Train Loss: 0.0922, Val Loss: 1.2726, Val mIoU: 39.57%, Val PixelAcc: 78.50%
2026-01-03 11:09:18,167 - INFO -   Batch 50/2527: Loss = 0.1332
2026-01-03 11:09:46,920 - INFO -   Batch 100/2527: Loss = 0.0788
2026-01-03 11:10:15,615 - INFO -   Batch 150/2527: Loss = 0.0959
2026-01-03 11:10:44,338 - INFO -   Batch 200/2527: Loss = 0.0485
2026-01-03 11:11:13,128 - INFO -   Batch 250/2527: Loss = 0.0945
2026-01-03 11:11:41,937 - INFO -   Batch 300/2527: Loss = 0.0965
2026-01-03 11:12:10,996 - INFO -   Batch 350/2527: Loss = 0.0933
2026-01-03 11:12:39,738 - INFO -   Batch 400/2527: Loss = 0.0758
2026-01-03 11:13:08,576 - INFO -   Batch 450/2527: Loss = 0.0635
2026-01-03 11:13:37,086 - INFO -   Batch 500/2527: Loss = 0.0763
2026-01-03 11:14:05,809 - INFO -   Batch 550/2527: Loss = 0.0740
2026-01-03 11:14:34,897 - INFO -   Batch 600/2527: Loss = 0.1034
2026-01-03 11:15:03,516 - INFO -   Batch 650/2527: Loss = 0.0660
2026-01-03 11:15:32,498 - INFO -   Batch 700/2527: Loss = 0.0827
2026-01-03 11:16:01,355 - INFO -   Batch 750/2527: Loss = 0.1039
2026-01-03 11:16:29,944 - INFO -   Batch 800/2527: Loss = 0.0818
2026-01-03 11:16:58,770 - INFO -   Batch 850/2527: Loss = 0.0975
2026-01-03 11:17:27,390 - INFO -   Batch 900/2527: Loss = 0.0815
2026-01-03 11:17:56,238 - INFO -   Batch 950/2527: Loss = 0.0762
2026-01-03 11:18:25,047 - INFO -   Batch 1000/2527: Loss = 0.0858
2026-01-03 11:18:53,847 - INFO -   Batch 1050/2527: Loss = 0.0710
2026-01-03 11:19:22,612 - INFO -   Batch 1100/2527: Loss = 0.1054
2026-01-03 11:19:51,418 - INFO -   Batch 1150/2527: Loss = 0.0820
2026-01-03 11:20:20,347 - INFO -   Batch 1200/2527: Loss = 0.1313
2026-01-03 11:20:48,958 - INFO -   Batch 1250/2527: Loss = 0.0738
2026-01-03 11:21:17,762 - INFO -   Batch 1300/2527: Loss = 0.0886
2026-01-03 11:21:46,411 - INFO -   Batch 1350/2527: Loss = 0.0683
2026-01-03 11:22:15,489 - INFO -   Batch 1400/2527: Loss = 0.0743
2026-01-03 11:22:44,027 - INFO -   Batch 1450/2527: Loss = 0.1094
2026-01-03 11:23:12,933 - INFO -   Batch 1500/2527: Loss = 0.0583
2026-01-03 11:23:42,041 - INFO -   Batch 1550/2527: Loss = 0.0943
2026-01-03 11:24:10,742 - INFO -   Batch 1600/2527: Loss = 0.0718
2026-01-03 11:24:39,579 - INFO -   Batch 1650/2527: Loss = 0.1217
2026-01-03 11:25:08,172 - INFO -   Batch 1700/2527: Loss = 0.1182
2026-01-03 11:25:36,711 - INFO -   Batch 1750/2527: Loss = 0.1085
2026-01-03 11:26:05,813 - INFO -   Batch 1800/2527: Loss = 0.1006
2026-01-03 11:26:34,447 - INFO -   Batch 1850/2527: Loss = 0.0839
2026-01-03 11:27:03,390 - INFO -   Batch 1900/2527: Loss = 0.0961
2026-01-03 11:27:32,284 - INFO -   Batch 1950/2527: Loss = 0.0619
2026-01-03 11:28:01,172 - INFO -   Batch 2000/2527: Loss = 0.0888
2026-01-03 11:28:29,829 - INFO -   Batch 2050/2527: Loss = 0.0678
2026-01-03 11:28:58,342 - INFO -   Batch 2100/2527: Loss = 0.0525
2026-01-03 11:29:27,226 - INFO -   Batch 2150/2527: Loss = 0.0785
2026-01-03 11:29:56,117 - INFO -   Batch 2200/2527: Loss = 0.0815
2026-01-03 11:30:24,961 - INFO -   Batch 2250/2527: Loss = 0.0812
2026-01-03 11:30:53,717 - INFO -   Batch 2300/2527: Loss = 0.1563
2026-01-03 11:31:22,532 - INFO -   Batch 2350/2527: Loss = 0.0885
2026-01-03 11:31:51,489 - INFO -   Batch 2400/2527: Loss = 0.0513
2026-01-03 11:32:20,029 - INFO -   Batch 2450/2527: Loss = 0.0520
2026-01-03 11:32:48,874 - INFO -   Batch 2500/2527: Loss = 0.1022
2026-01-03 11:37:28,919 - INFO - Epoch 79/160: Train Loss: 0.0877, Val Loss: 1.2978, Val mIoU: 39.61%, Val PixelAcc: 78.60%
2026-01-03 11:37:59,055 - INFO -   Batch 50/2527: Loss = 0.0975
2026-01-03 11:38:28,128 - INFO -   Batch 100/2527: Loss = 0.0716
2026-01-03 11:38:56,757 - INFO -   Batch 150/2527: Loss = 0.0797
2026-01-03 11:39:25,551 - INFO -   Batch 200/2527: Loss = 0.0798
2026-01-03 11:39:54,055 - INFO -   Batch 250/2527: Loss = 0.0608
2026-01-03 11:40:22,593 - INFO -   Batch 300/2527: Loss = 0.0909
2026-01-03 11:40:51,667 - INFO -   Batch 350/2527: Loss = 0.0781
2026-01-03 11:41:20,309 - INFO -   Batch 400/2527: Loss = 0.0884
2026-01-03 11:41:49,234 - INFO -   Batch 450/2527: Loss = 0.0909
2026-01-03 11:42:18,078 - INFO -   Batch 500/2527: Loss = 0.0908
2026-01-03 11:42:46,950 - INFO -   Batch 550/2527: Loss = 0.0802
2026-01-03 11:43:15,559 - INFO -   Batch 600/2527: Loss = 0.1097
2026-01-03 11:43:44,048 - INFO -   Batch 650/2527: Loss = 0.0757
2026-01-03 11:44:12,873 - INFO -   Batch 700/2527: Loss = 0.0910
2026-01-03 11:44:41,721 - INFO -   Batch 750/2527: Loss = 0.1281
2026-01-03 11:45:10,537 - INFO -   Batch 800/2527: Loss = 0.0955
2026-01-03 11:45:39,210 - INFO -   Batch 850/2527: Loss = 0.1305
2026-01-03 11:46:08,317 - INFO -   Batch 900/2527: Loss = 0.0939
2026-01-03 11:46:36,976 - INFO -   Batch 950/2527: Loss = 0.0730
2026-01-03 11:47:05,449 - INFO -   Batch 1000/2527: Loss = 0.0774
2026-01-03 11:47:34,287 - INFO -   Batch 1050/2527: Loss = 0.1152
2026-01-03 11:48:02,919 - INFO -   Batch 1100/2527: Loss = 0.0638
2026-01-03 11:48:32,025 - INFO -   Batch 1150/2527: Loss = 0.0533
2026-01-03 11:49:00,785 - INFO -   Batch 1200/2527: Loss = 0.0850
2026-01-03 11:49:29,381 - INFO -   Batch 1250/2527: Loss = 0.0752
2026-01-03 11:49:58,485 - INFO -   Batch 1300/2527: Loss = 0.0882
2026-01-03 11:50:27,180 - INFO -   Batch 1350/2527: Loss = 0.0758
2026-01-03 11:50:55,951 - INFO -   Batch 1400/2527: Loss = 0.1148
2026-01-03 11:51:24,556 - INFO -   Batch 1450/2527: Loss = 0.0702
2026-01-03 11:51:53,381 - INFO -   Batch 1500/2527: Loss = 0.0892
2026-01-03 11:52:22,287 - INFO -   Batch 1550/2527: Loss = 0.0667
2026-01-03 11:52:50,781 - INFO -   Batch 1600/2527: Loss = 0.1224
2026-01-03 11:53:19,685 - INFO -   Batch 1650/2527: Loss = 0.0743
2026-01-03 11:53:48,552 - INFO -   Batch 1700/2527: Loss = 0.1105
2026-01-03 11:54:17,448 - INFO -   Batch 1750/2527: Loss = 0.1590
2026-01-03 11:54:46,058 - INFO -   Batch 1800/2527: Loss = 0.0763
2026-01-03 11:55:14,867 - INFO -   Batch 1850/2527: Loss = 0.0706
2026-01-03 11:55:43,414 - INFO -   Batch 1900/2527: Loss = 0.0979
2026-01-03 11:56:12,292 - INFO -   Batch 1950/2527: Loss = 0.0765
2026-01-03 11:56:41,128 - INFO -   Batch 2000/2527: Loss = 0.0779
2026-01-03 11:57:09,889 - INFO -   Batch 2050/2527: Loss = 0.1381
2026-01-03 11:57:38,985 - INFO -   Batch 2100/2527: Loss = 0.0800
2026-01-03 11:58:07,601 - INFO -   Batch 2150/2527: Loss = 0.0863
2026-01-03 11:58:36,082 - INFO -   Batch 2200/2527: Loss = 0.0684
2026-01-03 11:59:04,914 - INFO -   Batch 2250/2527: Loss = 0.0648
2026-01-03 11:59:33,599 - INFO -   Batch 2300/2527: Loss = 0.0779
2026-01-03 12:00:02,609 - INFO -   Batch 2350/2527: Loss = 0.1010
2026-01-03 12:00:31,222 - INFO -   Batch 2400/2527: Loss = 0.0812
2026-01-03 12:01:00,147 - INFO -   Batch 2450/2527: Loss = 0.0712
2026-01-03 12:01:29,072 - INFO -   Batch 2500/2527: Loss = 0.0667
2026-01-03 12:06:08,560 - INFO - Epoch 80/160: Train Loss: 0.0892, Val Loss: 1.3386, Val mIoU: 38.89%, Val PixelAcc: 77.71%
2026-01-03 12:06:12,160 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_80.pth
2026-01-03 12:06:12,163 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_80.pth
2026-01-03 12:06:42,994 - INFO -   Batch 50/2527: Loss = 0.0839
2026-01-03 12:07:12,043 - INFO -   Batch 100/2527: Loss = 0.0892
2026-01-03 12:07:40,740 - INFO -   Batch 150/2527: Loss = 0.0762
2026-01-03 12:08:09,209 - INFO -   Batch 200/2527: Loss = 0.0835
2026-01-03 12:08:38,061 - INFO -   Batch 250/2527: Loss = 0.0884
2026-01-03 12:09:06,703 - INFO -   Batch 300/2527: Loss = 0.0718
2026-01-03 12:09:35,556 - INFO -   Batch 350/2527: Loss = 0.0924
2026-01-03 12:10:04,313 - INFO -   Batch 400/2527: Loss = 0.0817
2026-01-03 12:10:33,175 - INFO -   Batch 450/2527: Loss = 0.0985
2026-01-03 12:11:01,887 - INFO -   Batch 500/2527: Loss = 0.0917
2026-01-03 12:11:30,577 - INFO -   Batch 550/2527: Loss = 0.0523
2026-01-03 12:11:59,398 - INFO -   Batch 600/2527: Loss = 0.0702
2026-01-03 12:12:28,036 - INFO -   Batch 650/2527: Loss = 0.1021
2026-01-03 12:12:56,925 - INFO -   Batch 700/2527: Loss = 0.0982
2026-01-03 12:13:25,717 - INFO -   Batch 750/2527: Loss = 0.0884
2026-01-03 12:13:54,365 - INFO -   Batch 800/2527: Loss = 0.0487
2026-01-03 12:14:23,296 - INFO -   Batch 850/2527: Loss = 0.0641
2026-01-03 12:14:52,116 - INFO -   Batch 900/2527: Loss = 0.1004
2026-01-03 12:15:21,053 - INFO -   Batch 950/2527: Loss = 0.0707
2026-01-03 12:15:49,651 - INFO -   Batch 1000/2527: Loss = 0.0747
2026-01-03 12:16:18,462 - INFO -   Batch 1050/2527: Loss = 0.0578
2026-01-03 12:16:47,166 - INFO -   Batch 1100/2527: Loss = 0.0765
2026-01-03 12:17:15,689 - INFO -   Batch 1150/2527: Loss = 0.1075
2026-01-03 12:17:44,664 - INFO -   Batch 1200/2527: Loss = 0.0783
2026-01-03 12:18:13,381 - INFO -   Batch 1250/2527: Loss = 0.0734
2026-01-03 12:18:42,412 - INFO -   Batch 1300/2527: Loss = 0.1089
2026-01-03 12:19:11,276 - INFO -   Batch 1350/2527: Loss = 0.0949
2026-01-03 12:19:40,076 - INFO -   Batch 1400/2527: Loss = 0.1196
2026-01-03 12:20:08,583 - INFO -   Batch 1450/2527: Loss = 0.0625
2026-01-03 12:20:37,284 - INFO -   Batch 1500/2527: Loss = 0.0686
2026-01-03 12:21:06,157 - INFO -   Batch 1550/2527: Loss = 0.0768
2026-01-03 12:21:34,932 - INFO -   Batch 1600/2527: Loss = 0.0664
2026-01-03 12:22:03,891 - INFO -   Batch 1650/2527: Loss = 0.0809
2026-01-03 12:22:32,688 - INFO -   Batch 1700/2527: Loss = 0.1130
2026-01-03 12:23:01,268 - INFO -   Batch 1750/2527: Loss = 0.0419
2026-01-03 12:23:30,070 - INFO -   Batch 1800/2527: Loss = 0.0823
2026-01-03 12:23:58,732 - INFO -   Batch 1850/2527: Loss = 0.0808
2026-01-03 12:24:27,596 - INFO -   Batch 1900/2527: Loss = 0.1074
2026-01-03 12:24:56,224 - INFO -   Batch 1950/2527: Loss = 0.0841
2026-01-03 12:25:25,172 - INFO -   Batch 2000/2527: Loss = 0.0489
2026-01-03 12:25:53,909 - INFO -   Batch 2050/2527: Loss = 0.0717
2026-01-03 12:26:22,640 - INFO -   Batch 2100/2527: Loss = 0.0909
2026-01-03 12:26:51,547 - INFO -   Batch 2150/2527: Loss = 0.1083
2026-01-03 12:27:20,088 - INFO -   Batch 2200/2527: Loss = 0.0562
2026-01-03 12:27:48,889 - INFO -   Batch 2250/2527: Loss = 0.0893
2026-01-03 12:28:17,560 - INFO -   Batch 2300/2527: Loss = 0.0913
2026-01-03 12:28:46,119 - INFO -   Batch 2350/2527: Loss = 0.1055
2026-01-03 12:29:15,106 - INFO -   Batch 2400/2527: Loss = 0.1061
2026-01-03 12:29:43,788 - INFO -   Batch 2450/2527: Loss = 0.0857
2026-01-03 12:30:12,858 - INFO -   Batch 2500/2527: Loss = 0.0590
2026-01-03 12:34:58,600 - INFO - Epoch 81/160: Train Loss: 0.0872, Val Loss: 1.2575, Val mIoU: 40.14%, Val PixelAcc: 78.61%
2026-01-03 12:35:28,871 - INFO -   Batch 50/2527: Loss = 0.0673
2026-01-03 12:35:57,627 - INFO -   Batch 100/2527: Loss = 0.1805
2026-01-03 12:36:26,230 - INFO -   Batch 150/2527: Loss = 0.0796
2026-01-03 12:36:55,253 - INFO -   Batch 200/2527: Loss = 0.0741
2026-01-03 12:37:23,875 - INFO -   Batch 250/2527: Loss = 0.0812
2026-01-03 12:37:52,820 - INFO -   Batch 300/2527: Loss = 0.0670
2026-01-03 12:38:21,589 - INFO -   Batch 350/2527: Loss = 0.0939
2026-01-03 12:38:50,253 - INFO -   Batch 400/2527: Loss = 0.1402
2026-01-03 12:39:19,032 - INFO -   Batch 450/2527: Loss = 0.1138
2026-01-03 12:39:47,623 - INFO -   Batch 500/2527: Loss = 0.1003
2026-01-03 12:40:16,486 - INFO -   Batch 550/2527: Loss = 0.0726
2026-01-03 12:40:45,279 - INFO -   Batch 600/2527: Loss = 0.0682
2026-01-03 12:41:13,755 - INFO -   Batch 650/2527: Loss = 0.0534
2026-01-03 12:41:42,737 - INFO -   Batch 700/2527: Loss = 0.0618
2026-01-03 12:42:11,601 - INFO -   Batch 750/2527: Loss = 0.0704
2026-01-03 12:42:40,540 - INFO -   Batch 800/2527: Loss = 0.0980
2026-01-03 12:43:09,267 - INFO -   Batch 850/2527: Loss = 0.0558
2026-01-03 12:43:38,101 - INFO -   Batch 900/2527: Loss = 0.0704
2026-01-03 12:44:06,715 - INFO -   Batch 950/2527: Loss = 0.0984
2026-01-03 12:44:35,456 - INFO -   Batch 1000/2527: Loss = 0.0938
2026-01-03 12:45:04,295 - INFO -   Batch 1050/2527: Loss = 0.0629
2026-01-03 12:45:33,081 - INFO -   Batch 1100/2527: Loss = 0.0936
2026-01-03 12:46:02,138 - INFO -   Batch 1150/2527: Loss = 0.0705
2026-01-03 12:46:30,849 - INFO -   Batch 1200/2527: Loss = 0.0844
2026-01-03 12:46:59,334 - INFO -   Batch 1250/2527: Loss = 0.1092
2026-01-03 12:47:28,122 - INFO -   Batch 1300/2527: Loss = 0.0827
2026-01-03 12:47:56,815 - INFO -   Batch 1350/2527: Loss = 0.0740
2026-01-03 12:48:25,854 - INFO -   Batch 1400/2527: Loss = 0.0902
2026-01-03 12:48:54,522 - INFO -   Batch 1450/2527: Loss = 0.1138
2026-01-03 12:49:23,491 - INFO -   Batch 1500/2527: Loss = 0.0791
2026-01-03 12:49:52,403 - INFO -   Batch 1550/2527: Loss = 0.1068
2026-01-03 12:50:20,974 - INFO -   Batch 1600/2527: Loss = 0.0878
2026-01-03 12:50:49,789 - INFO -   Batch 1650/2527: Loss = 0.1038
2026-01-03 12:51:18,392 - INFO -   Batch 1700/2527: Loss = 0.0642
2026-01-03 12:51:47,256 - INFO -   Batch 1750/2527: Loss = 0.0773
2026-01-03 12:52:16,038 - INFO -   Batch 1800/2527: Loss = 0.0987
2026-01-03 12:52:44,863 - INFO -   Batch 1850/2527: Loss = 0.1040
2026-01-03 12:53:13,470 - INFO -   Batch 1900/2527: Loss = 0.0955
2026-01-03 12:53:42,344 - INFO -   Batch 1950/2527: Loss = 0.0778
2026-01-03 12:54:11,272 - INFO -   Batch 2000/2527: Loss = 0.0676
2026-01-03 12:54:39,833 - INFO -   Batch 2050/2527: Loss = 0.0835
2026-01-03 12:55:08,614 - INFO -   Batch 2100/2527: Loss = 0.1086
2026-01-03 12:55:37,278 - INFO -   Batch 2150/2527: Loss = 0.0591
2026-01-03 12:56:05,979 - INFO -   Batch 2200/2527: Loss = 0.0701
2026-01-03 12:56:34,814 - INFO -   Batch 2250/2527: Loss = 0.0606
2026-01-03 12:57:03,525 - INFO -   Batch 2300/2527: Loss = 0.0697
2026-01-03 12:57:32,608 - INFO -   Batch 2350/2527: Loss = 0.1089
2026-01-03 12:58:01,299 - INFO -   Batch 2400/2527: Loss = 0.1135
2026-01-03 12:58:30,094 - INFO -   Batch 2450/2527: Loss = 0.0620
2026-01-03 12:58:58,700 - INFO -   Batch 2500/2527: Loss = 0.0818
2026-01-03 13:03:39,115 - INFO - Epoch 82/160: Train Loss: 0.0861, Val Loss: 1.2841, Val mIoU: 40.06%, Val PixelAcc: 78.59%
2026-01-03 13:04:09,752 - INFO -   Batch 50/2527: Loss = 0.1391
2026-01-03 13:04:38,541 - INFO -   Batch 100/2527: Loss = 0.1170
2026-01-03 13:05:07,101 - INFO -   Batch 150/2527: Loss = 0.0789
2026-01-03 13:05:35,941 - INFO -   Batch 200/2527: Loss = 0.0707
2026-01-03 13:06:04,458 - INFO -   Batch 250/2527: Loss = 0.0715
2026-01-03 13:06:33,319 - INFO -   Batch 300/2527: Loss = 0.0884
2026-01-03 13:07:02,107 - INFO -   Batch 350/2527: Loss = 0.0850
2026-01-03 13:07:30,954 - INFO -   Batch 400/2527: Loss = 0.0949
2026-01-03 13:07:59,747 - INFO -   Batch 450/2527: Loss = 0.1059
2026-01-03 13:08:28,492 - INFO -   Batch 500/2527: Loss = 0.0778
2026-01-03 13:08:57,371 - INFO -   Batch 550/2527: Loss = 0.0990
2026-01-03 13:09:26,163 - INFO -   Batch 600/2527: Loss = 0.1302
2026-01-03 13:09:54,937 - INFO -   Batch 650/2527: Loss = 0.0913
2026-01-03 13:10:23,614 - INFO -   Batch 700/2527: Loss = 0.0966
2026-01-03 13:10:52,684 - INFO -   Batch 750/2527: Loss = 0.0768
2026-01-03 13:11:21,178 - INFO -   Batch 800/2527: Loss = 0.0816
2026-01-03 13:11:49,944 - INFO -   Batch 850/2527: Loss = 0.1105
2026-01-03 13:12:19,039 - INFO -   Batch 900/2527: Loss = 0.0989
2026-01-03 13:12:47,753 - INFO -   Batch 950/2527: Loss = 0.0836
2026-01-03 13:13:16,576 - INFO -   Batch 1000/2527: Loss = 0.0856
2026-01-03 13:13:45,126 - INFO -   Batch 1050/2527: Loss = 0.0640
2026-01-03 13:14:13,677 - INFO -   Batch 1100/2527: Loss = 0.1018
2026-01-03 13:14:42,756 - INFO -   Batch 1150/2527: Loss = 0.0718
2026-01-03 13:15:11,377 - INFO -   Batch 1200/2527: Loss = 0.0861
2026-01-03 13:15:40,407 - INFO -   Batch 1250/2527: Loss = 0.0942
2026-01-03 13:16:09,463 - INFO -   Batch 1300/2527: Loss = 0.0561
2026-01-03 13:16:38,397 - INFO -   Batch 1350/2527: Loss = 0.0799
2026-01-03 13:17:07,073 - INFO -   Batch 1400/2527: Loss = 0.0837
2026-01-03 13:17:35,619 - INFO -   Batch 1450/2527: Loss = 0.1238
2026-01-03 13:18:04,478 - INFO -   Batch 1500/2527: Loss = 0.1123
2026-01-03 13:18:33,339 - INFO -   Batch 1550/2527: Loss = 0.0977
2026-01-03 13:19:02,248 - INFO -   Batch 1600/2527: Loss = 0.0631
2026-01-03 13:19:31,031 - INFO -   Batch 1650/2527: Loss = 0.0807
2026-01-03 13:20:00,193 - INFO -   Batch 1700/2527: Loss = 0.0579
2026-01-03 13:20:28,803 - INFO -   Batch 1750/2527: Loss = 0.0817
2026-01-03 13:20:57,403 - INFO -   Batch 1800/2527: Loss = 0.0938
2026-01-03 13:21:26,267 - INFO -   Batch 1850/2527: Loss = 0.0599
2026-01-03 13:21:54,928 - INFO -   Batch 1900/2527: Loss = 0.0700
2026-01-03 13:22:24,043 - INFO -   Batch 1950/2527: Loss = 0.1606
2026-01-03 13:22:52,653 - INFO -   Batch 2000/2527: Loss = 0.0863
2026-01-03 13:23:21,300 - INFO -   Batch 2050/2527: Loss = 0.0887
2026-01-03 13:23:50,431 - INFO -   Batch 2100/2527: Loss = 0.0881
2026-01-03 13:24:19,113 - INFO -   Batch 2150/2527: Loss = 0.0819
2026-01-03 13:24:47,934 - INFO -   Batch 2200/2527: Loss = 0.1199
2026-01-03 13:25:16,784 - INFO -   Batch 2250/2527: Loss = 0.1022
2026-01-03 13:25:45,818 - INFO -   Batch 2300/2527: Loss = 0.0743
2026-01-03 13:26:15,154 - INFO -   Batch 2350/2527: Loss = 0.0784
2026-01-03 13:26:43,945 - INFO -   Batch 2400/2527: Loss = 0.0800
2026-01-03 13:27:13,150 - INFO -   Batch 2450/2527: Loss = 0.0971
2026-01-03 13:27:42,755 - INFO -   Batch 2500/2527: Loss = 0.1224
2026-01-03 13:32:23,024 - INFO - Epoch 83/160: Train Loss: 0.0865, Val Loss: 1.3241, Val mIoU: 39.22%, Val PixelAcc: 78.36%
2026-01-03 13:32:53,881 - INFO -   Batch 50/2527: Loss = 0.0900
2026-01-03 13:33:22,743 - INFO -   Batch 100/2527: Loss = 0.0832
2026-01-03 13:33:51,535 - INFO -   Batch 150/2527: Loss = 0.1388
2026-01-03 13:34:20,333 - INFO -   Batch 200/2527: Loss = 0.0576
2026-01-03 13:34:48,920 - INFO -   Batch 250/2527: Loss = 0.0768
2026-01-03 13:35:17,460 - INFO -   Batch 300/2527: Loss = 0.0936
2026-01-03 13:35:46,318 - INFO -   Batch 350/2527: Loss = 0.0640
2026-01-03 13:36:15,089 - INFO -   Batch 400/2527: Loss = 0.0687
2026-01-03 13:36:43,997 - INFO -   Batch 450/2527: Loss = 0.1182
2026-01-03 13:37:12,900 - INFO -   Batch 500/2527: Loss = 0.0558
2026-01-03 13:37:41,788 - INFO -   Batch 550/2527: Loss = 0.0764
2026-01-03 13:38:10,570 - INFO -   Batch 600/2527: Loss = 0.0681
2026-01-03 13:38:39,087 - INFO -   Batch 650/2527: Loss = 0.0672
2026-01-03 13:39:07,940 - INFO -   Batch 700/2527: Loss = 0.0763
2026-01-03 13:39:36,625 - INFO -   Batch 750/2527: Loss = 0.0878
2026-01-03 13:40:05,637 - INFO -   Batch 800/2527: Loss = 0.0677
2026-01-03 13:40:34,358 - INFO -   Batch 850/2527: Loss = 0.0551
2026-01-03 13:41:03,409 - INFO -   Batch 900/2527: Loss = 0.1094
2026-01-03 13:41:31,957 - INFO -   Batch 950/2527: Loss = 0.0995
2026-01-03 13:42:00,772 - INFO -   Batch 1000/2527: Loss = 0.0932
2026-01-03 13:42:29,790 - INFO -   Batch 1050/2527: Loss = 0.0614
2026-01-03 13:42:58,522 - INFO -   Batch 1100/2527: Loss = 0.0725
2026-01-03 13:43:27,484 - INFO -   Batch 1150/2527: Loss = 0.0531
2026-01-03 13:43:56,416 - INFO -   Batch 1200/2527: Loss = 0.1137
2026-01-03 13:44:25,145 - INFO -   Batch 1250/2527: Loss = 0.0814
2026-01-03 13:44:54,412 - INFO -   Batch 1300/2527: Loss = 0.0801
2026-01-03 13:45:23,346 - INFO -   Batch 1350/2527: Loss = 0.0721
2026-01-03 13:45:52,195 - INFO -   Batch 1400/2527: Loss = 0.0873
2026-01-03 13:46:20,985 - INFO -   Batch 1450/2527: Loss = 0.0647
2026-01-03 13:46:49,822 - INFO -   Batch 1500/2527: Loss = 0.0588
2026-01-03 13:47:18,643 - INFO -   Batch 1550/2527: Loss = 0.0927
2026-01-03 13:47:47,518 - INFO -   Batch 1600/2527: Loss = 0.1112
2026-01-03 13:48:16,537 - INFO -   Batch 1650/2527: Loss = 0.0924
2026-01-03 13:48:45,517 - INFO -   Batch 1700/2527: Loss = 0.0782
2026-01-03 13:49:14,477 - INFO -   Batch 1750/2527: Loss = 0.1224
2026-01-03 13:49:43,164 - INFO -   Batch 1800/2527: Loss = 0.0943
2026-01-03 13:50:12,032 - INFO -   Batch 1850/2527: Loss = 0.0570
2026-01-03 13:50:40,737 - INFO -   Batch 1900/2527: Loss = 0.0665
2026-01-03 13:51:09,524 - INFO -   Batch 1950/2527: Loss = 0.0872
2026-01-03 13:51:38,504 - INFO -   Batch 2000/2527: Loss = 0.0959
2026-01-03 13:52:07,281 - INFO -   Batch 2050/2527: Loss = 0.0888
2026-01-03 13:52:36,578 - INFO -   Batch 2100/2527: Loss = 0.0883
2026-01-03 13:53:05,259 - INFO -   Batch 2150/2527: Loss = 0.0601
2026-01-03 13:53:33,818 - INFO -   Batch 2200/2527: Loss = 0.0676
2026-01-03 13:54:02,810 - INFO -   Batch 2250/2527: Loss = 0.0429
2026-01-03 13:54:31,561 - INFO -   Batch 2300/2527: Loss = 0.1195
2026-01-03 13:55:00,505 - INFO -   Batch 2350/2527: Loss = 0.0879
2026-01-03 13:55:29,616 - INFO -   Batch 2400/2527: Loss = 0.0966
2026-01-03 13:55:58,589 - INFO -   Batch 2450/2527: Loss = 0.0718
2026-01-03 13:56:27,695 - INFO -   Batch 2500/2527: Loss = 0.0661
2026-01-03 14:01:22,041 - INFO - Epoch 84/160: Train Loss: 0.0852, Val Loss: 1.2987, Val mIoU: 39.45%, Val PixelAcc: 78.62%
2026-01-03 14:01:52,820 - INFO -   Batch 50/2527: Loss = 0.1325
2026-01-03 14:02:21,458 - INFO -   Batch 100/2527: Loss = 0.0979
2026-01-03 14:02:49,962 - INFO -   Batch 150/2527: Loss = 0.1008
2026-01-03 14:03:19,055 - INFO -   Batch 200/2527: Loss = 0.1384
2026-01-03 14:03:47,978 - INFO -   Batch 250/2527: Loss = 0.0696
2026-01-03 14:04:17,036 - INFO -   Batch 300/2527: Loss = 0.0640
2026-01-03 14:04:45,919 - INFO -   Batch 350/2527: Loss = 0.0724
2026-01-03 14:05:14,897 - INFO -   Batch 400/2527: Loss = 0.0698
2026-01-03 14:05:43,583 - INFO -   Batch 450/2527: Loss = 0.0715
2026-01-03 14:06:12,066 - INFO -   Batch 500/2527: Loss = 0.0719
2026-01-03 14:06:40,978 - INFO -   Batch 550/2527: Loss = 0.1020
2026-01-03 14:07:09,868 - INFO -   Batch 600/2527: Loss = 0.0806
2026-01-03 14:07:38,762 - INFO -   Batch 650/2527: Loss = 0.0943
2026-01-03 14:08:07,794 - INFO -   Batch 700/2527: Loss = 0.0821
2026-01-03 14:08:36,586 - INFO -   Batch 750/2527: Loss = 0.0473
2026-01-03 14:09:05,570 - INFO -   Batch 800/2527: Loss = 0.1075
2026-01-03 14:09:34,285 - INFO -   Batch 850/2527: Loss = 0.1065
2026-01-03 14:10:03,078 - INFO -   Batch 900/2527: Loss = 0.0920
2026-01-03 14:10:31,806 - INFO -   Batch 950/2527: Loss = 0.0999
2026-01-03 14:11:00,848 - INFO -   Batch 1000/2527: Loss = 0.0867
2026-01-03 14:11:29,518 - INFO -   Batch 1050/2527: Loss = 0.0892
2026-01-03 14:11:58,119 - INFO -   Batch 1100/2527: Loss = 0.0942
2026-01-03 14:12:27,274 - INFO -   Batch 1150/2527: Loss = 0.0911
2026-01-03 14:12:55,990 - INFO -   Batch 1200/2527: Loss = 0.1027
2026-01-03 14:13:24,823 - INFO -   Batch 1250/2527: Loss = 0.0975
2026-01-03 14:13:53,468 - INFO -   Batch 1300/2527: Loss = 0.1115
2026-01-03 14:14:22,373 - INFO -   Batch 1350/2527: Loss = 0.1241
2026-01-03 14:14:51,282 - INFO -   Batch 1400/2527: Loss = 0.0826
2026-01-03 14:15:19,794 - INFO -   Batch 1450/2527: Loss = 0.0982
2026-01-03 14:15:48,754 - INFO -   Batch 1500/2527: Loss = 0.0958
2026-01-03 14:16:17,698 - INFO -   Batch 1550/2527: Loss = 0.0612
2026-01-03 14:16:46,581 - INFO -   Batch 1600/2527: Loss = 0.1118
2026-01-03 14:17:15,162 - INFO -   Batch 1650/2527: Loss = 0.0719
2026-01-03 14:17:43,700 - INFO -   Batch 1700/2527: Loss = 0.0703
2026-01-03 14:18:12,560 - INFO -   Batch 1750/2527: Loss = 0.0719
2026-01-03 14:18:41,487 - INFO -   Batch 1800/2527: Loss = 0.1197
2026-01-03 14:19:10,243 - INFO -   Batch 1850/2527: Loss = 0.0684
2026-01-03 14:19:38,979 - INFO -   Batch 1900/2527: Loss = 0.1037
2026-01-03 14:20:08,188 - INFO -   Batch 1950/2527: Loss = 0.0804
2026-01-03 14:20:36,839 - INFO -   Batch 2000/2527: Loss = 0.0944
2026-01-03 14:21:05,333 - INFO -   Batch 2050/2527: Loss = 0.1031
2026-01-03 14:21:34,210 - INFO -   Batch 2100/2527: Loss = 0.0798
2026-01-03 14:22:02,837 - INFO -   Batch 2150/2527: Loss = 0.1203
2026-01-03 14:22:32,022 - INFO -   Batch 2200/2527: Loss = 0.0759
2026-01-03 14:23:00,641 - INFO -   Batch 2250/2527: Loss = 0.1012
2026-01-03 14:23:29,702 - INFO -   Batch 2300/2527: Loss = 0.0585
2026-01-03 14:23:58,751 - INFO -   Batch 2350/2527: Loss = 0.0698
2026-01-03 14:24:27,623 - INFO -   Batch 2400/2527: Loss = 0.0868
2026-01-03 14:24:56,673 - INFO -   Batch 2450/2527: Loss = 0.0697
2026-01-03 14:25:25,560 - INFO -   Batch 2500/2527: Loss = 0.0948
2026-01-03 14:30:10,232 - INFO - Epoch 85/160: Train Loss: 0.0834, Val Loss: 1.2673, Val mIoU: 40.25%, Val PixelAcc: 78.90%
2026-01-03 14:30:10,234 - INFO -   → New best mIoU: 40.25%
2026-01-03 14:30:14,084 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-03 14:30:14,087 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-03 14:30:44,531 - INFO -   Batch 50/2527: Loss = 0.0698
2026-01-03 14:31:13,633 - INFO -   Batch 100/2527: Loss = 0.0835
2026-01-03 14:31:42,311 - INFO -   Batch 150/2527: Loss = 0.0613
2026-01-03 14:32:11,122 - INFO -   Batch 200/2527: Loss = 0.0961
2026-01-03 14:32:39,764 - INFO -   Batch 250/2527: Loss = 0.0702
2026-01-03 14:33:08,305 - INFO -   Batch 300/2527: Loss = 0.0740
2026-01-03 14:33:37,365 - INFO -   Batch 350/2527: Loss = 0.1635
2026-01-03 14:34:05,970 - INFO -   Batch 400/2527: Loss = 0.1073
2026-01-03 14:34:34,980 - INFO -   Batch 450/2527: Loss = 0.1212
2026-01-03 14:35:03,912 - INFO -   Batch 500/2527: Loss = 0.0709
2026-01-03 14:35:32,828 - INFO -   Batch 550/2527: Loss = 0.1023
2026-01-03 14:36:01,400 - INFO -   Batch 600/2527: Loss = 0.0970
2026-01-03 14:36:29,934 - INFO -   Batch 650/2527: Loss = 0.1586
2026-01-03 14:36:58,823 - INFO -   Batch 700/2527: Loss = 0.0629
2026-01-03 14:37:27,713 - INFO -   Batch 750/2527: Loss = 0.0846
2026-01-03 14:37:56,564 - INFO -   Batch 800/2527: Loss = 0.1139
2026-01-03 14:38:25,302 - INFO -   Batch 850/2527: Loss = 0.0811
2026-01-03 14:38:54,115 - INFO -   Batch 900/2527: Loss = 0.1009
2026-01-03 14:39:23,282 - INFO -   Batch 950/2527: Loss = 0.0770
2026-01-03 14:39:51,917 - INFO -   Batch 1000/2527: Loss = 0.0702
2026-01-03 14:40:20,719 - INFO -   Batch 1050/2527: Loss = 0.1239
2026-01-03 14:40:49,524 - INFO -   Batch 1100/2527: Loss = 0.0638
2026-01-03 14:41:18,588 - INFO -   Batch 1150/2527: Loss = 0.0748
2026-01-03 14:41:47,303 - INFO -   Batch 1200/2527: Loss = 0.1152
2026-01-03 14:42:15,891 - INFO -   Batch 1250/2527: Loss = 0.1004
2026-01-03 14:42:44,988 - INFO -   Batch 1300/2527: Loss = 0.0824
2026-01-03 14:43:13,812 - INFO -   Batch 1350/2527: Loss = 0.0559
2026-01-03 14:43:42,818 - INFO -   Batch 1400/2527: Loss = 0.0722
2026-01-03 14:44:11,459 - INFO -   Batch 1450/2527: Loss = 0.0852
2026-01-03 14:44:40,347 - INFO -   Batch 1500/2527: Loss = 0.0919
2026-01-03 14:45:09,050 - INFO -   Batch 1550/2527: Loss = 0.1157
2026-01-03 14:45:37,776 - INFO -   Batch 1600/2527: Loss = 0.0753
2026-01-03 14:46:06,686 - INFO -   Batch 1650/2527: Loss = 0.0827
2026-01-03 14:46:35,609 - INFO -   Batch 1700/2527: Loss = 0.0994
2026-01-03 14:47:04,556 - INFO -   Batch 1750/2527: Loss = 0.0632
2026-01-03 14:47:33,263 - INFO -   Batch 1800/2527: Loss = 0.1473
2026-01-03 14:48:01,924 - INFO -   Batch 1850/2527: Loss = 0.0570
2026-01-03 14:48:31,002 - INFO -   Batch 1900/2527: Loss = 0.0933
2026-01-03 14:49:00,186 - INFO -   Batch 1950/2527: Loss = 0.0791
2026-01-03 14:49:29,081 - INFO -   Batch 2000/2527: Loss = 0.0844
2026-01-03 14:49:57,801 - INFO -   Batch 2050/2527: Loss = 0.0886
2026-01-03 14:50:26,898 - INFO -   Batch 2100/2527: Loss = 0.0501
2026-01-03 14:50:55,760 - INFO -   Batch 2150/2527: Loss = 0.0722
2026-01-03 14:51:24,338 - INFO -   Batch 2200/2527: Loss = 0.0819
2026-01-03 14:51:53,243 - INFO -   Batch 2250/2527: Loss = 0.0977
2026-01-03 14:52:21,913 - INFO -   Batch 2300/2527: Loss = 0.1022
2026-01-03 14:52:50,990 - INFO -   Batch 2350/2527: Loss = 0.0698
2026-01-03 14:53:19,759 - INFO -   Batch 2400/2527: Loss = 0.0897
2026-01-03 14:53:48,799 - INFO -   Batch 2450/2527: Loss = 0.1014
2026-01-03 14:54:17,618 - INFO -   Batch 2500/2527: Loss = 0.0860
2026-01-03 14:59:01,445 - INFO - Epoch 86/160: Train Loss: 0.0842, Val Loss: 1.3256, Val mIoU: 39.85%, Val PixelAcc: 78.55%
2026-01-03 14:59:32,219 - INFO -   Batch 50/2527: Loss = 0.0989
2026-01-03 15:00:01,010 - INFO -   Batch 100/2527: Loss = 0.0777
2026-01-03 15:00:29,589 - INFO -   Batch 150/2527: Loss = 0.0853
2026-01-03 15:00:58,421 - INFO -   Batch 200/2527: Loss = 0.0829
2026-01-03 15:01:27,034 - INFO -   Batch 250/2527: Loss = 0.0874
2026-01-03 15:01:55,903 - INFO -   Batch 300/2527: Loss = 0.0865
2026-01-03 15:02:24,555 - INFO -   Batch 350/2527: Loss = 0.0624
2026-01-03 15:02:53,553 - INFO -   Batch 400/2527: Loss = 0.0863
2026-01-03 15:03:22,184 - INFO -   Batch 450/2527: Loss = 0.0611
2026-01-03 15:03:51,032 - INFO -   Batch 500/2527: Loss = 0.0902
2026-01-03 15:04:19,930 - INFO -   Batch 550/2527: Loss = 0.0882
2026-01-03 15:04:48,588 - INFO -   Batch 600/2527: Loss = 0.1268
2026-01-03 15:05:17,552 - INFO -   Batch 650/2527: Loss = 0.1015
2026-01-03 15:05:46,428 - INFO -   Batch 700/2527: Loss = 0.0846
2026-01-03 15:06:14,998 - INFO -   Batch 750/2527: Loss = 0.0787
2026-01-03 15:06:44,036 - INFO -   Batch 800/2527: Loss = 0.0814
2026-01-03 15:07:12,788 - INFO -   Batch 850/2527: Loss = 0.0674
2026-01-03 15:07:41,852 - INFO -   Batch 900/2527: Loss = 0.0947
2026-01-03 15:08:10,648 - INFO -   Batch 950/2527: Loss = 0.0867
2026-01-03 15:08:39,494 - INFO -   Batch 1000/2527: Loss = 0.0828
2026-01-03 15:09:08,197 - INFO -   Batch 1050/2527: Loss = 0.0613
2026-01-03 15:09:36,765 - INFO -   Batch 1100/2527: Loss = 0.0644
2026-01-03 15:10:05,722 - INFO -   Batch 1150/2527: Loss = 0.0959
2026-01-03 15:10:34,532 - INFO -   Batch 1200/2527: Loss = 0.0645
2026-01-03 15:11:03,487 - INFO -   Batch 1250/2527: Loss = 0.0820
2026-01-03 15:11:32,395 - INFO -   Batch 1300/2527: Loss = 0.0888
2026-01-03 15:12:01,001 - INFO -   Batch 1350/2527: Loss = 0.0549
2026-01-03 15:12:29,824 - INFO -   Batch 1400/2527: Loss = 0.0496
2026-01-03 15:12:58,555 - INFO -   Batch 1450/2527: Loss = 0.0826
2026-01-03 15:13:27,465 - INFO -   Batch 1500/2527: Loss = 0.0862
2026-01-03 15:13:56,199 - INFO -   Batch 1550/2527: Loss = 0.0602
2026-01-03 15:14:25,201 - INFO -   Batch 1600/2527: Loss = 0.0671
2026-01-03 15:14:53,889 - INFO -   Batch 1650/2527: Loss = 0.0792
2026-01-03 15:15:22,653 - INFO -   Batch 1700/2527: Loss = 0.1124
2026-01-03 15:15:51,621 - INFO -   Batch 1750/2527: Loss = 0.0756
2026-01-03 15:16:20,208 - INFO -   Batch 1800/2527: Loss = 0.0755
2026-01-03 15:16:49,068 - INFO -   Batch 1850/2527: Loss = 0.1126
2026-01-03 15:17:17,701 - INFO -   Batch 1900/2527: Loss = 0.0773
2026-01-03 15:17:46,615 - INFO -   Batch 1950/2527: Loss = 0.1193
2026-01-03 15:18:15,457 - INFO -   Batch 2000/2527: Loss = 0.0868
2026-01-03 15:18:44,059 - INFO -   Batch 2050/2527: Loss = 0.0743
2026-01-03 15:19:13,124 - INFO -   Batch 2100/2527: Loss = 0.0708
2026-01-03 15:19:41,865 - INFO -   Batch 2150/2527: Loss = 0.1076
2026-01-03 15:20:10,676 - INFO -   Batch 2200/2527: Loss = 0.0843
2026-01-03 15:20:39,333 - INFO -   Batch 2250/2527: Loss = 0.0962
2026-01-03 15:21:07,953 - INFO -   Batch 2300/2527: Loss = 0.0646
2026-01-03 15:21:36,807 - INFO -   Batch 2350/2527: Loss = 0.0876
2026-01-03 15:22:05,610 - INFO -   Batch 2400/2527: Loss = 0.0885
2026-01-03 15:22:34,555 - INFO -   Batch 2450/2527: Loss = 0.0605
2026-01-03 15:23:03,411 - INFO -   Batch 2500/2527: Loss = 0.0649
2026-01-03 15:27:48,898 - INFO - Epoch 87/160: Train Loss: 0.0808, Val Loss: 1.2815, Val mIoU: 39.99%, Val PixelAcc: 78.87%
2026-01-03 15:28:19,554 - INFO -   Batch 50/2527: Loss = 0.0769
2026-01-03 15:28:48,035 - INFO -   Batch 100/2527: Loss = 0.0644
2026-01-03 15:29:16,881 - INFO -   Batch 150/2527: Loss = 0.0669
2026-01-03 15:29:45,733 - INFO -   Batch 200/2527: Loss = 0.0578
2026-01-03 15:30:14,205 - INFO -   Batch 250/2527: Loss = 0.0619
2026-01-03 15:30:43,149 - INFO -   Batch 300/2527: Loss = 0.0810
2026-01-03 15:31:11,957 - INFO -   Batch 350/2527: Loss = 0.0624
2026-01-03 15:31:40,890 - INFO -   Batch 400/2527: Loss = 0.0722
2026-01-03 15:32:09,485 - INFO -   Batch 450/2527: Loss = 0.0555
2026-01-03 15:32:38,253 - INFO -   Batch 500/2527: Loss = 0.0851
2026-01-03 15:33:06,872 - INFO -   Batch 550/2527: Loss = 0.0885
2026-01-03 15:33:35,623 - INFO -   Batch 600/2527: Loss = 0.0625
2026-01-03 15:34:04,457 - INFO -   Batch 650/2527: Loss = 0.0738
2026-01-03 15:34:33,256 - INFO -   Batch 700/2527: Loss = 0.0876
2026-01-03 15:35:02,344 - INFO -   Batch 750/2527: Loss = 0.0695
2026-01-03 15:35:31,247 - INFO -   Batch 800/2527: Loss = 0.0677
2026-01-03 15:36:00,088 - INFO -   Batch 850/2527: Loss = 0.0502
2026-01-03 15:36:28,598 - INFO -   Batch 900/2527: Loss = 0.0687
2026-01-03 15:36:57,253 - INFO -   Batch 950/2527: Loss = 0.0736
2026-01-03 15:37:26,357 - INFO -   Batch 1000/2527: Loss = 0.0882
2026-01-03 15:37:54,962 - INFO -   Batch 1050/2527: Loss = 0.0667
2026-01-03 15:38:23,947 - INFO -   Batch 1100/2527: Loss = 0.0802
2026-01-03 15:38:52,824 - INFO -   Batch 1150/2527: Loss = 0.0630
2026-01-03 15:39:21,373 - INFO -   Batch 1200/2527: Loss = 0.0627
2026-01-03 15:39:50,195 - INFO -   Batch 1250/2527: Loss = 0.0571
2026-01-03 15:40:18,753 - INFO -   Batch 1300/2527: Loss = 0.0638
2026-01-03 15:40:47,641 - INFO -   Batch 1350/2527: Loss = 0.0771
2026-01-03 15:41:16,531 - INFO -   Batch 1400/2527: Loss = 0.0909
2026-01-03 15:41:45,358 - INFO -   Batch 1450/2527: Loss = 0.0827
2026-01-03 15:42:14,158 - INFO -   Batch 1500/2527: Loss = 0.0700
2026-01-03 15:42:42,932 - INFO -   Batch 1550/2527: Loss = 0.0869
2026-01-03 15:43:11,804 - INFO -   Batch 1600/2527: Loss = 0.0623
2026-01-03 15:43:40,463 - INFO -   Batch 1650/2527: Loss = 0.0887
2026-01-03 15:44:09,241 - INFO -   Batch 1700/2527: Loss = 0.0838
2026-01-03 15:44:37,861 - INFO -   Batch 1750/2527: Loss = 0.0849
2026-01-03 15:45:06,946 - INFO -   Batch 1800/2527: Loss = 0.0539
2026-01-03 15:45:35,439 - INFO -   Batch 1850/2527: Loss = 0.1071
2026-01-03 15:46:04,193 - INFO -   Batch 1900/2527: Loss = 0.0714
2026-01-03 15:46:33,295 - INFO -   Batch 1950/2527: Loss = 0.0835
2026-01-03 15:47:01,924 - INFO -   Batch 2000/2527: Loss = 0.0671
2026-01-03 15:47:30,755 - INFO -   Batch 2050/2527: Loss = 0.0616
2026-01-03 15:47:59,316 - INFO -   Batch 2100/2527: Loss = 0.0869
2026-01-03 15:48:27,844 - INFO -   Batch 2150/2527: Loss = 0.0964
2026-01-03 15:48:56,924 - INFO -   Batch 2200/2527: Loss = 0.0886
2026-01-03 15:49:25,463 - INFO -   Batch 2250/2527: Loss = 0.0816
2026-01-03 15:49:54,421 - INFO -   Batch 2300/2527: Loss = 0.0746
2026-01-03 15:50:23,304 - INFO -   Batch 2350/2527: Loss = 0.0784
2026-01-03 15:50:52,167 - INFO -   Batch 2400/2527: Loss = 0.0954
2026-01-03 15:51:20,750 - INFO -   Batch 2450/2527: Loss = 0.0655
2026-01-03 15:51:49,245 - INFO -   Batch 2500/2527: Loss = 0.0690
2026-01-03 15:56:29,435 - INFO - Epoch 88/160: Train Loss: 0.0780, Val Loss: 1.3311, Val mIoU: 40.09%, Val PixelAcc: 78.68%
2026-01-03 15:56:59,599 - INFO -   Batch 50/2527: Loss = 0.0598
2026-01-03 15:57:28,316 - INFO -   Batch 100/2527: Loss = 0.0684
2026-01-03 15:57:57,179 - INFO -   Batch 150/2527: Loss = 0.0730
2026-01-03 15:58:25,713 - INFO -   Batch 200/2527: Loss = 0.0645
2026-01-03 15:58:54,501 - INFO -   Batch 250/2527: Loss = 0.0589
2026-01-03 15:59:23,098 - INFO -   Batch 300/2527: Loss = 0.1080
2026-01-03 15:59:52,188 - INFO -   Batch 350/2527: Loss = 0.0729
2026-01-03 16:00:20,802 - INFO -   Batch 400/2527: Loss = 0.0915
2026-01-03 16:00:49,434 - INFO -   Batch 450/2527: Loss = 0.0910
2026-01-03 16:01:18,585 - INFO -   Batch 500/2527: Loss = 0.0684
2026-01-03 16:01:47,417 - INFO -   Batch 550/2527: Loss = 0.0626
2026-01-03 16:02:16,245 - INFO -   Batch 600/2527: Loss = 0.1006
2026-01-03 16:02:44,858 - INFO -   Batch 650/2527: Loss = 0.0795
2026-01-03 16:03:13,733 - INFO -   Batch 700/2527: Loss = 0.1041
2026-01-03 16:03:42,460 - INFO -   Batch 750/2527: Loss = 0.0966
2026-01-03 16:04:11,104 - INFO -   Batch 800/2527: Loss = 0.0930
2026-01-03 16:04:40,026 - INFO -   Batch 850/2527: Loss = 0.0935
2026-01-03 16:05:08,948 - INFO -   Batch 900/2527: Loss = 0.0790
2026-01-03 16:05:37,849 - INFO -   Batch 950/2527: Loss = 0.0612
2026-01-03 16:06:06,401 - INFO -   Batch 1000/2527: Loss = 0.0955
2026-01-03 16:06:34,891 - INFO -   Batch 1050/2527: Loss = 0.0446
2026-01-03 16:07:03,776 - INFO -   Batch 1100/2527: Loss = 0.1018
2026-01-03 16:07:32,632 - INFO -   Batch 1150/2527: Loss = 0.1353
2026-01-03 16:08:01,469 - INFO -   Batch 1200/2527: Loss = 0.0825
2026-01-03 16:08:30,216 - INFO -   Batch 1250/2527: Loss = 0.0609
2026-01-03 16:08:59,342 - INFO -   Batch 1300/2527: Loss = 0.0587
2026-01-03 16:09:28,056 - INFO -   Batch 1350/2527: Loss = 0.0727
2026-01-03 16:09:56,526 - INFO -   Batch 1400/2527: Loss = 0.0778
2026-01-03 16:10:25,306 - INFO -   Batch 1450/2527: Loss = 0.0835
2026-01-03 16:10:53,908 - INFO -   Batch 1500/2527: Loss = 0.1291
2026-01-03 16:11:22,950 - INFO -   Batch 1550/2527: Loss = 0.0469
2026-01-03 16:11:51,592 - INFO -   Batch 1600/2527: Loss = 0.0967
2026-01-03 16:12:20,220 - INFO -   Batch 1650/2527: Loss = 0.0683
2026-01-03 16:12:49,327 - INFO -   Batch 1700/2527: Loss = 0.0982
2026-01-03 16:13:17,988 - INFO -   Batch 1750/2527: Loss = 0.0747
2026-01-03 16:13:46,772 - INFO -   Batch 1800/2527: Loss = 0.0871
2026-01-03 16:14:15,397 - INFO -   Batch 1850/2527: Loss = 0.0641
2026-01-03 16:14:44,260 - INFO -   Batch 1900/2527: Loss = 0.0640
2026-01-03 16:15:13,089 - INFO -   Batch 1950/2527: Loss = 0.0832
2026-01-03 16:15:41,625 - INFO -   Batch 2000/2527: Loss = 0.0997
2026-01-03 16:16:10,576 - INFO -   Batch 2050/2527: Loss = 0.1080
2026-01-03 16:16:39,397 - INFO -   Batch 2100/2527: Loss = 0.0757
2026-01-03 16:17:08,311 - INFO -   Batch 2150/2527: Loss = 0.0762
2026-01-03 16:17:36,887 - INFO -   Batch 2200/2527: Loss = 0.1038
2026-01-03 16:18:05,695 - INFO -   Batch 2250/2527: Loss = 0.0922
2026-01-03 16:18:34,249 - INFO -   Batch 2300/2527: Loss = 0.0563
2026-01-03 16:19:03,149 - INFO -   Batch 2350/2527: Loss = 0.0634
2026-01-03 16:19:31,952 - INFO -   Batch 2400/2527: Loss = 0.0523
2026-01-03 16:20:00,866 - INFO -   Batch 2450/2527: Loss = 0.0662
2026-01-03 16:20:29,972 - INFO -   Batch 2500/2527: Loss = 0.0679
2026-01-03 16:25:09,431 - INFO - Epoch 89/160: Train Loss: 0.0781, Val Loss: 1.3247, Val mIoU: 39.96%, Val PixelAcc: 78.76%
2026-01-03 16:25:40,111 - INFO -   Batch 50/2527: Loss = 0.0798
2026-01-03 16:26:09,095 - INFO -   Batch 100/2527: Loss = 0.0543
2026-01-03 16:26:37,951 - INFO -   Batch 150/2527: Loss = 0.1011
2026-01-03 16:27:06,749 - INFO -   Batch 200/2527: Loss = 0.0970
2026-01-03 16:27:35,374 - INFO -   Batch 250/2527: Loss = 0.0781
2026-01-03 16:28:03,934 - INFO -   Batch 300/2527: Loss = 0.0868
2026-01-03 16:28:32,796 - INFO -   Batch 350/2527: Loss = 0.0765
2026-01-03 16:29:01,552 - INFO -   Batch 400/2527: Loss = 0.0725
2026-01-03 16:29:30,503 - INFO -   Batch 450/2527: Loss = 0.0788
2026-01-03 16:29:59,361 - INFO -   Batch 500/2527: Loss = 0.0665
2026-01-03 16:30:27,941 - INFO -   Batch 550/2527: Loss = 0.0841
2026-01-03 16:30:56,759 - INFO -   Batch 600/2527: Loss = 0.0643
2026-01-03 16:31:25,336 - INFO -   Batch 650/2527: Loss = 0.1201
2026-01-03 16:31:54,233 - INFO -   Batch 700/2527: Loss = 0.1007
2026-01-03 16:32:22,899 - INFO -   Batch 750/2527: Loss = 0.0692
2026-01-03 16:32:51,899 - INFO -   Batch 800/2527: Loss = 0.0862
2026-01-03 16:33:20,659 - INFO -   Batch 850/2527: Loss = 0.0686
2026-01-03 16:33:49,403 - INFO -   Batch 900/2527: Loss = 0.0893
2026-01-03 16:34:18,293 - INFO -   Batch 950/2527: Loss = 0.0848
2026-01-03 16:34:46,946 - INFO -   Batch 1000/2527: Loss = 0.0611
2026-01-03 16:35:15,751 - INFO -   Batch 1050/2527: Loss = 0.0877
2026-01-03 16:35:44,410 - INFO -   Batch 1100/2527: Loss = 0.0871
2026-01-03 16:36:13,294 - INFO -   Batch 1150/2527: Loss = 0.0696
2026-01-03 16:36:41,970 - INFO -   Batch 1200/2527: Loss = 0.0513
2026-01-03 16:37:10,698 - INFO -   Batch 1250/2527: Loss = 0.0648
2026-01-03 16:37:39,771 - INFO -   Batch 1300/2527: Loss = 0.0763
2026-01-03 16:38:08,451 - INFO -   Batch 1350/2527: Loss = 0.0590
2026-01-03 16:38:37,313 - INFO -   Batch 1400/2527: Loss = 0.0626
2026-01-03 16:39:05,963 - INFO -   Batch 1450/2527: Loss = 0.0578
2026-01-03 16:39:34,537 - INFO -   Batch 1500/2527: Loss = 0.0675
2026-01-03 16:40:03,402 - INFO -   Batch 1550/2527: Loss = 0.1248
2026-01-03 16:40:32,152 - INFO -   Batch 1600/2527: Loss = 0.0652
2026-01-03 16:41:01,054 - INFO -   Batch 1650/2527: Loss = 0.0607
2026-01-03 16:41:29,881 - INFO -   Batch 1700/2527: Loss = 0.0612
2026-01-03 16:41:58,766 - INFO -   Batch 1750/2527: Loss = 0.0654
2026-01-03 16:42:27,377 - INFO -   Batch 1800/2527: Loss = 0.0788
2026-01-03 16:42:55,919 - INFO -   Batch 1850/2527: Loss = 0.0737
2026-01-03 16:43:24,786 - INFO -   Batch 1900/2527: Loss = 0.1061
2026-01-03 16:43:53,482 - INFO -   Batch 1950/2527: Loss = 0.0956
2026-01-03 16:44:22,447 - INFO -   Batch 2000/2527: Loss = 0.0568
2026-01-03 16:44:51,335 - INFO -   Batch 2050/2527: Loss = 0.0980
2026-01-03 16:45:20,411 - INFO -   Batch 2100/2527: Loss = 0.0761
2026-01-03 16:45:48,978 - INFO -   Batch 2150/2527: Loss = 0.0780
2026-01-03 16:46:17,590 - INFO -   Batch 2200/2527: Loss = 0.0678
2026-01-03 16:46:46,486 - INFO -   Batch 2250/2527: Loss = 0.0651
2026-01-03 16:47:15,213 - INFO -   Batch 2300/2527: Loss = 0.0644
2026-01-03 16:47:44,079 - INFO -   Batch 2350/2527: Loss = 0.0566
2026-01-03 16:48:13,005 - INFO -   Batch 2400/2527: Loss = 0.0743
2026-01-03 16:48:41,646 - INFO -   Batch 2450/2527: Loss = 0.0453
2026-01-03 16:49:10,834 - INFO -   Batch 2500/2527: Loss = 0.0941
2026-01-03 16:54:04,367 - INFO - Epoch 90/160: Train Loss: 0.0770, Val Loss: 1.2980, Val mIoU: 40.21%, Val PixelAcc: 78.82%
2026-01-03 16:54:07,765 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_90.pth
2026-01-03 16:54:07,768 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_90.pth
2026-01-03 16:54:44,647 - INFO -   Batch 50/2527: Loss = 0.0524
2026-01-03 16:55:13,486 - INFO -   Batch 100/2527: Loss = 0.0729
2026-01-03 16:55:42,541 - INFO -   Batch 150/2527: Loss = 0.1572
2026-01-03 16:56:11,827 - INFO -   Batch 200/2527: Loss = 0.0539
2026-01-03 16:56:40,842 - INFO -   Batch 250/2527: Loss = 0.0752
2026-01-03 16:57:09,815 - INFO -   Batch 300/2527: Loss = 0.0601
2026-01-03 16:57:39,222 - INFO -   Batch 350/2527: Loss = 0.0801
2026-01-03 16:58:08,304 - INFO -   Batch 400/2527: Loss = 0.0747
2026-01-03 16:58:37,158 - INFO -   Batch 450/2527: Loss = 0.0735
2026-01-03 16:59:06,313 - INFO -   Batch 500/2527: Loss = 0.0507
2026-01-03 16:59:35,372 - INFO -   Batch 550/2527: Loss = 0.0828
2026-01-03 17:00:04,977 - INFO -   Batch 600/2527: Loss = 0.0685
2026-01-03 17:00:33,926 - INFO -   Batch 650/2527: Loss = 0.0674
2026-01-03 17:01:03,250 - INFO -   Batch 700/2527: Loss = 0.0922
2026-01-03 17:01:32,308 - INFO -   Batch 750/2527: Loss = 0.0577
2026-01-03 17:02:01,234 - INFO -   Batch 800/2527: Loss = 0.0711
2026-01-03 17:02:30,438 - INFO -   Batch 850/2527: Loss = 0.0570
2026-01-03 17:02:59,459 - INFO -   Batch 900/2527: Loss = 0.0969
2026-01-03 17:03:28,560 - INFO -   Batch 950/2527: Loss = 0.0808
2026-01-03 17:03:57,516 - INFO -   Batch 1000/2527: Loss = 0.0728
2026-01-03 17:04:26,149 - INFO -   Batch 1050/2527: Loss = 0.0810
2026-01-03 17:04:55,387 - INFO -   Batch 1100/2527: Loss = 0.0715
2026-01-03 17:05:24,431 - INFO -   Batch 1150/2527: Loss = 0.0589
2026-01-03 17:05:53,472 - INFO -   Batch 1200/2527: Loss = 0.0550
2026-01-03 17:06:22,296 - INFO -   Batch 1250/2527: Loss = 0.0535
2026-01-03 17:06:51,311 - INFO -   Batch 1300/2527: Loss = 0.0860
2026-01-03 17:07:20,151 - INFO -   Batch 1350/2527: Loss = 0.0691
2026-01-03 17:07:49,254 - INFO -   Batch 1400/2527: Loss = 0.0652
2026-01-03 17:08:18,218 - INFO -   Batch 1450/2527: Loss = 0.1028
2026-01-03 17:08:47,340 - INFO -   Batch 1500/2527: Loss = 0.0836
2026-01-03 17:09:16,571 - INFO -   Batch 1550/2527: Loss = 0.1041
2026-01-03 17:09:45,516 - INFO -   Batch 1600/2527: Loss = 0.0698
2026-01-03 17:10:14,343 - INFO -   Batch 1650/2527: Loss = 0.0774
2026-01-03 17:10:43,389 - INFO -   Batch 1700/2527: Loss = 0.0997
2026-01-03 17:11:12,301 - INFO -   Batch 1750/2527: Loss = 0.1007
2026-01-03 17:11:41,559 - INFO -   Batch 1800/2527: Loss = 0.0621
2026-01-03 17:12:10,461 - INFO -   Batch 1850/2527: Loss = 0.0923
2026-01-03 17:12:39,721 - INFO -   Batch 1900/2527: Loss = 0.0846
2026-01-03 17:13:08,855 - INFO -   Batch 1950/2527: Loss = 0.0607
2026-01-03 17:13:37,642 - INFO -   Batch 2000/2527: Loss = 0.0894
2026-01-03 17:14:06,666 - INFO -   Batch 2050/2527: Loss = 0.1043
2026-01-03 17:14:35,673 - INFO -   Batch 2100/2527: Loss = 0.0713
2026-01-03 17:15:04,726 - INFO -   Batch 2150/2527: Loss = 0.0712
2026-01-03 17:15:34,034 - INFO -   Batch 2200/2527: Loss = 0.0594
2026-01-03 17:16:03,124 - INFO -   Batch 2250/2527: Loss = 0.0629
2026-01-03 17:16:32,140 - INFO -   Batch 2300/2527: Loss = 0.0695
2026-01-03 17:17:01,137 - INFO -   Batch 2350/2527: Loss = 0.0487
2026-01-03 17:17:30,371 - INFO -   Batch 2400/2527: Loss = 0.0726
2026-01-03 17:17:59,248 - INFO -   Batch 2450/2527: Loss = 0.0795
2026-01-03 17:18:28,233 - INFO -   Batch 2500/2527: Loss = 0.1057
2026-01-03 17:23:18,313 - INFO - Epoch 91/160: Train Loss: 0.0768, Val Loss: 1.2911, Val mIoU: 40.39%, Val PixelAcc: 78.95%
2026-01-03 17:23:18,329 - INFO -   → New best mIoU: 40.39%
2026-01-03 17:23:22,057 - INFO - ✅ Checkpoint saved: runs/run_259/best_model.pth
2026-01-03 17:23:22,059 - INFO -   → Best model saved to runs/run_259/best_model.pth
2026-01-03 17:23:52,649 - INFO -   Batch 50/2527: Loss = 0.0548
2026-01-03 17:24:21,860 - INFO -   Batch 100/2527: Loss = 0.0757
2026-01-03 17:24:50,626 - INFO -   Batch 150/2527: Loss = 0.0694
2026-01-03 17:25:19,399 - INFO -   Batch 200/2527: Loss = 0.0801
2026-01-03 17:25:47,887 - INFO -   Batch 250/2527: Loss = 0.0587
2026-01-03 17:26:16,529 - INFO -   Batch 300/2527: Loss = 0.0871
2026-01-03 17:26:45,799 - INFO -   Batch 350/2527: Loss = 0.1277
2026-01-03 17:27:14,583 - INFO -   Batch 400/2527: Loss = 0.0760
2026-01-03 17:27:43,763 - INFO -   Batch 450/2527: Loss = 0.0974
2026-01-03 17:28:12,890 - INFO -   Batch 500/2527: Loss = 0.0700
2026-01-03 17:28:41,701 - INFO -   Batch 550/2527: Loss = 0.1020
2026-01-03 17:29:10,746 - INFO -   Batch 600/2527: Loss = 0.0831
2026-01-03 17:29:39,655 - INFO -   Batch 650/2527: Loss = 0.0563
2026-01-03 17:30:08,627 - INFO -   Batch 700/2527: Loss = 0.0797
2026-01-03 17:30:37,521 - INFO -   Batch 750/2527: Loss = 0.0720
2026-01-03 17:31:06,456 - INFO -   Batch 800/2527: Loss = 0.0723
2026-01-03 17:31:35,208 - INFO -   Batch 850/2527: Loss = 0.0845
2026-01-03 17:32:03,952 - INFO -   Batch 900/2527: Loss = 0.0746
2026-01-03 17:32:32,948 - INFO -   Batch 950/2527: Loss = 0.0977
2026-01-03 17:33:01,962 - INFO -   Batch 1000/2527: Loss = 0.0727
2026-01-03 17:33:30,966 - INFO -   Batch 1050/2527: Loss = 0.1139
2026-01-03 17:33:59,857 - INFO -   Batch 1100/2527: Loss = 0.0845
2026-01-03 17:34:28,845 - INFO -   Batch 1150/2527: Loss = 0.0634
2026-01-03 17:34:57,408 - INFO -   Batch 1200/2527: Loss = 0.0526
2026-01-03 17:35:26,365 - INFO -   Batch 1250/2527: Loss = 0.0683
2026-01-03 17:35:55,248 - INFO -   Batch 1300/2527: Loss = 0.0698
2026-01-03 17:36:24,127 - INFO -   Batch 1350/2527: Loss = 0.0660
2026-01-03 17:36:52,851 - INFO -   Batch 1400/2527: Loss = 0.0826
2026-01-03 17:37:21,587 - INFO -   Batch 1450/2527: Loss = 0.0776
2026-01-03 17:37:50,689 - INFO -   Batch 1500/2527: Loss = 0.0549
2026-01-03 17:38:19,535 - INFO -   Batch 1550/2527: Loss = 0.0657
2026-01-03 17:38:48,343 - INFO -   Batch 1600/2527: Loss = 0.0748
2026-01-03 17:39:17,245 - INFO -   Batch 1650/2527: Loss = 0.0659
2026-01-03 17:39:46,497 - INFO -   Batch 1700/2527: Loss = 0.0795
2026-01-03 17:40:15,417 - INFO -   Batch 1750/2527: Loss = 0.1117
2026-01-03 17:40:44,237 - INFO -   Batch 1800/2527: Loss = 0.0975
2026-01-03 17:41:12,715 - INFO -   Batch 1850/2527: Loss = 0.0764
2026-01-03 17:41:41,329 - INFO -   Batch 1900/2527: Loss = 0.0820
2026-01-03 17:42:10,327 - INFO -   Batch 1950/2527: Loss = 0.0839
2026-01-03 17:42:39,079 - INFO -   Batch 2000/2527: Loss = 0.0819
2026-01-03 17:43:07,853 - INFO -   Batch 2050/2527: Loss = 0.0538
2026-01-03 17:43:37,151 - INFO -   Batch 2100/2527: Loss = 0.0643
2026-01-03 17:44:05,910 - INFO -   Batch 2150/2527: Loss = 0.0998
2026-01-03 17:44:34,491 - INFO -   Batch 2200/2527: Loss = 0.0555
2026-01-03 17:45:03,302 - INFO -   Batch 2250/2527: Loss = 0.0572
2026-01-03 17:45:31,892 - INFO -   Batch 2300/2527: Loss = 0.0748
2026-01-03 17:46:00,902 - INFO -   Batch 2350/2527: Loss = 0.0876
2026-01-03 17:46:29,611 - INFO -   Batch 2400/2527: Loss = 0.0634
2026-01-03 17:46:58,513 - INFO -   Batch 2450/2527: Loss = 0.0624
2026-01-03 17:47:27,453 - INFO -   Batch 2500/2527: Loss = 0.0925
2026-01-03 17:52:10,946 - INFO - Epoch 92/160: Train Loss: 0.0766, Val Loss: 1.3247, Val mIoU: 40.27%, Val PixelAcc: 78.92%
2026-01-03 17:52:41,858 - INFO -   Batch 50/2527: Loss = 0.0814
2026-01-03 17:53:10,648 - INFO -   Batch 100/2527: Loss = 0.0696
2026-01-03 17:53:39,231 - INFO -   Batch 150/2527: Loss = 0.0755
2026-01-03 17:54:08,028 - INFO -   Batch 200/2527: Loss = 0.0817
2026-01-03 17:54:36,599 - INFO -   Batch 250/2527: Loss = 0.0653
2026-01-03 17:55:05,520 - INFO -   Batch 300/2527: Loss = 0.0619
2026-01-03 17:55:34,180 - INFO -   Batch 350/2527: Loss = 0.1024
2026-01-03 17:56:03,209 - INFO -   Batch 400/2527: Loss = 0.0683
2026-01-03 17:56:31,833 - INFO -   Batch 450/2527: Loss = 0.0894
2026-01-03 17:57:00,655 - INFO -   Batch 500/2527: Loss = 0.0992
2026-01-03 17:57:29,608 - INFO -   Batch 550/2527: Loss = 0.0962
2026-01-03 17:57:58,256 - INFO -   Batch 600/2527: Loss = 0.0539
2026-01-03 17:58:27,083 - INFO -   Batch 650/2527: Loss = 0.0782
2026-01-03 17:58:55,976 - INFO -   Batch 700/2527: Loss = 0.0675
2026-01-03 17:59:24,515 - INFO -   Batch 750/2527: Loss = 0.0557
2026-01-03 17:59:53,501 - INFO -   Batch 800/2527: Loss = 0.0965
2026-01-03 18:00:22,289 - INFO -   Batch 850/2527: Loss = 0.0705
2026-01-03 18:00:51,378 - INFO -   Batch 900/2527: Loss = 0.0615
2026-01-03 18:01:20,150 - INFO -   Batch 950/2527: Loss = 0.0645
2026-01-03 18:01:49,036 - INFO -   Batch 1000/2527: Loss = 0.0888
2026-01-03 18:02:17,690 - INFO -   Batch 1050/2527: Loss = 0.0778
2026-01-03 18:02:46,254 - INFO -   Batch 1100/2527: Loss = 0.0588
2026-01-03 18:03:15,208 - INFO -   Batch 1150/2527: Loss = 0.0516
2026-01-03 18:03:43,960 - INFO -   Batch 1200/2527: Loss = 0.0648
2026-01-03 18:04:12,963 - INFO -   Batch 1250/2527: Loss = 0.0515
2026-01-03 18:04:41,871 - INFO -   Batch 1300/2527: Loss = 0.0575
2026-01-03 18:05:10,458 - INFO -   Batch 1350/2527: Loss = 0.0822
2026-01-03 18:05:39,274 - INFO -   Batch 1400/2527: Loss = 0.0804
2026-01-03 18:06:08,012 - INFO -   Batch 1450/2527: Loss = 0.0971
2026-01-03 18:06:36,895 - INFO -   Batch 1500/2527: Loss = 0.0761
2026-01-03 18:07:05,599 - INFO -   Batch 1550/2527: Loss = 0.0537
2026-01-03 18:07:34,573 - INFO -   Batch 1600/2527: Loss = 0.0676
2026-01-03 18:08:03,279 - INFO -   Batch 1650/2527: Loss = 0.0717
2026-01-03 18:08:32,016 - INFO -   Batch 1700/2527: Loss = 0.0417
2026-01-03 18:09:00,993 - INFO -   Batch 1750/2527: Loss = 0.0577
2026-01-03 18:09:29,595 - INFO -   Batch 1800/2527: Loss = 0.0701
2026-01-03 18:09:58,489 - INFO -   Batch 1850/2527: Loss = 0.0731
2026-01-03 18:10:27,185 - INFO -   Batch 1900/2527: Loss = 0.0598
2026-01-03 18:10:56,075 - INFO -   Batch 1950/2527: Loss = 0.0817
2026-01-03 18:11:24,860 - INFO -   Batch 2000/2527: Loss = 0.1072
2026-01-03 18:11:53,461 - INFO -   Batch 2050/2527: Loss = 0.0662
2026-01-03 18:12:22,573 - INFO -   Batch 2100/2527: Loss = 0.0767
2026-01-03 18:12:51,221 - INFO -   Batch 2150/2527: Loss = 0.0778
2026-01-03 18:13:20,061 - INFO -   Batch 2200/2527: Loss = 0.0485
2026-01-03 18:13:48,724 - INFO -   Batch 2250/2527: Loss = 0.0535
2026-01-03 18:14:17,302 - INFO -   Batch 2300/2527: Loss = 0.0650
2026-01-03 18:14:46,198 - INFO -   Batch 2350/2527: Loss = 0.0752
2026-01-03 18:15:14,978 - INFO -   Batch 2400/2527: Loss = 0.0569
2026-01-03 18:15:43,909 - INFO -   Batch 2450/2527: Loss = 0.0768
2026-01-03 18:16:12,795 - INFO -   Batch 2500/2527: Loss = 0.0623
2026-01-03 18:20:58,276 - INFO - Epoch 93/160: Train Loss: 0.0744, Val Loss: 1.3218, Val mIoU: 40.13%, Val PixelAcc: 78.83%
2026-01-03 18:21:28,744 - INFO -   Batch 50/2527: Loss = 0.0589
2026-01-03 18:21:57,274 - INFO -   Batch 100/2527: Loss = 0.0479
2026-01-03 18:22:26,115 - INFO -   Batch 150/2527: Loss = 0.0735
2026-01-03 18:22:54,929 - INFO -   Batch 200/2527: Loss = 0.0464
2026-01-03 18:23:23,405 - INFO -   Batch 250/2527: Loss = 0.0801
2026-01-03 18:23:52,358 - INFO -   Batch 300/2527: Loss = 0.0886
2026-01-03 18:24:21,164 - INFO -   Batch 350/2527: Loss = 0.0635
2026-01-03 18:24:50,062 - INFO -   Batch 400/2527: Loss = 0.0758
2026-01-03 18:25:18,591 - INFO -   Batch 450/2527: Loss = 0.0732
2026-01-03 18:25:47,411 - INFO -   Batch 500/2527: Loss = 0.0645
2026-01-03 18:26:15,994 - INFO -   Batch 550/2527: Loss = 0.1024
2026-01-03 18:26:44,748 - INFO -   Batch 600/2527: Loss = 0.0411
2026-01-03 18:27:13,604 - INFO -   Batch 650/2527: Loss = 0.0798
2026-01-03 18:27:42,377 - INFO -   Batch 700/2527: Loss = 0.0577
2026-01-03 18:28:11,463 - INFO -   Batch 750/2527: Loss = 0.0653
2026-01-03 18:28:40,337 - INFO -   Batch 800/2527: Loss = 0.1186
2026-01-03 18:29:09,143 - INFO -   Batch 850/2527: Loss = 0.0980
2026-01-03 18:29:37,624 - INFO -   Batch 900/2527: Loss = 0.1087
2026-01-03 18:30:06,330 - INFO -   Batch 950/2527: Loss = 0.0714
2026-01-03 18:30:35,397 - INFO -   Batch 1000/2527: Loss = 0.0736
2026-01-03 18:31:04,030 - INFO -   Batch 1050/2527: Loss = 0.0632
2026-01-03 18:31:32,978 - INFO -   Batch 1100/2527: Loss = 0.0549
2026-01-03 18:32:01,865 - INFO -   Batch 1150/2527: Loss = 0.0707
2026-01-03 18:32:30,439 - INFO -   Batch 1200/2527: Loss = 0.0927
2026-01-03 18:32:59,259 - INFO -   Batch 1250/2527: Loss = 0.0789
2026-01-03 18:33:27,821 - INFO -   Batch 1300/2527: Loss = 0.0708
2026-01-03 18:33:56,689 - INFO -   Batch 1350/2527: Loss = 0.0595
2026-01-03 18:34:25,517 - INFO -   Batch 1400/2527: Loss = 0.0717
2026-01-03 18:34:54,352 - INFO -   Batch 1450/2527: Loss = 0.0878
2026-01-03 18:35:23,112 - INFO -   Batch 1500/2527: Loss = 0.0758
2026-01-03 18:35:51,866 - INFO -   Batch 1550/2527: Loss = 0.0830
2026-01-03 18:36:20,799 - INFO -   Batch 1600/2527: Loss = 0.0842
2026-01-03 18:36:49,410 - INFO -   Batch 1650/2527: Loss = 0.0678
2026-01-03 18:37:18,213 - INFO -   Batch 1700/2527: Loss = 0.0537
2026-01-03 18:37:46,817 - INFO -   Batch 1750/2527: Loss = 0.0596
2026-01-03 18:38:15,837 - INFO -   Batch 1800/2527: Loss = 0.0859
2026-01-03 18:38:44,326 - INFO -   Batch 1850/2527: Loss = 0.0625
2026-01-03 18:39:13,094 - INFO -   Batch 1900/2527: Loss = 0.0573
2026-01-03 18:39:42,158 - INFO -   Batch 1950/2527: Loss = 0.0477
2026-01-03 18:40:10,815 - INFO -   Batch 2000/2527: Loss = 0.0600
2026-01-03 18:40:39,635 - INFO -   Batch 2050/2527: Loss = 0.0710
2026-01-03 18:41:08,241 - INFO -   Batch 2100/2527: Loss = 0.0817
2026-01-03 18:41:36,756 - INFO -   Batch 2150/2527: Loss = 0.0672
2026-01-03 18:42:05,823 - INFO -   Batch 2200/2527: Loss = 0.0653
2026-01-03 18:42:34,402 - INFO -   Batch 2250/2527: Loss = 0.0906
2026-01-03 18:43:03,410 - INFO -   Batch 2300/2527: Loss = 0.0780
2026-01-03 18:43:32,273 - INFO -   Batch 2350/2527: Loss = 0.0521
2026-01-03 18:44:01,194 - INFO -   Batch 2400/2527: Loss = 0.0728
2026-01-03 18:44:29,780 - INFO -   Batch 2450/2527: Loss = 0.0625
2026-01-03 18:44:58,287 - INFO -   Batch 2500/2527: Loss = 0.0778
2026-01-03 18:49:38,495 - INFO - Epoch 94/160: Train Loss: 0.0734, Val Loss: 1.3078, Val mIoU: 40.04%, Val PixelAcc: 78.84%
2026-01-03 18:50:08,757 - INFO -   Batch 50/2527: Loss = 0.0812
2026-01-03 18:50:37,460 - INFO -   Batch 100/2527: Loss = 0.0467
2026-01-03 18:51:06,302 - INFO -   Batch 150/2527: Loss = 0.0912
2026-01-03 18:51:34,875 - INFO -   Batch 200/2527: Loss = 0.0727
2026-01-03 18:52:03,614 - INFO -   Batch 250/2527: Loss = 0.0665
2026-01-03 18:52:32,238 - INFO -   Batch 300/2527: Loss = 0.0706
2026-01-03 18:53:01,277 - INFO -   Batch 350/2527: Loss = 0.0796
2026-01-03 18:53:29,925 - INFO -   Batch 400/2527: Loss = 0.0567
2026-01-03 18:53:58,544 - INFO -   Batch 450/2527: Loss = 0.0885
2026-01-03 18:54:27,659 - INFO -   Batch 500/2527: Loss = 0.0776
2026-01-03 18:54:56,497 - INFO -   Batch 550/2527: Loss = 0.0798
2026-01-03 18:55:25,299 - INFO -   Batch 600/2527: Loss = 0.0826
2026-01-03 18:55:53,879 - INFO -   Batch 650/2527: Loss = 0.0683
2026-01-03 18:56:22,781 - INFO -   Batch 700/2527: Loss = 0.0641
2026-01-03 18:56:51,508 - INFO -   Batch 750/2527: Loss = 0.0926
2026-01-03 18:57:20,091 - INFO -   Batch 800/2527: Loss = 0.0720
2026-01-03 18:57:49,052 - INFO -   Batch 850/2527: Loss = 0.1169
2026-01-03 18:58:17,904 - INFO -   Batch 900/2527: Loss = 0.0795
2026-01-03 18:58:46,812 - INFO -   Batch 950/2527: Loss = 0.0473
2026-01-03 18:59:15,371 - INFO -   Batch 1000/2527: Loss = 0.0878
2026-01-03 18:59:43,838 - INFO -   Batch 1050/2527: Loss = 0.1018
2026-01-03 19:00:12,700 - INFO -   Batch 1100/2527: Loss = 0.0757
2026-01-03 19:00:41,550 - INFO -   Batch 1150/2527: Loss = 0.0592
2026-01-03 19:01:10,354 - INFO -   Batch 1200/2527: Loss = 0.0463
2026-01-03 19:01:39,134 - INFO -   Batch 1250/2527: Loss = 0.0679
2026-01-03 19:02:08,205 - INFO -   Batch 1300/2527: Loss = 0.0807
2026-01-03 19:02:36,925 - INFO -   Batch 1350/2527: Loss = 0.0844
2026-01-03 19:03:05,403 - INFO -   Batch 1400/2527: Loss = 0.0804
2026-01-03 19:03:34,247 - INFO -   Batch 1450/2527: Loss = 0.0793
2026-01-03 19:04:02,817 - INFO -   Batch 1500/2527: Loss = 0.0750
2026-01-03 19:04:31,904 - INFO -   Batch 1550/2527: Loss = 0.0928
2026-01-03 19:05:00,524 - INFO -   Batch 1600/2527: Loss = 0.0950
2026-01-03 19:05:29,136 - INFO -   Batch 1650/2527: Loss = 0.0549
2026-01-03 19:05:58,286 - INFO -   Batch 1700/2527: Loss = 0.0561
2026-01-03 19:06:26,911 - INFO -   Batch 1750/2527: Loss = 0.0647
2026-01-03 19:06:55,722 - INFO -   Batch 1800/2527: Loss = 0.0829
2026-01-03 19:07:24,301 - INFO -   Batch 1850/2527: Loss = 0.0899
2026-01-03 19:07:53,162 - INFO -   Batch 1900/2527: Loss = 0.0621
2026-01-03 19:08:21,978 - INFO -   Batch 1950/2527: Loss = 0.0841
2026-01-03 19:08:50,483 - INFO -   Batch 2000/2527: Loss = 0.0727
2026-01-03 19:09:19,424 - INFO -   Batch 2050/2527: Loss = 0.0870
2026-01-03 19:09:48,284 - INFO -   Batch 2100/2527: Loss = 0.0552
2026-01-03 19:10:17,150 - INFO -   Batch 2150/2527: Loss = 0.0715
2026-01-03 19:10:45,704 - INFO -   Batch 2200/2527: Loss = 0.0880
2026-01-03 19:11:14,467 - INFO -   Batch 2250/2527: Loss = 0.0733
2026-01-03 19:11:42,973 - INFO -   Batch 2300/2527: Loss = 0.1098
2026-01-03 19:12:11,858 - INFO -   Batch 2350/2527: Loss = 0.0963
2026-01-03 19:12:40,678 - INFO -   Batch 2400/2527: Loss = 0.0863
2026-01-03 19:13:09,570 - INFO -   Batch 2450/2527: Loss = 0.1243
2026-01-03 19:13:38,684 - INFO -   Batch 2500/2527: Loss = 0.0930
2026-01-03 19:18:15,568 - INFO - Epoch 95/160: Train Loss: 0.0726, Val Loss: 1.3608, Val mIoU: 39.68%, Val PixelAcc: 78.45%
2026-01-03 19:18:45,978 - INFO -   Batch 50/2527: Loss = 0.0922
2026-01-03 19:19:14,991 - INFO -   Batch 100/2527: Loss = 0.1013
2026-01-03 19:19:43,850 - INFO -   Batch 150/2527: Loss = 0.0628
2026-01-03 19:20:12,605 - INFO -   Batch 200/2527: Loss = 0.0697
2026-01-03 19:20:41,284 - INFO -   Batch 250/2527: Loss = 0.0882
2026-01-03 19:21:09,862 - INFO -   Batch 300/2527: Loss = 0.0542
2026-01-03 19:21:38,710 - INFO -   Batch 350/2527: Loss = 0.0750
2026-01-03 19:22:07,487 - INFO -   Batch 400/2527: Loss = 0.0927
2026-01-03 19:22:36,391 - INFO -   Batch 450/2527: Loss = 0.0735
2026-01-03 19:23:05,281 - INFO -   Batch 500/2527: Loss = 0.0549
2026-01-03 19:23:33,851 - INFO -   Batch 550/2527: Loss = 0.0788
2026-01-03 19:24:02,691 - INFO -   Batch 600/2527: Loss = 0.0543
2026-01-03 19:24:31,318 - INFO -   Batch 650/2527: Loss = 0.0925
2026-01-03 19:25:00,235 - INFO -   Batch 700/2527: Loss = 0.0655
2026-01-03 19:25:28,942 - INFO -   Batch 750/2527: Loss = 0.0813
2026-01-03 19:25:57,973 - INFO -   Batch 800/2527: Loss = 0.1102
2026-01-03 19:26:26,700 - INFO -   Batch 850/2527: Loss = 0.1087
2026-01-03 19:26:55,407 - INFO -   Batch 900/2527: Loss = 0.0657
2026-01-03 19:27:24,332 - INFO -   Batch 950/2527: Loss = 0.0553
2026-01-03 19:27:52,954 - INFO -   Batch 1000/2527: Loss = 0.0672
2026-01-03 19:28:21,778 - INFO -   Batch 1050/2527: Loss = 0.0877
2026-01-03 19:28:50,437 - INFO -   Batch 1100/2527: Loss = 0.0780
2026-01-03 19:29:19,294 - INFO -   Batch 1150/2527: Loss = 0.0521
2026-01-03 19:29:47,941 - INFO -   Batch 1200/2527: Loss = 0.0771
2026-01-03 19:30:16,709 - INFO -   Batch 1250/2527: Loss = 0.0746
2026-01-03 19:30:45,728 - INFO -   Batch 1300/2527: Loss = 0.0694
2026-01-03 19:31:14,379 - INFO -   Batch 1350/2527: Loss = 0.0985
2026-01-03 19:31:43,215 - INFO -   Batch 1400/2527: Loss = 0.0786
2026-01-03 19:32:11,848 - INFO -   Batch 1450/2527: Loss = 0.0788
2026-01-03 19:32:40,414 - INFO -   Batch 1500/2527: Loss = 0.0681
2026-01-03 19:33:09,260 - INFO -   Batch 1550/2527: Loss = 0.0596
2026-01-03 19:33:37,965 - INFO -   Batch 1600/2527: Loss = 0.0657
2026-01-03 19:34:06,905 - INFO -   Batch 1650/2527: Loss = 0.0501
2026-01-03 19:34:35,687 - INFO -   Batch 1700/2527: Loss = 0.0716
2026-01-03 19:35:04,584 - INFO -   Batch 1750/2527: Loss = 0.0931
2026-01-03 19:35:33,121 - INFO -   Batch 1800/2527: Loss = 0.0815
2026-01-03 19:36:01,651 - INFO -   Batch 1850/2527: Loss = 0.0848
2026-01-03 19:36:30,568 - INFO -   Batch 1900/2527: Loss = 0.0711
2026-01-03 19:36:59,241 - INFO -   Batch 1950/2527: Loss = 0.0599
2026-01-03 19:37:28,189 - INFO -   Batch 2000/2527: Loss = 0.0870
2026-01-03 19:37:57,074 - INFO -   Batch 2050/2527: Loss = 0.0659
2026-01-03 19:38:26,099 - INFO -   Batch 2100/2527: Loss = 0.0906
2026-01-03 19:38:54,661 - INFO -   Batch 2150/2527: Loss = 0.0967
2026-01-03 19:39:23,280 - INFO -   Batch 2200/2527: Loss = 0.0819
2026-01-03 19:39:52,113 - INFO -   Batch 2250/2527: Loss = 0.0701
2026-01-03 19:40:20,818 - INFO -   Batch 2300/2527: Loss = 0.0588
2026-01-03 19:40:49,696 - INFO -   Batch 2350/2527: Loss = 0.0892
2026-01-03 19:41:18,448 - INFO -   Batch 2400/2527: Loss = 0.0518
2026-01-03 19:41:47,040 - INFO -   Batch 2450/2527: Loss = 0.0519
2026-01-03 19:42:16,085 - INFO -   Batch 2500/2527: Loss = 0.0647
2026-01-03 19:47:01,575 - INFO - Epoch 96/160: Train Loss: 0.0729, Val Loss: 1.3477, Val mIoU: 39.66%, Val PixelAcc: 78.66%
2026-01-03 19:47:32,048 - INFO -   Batch 50/2527: Loss = 0.0629
2026-01-03 19:48:00,449 - INFO -   Batch 100/2527: Loss = 0.0796
2026-01-03 19:48:29,069 - INFO -   Batch 150/2527: Loss = 0.0966
2026-01-03 19:48:58,089 - INFO -   Batch 200/2527: Loss = 0.0850
2026-01-03 19:49:26,878 - INFO -   Batch 250/2527: Loss = 0.0747
2026-01-03 19:49:55,797 - INFO -   Batch 300/2527: Loss = 0.0578
2026-01-03 19:50:24,700 - INFO -   Batch 350/2527: Loss = 0.0805
2026-01-03 19:50:53,285 - INFO -   Batch 400/2527: Loss = 0.0672
2026-01-03 19:51:22,081 - INFO -   Batch 450/2527: Loss = 0.0851
2026-01-03 19:51:50,662 - INFO -   Batch 500/2527: Loss = 0.0712
2026-01-03 19:52:19,530 - INFO -   Batch 550/2527: Loss = 0.0957
2026-01-03 19:52:48,343 - INFO -   Batch 600/2527: Loss = 0.0804
2026-01-03 19:53:17,210 - INFO -   Batch 650/2527: Loss = 0.0554
2026-01-03 19:53:45,890 - INFO -   Batch 700/2527: Loss = 0.0649
2026-01-03 19:54:14,675 - INFO -   Batch 750/2527: Loss = 0.0936
2026-01-03 19:54:43,582 - INFO -   Batch 800/2527: Loss = 0.1111
2026-01-03 19:55:12,163 - INFO -   Batch 850/2527: Loss = 0.0914
2026-01-03 19:55:40,996 - INFO -   Batch 900/2527: Loss = 0.0582
2026-01-03 19:56:09,676 - INFO -   Batch 950/2527: Loss = 0.0710
2026-01-03 19:56:38,406 - INFO -   Batch 1000/2527: Loss = 0.0641
2026-01-03 19:57:07,207 - INFO -   Batch 1050/2527: Loss = 0.0656
2026-01-03 19:57:36,006 - INFO -   Batch 1100/2527: Loss = 0.0802
2026-01-03 19:58:05,056 - INFO -   Batch 1150/2527: Loss = 0.1030
2026-01-03 19:58:33,705 - INFO -   Batch 1200/2527: Loss = 0.0997
2026-01-03 19:59:02,465 - INFO -   Batch 1250/2527: Loss = 0.0915
2026-01-03 19:59:31,104 - INFO -   Batch 1300/2527: Loss = 0.0645
2026-01-03 19:59:59,618 - INFO -   Batch 1350/2527: Loss = 0.0535
2026-01-03 20:00:28,700 - INFO -   Batch 1400/2527: Loss = 0.0542
2026-01-03 20:00:57,280 - INFO -   Batch 1450/2527: Loss = 0.0695
2026-01-03 20:01:26,259 - INFO -   Batch 1500/2527: Loss = 0.0706
2026-01-03 20:01:55,110 - INFO -   Batch 1550/2527: Loss = 0.0850
2026-01-03 20:02:24,040 - INFO -   Batch 1600/2527: Loss = 0.0772
2026-01-03 20:02:52,590 - INFO -   Batch 1650/2527: Loss = 0.0671
2026-01-03 20:03:21,052 - INFO -   Batch 1700/2527: Loss = 0.0520
2026-01-03 20:03:49,931 - INFO -   Batch 1750/2527: Loss = 0.0691
2026-01-03 20:04:18,717 - INFO -   Batch 1800/2527: Loss = 0.0482
2026-01-03 20:04:47,539 - INFO -   Batch 1850/2527: Loss = 0.0561
2026-01-03 20:05:16,296 - INFO -   Batch 1900/2527: Loss = 0.0956
2026-01-03 20:05:45,061 - INFO -   Batch 1950/2527: Loss = 0.0606
2026-01-03 20:06:13,983 - INFO -   Batch 2000/2527: Loss = 0.0652
2026-01-03 20:06:42,607 - INFO -   Batch 2050/2527: Loss = 0.0504
2026-01-03 20:07:11,385 - INFO -   Batch 2100/2527: Loss = 0.0664
2026-01-03 20:07:40,210 - INFO -   Batch 2150/2527: Loss = 0.1043
2026-01-03 20:08:09,250 - INFO -   Batch 2200/2527: Loss = 0.0498
2026-01-03 20:08:37,880 - INFO -   Batch 2250/2527: Loss = 0.0546
2026-01-03 20:09:06,506 - INFO -   Batch 2300/2527: Loss = 0.0619
2026-01-03 20:09:35,656 - INFO -   Batch 2350/2527: Loss = 0.0571
2026-01-03 20:10:04,329 - INFO -   Batch 2400/2527: Loss = 0.0841
2026-01-03 20:10:33,170 - INFO -   Batch 2450/2527: Loss = 0.0528
2026-01-03 20:11:01,727 - INFO -   Batch 2500/2527: Loss = 0.0843
2026-01-03 20:15:42,108 - INFO - Epoch 97/160: Train Loss: 0.0719, Val Loss: 1.3754, Val mIoU: 39.61%, Val PixelAcc: 78.71%
2026-01-03 20:16:12,528 - INFO -   Batch 50/2527: Loss = 0.0983
2026-01-03 20:16:41,348 - INFO -   Batch 100/2527: Loss = 0.1054
2026-01-03 20:17:10,237 - INFO -   Batch 150/2527: Loss = 0.0655
2026-01-03 20:17:38,804 - INFO -   Batch 200/2527: Loss = 0.1082
2026-01-03 20:18:07,274 - INFO -   Batch 250/2527: Loss = 0.0751
2026-01-03 20:18:36,143 - INFO -   Batch 300/2527: Loss = 0.0718
2026-01-03 20:19:05,011 - INFO -   Batch 350/2527: Loss = 0.0605
2026-01-03 20:19:33,825 - INFO -   Batch 400/2527: Loss = 0.0723
2026-01-03 20:20:02,602 - INFO -   Batch 450/2527: Loss = 0.0725
2026-01-03 20:20:31,670 - INFO -   Batch 500/2527: Loss = 0.0784
2026-01-03 20:21:00,259 - INFO -   Batch 550/2527: Loss = 0.0643
2026-01-03 20:21:28,877 - INFO -   Batch 600/2527: Loss = 0.0611
2026-01-03 20:21:57,667 - INFO -   Batch 650/2527: Loss = 0.1154
2026-01-03 20:22:26,380 - INFO -   Batch 700/2527: Loss = 0.0752
2026-01-03 20:22:55,464 - INFO -   Batch 750/2527: Loss = 0.0790
2026-01-03 20:23:24,094 - INFO -   Batch 800/2527: Loss = 0.0872
2026-01-03 20:23:52,755 - INFO -   Batch 850/2527: Loss = 0.0818
2026-01-03 20:24:21,869 - INFO -   Batch 900/2527: Loss = 0.0752
2026-01-03 20:24:50,551 - INFO -   Batch 950/2527: Loss = 0.0711
2026-01-03 20:25:19,395 - INFO -   Batch 1000/2527: Loss = 0.0908
2026-01-03 20:25:48,033 - INFO -   Batch 1050/2527: Loss = 0.0796
2026-01-03 20:26:16,917 - INFO -   Batch 1100/2527: Loss = 0.0356
2026-01-03 20:26:45,776 - INFO -   Batch 1150/2527: Loss = 0.0681
2026-01-03 20:27:14,244 - INFO -   Batch 1200/2527: Loss = 0.0505
2026-01-03 20:27:43,188 - INFO -   Batch 1250/2527: Loss = 0.0485
2026-01-03 20:28:12,108 - INFO -   Batch 1300/2527: Loss = 0.0587
2026-01-03 20:28:41,038 - INFO -   Batch 1350/2527: Loss = 0.0772
2026-01-03 20:29:09,657 - INFO -   Batch 1400/2527: Loss = 0.0475
2026-01-03 20:29:38,475 - INFO -   Batch 1450/2527: Loss = 0.0629
2026-01-03 20:30:07,047 - INFO -   Batch 1500/2527: Loss = 0.0750
2026-01-03 20:30:35,927 - INFO -   Batch 1550/2527: Loss = 0.0888
2026-01-03 20:31:04,798 - INFO -   Batch 1600/2527: Loss = 0.0537
2026-01-03 20:31:33,573 - INFO -   Batch 1650/2527: Loss = 0.0434
2026-01-03 20:32:02,699 - INFO -   Batch 1700/2527: Loss = 0.0968
2026-01-03 20:32:31,414 - INFO -   Batch 1750/2527: Loss = 0.0643
2026-01-03 20:32:59,940 - INFO -   Batch 1800/2527: Loss = 0.0937
2026-01-03 20:33:28,806 - INFO -   Batch 1850/2527: Loss = 0.0615
2026-01-03 20:33:57,719 - INFO -   Batch 1900/2527: Loss = 0.0590
2026-01-03 20:34:26,824 - INFO -   Batch 1950/2527: Loss = 0.0678
2026-01-03 20:34:55,480 - INFO -   Batch 2000/2527: Loss = 0.0471
2026-01-03 20:35:24,440 - INFO -   Batch 2050/2527: Loss = 0.0588
2026-01-03 20:35:53,423 - INFO -   Batch 2100/2527: Loss = 0.0774
2026-01-03 20:36:21,993 - INFO -   Batch 2150/2527: Loss = 0.0941
2026-01-03 20:36:50,812 - INFO -   Batch 2200/2527: Loss = 0.0523
2026-01-03 20:37:19,488 - INFO -   Batch 2250/2527: Loss = 0.0687
2026-01-03 20:37:48,424 - INFO -   Batch 2300/2527: Loss = 0.0541
2026-01-03 20:38:17,291 - INFO -   Batch 2350/2527: Loss = 0.0592
2026-01-03 20:38:45,899 - INFO -   Batch 2400/2527: Loss = 0.0514
2026-01-03 20:39:14,881 - INFO -   Batch 2450/2527: Loss = 0.0462
2026-01-03 20:39:43,817 - INFO -   Batch 2500/2527: Loss = 0.0606
2026-01-03 20:44:24,858 - INFO - Epoch 98/160: Train Loss: 0.0698, Val Loss: 1.3433, Val mIoU: 39.73%, Val PixelAcc: 78.80%
2026-01-03 20:44:55,433 - INFO -   Batch 50/2527: Loss = 0.0746
2026-01-03 20:45:24,108 - INFO -   Batch 100/2527: Loss = 0.0886
2026-01-03 20:45:53,047 - INFO -   Batch 150/2527: Loss = 0.0575
2026-01-03 20:46:21,650 - INFO -   Batch 200/2527: Loss = 0.0848
2026-01-03 20:46:50,485 - INFO -   Batch 250/2527: Loss = 0.0755
2026-01-03 20:47:19,251 - INFO -   Batch 300/2527: Loss = 0.0522
2026-01-03 20:47:48,099 - INFO -   Batch 350/2527: Loss = 0.0504
2026-01-03 20:48:16,768 - INFO -   Batch 400/2527: Loss = 0.0598
2026-01-03 20:48:45,520 - INFO -   Batch 450/2527: Loss = 0.0566
2026-01-03 20:49:14,563 - INFO -   Batch 500/2527: Loss = 0.0813
2026-01-03 20:49:43,243 - INFO -   Batch 550/2527: Loss = 0.0743
2026-01-03 20:50:12,080 - INFO -   Batch 600/2527: Loss = 0.0433
2026-01-03 20:50:40,765 - INFO -   Batch 650/2527: Loss = 0.0620
2026-01-03 20:51:09,349 - INFO -   Batch 700/2527: Loss = 0.0687
2026-01-03 20:51:38,242 - INFO -   Batch 750/2527: Loss = 0.0645
2026-01-03 20:52:06,933 - INFO -   Batch 800/2527: Loss = 0.0805
2026-01-03 20:52:35,920 - INFO -   Batch 850/2527: Loss = 0.0594
2026-01-03 20:53:04,777 - INFO -   Batch 900/2527: Loss = 0.1018
2026-01-03 20:53:33,740 - INFO -   Batch 950/2527: Loss = 0.0715
2026-01-03 20:54:02,327 - INFO -   Batch 1000/2527: Loss = 0.0497
2026-01-03 20:54:30,855 - INFO -   Batch 1050/2527: Loss = 0.0357
2026-01-03 20:54:59,738 - INFO -   Batch 1100/2527: Loss = 0.0815
2026-01-03 20:55:28,360 - INFO -   Batch 1150/2527: Loss = 0.0779
2026-01-03 20:55:57,318 - INFO -   Batch 1200/2527: Loss = 0.0697
2026-01-03 20:56:26,041 - INFO -   Batch 1250/2527: Loss = 0.0610
2026-01-03 20:56:54,745 - INFO -   Batch 1300/2527: Loss = 0.0540
2026-01-03 20:57:23,653 - INFO -   Batch 1350/2527: Loss = 0.0581
2026-01-03 20:57:52,269 - INFO -   Batch 1400/2527: Loss = 0.0481
2026-01-03 20:58:21,139 - INFO -   Batch 1450/2527: Loss = 0.0719
2026-01-03 20:58:50,034 - INFO -   Batch 1500/2527: Loss = 0.0591
2026-01-03 20:59:18,986 - INFO -   Batch 1550/2527: Loss = 0.0725
2026-01-03 20:59:47,822 - INFO -   Batch 1600/2527: Loss = 0.0626
2026-01-03 21:00:16,446 - INFO -   Batch 1650/2527: Loss = 0.0912
2026-01-03 21:00:45,549 - INFO -   Batch 1700/2527: Loss = 0.0687
2026-01-03 21:01:14,281 - INFO -   Batch 1750/2527: Loss = 0.0765
2026-01-03 21:01:43,090 - INFO -   Batch 1800/2527: Loss = 0.0652
2026-01-03 21:02:11,803 - INFO -   Batch 1850/2527: Loss = 0.0884
2026-01-03 21:02:40,698 - INFO -   Batch 1900/2527: Loss = 0.0742
2026-01-03 21:03:09,254 - INFO -   Batch 1950/2527: Loss = 0.1033
2026-01-03 21:03:38,017 - INFO -   Batch 2000/2527: Loss = 0.0719
2026-01-03 21:04:07,030 - INFO -   Batch 2050/2527: Loss = 0.0724
2026-01-03 21:04:35,906 - INFO -   Batch 2100/2527: Loss = 0.0396
2026-01-03 21:05:04,825 - INFO -   Batch 2150/2527: Loss = 0.0681
2026-01-03 21:05:33,533 - INFO -   Batch 2200/2527: Loss = 0.0724
2026-01-03 21:06:02,128 - INFO -   Batch 2250/2527: Loss = 0.0590
2026-01-03 21:06:31,063 - INFO -   Batch 2300/2527: Loss = 0.0638
2026-01-03 21:06:59,757 - INFO -   Batch 2350/2527: Loss = 0.0464
2026-01-03 21:07:28,757 - INFO -   Batch 2400/2527: Loss = 0.0572
2026-01-03 21:07:57,467 - INFO -   Batch 2450/2527: Loss = 0.0523
2026-01-03 21:08:26,574 - INFO -   Batch 2500/2527: Loss = 0.0621
2026-01-03 21:13:09,998 - INFO - Epoch 99/160: Train Loss: 0.0680, Val Loss: 1.3594, Val mIoU: 39.67%, Val PixelAcc: 78.69%
2026-01-03 21:13:40,312 - INFO -   Batch 50/2527: Loss = 0.0847
2026-01-03 21:14:09,177 - INFO -   Batch 100/2527: Loss = 0.0406
2026-01-03 21:14:37,762 - INFO -   Batch 150/2527: Loss = 0.0532
2026-01-03 21:15:06,500 - INFO -   Batch 200/2527: Loss = 0.0771
2026-01-03 21:15:35,317 - INFO -   Batch 250/2527: Loss = 0.1256
2026-01-03 21:16:04,063 - INFO -   Batch 300/2527: Loss = 0.0510
2026-01-03 21:16:33,156 - INFO -   Batch 350/2527: Loss = 0.0594
2026-01-03 21:17:01,875 - INFO -   Batch 400/2527: Loss = 0.0579
2026-01-03 21:17:30,653 - INFO -   Batch 450/2527: Loss = 0.0772
2026-01-03 21:17:59,281 - INFO -   Batch 500/2527: Loss = 0.0626
2026-01-03 21:18:27,794 - INFO -   Batch 550/2527: Loss = 0.0745
2026-01-03 21:18:56,814 - INFO -   Batch 600/2527: Loss = 0.0547
2026-01-03 21:19:25,425 - INFO -   Batch 650/2527: Loss = 0.0418
2026-01-03 21:19:54,402 - INFO -   Batch 700/2527: Loss = 0.0506
2026-01-03 21:20:23,295 - INFO -   Batch 750/2527: Loss = 0.0430
2026-01-03 21:20:52,146 - INFO -   Batch 800/2527: Loss = 0.0856
2026-01-03 21:21:20,611 - INFO -   Batch 850/2527: Loss = 0.1043
2026-01-03 21:21:49,455 - INFO -   Batch 900/2527: Loss = 0.0533
2026-01-03 21:22:18,121 - INFO -   Batch 950/2527: Loss = 0.0677
2026-01-03 21:22:47,204 - INFO -   Batch 1000/2527: Loss = 0.0725
2026-01-03 21:23:15,848 - INFO -   Batch 1050/2527: Loss = 0.0807
2026-01-03 21:23:44,791 - INFO -   Batch 1100/2527: Loss = 0.0537
2026-01-03 21:24:13,750 - INFO -   Batch 1150/2527: Loss = 0.0647
2026-01-03 21:24:42,363 - INFO -   Batch 1200/2527: Loss = 0.0888
2026-01-03 21:25:11,213 - INFO -   Batch 1250/2527: Loss = 0.0554
2026-01-03 21:25:39,786 - INFO -   Batch 1300/2527: Loss = 0.0624
2026-01-03 21:26:08,689 - INFO -   Batch 1350/2527: Loss = 0.0594
2026-01-03 21:26:37,513 - INFO -   Batch 1400/2527: Loss = 0.0593
2026-01-03 21:27:06,024 - INFO -   Batch 1450/2527: Loss = 0.0816
2026-01-03 21:27:35,031 - INFO -   Batch 1500/2527: Loss = 0.0320
2026-01-03 21:28:03,885 - INFO -   Batch 1550/2527: Loss = 0.0684
2026-01-03 21:28:32,867 - INFO -   Batch 1600/2527: Loss = 0.0771
2026-01-03 21:29:01,467 - INFO -   Batch 1650/2527: Loss = 0.1165
2026-01-03 21:29:30,248 - INFO -   Batch 1700/2527: Loss = 0.1157
2026-01-03 21:29:58,960 - INFO -   Batch 1750/2527: Loss = 0.0540
2026-01-03 21:30:27,704 - INFO -   Batch 1800/2527: Loss = 0.0620
2026-01-03 21:30:56,534 - INFO -   Batch 1850/2527: Loss = 0.0573
2026-01-03 21:31:25,312 - INFO -   Batch 1900/2527: Loss = 0.0672
2026-01-03 21:31:54,417 - INFO -   Batch 1950/2527: Loss = 0.0407
2026-01-03 21:32:23,325 - INFO -   Batch 2000/2527: Loss = 0.0675
2026-01-03 21:32:52,174 - INFO -   Batch 2050/2527: Loss = 0.0663
2026-01-03 21:33:20,681 - INFO -   Batch 2100/2527: Loss = 0.0784
2026-01-03 21:33:49,328 - INFO -   Batch 2150/2527: Loss = 0.0592
2026-01-03 21:34:18,432 - INFO -   Batch 2200/2527: Loss = 0.0578
2026-01-03 21:34:47,081 - INFO -   Batch 2250/2527: Loss = 0.0481
2026-01-03 21:35:16,100 - INFO -   Batch 2300/2527: Loss = 0.0534
2026-01-03 21:35:44,992 - INFO -   Batch 2350/2527: Loss = 0.0795
2026-01-03 21:36:13,603 - INFO -   Batch 2400/2527: Loss = 0.0747
2026-01-03 21:36:42,471 - INFO -   Batch 2450/2527: Loss = 0.0627
2026-01-03 21:37:11,050 - INFO -   Batch 2500/2527: Loss = 0.0826
2026-01-03 21:41:51,905 - INFO - Epoch 100/160: Train Loss: 0.0681, Val Loss: 1.3587, Val mIoU: 39.74%, Val PixelAcc: 78.87%
2026-01-03 21:41:55,471 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_100.pth
2026-01-03 21:41:55,474 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_100.pth
2026-01-03 21:42:25,698 - INFO -   Batch 50/2527: Loss = 0.0796
2026-01-03 21:42:54,508 - INFO -   Batch 100/2527: Loss = 0.0641
2026-01-03 21:43:23,311 - INFO -   Batch 150/2527: Loss = 0.0788
2026-01-03 21:43:52,111 - INFO -   Batch 200/2527: Loss = 0.0874
2026-01-03 21:44:20,648 - INFO -   Batch 250/2527: Loss = 0.0472
2026-01-03 21:44:49,523 - INFO -   Batch 300/2527: Loss = 0.0711
2026-01-03 21:45:18,401 - INFO -   Batch 350/2527: Loss = 0.0737
2026-01-03 21:45:46,884 - INFO -   Batch 400/2527: Loss = 0.0571
2026-01-03 21:46:15,810 - INFO -   Batch 450/2527: Loss = 0.0626
2026-01-03 21:46:44,861 - INFO -   Batch 500/2527: Loss = 0.0704
2026-01-03 21:47:13,745 - INFO -   Batch 550/2527: Loss = 0.0581
2026-01-03 21:47:42,400 - INFO -   Batch 600/2527: Loss = 0.0753
2026-01-03 21:48:10,866 - INFO -   Batch 650/2527: Loss = 0.0929
2026-01-03 21:48:39,760 - INFO -   Batch 700/2527: Loss = 0.0752
2026-01-03 21:49:08,634 - INFO -   Batch 750/2527: Loss = 0.0740
2026-01-03 21:49:37,452 - INFO -   Batch 800/2527: Loss = 0.0619
2026-01-03 21:50:06,184 - INFO -   Batch 850/2527: Loss = 0.0849
2026-01-03 21:50:35,309 - INFO -   Batch 900/2527: Loss = 0.0843
2026-01-03 21:51:03,965 - INFO -   Batch 950/2527: Loss = 0.0566
2026-01-03 21:51:32,430 - INFO -   Batch 1000/2527: Loss = 0.0436
2026-01-03 21:52:01,235 - INFO -   Batch 1050/2527: Loss = 0.0676
2026-01-03 21:52:29,879 - INFO -   Batch 1100/2527: Loss = 0.0747
2026-01-03 21:52:58,947 - INFO -   Batch 1150/2527: Loss = 0.0667
2026-01-03 21:53:27,576 - INFO -   Batch 1200/2527: Loss = 0.0609
2026-01-03 21:53:56,522 - INFO -   Batch 1250/2527: Loss = 0.0501
2026-01-03 21:54:25,293 - INFO -   Batch 1300/2527: Loss = 0.0669
2026-01-03 21:54:54,010 - INFO -   Batch 1350/2527: Loss = 0.0484
2026-01-03 21:55:22,810 - INFO -   Batch 1400/2527: Loss = 0.0419
2026-01-03 21:55:51,403 - INFO -   Batch 1450/2527: Loss = 0.0692
2026-01-03 21:56:20,222 - INFO -   Batch 1500/2527: Loss = 0.0753
2026-01-03 21:56:49,131 - INFO -   Batch 1550/2527: Loss = 0.0709
2026-01-03 21:57:17,633 - INFO -   Batch 1600/2527: Loss = 0.0590
2026-01-03 21:57:46,583 - INFO -   Batch 1650/2527: Loss = 0.0600
2026-01-03 21:58:15,433 - INFO -   Batch 1700/2527: Loss = 0.0770
2026-01-03 21:58:44,371 - INFO -   Batch 1750/2527: Loss = 0.0498
2026-01-03 21:59:13,049 - INFO -   Batch 1800/2527: Loss = 0.0592
2026-01-03 21:59:41,988 - INFO -   Batch 1850/2527: Loss = 0.0678
2026-01-03 22:00:10,571 - INFO -   Batch 1900/2527: Loss = 0.1000
2026-01-03 22:00:39,368 - INFO -   Batch 1950/2527: Loss = 0.0448
2026-01-03 22:01:08,195 - INFO -   Batch 2000/2527: Loss = 0.0801
2026-01-03 22:01:36,990 - INFO -   Batch 2050/2527: Loss = 0.0488
2026-01-03 22:02:06,125 - INFO -   Batch 2100/2527: Loss = 0.0738
2026-01-03 22:02:34,887 - INFO -   Batch 2150/2527: Loss = 0.0701
2026-01-03 22:03:03,732 - INFO -   Batch 2200/2527: Loss = 0.0786
2026-01-03 22:03:32,237 - INFO -   Batch 2250/2527: Loss = 0.0560
2026-01-03 22:04:00,969 - INFO -   Batch 2300/2527: Loss = 0.0723
2026-01-03 22:04:30,025 - INFO -   Batch 2350/2527: Loss = 0.0538
2026-01-03 22:04:58,826 - INFO -   Batch 2400/2527: Loss = 0.0955
2026-01-03 22:05:27,832 - INFO -   Batch 2450/2527: Loss = 0.0491
2026-01-03 22:05:56,724 - INFO -   Batch 2500/2527: Loss = 0.0678
2026-01-03 22:10:33,880 - INFO - Epoch 101/160: Train Loss: 0.0683, Val Loss: 1.4435, Val mIoU: 39.39%, Val PixelAcc: 78.26%
2026-01-03 22:11:04,716 - INFO -   Batch 50/2527: Loss = 0.0588
2026-01-03 22:11:33,708 - INFO -   Batch 100/2527: Loss = 0.0620
2026-01-03 22:12:02,599 - INFO -   Batch 150/2527: Loss = 0.0477
2026-01-03 22:12:31,053 - INFO -   Batch 200/2527: Loss = 0.0660
2026-01-03 22:12:59,701 - INFO -   Batch 250/2527: Loss = 0.1035
2026-01-03 22:13:28,608 - INFO -   Batch 300/2527: Loss = 0.0757
2026-01-03 22:13:57,246 - INFO -   Batch 350/2527: Loss = 0.0629
2026-01-03 22:14:26,215 - INFO -   Batch 400/2527: Loss = 0.0658
2026-01-03 22:14:54,907 - INFO -   Batch 450/2527: Loss = 0.0519
2026-01-03 22:15:23,642 - INFO -   Batch 500/2527: Loss = 0.0751
2026-01-03 22:15:52,589 - INFO -   Batch 550/2527: Loss = 0.0495
2026-01-03 22:16:21,145 - INFO -   Batch 600/2527: Loss = 0.0522
2026-01-03 22:16:50,051 - INFO -   Batch 650/2527: Loss = 0.0767
2026-01-03 22:17:18,811 - INFO -   Batch 700/2527: Loss = 0.0745
2026-01-03 22:17:47,654 - INFO -   Batch 750/2527: Loss = 0.0499
2026-01-03 22:18:16,512 - INFO -   Batch 800/2527: Loss = 0.0665
2026-01-03 22:18:45,137 - INFO -   Batch 850/2527: Loss = 0.0788
2026-01-03 22:19:14,177 - INFO -   Batch 900/2527: Loss = 0.0726
2026-01-03 22:19:42,921 - INFO -   Batch 950/2527: Loss = 0.0861
2026-01-03 22:20:11,717 - INFO -   Batch 1000/2527: Loss = 0.0624
2026-01-03 22:20:40,325 - INFO -   Batch 1050/2527: Loss = 0.0593
2026-01-03 22:21:09,171 - INFO -   Batch 1100/2527: Loss = 0.0734
2026-01-03 22:21:37,714 - INFO -   Batch 1150/2527: Loss = 0.0721
2026-01-03 22:22:06,525 - INFO -   Batch 1200/2527: Loss = 0.0752
2026-01-03 22:22:35,457 - INFO -   Batch 1250/2527: Loss = 0.0551
2026-01-03 22:23:04,271 - INFO -   Batch 1300/2527: Loss = 0.0678
2026-01-03 22:23:33,185 - INFO -   Batch 1350/2527: Loss = 0.0630
2026-01-03 22:24:01,763 - INFO -   Batch 1400/2527: Loss = 0.0665
2026-01-03 22:24:30,303 - INFO -   Batch 1450/2527: Loss = 0.0638
2026-01-03 22:24:59,241 - INFO -   Batch 1500/2527: Loss = 0.0619
2026-01-03 22:25:27,879 - INFO -   Batch 1550/2527: Loss = 0.1124
2026-01-03 22:25:56,874 - INFO -   Batch 1600/2527: Loss = 0.0697
2026-01-03 22:26:25,645 - INFO -   Batch 1650/2527: Loss = 0.0708
2026-01-03 22:26:54,660 - INFO -   Batch 1700/2527: Loss = 0.0691
2026-01-03 22:27:23,443 - INFO -   Batch 1750/2527: Loss = 0.0585
2026-01-03 22:27:51,983 - INFO -   Batch 1800/2527: Loss = 0.0536
2026-01-03 22:28:20,865 - INFO -   Batch 1850/2527: Loss = 0.0695
2026-01-03 22:28:49,636 - INFO -   Batch 1900/2527: Loss = 0.0553
2026-01-03 22:29:18,436 - INFO -   Batch 1950/2527: Loss = 0.0812
2026-01-03 22:29:47,406 - INFO -   Batch 2000/2527: Loss = 0.0771
2026-01-03 22:30:16,043 - INFO -   Batch 2050/2527: Loss = 0.0596
2026-01-03 22:30:45,052 - INFO -   Batch 2100/2527: Loss = 0.1085
2026-01-03 22:31:13,741 - INFO -   Batch 2150/2527: Loss = 0.0588
2026-01-03 22:31:42,568 - INFO -   Batch 2200/2527: Loss = 0.0703
2026-01-03 22:32:11,166 - INFO -   Batch 2250/2527: Loss = 0.0669
2026-01-03 22:32:40,078 - INFO -   Batch 2300/2527: Loss = 0.0611
2026-01-03 22:33:08,831 - INFO -   Batch 2350/2527: Loss = 0.0755
2026-01-03 22:33:37,553 - INFO -   Batch 2400/2527: Loss = 0.0490
2026-01-03 22:34:06,473 - INFO -   Batch 2450/2527: Loss = 0.0598
2026-01-03 22:34:35,306 - INFO -   Batch 2500/2527: Loss = 0.0685
2026-01-03 22:39:20,529 - INFO - Epoch 102/160: Train Loss: 0.0666, Val Loss: 1.3655, Val mIoU: 40.07%, Val PixelAcc: 78.84%
2026-01-03 22:39:51,284 - INFO -   Batch 50/2527: Loss = 0.0767
2026-01-03 22:40:19,865 - INFO -   Batch 100/2527: Loss = 0.0518
2026-01-03 22:40:48,694 - INFO -   Batch 150/2527: Loss = 0.0734
2026-01-03 22:41:17,734 - INFO -   Batch 200/2527: Loss = 0.0700
2026-01-03 22:41:46,539 - INFO -   Batch 250/2527: Loss = 0.0790
2026-01-03 22:42:15,299 - INFO -   Batch 300/2527: Loss = 0.0625
2026-01-03 22:42:44,081 - INFO -   Batch 350/2527: Loss = 0.0660
2026-01-03 22:43:13,027 - INFO -   Batch 400/2527: Loss = 0.0482
2026-01-03 22:43:41,596 - INFO -   Batch 450/2527: Loss = 0.0580
2026-01-03 22:44:10,381 - INFO -   Batch 500/2527: Loss = 0.0863
2026-01-03 22:44:39,033 - INFO -   Batch 550/2527: Loss = 0.0680
2026-01-03 22:45:08,203 - INFO -   Batch 600/2527: Loss = 0.0666
2026-01-03 22:45:36,806 - INFO -   Batch 650/2527: Loss = 0.0576
2026-01-03 22:46:05,476 - INFO -   Batch 700/2527: Loss = 0.0569
2026-01-03 22:46:34,607 - INFO -   Batch 750/2527: Loss = 0.0719
2026-01-03 22:47:03,316 - INFO -   Batch 800/2527: Loss = 0.0455
2026-01-03 22:47:32,154 - INFO -   Batch 850/2527: Loss = 0.0928
2026-01-03 22:48:00,856 - INFO -   Batch 900/2527: Loss = 0.0784
2026-01-03 22:48:29,405 - INFO -   Batch 950/2527: Loss = 0.0454
2026-01-03 22:48:58,473 - INFO -   Batch 1000/2527: Loss = 0.0703
2026-01-03 22:49:27,132 - INFO -   Batch 1050/2527: Loss = 0.0510
2026-01-03 22:49:56,059 - INFO -   Batch 1100/2527: Loss = 0.0661
2026-01-03 22:50:24,922 - INFO -   Batch 1150/2527: Loss = 0.0725
2026-01-03 22:50:53,841 - INFO -   Batch 1200/2527: Loss = 0.0917
2026-01-03 22:51:22,428 - INFO -   Batch 1250/2527: Loss = 0.0692
2026-01-03 22:51:50,924 - INFO -   Batch 1300/2527: Loss = 0.0819
2026-01-03 22:52:19,783 - INFO -   Batch 1350/2527: Loss = 0.0705
2026-01-03 22:52:48,580 - INFO -   Batch 1400/2527: Loss = 0.0766
2026-01-03 22:53:17,427 - INFO -   Batch 1450/2527: Loss = 0.0513
2026-01-03 22:53:46,169 - INFO -   Batch 1500/2527: Loss = 0.0786
2026-01-03 22:54:15,287 - INFO -   Batch 1550/2527: Loss = 0.1011
2026-01-03 22:54:43,869 - INFO -   Batch 1600/2527: Loss = 0.1448
2026-01-03 22:55:12,411 - INFO -   Batch 1650/2527: Loss = 0.0532
2026-01-03 22:55:41,241 - INFO -   Batch 1700/2527: Loss = 0.0803
2026-01-03 22:56:09,906 - INFO -   Batch 1750/2527: Loss = 0.0405
2026-01-03 22:56:38,973 - INFO -   Batch 1800/2527: Loss = 0.0593
2026-01-03 22:57:07,584 - INFO -   Batch 1850/2527: Loss = 0.0649
2026-01-03 22:57:36,213 - INFO -   Batch 1900/2527: Loss = 0.0429
2026-01-03 22:58:05,290 - INFO -   Batch 1950/2527: Loss = 0.0775
2026-01-03 22:58:34,030 - INFO -   Batch 2000/2527: Loss = 0.0741
2026-01-03 22:59:02,812 - INFO -   Batch 2050/2527: Loss = 0.0788
2026-01-03 22:59:31,614 - INFO -   Batch 2100/2527: Loss = 0.0691
2026-01-03 23:00:00,505 - INFO -   Batch 2150/2527: Loss = 0.0485
2026-01-03 23:00:29,365 - INFO -   Batch 2200/2527: Loss = 0.0588
2026-01-03 23:00:57,866 - INFO -   Batch 2250/2527: Loss = 0.0678
2026-01-03 23:01:26,868 - INFO -   Batch 2300/2527: Loss = 0.0653
2026-01-03 23:01:55,884 - INFO -   Batch 2350/2527: Loss = 0.0655
2026-01-03 23:02:24,794 - INFO -   Batch 2400/2527: Loss = 0.0523
2026-01-03 23:02:53,420 - INFO -   Batch 2450/2527: Loss = 0.0514
2026-01-03 23:03:22,282 - INFO -   Batch 2500/2527: Loss = 0.0613
2026-01-03 23:08:01,187 - INFO - Epoch 103/160: Train Loss: 0.0650, Val Loss: 1.3885, Val mIoU: 40.10%, Val PixelAcc: 78.83%
2026-01-03 23:08:31,400 - INFO -   Batch 50/2527: Loss = 0.0880
2026-01-03 23:09:00,461 - INFO -   Batch 100/2527: Loss = 0.0574
2026-01-03 23:09:29,132 - INFO -   Batch 150/2527: Loss = 0.0447
2026-01-03 23:09:57,624 - INFO -   Batch 200/2527: Loss = 0.0624
2026-01-03 23:10:26,432 - INFO -   Batch 250/2527: Loss = 0.0542
2026-01-03 23:10:55,052 - INFO -   Batch 300/2527: Loss = 0.0613
2026-01-03 23:11:24,173 - INFO -   Batch 350/2527: Loss = 0.0581
2026-01-03 23:11:52,756 - INFO -   Batch 400/2527: Loss = 0.0670
2026-01-03 23:12:21,717 - INFO -   Batch 450/2527: Loss = 0.0479
2026-01-03 23:12:50,492 - INFO -   Batch 500/2527: Loss = 0.0776
2026-01-03 23:13:19,224 - INFO -   Batch 550/2527: Loss = 0.0663
2026-01-03 23:13:48,043 - INFO -   Batch 600/2527: Loss = 0.0752
2026-01-03 23:14:16,745 - INFO -   Batch 650/2527: Loss = 0.0588
2026-01-03 23:14:45,578 - INFO -   Batch 700/2527: Loss = 0.0596
2026-01-03 23:15:14,466 - INFO -   Batch 750/2527: Loss = 0.0643
2026-01-03 23:15:42,944 - INFO -   Batch 800/2527: Loss = 0.0605
2026-01-03 23:16:11,901 - INFO -   Batch 850/2527: Loss = 0.0587
2026-01-03 23:16:40,728 - INFO -   Batch 900/2527: Loss = 0.0600
2026-01-03 23:17:09,641 - INFO -   Batch 950/2527: Loss = 0.0548
2026-01-03 23:17:38,240 - INFO -   Batch 1000/2527: Loss = 0.0742
2026-01-03 23:18:07,053 - INFO -   Batch 1050/2527: Loss = 0.0851
2026-01-03 23:18:35,620 - INFO -   Batch 1100/2527: Loss = 0.0742
2026-01-03 23:19:04,383 - INFO -   Batch 1150/2527: Loss = 0.0782
2026-01-03 23:19:33,260 - INFO -   Batch 1200/2527: Loss = 0.0774
2026-01-03 23:20:02,015 - INFO -   Batch 1250/2527: Loss = 0.0627
2026-01-03 23:20:31,130 - INFO -   Batch 1300/2527: Loss = 0.0518
2026-01-03 23:20:59,771 - INFO -   Batch 1350/2527: Loss = 0.0499
2026-01-03 23:21:28,264 - INFO -   Batch 1400/2527: Loss = 0.0315
2026-01-03 23:21:57,054 - INFO -   Batch 1450/2527: Loss = 0.0648
2026-01-03 23:22:25,697 - INFO -   Batch 1500/2527: Loss = 0.0508
2026-01-03 23:22:54,738 - INFO -   Batch 1550/2527: Loss = 0.0638
2026-01-03 23:23:23,360 - INFO -   Batch 1600/2527: Loss = 0.0556
2026-01-03 23:23:52,278 - INFO -   Batch 1650/2527: Loss = 0.0625
2026-01-03 23:24:21,218 - INFO -   Batch 1700/2527: Loss = 0.0665
2026-01-03 23:24:49,792 - INFO -   Batch 1750/2527: Loss = 0.0501
2026-01-03 23:25:18,648 - INFO -   Batch 1800/2527: Loss = 0.0494
2026-01-03 23:25:47,369 - INFO -   Batch 1850/2527: Loss = 0.0388
2026-01-03 23:26:16,240 - INFO -   Batch 1900/2527: Loss = 0.0588
2026-01-03 23:26:45,088 - INFO -   Batch 1950/2527: Loss = 0.0822
2026-01-03 23:27:13,932 - INFO -   Batch 2000/2527: Loss = 0.0437
2026-01-03 23:27:42,681 - INFO -   Batch 2050/2527: Loss = 0.0534
2026-01-03 23:28:11,452 - INFO -   Batch 2100/2527: Loss = 0.0628
2026-01-03 23:28:40,380 - INFO -   Batch 2150/2527: Loss = 0.0675
2026-01-03 23:29:09,043 - INFO -   Batch 2200/2527: Loss = 0.0731
2026-01-03 23:29:37,867 - INFO -   Batch 2250/2527: Loss = 0.0673
2026-01-03 23:30:06,656 - INFO -   Batch 2300/2527: Loss = 0.0634
2026-01-03 23:30:35,413 - INFO -   Batch 2350/2527: Loss = 0.0492
2026-01-03 23:31:04,322 - INFO -   Batch 2400/2527: Loss = 0.0612
2026-01-03 23:31:33,179 - INFO -   Batch 2450/2527: Loss = 0.0854
2026-01-03 23:32:02,274 - INFO -   Batch 2500/2527: Loss = 0.0827
2026-01-03 23:36:42,013 - INFO - Epoch 104/160: Train Loss: 0.0647, Val Loss: 1.3798, Val mIoU: 39.86%, Val PixelAcc: 78.55%
2026-01-03 23:37:12,641 - INFO -   Batch 50/2527: Loss = 0.0529
2026-01-03 23:37:41,654 - INFO -   Batch 100/2527: Loss = 0.0969
2026-01-03 23:38:10,344 - INFO -   Batch 150/2527: Loss = 0.0470
2026-01-03 23:38:39,130 - INFO -   Batch 200/2527: Loss = 0.0611
2026-01-03 23:39:07,812 - INFO -   Batch 250/2527: Loss = 0.0594
2026-01-03 23:39:36,361 - INFO -   Batch 300/2527: Loss = 0.0672
2026-01-03 23:40:05,205 - INFO -   Batch 350/2527: Loss = 0.0600
2026-01-03 23:40:34,001 - INFO -   Batch 400/2527: Loss = 0.0498
2026-01-03 23:41:02,958 - INFO -   Batch 450/2527: Loss = 0.0718
2026-01-03 23:41:31,739 - INFO -   Batch 500/2527: Loss = 0.0748
2026-01-03 23:42:00,640 - INFO -   Batch 550/2527: Loss = 0.0600
2026-01-03 23:42:29,236 - INFO -   Batch 600/2527: Loss = 0.0570
2026-01-03 23:42:57,777 - INFO -   Batch 650/2527: Loss = 0.0513
2026-01-03 23:43:26,730 - INFO -   Batch 700/2527: Loss = 0.0600
2026-01-03 23:43:55,334 - INFO -   Batch 750/2527: Loss = 0.0660
2026-01-03 23:44:24,335 - INFO -   Batch 800/2527: Loss = 0.0590
2026-01-03 23:44:53,038 - INFO -   Batch 850/2527: Loss = 0.0653
2026-01-03 23:45:22,088 - INFO -   Batch 900/2527: Loss = 0.0676
2026-01-03 23:45:50,681 - INFO -   Batch 950/2527: Loss = 0.0769
2026-01-03 23:46:19,269 - INFO -   Batch 1000/2527: Loss = 0.0543
2026-01-03 23:46:48,102 - INFO -   Batch 1050/2527: Loss = 0.0622
2026-01-03 23:47:16,771 - INFO -   Batch 1100/2527: Loss = 0.0532
2026-01-03 23:47:45,620 - INFO -   Batch 1150/2527: Loss = 0.0573
2026-01-03 23:48:14,379 - INFO -   Batch 1200/2527: Loss = 0.0590
2026-01-03 23:48:42,995 - INFO -   Batch 1250/2527: Loss = 0.0741
2026-01-03 23:49:12,012 - INFO -   Batch 1300/2527: Loss = 0.0632
2026-01-03 23:49:40,738 - INFO -   Batch 1350/2527: Loss = 0.0464
2026-01-03 23:50:09,554 - INFO -   Batch 1400/2527: Loss = 0.0609
2026-01-03 23:50:38,357 - INFO -   Batch 1450/2527: Loss = 0.0592
2026-01-03 23:51:07,251 - INFO -   Batch 1500/2527: Loss = 0.0641
2026-01-03 23:51:35,883 - INFO -   Batch 1550/2527: Loss = 0.0593
2026-01-03 23:52:04,514 - INFO -   Batch 1600/2527: Loss = 0.0761
2026-01-03 23:52:33,486 - INFO -   Batch 1650/2527: Loss = 0.0698
2026-01-03 23:53:02,313 - INFO -   Batch 1700/2527: Loss = 0.0599
2026-01-03 23:53:31,229 - INFO -   Batch 1750/2527: Loss = 0.0449
2026-01-03 23:53:59,821 - INFO -   Batch 1800/2527: Loss = 0.0685
2026-01-03 23:54:28,712 - INFO -   Batch 1850/2527: Loss = 0.0581
2026-01-03 23:54:57,267 - INFO -   Batch 1900/2527: Loss = 0.0985
2026-01-03 23:55:25,885 - INFO -   Batch 1950/2527: Loss = 0.0611
2026-01-03 23:55:54,876 - INFO -   Batch 2000/2527: Loss = 0.0479
2026-01-03 23:56:23,603 - INFO -   Batch 2050/2527: Loss = 0.0426
2026-01-03 23:56:52,633 - INFO -   Batch 2100/2527: Loss = 0.0750
2026-01-03 23:57:21,390 - INFO -   Batch 2150/2527: Loss = 0.0824
2026-01-03 23:57:49,858 - INFO -   Batch 2200/2527: Loss = 0.0729
2026-01-03 23:58:18,692 - INFO -   Batch 2250/2527: Loss = 0.0706
2026-01-03 23:58:47,406 - INFO -   Batch 2300/2527: Loss = 0.0633
2026-01-03 23:59:16,254 - INFO -   Batch 2350/2527: Loss = 0.0481
2026-01-03 23:59:44,944 - INFO -   Batch 2400/2527: Loss = 0.0657
2026-01-04 00:00:13,867 - INFO -   Batch 2450/2527: Loss = 0.0494
2026-01-04 00:00:42,689 - INFO -   Batch 2500/2527: Loss = 0.0426
2026-01-04 00:05:26,822 - INFO - Epoch 105/160: Train Loss: 0.0644, Val Loss: 1.3943, Val mIoU: 39.83%, Val PixelAcc: 78.52%
2026-01-04 00:05:57,347 - INFO -   Batch 50/2527: Loss = 0.0615
2026-01-04 00:06:25,819 - INFO -   Batch 100/2527: Loss = 0.0615
2026-01-04 00:06:54,285 - INFO -   Batch 150/2527: Loss = 0.0511
2026-01-04 00:07:23,352 - INFO -   Batch 200/2527: Loss = 0.0676
2026-01-04 00:07:52,003 - INFO -   Batch 250/2527: Loss = 0.0560
2026-01-04 00:08:20,971 - INFO -   Batch 300/2527: Loss = 0.0742
2026-01-04 00:08:49,892 - INFO -   Batch 350/2527: Loss = 0.0730
2026-01-04 00:09:18,766 - INFO -   Batch 400/2527: Loss = 0.0632
2026-01-04 00:09:47,369 - INFO -   Batch 450/2527: Loss = 0.0779
2026-01-04 00:10:15,832 - INFO -   Batch 500/2527: Loss = 0.0946
2026-01-04 00:10:44,675 - INFO -   Batch 550/2527: Loss = 0.0631
2026-01-04 00:11:13,516 - INFO -   Batch 600/2527: Loss = 0.0660
2026-01-04 00:11:42,323 - INFO -   Batch 650/2527: Loss = 0.0432
2026-01-04 00:12:11,072 - INFO -   Batch 700/2527: Loss = 0.0375
2026-01-04 00:12:40,158 - INFO -   Batch 750/2527: Loss = 0.0499
2026-01-04 00:13:08,726 - INFO -   Batch 800/2527: Loss = 0.0896
2026-01-04 00:13:37,267 - INFO -   Batch 850/2527: Loss = 0.0637
2026-01-04 00:14:06,059 - INFO -   Batch 900/2527: Loss = 0.0555
2026-01-04 00:14:34,670 - INFO -   Batch 950/2527: Loss = 0.0672
2026-01-04 00:15:03,746 - INFO -   Batch 1000/2527: Loss = 0.0855
2026-01-04 00:15:32,296 - INFO -   Batch 1050/2527: Loss = 0.0578
2026-01-04 00:16:00,929 - INFO -   Batch 1100/2527: Loss = 0.0731
2026-01-04 00:16:30,044 - INFO -   Batch 1150/2527: Loss = 0.0710
2026-01-04 00:16:58,732 - INFO -   Batch 1200/2527: Loss = 0.0667
2026-01-04 00:17:27,515 - INFO -   Batch 1250/2527: Loss = 0.0502
2026-01-04 00:17:56,164 - INFO -   Batch 1300/2527: Loss = 0.0740
2026-01-04 00:18:25,036 - INFO -   Batch 1350/2527: Loss = 0.0451
2026-01-04 00:18:53,915 - INFO -   Batch 1400/2527: Loss = 0.0465
2026-01-04 00:19:22,431 - INFO -   Batch 1450/2527: Loss = 0.0498
2026-01-04 00:19:51,388 - INFO -   Batch 1500/2527: Loss = 0.0597
2026-01-04 00:20:20,419 - INFO -   Batch 1550/2527: Loss = 0.0520
2026-01-04 00:20:49,311 - INFO -   Batch 1600/2527: Loss = 0.0463
2026-01-04 00:21:17,889 - INFO -   Batch 1650/2527: Loss = 0.0386
2026-01-04 00:21:46,350 - INFO -   Batch 1700/2527: Loss = 0.0592
2026-01-04 00:22:15,232 - INFO -   Batch 1750/2527: Loss = 0.0767
2026-01-04 00:22:44,079 - INFO -   Batch 1800/2527: Loss = 0.0899
2026-01-04 00:23:12,922 - INFO -   Batch 1850/2527: Loss = 0.0642
2026-01-04 00:23:41,653 - INFO -   Batch 1900/2527: Loss = 0.0730
2026-01-04 00:24:10,778 - INFO -   Batch 1950/2527: Loss = 0.0443
2026-01-04 00:24:39,448 - INFO -   Batch 2000/2527: Loss = 0.0523
2026-01-04 00:25:07,915 - INFO -   Batch 2050/2527: Loss = 0.0563
2026-01-04 00:25:36,691 - INFO -   Batch 2100/2527: Loss = 0.0599
2026-01-04 00:26:05,399 - INFO -   Batch 2150/2527: Loss = 0.1038
2026-01-04 00:26:34,440 - INFO -   Batch 2200/2527: Loss = 0.0690
2026-01-04 00:27:03,102 - INFO -   Batch 2250/2527: Loss = 0.1042
2026-01-04 00:27:32,059 - INFO -   Batch 2300/2527: Loss = 0.0477
2026-01-04 00:28:00,824 - INFO -   Batch 2350/2527: Loss = 0.0515
2026-01-04 00:28:29,544 - INFO -   Batch 2400/2527: Loss = 0.0771
2026-01-04 00:28:58,346 - INFO -   Batch 2450/2527: Loss = 0.0773
2026-01-04 00:29:26,899 - INFO -   Batch 2500/2527: Loss = 0.0663
2026-01-04 00:34:05,394 - INFO - Epoch 106/160: Train Loss: 0.0643, Val Loss: 1.4094, Val mIoU: 39.99%, Val PixelAcc: 78.65%
2026-01-04 00:34:35,784 - INFO -   Batch 50/2527: Loss = 0.0872
2026-01-04 00:35:04,648 - INFO -   Batch 100/2527: Loss = 0.0575
2026-01-04 00:35:33,531 - INFO -   Batch 150/2527: Loss = 0.0470
2026-01-04 00:36:02,120 - INFO -   Batch 200/2527: Loss = 0.0566
2026-01-04 00:36:30,920 - INFO -   Batch 250/2527: Loss = 0.0628
2026-01-04 00:36:59,519 - INFO -   Batch 300/2527: Loss = 0.0461
2026-01-04 00:37:28,222 - INFO -   Batch 350/2527: Loss = 0.0416
2026-01-04 00:37:57,017 - INFO -   Batch 400/2527: Loss = 0.0692
2026-01-04 00:38:25,778 - INFO -   Batch 450/2527: Loss = 0.0596
2026-01-04 00:38:54,840 - INFO -   Batch 500/2527: Loss = 0.0692
2026-01-04 00:39:23,480 - INFO -   Batch 550/2527: Loss = 0.0495
2026-01-04 00:39:51,938 - INFO -   Batch 600/2527: Loss = 0.0543
2026-01-04 00:40:20,738 - INFO -   Batch 650/2527: Loss = 0.0825
2026-01-04 00:40:49,399 - INFO -   Batch 700/2527: Loss = 0.0669
2026-01-04 00:41:18,462 - INFO -   Batch 750/2527: Loss = 0.0596
2026-01-04 00:41:47,009 - INFO -   Batch 800/2527: Loss = 0.0821
2026-01-04 00:42:15,962 - INFO -   Batch 850/2527: Loss = 0.0636
2026-01-04 00:42:44,810 - INFO -   Batch 900/2527: Loss = 0.0627
2026-01-04 00:43:13,403 - INFO -   Batch 950/2527: Loss = 0.0641
2026-01-04 00:43:42,245 - INFO -   Batch 1000/2527: Loss = 0.0654
2026-01-04 00:44:10,780 - INFO -   Batch 1050/2527: Loss = 0.0672
2026-01-04 00:44:39,652 - INFO -   Batch 1100/2527: Loss = 0.0621
2026-01-04 00:45:08,475 - INFO -   Batch 1150/2527: Loss = 0.0619
2026-01-04 00:45:37,284 - INFO -   Batch 1200/2527: Loss = 0.0587
2026-01-04 00:46:05,897 - INFO -   Batch 1250/2527: Loss = 0.0713
2026-01-04 00:46:34,970 - INFO -   Batch 1300/2527: Loss = 0.0660
2026-01-04 00:47:03,849 - INFO -   Batch 1350/2527: Loss = 0.0598
2026-01-04 00:47:32,457 - INFO -   Batch 1400/2527: Loss = 0.0424
2026-01-04 00:48:01,241 - INFO -   Batch 1450/2527: Loss = 0.0591
2026-01-04 00:48:29,920 - INFO -   Batch 1500/2527: Loss = 0.0569
2026-01-04 00:48:58,658 - INFO -   Batch 1550/2527: Loss = 0.0453
2026-01-04 00:49:27,445 - INFO -   Batch 1600/2527: Loss = 0.0553
2026-01-04 00:49:56,253 - INFO -   Batch 1650/2527: Loss = 0.0645
2026-01-04 00:50:25,431 - INFO -   Batch 1700/2527: Loss = 0.0774
2026-01-04 00:50:54,112 - INFO -   Batch 1750/2527: Loss = 0.0829
2026-01-04 00:51:22,924 - INFO -   Batch 1800/2527: Loss = 0.0468
2026-01-04 00:51:51,545 - INFO -   Batch 1850/2527: Loss = 0.0483
2026-01-04 00:52:20,124 - INFO -   Batch 1900/2527: Loss = 0.0780
2026-01-04 00:52:49,211 - INFO -   Batch 1950/2527: Loss = 0.0808
2026-01-04 00:53:17,873 - INFO -   Batch 2000/2527: Loss = 0.0609
2026-01-04 00:53:46,872 - INFO -   Batch 2050/2527: Loss = 0.0858
2026-01-04 00:54:15,800 - INFO -   Batch 2100/2527: Loss = 0.0462
2026-01-04 00:54:44,721 - INFO -   Batch 2150/2527: Loss = 0.0502
2026-01-04 00:55:13,246 - INFO -   Batch 2200/2527: Loss = 0.0645
2026-01-04 00:55:41,822 - INFO -   Batch 2250/2527: Loss = 0.0774
2026-01-04 00:56:10,645 - INFO -   Batch 2300/2527: Loss = 0.0726
2026-01-04 00:56:39,588 - INFO -   Batch 2350/2527: Loss = 0.0863
2026-01-04 00:57:08,380 - INFO -   Batch 2400/2527: Loss = 0.0741
2026-01-04 00:57:37,157 - INFO -   Batch 2450/2527: Loss = 0.0647
2026-01-04 00:58:05,919 - INFO -   Batch 2500/2527: Loss = 0.0593
2026-01-04 01:02:44,561 - INFO - Epoch 107/160: Train Loss: 0.0636, Val Loss: 1.4173, Val mIoU: 39.98%, Val PixelAcc: 78.81%
2026-01-04 01:03:15,171 - INFO -   Batch 50/2527: Loss = 0.0551
2026-01-04 01:03:44,148 - INFO -   Batch 100/2527: Loss = 0.0626
2026-01-04 01:04:12,697 - INFO -   Batch 150/2527: Loss = 0.0606
2026-01-04 01:04:41,480 - INFO -   Batch 200/2527: Loss = 0.0480
2026-01-04 01:05:10,171 - INFO -   Batch 250/2527: Loss = 0.0710
2026-01-04 01:05:39,050 - INFO -   Batch 300/2527: Loss = 0.0610
2026-01-04 01:06:07,679 - INFO -   Batch 350/2527: Loss = 0.0688
2026-01-04 01:06:36,685 - INFO -   Batch 400/2527: Loss = 0.0696
2026-01-04 01:07:05,443 - INFO -   Batch 450/2527: Loss = 0.0575
2026-01-04 01:07:34,171 - INFO -   Batch 500/2527: Loss = 0.0695
2026-01-04 01:08:03,100 - INFO -   Batch 550/2527: Loss = 0.0389
2026-01-04 01:08:31,627 - INFO -   Batch 600/2527: Loss = 0.0565
2026-01-04 01:09:00,496 - INFO -   Batch 650/2527: Loss = 0.0661
2026-01-04 01:09:29,176 - INFO -   Batch 700/2527: Loss = 0.0737
2026-01-04 01:09:57,729 - INFO -   Batch 750/2527: Loss = 0.0385
2026-01-04 01:10:26,742 - INFO -   Batch 800/2527: Loss = 0.0604
2026-01-04 01:10:55,408 - INFO -   Batch 850/2527: Loss = 0.0580
2026-01-04 01:11:24,500 - INFO -   Batch 900/2527: Loss = 0.0404
2026-01-04 01:11:53,236 - INFO -   Batch 950/2527: Loss = 0.0529
2026-01-04 01:12:22,026 - INFO -   Batch 1000/2527: Loss = 0.0571
2026-01-04 01:12:50,655 - INFO -   Batch 1050/2527: Loss = 0.0776
2026-01-04 01:13:19,224 - INFO -   Batch 1100/2527: Loss = 0.0555
2026-01-04 01:13:48,060 - INFO -   Batch 1150/2527: Loss = 0.0543
2026-01-04 01:14:16,857 - INFO -   Batch 1200/2527: Loss = 0.0742
2026-01-04 01:14:45,750 - INFO -   Batch 1250/2527: Loss = 0.0503
2026-01-04 01:15:14,801 - INFO -   Batch 1300/2527: Loss = 0.0673
2026-01-04 01:15:43,724 - INFO -   Batch 1350/2527: Loss = 0.0442
2026-01-04 01:16:12,215 - INFO -   Batch 1400/2527: Loss = 0.0559
2026-01-04 01:16:40,867 - INFO -   Batch 1450/2527: Loss = 0.0745
2026-01-04 01:17:09,779 - INFO -   Batch 1500/2527: Loss = 0.0541
2026-01-04 01:17:38,461 - INFO -   Batch 1550/2527: Loss = 0.0654
2026-01-04 01:18:07,480 - INFO -   Batch 1600/2527: Loss = 0.0536
2026-01-04 01:18:36,175 - INFO -   Batch 1650/2527: Loss = 0.0466
2026-01-04 01:19:04,938 - INFO -   Batch 1700/2527: Loss = 0.0548
2026-01-04 01:19:33,906 - INFO -   Batch 1750/2527: Loss = 0.0465
2026-01-04 01:20:02,488 - INFO -   Batch 1800/2527: Loss = 0.0854
2026-01-04 01:20:31,392 - INFO -   Batch 1850/2527: Loss = 0.0739
2026-01-04 01:21:00,106 - INFO -   Batch 1900/2527: Loss = 0.0445
2026-01-04 01:21:28,994 - INFO -   Batch 1950/2527: Loss = 0.0541
2026-01-04 01:21:57,842 - INFO -   Batch 2000/2527: Loss = 0.0486
2026-01-04 01:22:26,465 - INFO -   Batch 2050/2527: Loss = 0.0472
2026-01-04 01:22:55,499 - INFO -   Batch 2100/2527: Loss = 0.0868
2026-01-04 01:23:24,250 - INFO -   Batch 2150/2527: Loss = 0.1133
2026-01-04 01:23:53,051 - INFO -   Batch 2200/2527: Loss = 0.0613
2026-01-04 01:24:21,712 - INFO -   Batch 2250/2527: Loss = 0.0809
2026-01-04 01:24:50,627 - INFO -   Batch 2300/2527: Loss = 0.0372
2026-01-04 01:25:19,178 - INFO -   Batch 2350/2527: Loss = 0.0744
2026-01-04 01:25:47,939 - INFO -   Batch 2400/2527: Loss = 0.0511
2026-01-04 01:26:16,870 - INFO -   Batch 2450/2527: Loss = 0.0673
2026-01-04 01:26:45,668 - INFO -   Batch 2500/2527: Loss = 0.0471
2026-01-04 01:31:31,343 - INFO - Epoch 108/160: Train Loss: 0.0624, Val Loss: 1.3832, Val mIoU: 40.05%, Val PixelAcc: 78.86%
2026-01-04 01:32:02,052 - INFO -   Batch 50/2527: Loss = 0.0551
2026-01-04 01:32:30,630 - INFO -   Batch 100/2527: Loss = 0.0526
2026-01-04 01:32:59,443 - INFO -   Batch 150/2527: Loss = 0.0571
2026-01-04 01:33:28,329 - INFO -   Batch 200/2527: Loss = 0.0645
2026-01-04 01:33:57,159 - INFO -   Batch 250/2527: Loss = 0.0744
2026-01-04 01:34:25,776 - INFO -   Batch 300/2527: Loss = 0.0449
2026-01-04 01:34:54,656 - INFO -   Batch 350/2527: Loss = 0.0542
2026-01-04 01:35:23,571 - INFO -   Batch 400/2527: Loss = 0.0527
2026-01-04 01:35:52,088 - INFO -   Batch 450/2527: Loss = 0.0530
2026-01-04 01:36:20,890 - INFO -   Batch 500/2527: Loss = 0.0601
2026-01-04 01:36:49,530 - INFO -   Batch 550/2527: Loss = 0.0790
2026-01-04 01:37:18,245 - INFO -   Batch 600/2527: Loss = 0.0762
2026-01-04 01:37:47,062 - INFO -   Batch 650/2527: Loss = 0.0680
2026-01-04 01:38:15,751 - INFO -   Batch 700/2527: Loss = 0.0494
2026-01-04 01:38:44,849 - INFO -   Batch 750/2527: Loss = 0.0492
2026-01-04 01:39:13,577 - INFO -   Batch 800/2527: Loss = 0.0683
2026-01-04 01:39:42,345 - INFO -   Batch 850/2527: Loss = 0.0513
2026-01-04 01:40:10,894 - INFO -   Batch 900/2527: Loss = 0.0648
2026-01-04 01:40:39,424 - INFO -   Batch 950/2527: Loss = 0.0564
2026-01-04 01:41:08,454 - INFO -   Batch 1000/2527: Loss = 0.0776
2026-01-04 01:41:37,058 - INFO -   Batch 1050/2527: Loss = 0.0500
2026-01-04 01:42:05,986 - INFO -   Batch 1100/2527: Loss = 0.0461
2026-01-04 01:42:34,854 - INFO -   Batch 1150/2527: Loss = 0.0765
2026-01-04 01:43:03,422 - INFO -   Batch 1200/2527: Loss = 0.0599
2026-01-04 01:43:32,227 - INFO -   Batch 1250/2527: Loss = 0.0538
2026-01-04 01:44:00,847 - INFO -   Batch 1300/2527: Loss = 0.0429
2026-01-04 01:44:29,736 - INFO -   Batch 1350/2527: Loss = 0.0499
2026-01-04 01:44:58,720 - INFO -   Batch 1400/2527: Loss = 0.0569
2026-01-04 01:45:27,565 - INFO -   Batch 1450/2527: Loss = 0.0740
2026-01-04 01:45:56,309 - INFO -   Batch 1500/2527: Loss = 0.0686
2026-01-04 01:46:25,054 - INFO -   Batch 1550/2527: Loss = 0.0666
2026-01-04 01:46:54,006 - INFO -   Batch 1600/2527: Loss = 0.0589
2026-01-04 01:47:22,615 - INFO -   Batch 1650/2527: Loss = 0.0598
2026-01-04 01:47:51,438 - INFO -   Batch 1700/2527: Loss = 0.0489
2026-01-04 01:48:20,082 - INFO -   Batch 1750/2527: Loss = 0.0612
2026-01-04 01:48:49,118 - INFO -   Batch 1800/2527: Loss = 0.0795
2026-01-04 01:49:17,705 - INFO -   Batch 1850/2527: Loss = 0.0942
2026-01-04 01:49:46,323 - INFO -   Batch 1900/2527: Loss = 0.0498
2026-01-04 01:50:15,384 - INFO -   Batch 1950/2527: Loss = 0.0403
2026-01-04 01:50:44,148 - INFO -   Batch 2000/2527: Loss = 0.0681
2026-01-04 01:51:12,913 - INFO -   Batch 2050/2527: Loss = 0.0759
2026-01-04 01:51:41,545 - INFO -   Batch 2100/2527: Loss = 0.0844
2026-01-04 01:52:10,078 - INFO -   Batch 2150/2527: Loss = 0.0456
2026-01-04 01:52:39,136 - INFO -   Batch 2200/2527: Loss = 0.0587
2026-01-04 01:53:07,737 - INFO -   Batch 2250/2527: Loss = 0.0646
2026-01-04 01:53:36,694 - INFO -   Batch 2300/2527: Loss = 0.0693
2026-01-04 01:54:05,523 - INFO -   Batch 2350/2527: Loss = 0.0525
2026-01-04 01:54:34,444 - INFO -   Batch 2400/2527: Loss = 0.0768
2026-01-04 01:55:03,022 - INFO -   Batch 2450/2527: Loss = 0.0623
2026-01-04 01:55:31,512 - INFO -   Batch 2500/2527: Loss = 0.0770
2026-01-04 02:00:11,923 - INFO - Epoch 109/160: Train Loss: 0.0615, Val Loss: 1.4052, Val mIoU: 39.79%, Val PixelAcc: 78.56%
2026-01-04 02:00:42,118 - INFO -   Batch 50/2527: Loss = 0.0925
2026-01-04 02:01:10,833 - INFO -   Batch 100/2527: Loss = 0.0604
2026-01-04 02:01:39,735 - INFO -   Batch 150/2527: Loss = 0.0400
2026-01-04 02:02:08,283 - INFO -   Batch 200/2527: Loss = 0.0634
2026-01-04 02:02:37,087 - INFO -   Batch 250/2527: Loss = 0.0435
2026-01-04 02:03:05,730 - INFO -   Batch 300/2527: Loss = 0.0504
2026-01-04 02:03:34,757 - INFO -   Batch 350/2527: Loss = 0.0451
2026-01-04 02:04:03,322 - INFO -   Batch 400/2527: Loss = 0.0421
2026-01-04 02:04:31,951 - INFO -   Batch 450/2527: Loss = 0.0407
2026-01-04 02:05:01,042 - INFO -   Batch 500/2527: Loss = 0.0445
2026-01-04 02:05:29,748 - INFO -   Batch 550/2527: Loss = 0.0612
2026-01-04 02:05:58,532 - INFO -   Batch 600/2527: Loss = 0.0568
2026-01-04 02:06:27,107 - INFO -   Batch 650/2527: Loss = 0.0538
2026-01-04 02:06:55,953 - INFO -   Batch 700/2527: Loss = 0.0517
2026-01-04 02:07:24,674 - INFO -   Batch 750/2527: Loss = 0.0614
2026-01-04 02:07:53,235 - INFO -   Batch 800/2527: Loss = 0.0703
2026-01-04 02:08:22,197 - INFO -   Batch 850/2527: Loss = 0.0711
2026-01-04 02:08:51,050 - INFO -   Batch 900/2527: Loss = 0.0649
2026-01-04 02:09:19,967 - INFO -   Batch 950/2527: Loss = 0.0435
2026-01-04 02:09:48,522 - INFO -   Batch 1000/2527: Loss = 0.0595
2026-01-04 02:10:17,014 - INFO -   Batch 1050/2527: Loss = 0.0728
2026-01-04 02:10:45,900 - INFO -   Batch 1100/2527: Loss = 0.0735
2026-01-04 02:11:14,874 - INFO -   Batch 1150/2527: Loss = 0.0604
2026-01-04 02:11:43,713 - INFO -   Batch 1200/2527: Loss = 0.0660
2026-01-04 02:12:12,513 - INFO -   Batch 1250/2527: Loss = 0.0909
2026-01-04 02:12:41,591 - INFO -   Batch 1300/2527: Loss = 0.0612
2026-01-04 02:13:10,326 - INFO -   Batch 1350/2527: Loss = 0.0872
2026-01-04 02:13:38,811 - INFO -   Batch 1400/2527: Loss = 0.0888
2026-01-04 02:14:07,587 - INFO -   Batch 1450/2527: Loss = 0.0419
2026-01-04 02:14:36,241 - INFO -   Batch 1500/2527: Loss = 0.0677
2026-01-04 02:15:05,278 - INFO -   Batch 1550/2527: Loss = 0.0620
2026-01-04 02:15:33,860 - INFO -   Batch 1600/2527: Loss = 0.0481
2026-01-04 02:16:02,845 - INFO -   Batch 1650/2527: Loss = 0.0441
2026-01-04 02:16:31,600 - INFO -   Batch 1700/2527: Loss = 0.0560
2026-01-04 02:17:00,320 - INFO -   Batch 1750/2527: Loss = 0.0889
2026-01-04 02:17:29,151 - INFO -   Batch 1800/2527: Loss = 0.0503
2026-01-04 02:17:57,793 - INFO -   Batch 1850/2527: Loss = 0.0476
2026-01-04 02:18:26,655 - INFO -   Batch 1900/2527: Loss = 0.0687
2026-01-04 02:18:55,483 - INFO -   Batch 1950/2527: Loss = 0.0499
2026-01-04 02:19:23,967 - INFO -   Batch 2000/2527: Loss = 0.0464
2026-01-04 02:19:52,925 - INFO -   Batch 2050/2527: Loss = 0.0498
2026-01-04 02:20:21,735 - INFO -   Batch 2100/2527: Loss = 0.0766
2026-01-04 02:20:50,647 - INFO -   Batch 2150/2527: Loss = 0.0492
2026-01-04 02:21:19,256 - INFO -   Batch 2200/2527: Loss = 0.0595
2026-01-04 02:21:48,015 - INFO -   Batch 2250/2527: Loss = 0.0470
2026-01-04 02:22:16,649 - INFO -   Batch 2300/2527: Loss = 0.0477
2026-01-04 02:22:45,366 - INFO -   Batch 2350/2527: Loss = 0.0409
2026-01-04 02:23:14,172 - INFO -   Batch 2400/2527: Loss = 0.1032
2026-01-04 02:23:42,931 - INFO -   Batch 2450/2527: Loss = 0.0578
2026-01-04 02:24:11,995 - INFO -   Batch 2500/2527: Loss = 0.0578
2026-01-04 02:28:50,520 - INFO - Epoch 110/160: Train Loss: 0.0617, Val Loss: 1.3869, Val mIoU: 39.45%, Val PixelAcc: 78.54%
2026-01-04 02:28:54,262 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_110.pth
2026-01-04 02:28:54,265 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_110.pth
2026-01-04 02:29:25,054 - INFO -   Batch 50/2527: Loss = 0.0772
2026-01-04 02:29:54,004 - INFO -   Batch 100/2527: Loss = 0.0722
2026-01-04 02:30:22,864 - INFO -   Batch 150/2527: Loss = 0.0600
2026-01-04 02:30:51,478 - INFO -   Batch 200/2527: Loss = 0.0764
2026-01-04 02:31:20,278 - INFO -   Batch 250/2527: Loss = 0.0720
2026-01-04 02:31:48,988 - INFO -   Batch 300/2527: Loss = 0.1029
2026-01-04 02:32:17,520 - INFO -   Batch 350/2527: Loss = 0.0578
2026-01-04 02:32:46,511 - INFO -   Batch 400/2527: Loss = 0.0640
2026-01-04 02:33:15,206 - INFO -   Batch 450/2527: Loss = 0.0682
2026-01-04 02:33:44,315 - INFO -   Batch 500/2527: Loss = 0.0579
2026-01-04 02:34:13,025 - INFO -   Batch 550/2527: Loss = 0.0489
2026-01-04 02:34:41,854 - INFO -   Batch 600/2527: Loss = 0.0586
2026-01-04 02:35:10,371 - INFO -   Batch 650/2527: Loss = 0.0628
2026-01-04 02:35:39,065 - INFO -   Batch 700/2527: Loss = 0.0591
2026-01-04 02:36:07,957 - INFO -   Batch 750/2527: Loss = 0.0599
2026-01-04 02:36:36,748 - INFO -   Batch 800/2527: Loss = 0.0516
2026-01-04 02:37:05,705 - INFO -   Batch 850/2527: Loss = 0.0388
2026-01-04 02:37:34,530 - INFO -   Batch 900/2527: Loss = 0.0652
2026-01-04 02:38:03,109 - INFO -   Batch 950/2527: Loss = 0.0608
2026-01-04 02:38:31,905 - INFO -   Batch 1000/2527: Loss = 0.0553
2026-01-04 02:39:00,591 - INFO -   Batch 1050/2527: Loss = 0.0505
2026-01-04 02:39:29,444 - INFO -   Batch 1100/2527: Loss = 0.0496
2026-01-04 02:39:58,062 - INFO -   Batch 1150/2527: Loss = 0.0585
2026-01-04 02:40:27,053 - INFO -   Batch 1200/2527: Loss = 0.0596
2026-01-04 02:40:55,775 - INFO -   Batch 1250/2527: Loss = 0.0581
2026-01-04 02:41:24,502 - INFO -   Batch 1300/2527: Loss = 0.0742
2026-01-04 02:41:53,453 - INFO -   Batch 1350/2527: Loss = 0.0522
2026-01-04 02:42:22,007 - INFO -   Batch 1400/2527: Loss = 0.0721
2026-01-04 02:42:50,872 - INFO -   Batch 1450/2527: Loss = 0.0715
2026-01-04 02:43:19,481 - INFO -   Batch 1500/2527: Loss = 0.0567
2026-01-04 02:43:48,003 - INFO -   Batch 1550/2527: Loss = 0.0537
2026-01-04 02:44:17,004 - INFO -   Batch 1600/2527: Loss = 0.0596
2026-01-04 02:44:45,670 - INFO -   Batch 1650/2527: Loss = 0.0709
2026-01-04 02:45:14,770 - INFO -   Batch 1700/2527: Loss = 0.0581
2026-01-04 02:45:43,484 - INFO -   Batch 1750/2527: Loss = 0.0537
2026-01-04 02:46:12,294 - INFO -   Batch 1800/2527: Loss = 0.0504
2026-01-04 02:46:41,115 - INFO -   Batch 1850/2527: Loss = 0.0677
2026-01-04 02:47:09,695 - INFO -   Batch 1900/2527: Loss = 0.0451
2026-01-04 02:47:38,534 - INFO -   Batch 1950/2527: Loss = 0.0570
2026-01-04 02:48:07,337 - INFO -   Batch 2000/2527: Loss = 0.0589
2026-01-04 02:48:36,247 - INFO -   Batch 2050/2527: Loss = 0.0659
2026-01-04 02:49:05,145 - INFO -   Batch 2100/2527: Loss = 0.0426
2026-01-04 02:49:34,087 - INFO -   Batch 2150/2527: Loss = 0.0513
2026-01-04 02:50:02,688 - INFO -   Batch 2200/2527: Loss = 0.0569
2026-01-04 02:50:31,203 - INFO -   Batch 2250/2527: Loss = 0.0792
2026-01-04 02:51:00,113 - INFO -   Batch 2300/2527: Loss = 0.0658
2026-01-04 02:51:28,712 - INFO -   Batch 2350/2527: Loss = 0.0448
2026-01-04 02:51:57,699 - INFO -   Batch 2400/2527: Loss = 0.0909
2026-01-04 02:52:26,414 - INFO -   Batch 2450/2527: Loss = 0.0924
2026-01-04 02:52:55,135 - INFO -   Batch 2500/2527: Loss = 0.0468
2026-01-04 02:57:41,647 - INFO - Epoch 111/160: Train Loss: 0.0610, Val Loss: 1.4110, Val mIoU: 40.01%, Val PixelAcc: 78.95%
2026-01-04 02:58:12,069 - INFO -   Batch 50/2527: Loss = 0.0719
2026-01-04 02:58:40,819 - INFO -   Batch 100/2527: Loss = 0.0604
2026-01-04 02:59:09,473 - INFO -   Batch 150/2527: Loss = 0.0691
2026-01-04 02:59:38,159 - INFO -   Batch 200/2527: Loss = 0.0601
2026-01-04 03:00:06,963 - INFO -   Batch 250/2527: Loss = 0.0471
2026-01-04 03:00:35,719 - INFO -   Batch 300/2527: Loss = 0.0685
2026-01-04 03:01:04,760 - INFO -   Batch 350/2527: Loss = 0.0441
2026-01-04 03:01:33,459 - INFO -   Batch 400/2527: Loss = 0.0441
2026-01-04 03:02:01,924 - INFO -   Batch 450/2527: Loss = 0.0537
2026-01-04 03:02:30,777 - INFO -   Batch 500/2527: Loss = 0.0716
2026-01-04 03:02:59,415 - INFO -   Batch 550/2527: Loss = 0.0893
2026-01-04 03:03:28,501 - INFO -   Batch 600/2527: Loss = 0.0744
2026-01-04 03:03:57,103 - INFO -   Batch 650/2527: Loss = 0.0587
2026-01-04 03:04:26,070 - INFO -   Batch 700/2527: Loss = 0.0463
2026-01-04 03:04:54,970 - INFO -   Batch 750/2527: Loss = 0.0576
2026-01-04 03:05:23,533 - INFO -   Batch 800/2527: Loss = 0.0554
2026-01-04 03:05:52,369 - INFO -   Batch 850/2527: Loss = 0.0922
2026-01-04 03:06:20,944 - INFO -   Batch 900/2527: Loss = 0.0606
2026-01-04 03:06:49,818 - INFO -   Batch 950/2527: Loss = 0.0686
2026-01-04 03:07:18,668 - INFO -   Batch 1000/2527: Loss = 0.0505
2026-01-04 03:07:47,444 - INFO -   Batch 1050/2527: Loss = 0.0557
2026-01-04 03:08:16,086 - INFO -   Batch 1100/2527: Loss = 0.0483
2026-01-04 03:08:45,000 - INFO -   Batch 1150/2527: Loss = 0.0660
2026-01-04 03:09:13,872 - INFO -   Batch 1200/2527: Loss = 0.0595
2026-01-04 03:09:42,435 - INFO -   Batch 1250/2527: Loss = 0.0751
2026-01-04 03:10:11,198 - INFO -   Batch 1300/2527: Loss = 0.0503
2026-01-04 03:10:39,902 - INFO -   Batch 1350/2527: Loss = 0.0330
2026-01-04 03:11:08,684 - INFO -   Batch 1400/2527: Loss = 0.0914
2026-01-04 03:11:37,529 - INFO -   Batch 1450/2527: Loss = 0.0702
2026-01-04 03:12:06,218 - INFO -   Batch 1500/2527: Loss = 0.0355
2026-01-04 03:12:35,324 - INFO -   Batch 1550/2527: Loss = 0.0816
2026-01-04 03:13:03,958 - INFO -   Batch 1600/2527: Loss = 0.0530
2026-01-04 03:13:32,759 - INFO -   Batch 1650/2527: Loss = 0.0497
2026-01-04 03:14:01,311 - INFO -   Batch 1700/2527: Loss = 0.0640
2026-01-04 03:14:29,862 - INFO -   Batch 1750/2527: Loss = 0.0663
2026-01-04 03:15:03,171 - INFO -   Batch 1800/2527: Loss = 0.0551
2026-01-04 03:15:31,774 - INFO -   Batch 1850/2527: Loss = 0.0634
2026-01-04 03:16:00,765 - INFO -   Batch 1900/2527: Loss = 0.0490
2026-01-04 03:16:29,855 - INFO -   Batch 1950/2527: Loss = 0.0529
2026-01-04 03:16:58,732 - INFO -   Batch 2000/2527: Loss = 0.0629
2026-01-04 03:17:27,238 - INFO -   Batch 2050/2527: Loss = 0.0743
2026-01-04 03:17:55,848 - INFO -   Batch 2100/2527: Loss = 0.0485
2026-01-04 03:18:24,679 - INFO -   Batch 2150/2527: Loss = 0.0628
2026-01-04 03:18:53,555 - INFO -   Batch 2200/2527: Loss = 0.0770
2026-01-04 03:19:22,342 - INFO -   Batch 2250/2527: Loss = 0.0742
2026-01-04 03:19:51,089 - INFO -   Batch 2300/2527: Loss = 0.0505
2026-01-04 03:20:19,845 - INFO -   Batch 2350/2527: Loss = 0.0594
2026-01-04 03:20:48,749 - INFO -   Batch 2400/2527: Loss = 0.0645
2026-01-04 03:21:17,284 - INFO -   Batch 2450/2527: Loss = 0.0599
2026-01-04 03:21:46,086 - INFO -   Batch 2500/2527: Loss = 0.0459
2026-01-04 03:26:24,252 - INFO - Epoch 112/160: Train Loss: 0.0595, Val Loss: 1.4251, Val mIoU: 39.96%, Val PixelAcc: 78.84%
2026-01-04 03:26:54,642 - INFO -   Batch 50/2527: Loss = 0.0780
2026-01-04 03:27:23,694 - INFO -   Batch 100/2527: Loss = 0.0528
2026-01-04 03:27:52,346 - INFO -   Batch 150/2527: Loss = 0.0619
2026-01-04 03:28:21,130 - INFO -   Batch 200/2527: Loss = 0.0656
2026-01-04 03:28:49,665 - INFO -   Batch 250/2527: Loss = 0.0384
2026-01-04 03:29:18,184 - INFO -   Batch 300/2527: Loss = 0.0353
2026-01-04 03:29:47,239 - INFO -   Batch 350/2527: Loss = 0.0408
2026-01-04 03:30:15,870 - INFO -   Batch 400/2527: Loss = 0.0567
2026-01-04 03:30:44,852 - INFO -   Batch 450/2527: Loss = 0.0653
2026-01-04 03:31:13,741 - INFO -   Batch 500/2527: Loss = 0.0428
2026-01-04 03:31:42,699 - INFO -   Batch 550/2527: Loss = 0.0423
2026-01-04 03:32:11,296 - INFO -   Batch 600/2527: Loss = 0.0497
2026-01-04 03:32:39,754 - INFO -   Batch 650/2527: Loss = 0.0484
2026-01-04 03:33:08,613 - INFO -   Batch 700/2527: Loss = 0.0808
2026-01-04 03:33:37,424 - INFO -   Batch 750/2527: Loss = 0.0513
2026-01-04 03:34:06,223 - INFO -   Batch 800/2527: Loss = 0.0755
2026-01-04 03:34:35,002 - INFO -   Batch 850/2527: Loss = 0.0692
2026-01-04 03:35:04,087 - INFO -   Batch 900/2527: Loss = 0.0510
2026-01-04 03:35:32,663 - INFO -   Batch 950/2527: Loss = 0.0591
2026-01-04 03:36:01,215 - INFO -   Batch 1000/2527: Loss = 0.0513
2026-01-04 03:36:29,964 - INFO -   Batch 1050/2527: Loss = 0.0477
2026-01-04 03:36:58,635 - INFO -   Batch 1100/2527: Loss = 0.0501
2026-01-04 03:37:27,689 - INFO -   Batch 1150/2527: Loss = 0.0583
2026-01-04 03:37:56,230 - INFO -   Batch 1200/2527: Loss = 0.0389
2026-01-04 03:38:24,852 - INFO -   Batch 1250/2527: Loss = 0.0806
2026-01-04 03:38:53,952 - INFO -   Batch 1300/2527: Loss = 0.0468
2026-01-04 03:39:22,605 - INFO -   Batch 1350/2527: Loss = 0.0561
2026-01-04 03:39:51,445 - INFO -   Batch 1400/2527: Loss = 0.0545
2026-01-04 03:40:19,967 - INFO -   Batch 1450/2527: Loss = 0.0596
2026-01-04 03:40:48,827 - INFO -   Batch 1500/2527: Loss = 0.0562
2026-01-04 03:41:17,694 - INFO -   Batch 1550/2527: Loss = 0.0768
2026-01-04 03:41:46,163 - INFO -   Batch 1600/2527: Loss = 0.0592
2026-01-04 03:42:15,151 - INFO -   Batch 1650/2527: Loss = 0.0868
2026-01-04 03:42:44,198 - INFO -   Batch 1700/2527: Loss = 0.0811
2026-01-04 03:43:13,070 - INFO -   Batch 1750/2527: Loss = 0.0556
2026-01-04 03:43:41,690 - INFO -   Batch 1800/2527: Loss = 0.0788
2026-01-04 03:44:10,166 - INFO -   Batch 1850/2527: Loss = 0.0579
2026-01-04 03:44:39,021 - INFO -   Batch 1900/2527: Loss = 0.0693
2026-01-04 03:45:07,906 - INFO -   Batch 1950/2527: Loss = 0.0457
2026-01-04 03:45:36,698 - INFO -   Batch 2000/2527: Loss = 0.0568
2026-01-04 03:46:05,474 - INFO -   Batch 2050/2527: Loss = 0.0763
2026-01-04 03:46:34,611 - INFO -   Batch 2100/2527: Loss = 0.0469
2026-01-04 03:47:03,283 - INFO -   Batch 2150/2527: Loss = 0.0469
2026-01-04 03:47:31,753 - INFO -   Batch 2200/2527: Loss = 0.0701
2026-01-04 03:48:00,546 - INFO -   Batch 2250/2527: Loss = 0.0500
2026-01-04 03:48:29,174 - INFO -   Batch 2300/2527: Loss = 0.0489
2026-01-04 03:48:58,246 - INFO -   Batch 2350/2527: Loss = 0.0485
2026-01-04 03:49:26,855 - INFO -   Batch 2400/2527: Loss = 0.0469
2026-01-04 03:49:55,834 - INFO -   Batch 2450/2527: Loss = 0.0570
2026-01-04 03:50:24,594 - INFO -   Batch 2500/2527: Loss = 0.0615
2026-01-04 03:55:04,197 - INFO - Epoch 113/160: Train Loss: 0.0594, Val Loss: 1.4109, Val mIoU: 40.16%, Val PixelAcc: 78.79%
2026-01-04 03:55:35,004 - INFO -   Batch 50/2527: Loss = 0.1398
2026-01-04 03:56:03,872 - INFO -   Batch 100/2527: Loss = 0.0463
2026-01-04 03:56:32,405 - INFO -   Batch 150/2527: Loss = 0.0616
2026-01-04 03:57:01,181 - INFO -   Batch 200/2527: Loss = 0.0571
2026-01-04 03:57:29,864 - INFO -   Batch 250/2527: Loss = 0.0517
2026-01-04 03:57:58,714 - INFO -   Batch 300/2527: Loss = 0.0477
2026-01-04 03:58:27,333 - INFO -   Batch 350/2527: Loss = 0.0696
2026-01-04 03:58:56,348 - INFO -   Batch 400/2527: Loss = 0.0458
2026-01-04 03:59:25,060 - INFO -   Batch 450/2527: Loss = 0.0584
2026-01-04 03:59:53,806 - INFO -   Batch 500/2527: Loss = 0.0731
2026-01-04 04:00:22,718 - INFO -   Batch 550/2527: Loss = 0.0675
2026-01-04 04:00:51,252 - INFO -   Batch 600/2527: Loss = 0.0670
2026-01-04 04:01:20,111 - INFO -   Batch 650/2527: Loss = 0.0582
2026-01-04 04:01:48,809 - INFO -   Batch 700/2527: Loss = 0.0650
2026-01-04 04:02:17,332 - INFO -   Batch 750/2527: Loss = 0.0758
2026-01-04 04:02:46,365 - INFO -   Batch 800/2527: Loss = 0.0593
2026-01-04 04:03:15,034 - INFO -   Batch 850/2527: Loss = 0.0825
2026-01-04 04:03:44,113 - INFO -   Batch 900/2527: Loss = 0.0466
2026-01-04 04:04:12,794 - INFO -   Batch 950/2527: Loss = 0.0459
2026-01-04 04:04:41,560 - INFO -   Batch 1000/2527: Loss = 0.0731
2026-01-04 04:05:10,196 - INFO -   Batch 1050/2527: Loss = 0.0649
2026-01-04 04:05:38,777 - INFO -   Batch 1100/2527: Loss = 0.0546
2026-01-04 04:06:07,608 - INFO -   Batch 1150/2527: Loss = 0.0994
2026-01-04 04:06:36,420 - INFO -   Batch 1200/2527: Loss = 0.0648
2026-01-04 04:07:05,347 - INFO -   Batch 1250/2527: Loss = 0.0507
2026-01-04 04:07:34,361 - INFO -   Batch 1300/2527: Loss = 0.0505
2026-01-04 04:08:03,275 - INFO -   Batch 1350/2527: Loss = 0.0980
2026-01-04 04:08:31,757 - INFO -   Batch 1400/2527: Loss = 0.0350
2026-01-04 04:09:00,400 - INFO -   Batch 1450/2527: Loss = 0.0573
2026-01-04 04:09:29,299 - INFO -   Batch 1500/2527: Loss = 0.0394
2026-01-04 04:09:57,982 - INFO -   Batch 1550/2527: Loss = 0.0468
2026-01-04 04:10:26,982 - INFO -   Batch 1600/2527: Loss = 0.0513
2026-01-04 04:10:55,688 - INFO -   Batch 1650/2527: Loss = 0.0557
2026-01-04 04:11:24,388 - INFO -   Batch 1700/2527: Loss = 0.0703
2026-01-04 04:11:53,336 - INFO -   Batch 1750/2527: Loss = 0.0577
2026-01-04 04:12:21,866 - INFO -   Batch 1800/2527: Loss = 0.0733
2026-01-04 04:12:50,759 - INFO -   Batch 1850/2527: Loss = 0.0650
2026-01-04 04:13:19,449 - INFO -   Batch 1900/2527: Loss = 0.0604
2026-01-04 04:13:48,294 - INFO -   Batch 1950/2527: Loss = 0.0634
2026-01-04 04:14:17,125 - INFO -   Batch 2000/2527: Loss = 0.0488
2026-01-04 04:14:45,738 - INFO -   Batch 2050/2527: Loss = 0.0587
2026-01-04 04:15:14,778 - INFO -   Batch 2100/2527: Loss = 0.0685
2026-01-04 04:15:43,541 - INFO -   Batch 2150/2527: Loss = 0.0660
2026-01-04 04:16:12,310 - INFO -   Batch 2200/2527: Loss = 0.0602
2026-01-04 04:16:40,927 - INFO -   Batch 2250/2527: Loss = 0.0617
2026-01-04 04:17:09,826 - INFO -   Batch 2300/2527: Loss = 0.0557
2026-01-04 04:17:38,347 - INFO -   Batch 2350/2527: Loss = 0.0605
2026-01-04 04:18:07,127 - INFO -   Batch 2400/2527: Loss = 0.0614
2026-01-04 04:18:36,070 - INFO -   Batch 2450/2527: Loss = 0.0631
2026-01-04 04:19:04,848 - INFO -   Batch 2500/2527: Loss = 0.0649
2026-01-04 04:23:48,777 - INFO - Epoch 114/160: Train Loss: 0.0588, Val Loss: 1.4103, Val mIoU: 40.04%, Val PixelAcc: 78.68%
2026-01-04 04:24:19,444 - INFO -   Batch 50/2527: Loss = 0.0485
2026-01-04 04:24:48,015 - INFO -   Batch 100/2527: Loss = 0.0634
2026-01-04 04:25:16,893 - INFO -   Batch 150/2527: Loss = 0.0570
2026-01-04 04:25:45,791 - INFO -   Batch 200/2527: Loss = 0.0640
2026-01-04 04:26:14,574 - INFO -   Batch 250/2527: Loss = 0.0479
2026-01-04 04:26:43,183 - INFO -   Batch 300/2527: Loss = 0.0479
2026-01-04 04:27:12,100 - INFO -   Batch 350/2527: Loss = 0.0535
2026-01-04 04:27:40,961 - INFO -   Batch 400/2527: Loss = 0.0546
2026-01-04 04:28:09,557 - INFO -   Batch 450/2527: Loss = 0.0684
2026-01-04 04:28:38,312 - INFO -   Batch 500/2527: Loss = 0.0585
2026-01-04 04:29:06,978 - INFO -   Batch 550/2527: Loss = 0.0656
2026-01-04 04:29:35,704 - INFO -   Batch 600/2527: Loss = 0.0539
2026-01-04 04:30:04,524 - INFO -   Batch 650/2527: Loss = 0.0479
2026-01-04 04:30:33,202 - INFO -   Batch 700/2527: Loss = 0.0477
2026-01-04 04:31:02,296 - INFO -   Batch 750/2527: Loss = 0.0965
2026-01-04 04:31:30,991 - INFO -   Batch 800/2527: Loss = 0.0626
2026-01-04 04:31:59,803 - INFO -   Batch 850/2527: Loss = 0.0679
2026-01-04 04:32:28,353 - INFO -   Batch 900/2527: Loss = 0.0638
2026-01-04 04:32:56,880 - INFO -   Batch 950/2527: Loss = 0.0722
2026-01-04 04:33:25,955 - INFO -   Batch 1000/2527: Loss = 0.0639
2026-01-04 04:33:54,495 - INFO -   Batch 1050/2527: Loss = 0.0584
2026-01-04 04:34:23,446 - INFO -   Batch 1100/2527: Loss = 0.0442
2026-01-04 04:34:52,303 - INFO -   Batch 1150/2527: Loss = 0.0463
2026-01-04 04:35:20,857 - INFO -   Batch 1200/2527: Loss = 0.0574
2026-01-04 04:35:49,660 - INFO -   Batch 1250/2527: Loss = 0.0550
2026-01-04 04:36:18,262 - INFO -   Batch 1300/2527: Loss = 0.0580
2026-01-04 04:36:47,098 - INFO -   Batch 1350/2527: Loss = 0.0617
2026-01-04 04:37:16,159 - INFO -   Batch 1400/2527: Loss = 0.0766
2026-01-04 04:37:44,952 - INFO -   Batch 1450/2527: Loss = 0.0580
2026-01-04 04:38:13,741 - INFO -   Batch 1500/2527: Loss = 0.0563
2026-01-04 04:38:42,499 - INFO -   Batch 1550/2527: Loss = 0.0685
2026-01-04 04:39:11,406 - INFO -   Batch 1600/2527: Loss = 0.0916
2026-01-04 04:39:39,995 - INFO -   Batch 1650/2527: Loss = 0.0529
2026-01-04 04:40:08,802 - INFO -   Batch 1700/2527: Loss = 0.0489
2026-01-04 04:40:37,446 - INFO -   Batch 1750/2527: Loss = 0.0425
2026-01-04 04:41:06,575 - INFO -   Batch 1800/2527: Loss = 0.0571
2026-01-04 04:41:35,129 - INFO -   Batch 1850/2527: Loss = 0.0636
2026-01-04 04:42:03,762 - INFO -   Batch 1900/2527: Loss = 0.0751
2026-01-04 04:42:32,877 - INFO -   Batch 1950/2527: Loss = 0.0883
2026-01-04 04:43:01,555 - INFO -   Batch 2000/2527: Loss = 0.0616
2026-01-04 04:43:30,410 - INFO -   Batch 2050/2527: Loss = 0.0396
2026-01-04 04:43:59,081 - INFO -   Batch 2100/2527: Loss = 0.0473
2026-01-04 04:44:27,589 - INFO -   Batch 2150/2527: Loss = 0.0607
2026-01-04 04:44:56,626 - INFO -   Batch 2200/2527: Loss = 0.0462
2026-01-04 04:45:25,267 - INFO -   Batch 2250/2527: Loss = 0.0618
2026-01-04 04:45:54,195 - INFO -   Batch 2300/2527: Loss = 0.0506
2026-01-04 04:46:23,067 - INFO -   Batch 2350/2527: Loss = 0.0571
2026-01-04 04:46:51,956 - INFO -   Batch 2400/2527: Loss = 0.0648
2026-01-04 04:47:20,599 - INFO -   Batch 2450/2527: Loss = 0.0779
2026-01-04 04:47:49,103 - INFO -   Batch 2500/2527: Loss = 0.0408
2026-01-04 04:52:29,806 - INFO - Epoch 115/160: Train Loss: 0.0583, Val Loss: 1.4147, Val mIoU: 39.98%, Val PixelAcc: 78.68%
2026-01-04 04:53:00,211 - INFO -   Batch 50/2527: Loss = 0.0451
2026-01-04 04:53:28,926 - INFO -   Batch 100/2527: Loss = 0.0483
2026-01-04 04:53:57,802 - INFO -   Batch 150/2527: Loss = 0.0623
2026-01-04 04:54:26,335 - INFO -   Batch 200/2527: Loss = 0.0648
2026-01-04 04:54:55,120 - INFO -   Batch 250/2527: Loss = 0.0582
2026-01-04 04:55:23,759 - INFO -   Batch 300/2527: Loss = 0.0439
2026-01-04 04:55:52,819 - INFO -   Batch 350/2527: Loss = 0.0727
2026-01-04 04:56:21,361 - INFO -   Batch 400/2527: Loss = 0.0531
2026-01-04 04:56:49,990 - INFO -   Batch 450/2527: Loss = 0.0444
2026-01-04 04:57:19,085 - INFO -   Batch 500/2527: Loss = 0.0668
2026-01-04 04:57:47,785 - INFO -   Batch 550/2527: Loss = 0.0845
2026-01-04 04:58:16,618 - INFO -   Batch 600/2527: Loss = 0.0447
2026-01-04 04:58:45,181 - INFO -   Batch 650/2527: Loss = 0.0558
2026-01-04 04:59:14,009 - INFO -   Batch 700/2527: Loss = 0.0487
2026-01-04 04:59:42,718 - INFO -   Batch 750/2527: Loss = 0.0809
2026-01-04 05:00:11,316 - INFO -   Batch 800/2527: Loss = 0.0864
2026-01-04 05:00:40,230 - INFO -   Batch 850/2527: Loss = 0.0619
2026-01-04 05:01:09,131 - INFO -   Batch 900/2527: Loss = 0.0623
2026-01-04 05:01:37,986 - INFO -   Batch 950/2527: Loss = 0.0614
2026-01-04 05:02:06,616 - INFO -   Batch 1000/2527: Loss = 0.0496
2026-01-04 05:02:35,094 - INFO -   Batch 1050/2527: Loss = 0.0322
2026-01-04 05:03:03,943 - INFO -   Batch 1100/2527: Loss = 0.0639
2026-01-04 05:03:32,928 - INFO -   Batch 1150/2527: Loss = 0.0398
2026-01-04 05:04:01,759 - INFO -   Batch 1200/2527: Loss = 0.0523
2026-01-04 05:04:30,484 - INFO -   Batch 1250/2527: Loss = 0.0782
2026-01-04 05:04:59,596 - INFO -   Batch 1300/2527: Loss = 0.0359
2026-01-04 05:05:28,254 - INFO -   Batch 1350/2527: Loss = 0.0551
2026-01-04 05:05:56,739 - INFO -   Batch 1400/2527: Loss = 0.0590
2026-01-04 05:06:25,551 - INFO -   Batch 1450/2527: Loss = 0.0909
2026-01-04 05:06:54,157 - INFO -   Batch 1500/2527: Loss = 0.0484
2026-01-04 05:07:23,227 - INFO -   Batch 1550/2527: Loss = 0.0529
2026-01-04 05:07:51,800 - INFO -   Batch 1600/2527: Loss = 0.0604
2026-01-04 05:08:20,719 - INFO -   Batch 1650/2527: Loss = 0.0389
2026-01-04 05:08:49,486 - INFO -   Batch 1700/2527: Loss = 0.0526
2026-01-04 05:09:18,228 - INFO -   Batch 1750/2527: Loss = 0.0570
2026-01-04 05:09:47,013 - INFO -   Batch 1800/2527: Loss = 0.0552
2026-01-04 05:10:15,685 - INFO -   Batch 1850/2527: Loss = 0.0572
2026-01-04 05:10:44,512 - INFO -   Batch 1900/2527: Loss = 0.0452
2026-01-04 05:11:13,385 - INFO -   Batch 1950/2527: Loss = 0.0596
2026-01-04 05:11:41,860 - INFO -   Batch 2000/2527: Loss = 0.0587
2026-01-04 05:12:10,813 - INFO -   Batch 2050/2527: Loss = 0.0682
2026-01-04 05:12:39,629 - INFO -   Batch 2100/2527: Loss = 0.0804
2026-01-04 05:13:08,552 - INFO -   Batch 2150/2527: Loss = 0.0515
2026-01-04 05:13:37,176 - INFO -   Batch 2200/2527: Loss = 0.0582
2026-01-04 05:14:05,985 - INFO -   Batch 2250/2527: Loss = 0.0460
2026-01-04 05:14:34,542 - INFO -   Batch 2300/2527: Loss = 0.1330
2026-01-04 05:15:03,258 - INFO -   Batch 2350/2527: Loss = 0.0678
2026-01-04 05:15:32,103 - INFO -   Batch 2400/2527: Loss = 0.0528
2026-01-04 05:16:00,810 - INFO -   Batch 2450/2527: Loss = 0.0436
2026-01-04 05:16:29,924 - INFO -   Batch 2500/2527: Loss = 0.0694
2026-01-04 05:21:08,316 - INFO - Epoch 116/160: Train Loss: 0.0581, Val Loss: 1.4328, Val mIoU: 40.15%, Val PixelAcc: 78.85%
2026-01-04 05:21:38,871 - INFO -   Batch 50/2527: Loss = 0.0729
2026-01-04 05:22:07,878 - INFO -   Batch 100/2527: Loss = 0.0515
2026-01-04 05:22:36,550 - INFO -   Batch 150/2527: Loss = 0.0509
2026-01-04 05:23:05,316 - INFO -   Batch 200/2527: Loss = 0.0502
2026-01-04 05:23:33,955 - INFO -   Batch 250/2527: Loss = 0.0514
2026-01-04 05:24:02,547 - INFO -   Batch 300/2527: Loss = 0.0612
2026-01-04 05:24:31,380 - INFO -   Batch 350/2527: Loss = 0.0420
2026-01-04 05:25:00,156 - INFO -   Batch 400/2527: Loss = 0.0500
2026-01-04 05:25:29,087 - INFO -   Batch 450/2527: Loss = 0.0502
2026-01-04 05:25:57,899 - INFO -   Batch 500/2527: Loss = 0.0410
2026-01-04 05:26:26,818 - INFO -   Batch 550/2527: Loss = 0.0430
2026-01-04 05:26:55,291 - INFO -   Batch 600/2527: Loss = 0.0409
2026-01-04 05:27:23,915 - INFO -   Batch 650/2527: Loss = 0.0568
2026-01-04 05:27:52,863 - INFO -   Batch 700/2527: Loss = 0.1135
2026-01-04 05:28:21,651 - INFO -   Batch 750/2527: Loss = 0.0597
2026-01-04 05:28:50,657 - INFO -   Batch 800/2527: Loss = 0.0629
2026-01-04 05:29:19,389 - INFO -   Batch 850/2527: Loss = 0.0511
2026-01-04 05:29:48,144 - INFO -   Batch 900/2527: Loss = 0.0511
2026-01-04 05:30:17,046 - INFO -   Batch 950/2527: Loss = 0.0471
2026-01-04 05:30:45,682 - INFO -   Batch 1000/2527: Loss = 0.0562
2026-01-04 05:31:14,511 - INFO -   Batch 1050/2527: Loss = 0.0468
2026-01-04 05:31:43,201 - INFO -   Batch 1100/2527: Loss = 0.0435
2026-01-04 05:32:12,055 - INFO -   Batch 1150/2527: Loss = 0.0439
2026-01-04 05:32:40,813 - INFO -   Batch 1200/2527: Loss = 0.0704
2026-01-04 05:33:09,445 - INFO -   Batch 1250/2527: Loss = 0.0754
2026-01-04 05:33:38,504 - INFO -   Batch 1300/2527: Loss = 0.0458
2026-01-04 05:34:07,230 - INFO -   Batch 1350/2527: Loss = 0.0642
2026-01-04 05:34:36,058 - INFO -   Batch 1400/2527: Loss = 0.0440
2026-01-04 05:35:04,725 - INFO -   Batch 1450/2527: Loss = 0.0453
2026-01-04 05:35:33,326 - INFO -   Batch 1500/2527: Loss = 0.0529
2026-01-04 05:36:02,200 - INFO -   Batch 1550/2527: Loss = 0.0618
2026-01-04 05:36:30,958 - INFO -   Batch 1600/2527: Loss = 0.0642
2026-01-04 05:36:59,910 - INFO -   Batch 1650/2527: Loss = 0.0752
2026-01-04 05:37:28,689 - INFO -   Batch 1700/2527: Loss = 0.0559
2026-01-04 05:37:57,602 - INFO -   Batch 1750/2527: Loss = 0.0520
2026-01-04 05:38:26,224 - INFO -   Batch 1800/2527: Loss = 0.0537
2026-01-04 05:38:54,735 - INFO -   Batch 1850/2527: Loss = 0.0533
2026-01-04 05:39:23,607 - INFO -   Batch 1900/2527: Loss = 0.0722
2026-01-04 05:39:52,227 - INFO -   Batch 1950/2527: Loss = 0.0490
2026-01-04 05:40:21,196 - INFO -   Batch 2000/2527: Loss = 0.0647
2026-01-04 05:40:49,934 - INFO -   Batch 2050/2527: Loss = 0.0550
2026-01-04 05:41:18,982 - INFO -   Batch 2100/2527: Loss = 0.0665
2026-01-04 05:41:47,590 - INFO -   Batch 2150/2527: Loss = 0.0605
2026-01-04 05:42:16,159 - INFO -   Batch 2200/2527: Loss = 0.0443
2026-01-04 05:42:45,035 - INFO -   Batch 2250/2527: Loss = 0.0488
2026-01-04 05:43:13,671 - INFO -   Batch 2300/2527: Loss = 0.0571
2026-01-04 05:43:42,530 - INFO -   Batch 2350/2527: Loss = 0.0581
2026-01-04 05:44:11,249 - INFO -   Batch 2400/2527: Loss = 0.0460
2026-01-04 05:44:39,831 - INFO -   Batch 2450/2527: Loss = 0.0406
2026-01-04 05:45:08,917 - INFO -   Batch 2500/2527: Loss = 0.0424
2026-01-04 05:49:54,916 - INFO - Epoch 117/160: Train Loss: 0.0576, Val Loss: 1.4237, Val mIoU: 40.31%, Val PixelAcc: 78.89%
2026-01-04 05:50:25,465 - INFO -   Batch 50/2527: Loss = 0.0415
2026-01-04 05:50:53,930 - INFO -   Batch 100/2527: Loss = 0.0626
2026-01-04 05:51:22,460 - INFO -   Batch 150/2527: Loss = 0.0803
2026-01-04 05:51:51,505 - INFO -   Batch 200/2527: Loss = 0.0614
2026-01-04 05:52:20,152 - INFO -   Batch 250/2527: Loss = 0.0684
2026-01-04 05:52:49,068 - INFO -   Batch 300/2527: Loss = 0.0660
2026-01-04 05:53:17,896 - INFO -   Batch 350/2527: Loss = 0.0502
2026-01-04 05:53:46,464 - INFO -   Batch 400/2527: Loss = 0.0468
2026-01-04 05:54:15,227 - INFO -   Batch 450/2527: Loss = 0.0440
2026-01-04 05:54:43,791 - INFO -   Batch 500/2527: Loss = 0.0526
2026-01-04 05:55:12,622 - INFO -   Batch 550/2527: Loss = 0.0631
2026-01-04 05:55:41,476 - INFO -   Batch 600/2527: Loss = 0.0543
2026-01-04 05:56:10,252 - INFO -   Batch 650/2527: Loss = 0.0754
2026-01-04 05:56:39,043 - INFO -   Batch 700/2527: Loss = 0.0489
2026-01-04 05:57:07,791 - INFO -   Batch 750/2527: Loss = 0.0604
2026-01-04 05:57:36,722 - INFO -   Batch 800/2527: Loss = 0.0382
2026-01-04 05:58:05,465 - INFO -   Batch 850/2527: Loss = 0.0610
2026-01-04 05:58:34,288 - INFO -   Batch 900/2527: Loss = 0.0453
2026-01-04 05:59:02,920 - INFO -   Batch 950/2527: Loss = 0.0649
2026-01-04 05:59:32,000 - INFO -   Batch 1000/2527: Loss = 0.0602
2026-01-04 06:00:00,500 - INFO -   Batch 1050/2527: Loss = 0.0442
2026-01-04 06:00:29,221 - INFO -   Batch 1100/2527: Loss = 0.0723
2026-01-04 06:00:58,322 - INFO -   Batch 1150/2527: Loss = 0.0392
2026-01-04 06:01:27,013 - INFO -   Batch 1200/2527: Loss = 0.0344
2026-01-04 06:01:55,827 - INFO -   Batch 1250/2527: Loss = 0.0546
2026-01-04 06:02:24,410 - INFO -   Batch 1300/2527: Loss = 0.0617
2026-01-04 06:02:52,928 - INFO -   Batch 1350/2527: Loss = 0.0491
2026-01-04 06:03:21,986 - INFO -   Batch 1400/2527: Loss = 0.0591
2026-01-04 06:03:50,641 - INFO -   Batch 1450/2527: Loss = 0.0567
2026-01-04 06:04:19,578 - INFO -   Batch 1500/2527: Loss = 0.0410
2026-01-04 06:04:48,515 - INFO -   Batch 1550/2527: Loss = 0.0610
2026-01-04 06:05:17,392 - INFO -   Batch 1600/2527: Loss = 0.0457
2026-01-04 06:05:46,021 - INFO -   Batch 1650/2527: Loss = 0.0480
2026-01-04 06:06:14,497 - INFO -   Batch 1700/2527: Loss = 0.0495
2026-01-04 06:06:43,381 - INFO -   Batch 1750/2527: Loss = 0.0647
2026-01-04 06:07:12,166 - INFO -   Batch 1800/2527: Loss = 0.0720
2026-01-04 06:07:40,987 - INFO -   Batch 1850/2527: Loss = 0.0671
2026-01-04 06:08:09,719 - INFO -   Batch 1900/2527: Loss = 0.0829
2026-01-04 06:08:38,975 - INFO -   Batch 1950/2527: Loss = 0.0372
2026-01-04 06:09:07,535 - INFO -   Batch 2000/2527: Loss = 0.0609
2026-01-04 06:09:36,067 - INFO -   Batch 2050/2527: Loss = 0.0393
2026-01-04 06:10:04,894 - INFO -   Batch 2100/2527: Loss = 0.0632
2026-01-04 06:10:33,538 - INFO -   Batch 2150/2527: Loss = 0.0446
2026-01-04 06:11:02,599 - INFO -   Batch 2200/2527: Loss = 0.0496
2026-01-04 06:11:31,181 - INFO -   Batch 2250/2527: Loss = 0.0584
2026-01-04 06:11:59,805 - INFO -   Batch 2300/2527: Loss = 0.0371
2026-01-04 06:12:28,894 - INFO -   Batch 2350/2527: Loss = 0.0660
2026-01-04 06:12:57,592 - INFO -   Batch 2400/2527: Loss = 0.0413
2026-01-04 06:13:26,365 - INFO -   Batch 2450/2527: Loss = 0.0573
2026-01-04 06:13:54,981 - INFO -   Batch 2500/2527: Loss = 0.0908
2026-01-04 06:18:34,606 - INFO - Epoch 118/160: Train Loss: 0.0571, Val Loss: 1.4342, Val mIoU: 40.16%, Val PixelAcc: 78.80%
2026-01-04 06:19:05,010 - INFO -   Batch 50/2527: Loss = 0.0730
2026-01-04 06:19:33,852 - INFO -   Batch 100/2527: Loss = 0.0539
2026-01-04 06:20:02,727 - INFO -   Batch 150/2527: Loss = 0.0666
2026-01-04 06:20:31,289 - INFO -   Batch 200/2527: Loss = 0.0495
2026-01-04 06:20:59,765 - INFO -   Batch 250/2527: Loss = 0.0446
2026-01-04 06:21:28,612 - INFO -   Batch 300/2527: Loss = 0.0617
2026-01-04 06:21:57,451 - INFO -   Batch 350/2527: Loss = 0.0716
2026-01-04 06:22:26,306 - INFO -   Batch 400/2527: Loss = 0.0503
2026-01-04 06:22:55,040 - INFO -   Batch 450/2527: Loss = 0.0480
2026-01-04 06:23:24,173 - INFO -   Batch 500/2527: Loss = 0.0378
2026-01-04 06:23:53,056 - INFO -   Batch 550/2527: Loss = 0.0647
2026-01-04 06:24:21,534 - INFO -   Batch 600/2527: Loss = 0.0435
2026-01-04 06:24:50,311 - INFO -   Batch 650/2527: Loss = 0.0580
2026-01-04 06:25:19,026 - INFO -   Batch 700/2527: Loss = 0.0523
2026-01-04 06:25:48,098 - INFO -   Batch 750/2527: Loss = 0.0673
2026-01-04 06:26:16,732 - INFO -   Batch 800/2527: Loss = 0.0638
2026-01-04 06:26:45,366 - INFO -   Batch 850/2527: Loss = 0.0571
2026-01-04 06:27:14,443 - INFO -   Batch 900/2527: Loss = 0.0448
2026-01-04 06:27:43,136 - INFO -   Batch 950/2527: Loss = 0.0676
2026-01-04 06:28:11,948 - INFO -   Batch 1000/2527: Loss = 0.0433
2026-01-04 06:28:40,491 - INFO -   Batch 1050/2527: Loss = 0.0567
2026-01-04 06:29:09,367 - INFO -   Batch 1100/2527: Loss = 0.0514
2026-01-04 06:29:38,186 - INFO -   Batch 1150/2527: Loss = 0.0603
2026-01-04 06:30:06,688 - INFO -   Batch 1200/2527: Loss = 0.0523
2026-01-04 06:30:35,657 - INFO -   Batch 1250/2527: Loss = 0.0532
2026-01-04 06:31:04,555 - INFO -   Batch 1300/2527: Loss = 0.0485
2026-01-04 06:31:33,461 - INFO -   Batch 1350/2527: Loss = 0.0444
2026-01-04 06:32:02,102 - INFO -   Batch 1400/2527: Loss = 0.0461
2026-01-04 06:32:30,872 - INFO -   Batch 1450/2527: Loss = 0.0576
2026-01-04 06:32:59,513 - INFO -   Batch 1500/2527: Loss = 0.0481
2026-01-04 06:33:28,223 - INFO -   Batch 1550/2527: Loss = 0.0531
2026-01-04 06:33:57,009 - INFO -   Batch 1600/2527: Loss = 0.0385
2026-01-04 06:34:25,787 - INFO -   Batch 1650/2527: Loss = 0.0476
2026-01-04 06:34:54,835 - INFO -   Batch 1700/2527: Loss = 0.0653
2026-01-04 06:35:23,483 - INFO -   Batch 1750/2527: Loss = 0.0529
2026-01-04 06:35:51,944 - INFO -   Batch 1800/2527: Loss = 0.0409
2026-01-04 06:36:20,771 - INFO -   Batch 1850/2527: Loss = 0.0779
2026-01-04 06:36:49,424 - INFO -   Batch 1900/2527: Loss = 0.0407
2026-01-04 06:37:18,505 - INFO -   Batch 1950/2527: Loss = 0.0452
2026-01-04 06:37:47,067 - INFO -   Batch 2000/2527: Loss = 0.0539
2026-01-04 06:38:16,021 - INFO -   Batch 2050/2527: Loss = 0.2475
2026-01-04 06:38:44,829 - INFO -   Batch 2100/2527: Loss = 0.0363
2026-01-04 06:39:13,409 - INFO -   Batch 2150/2527: Loss = 0.0608
2026-01-04 06:39:42,251 - INFO -   Batch 2200/2527: Loss = 0.0509
2026-01-04 06:40:10,781 - INFO -   Batch 2250/2527: Loss = 0.0621
2026-01-04 06:40:39,646 - INFO -   Batch 2300/2527: Loss = 0.0466
2026-01-04 06:41:08,519 - INFO -   Batch 2350/2527: Loss = 0.0592
2026-01-04 06:41:37,311 - INFO -   Batch 2400/2527: Loss = 0.0615
2026-01-04 06:42:05,932 - INFO -   Batch 2450/2527: Loss = 0.0740
2026-01-04 06:42:35,016 - INFO -   Batch 2500/2527: Loss = 0.0484
2026-01-04 06:47:14,652 - INFO - Epoch 119/160: Train Loss: 0.0562, Val Loss: 1.4349, Val mIoU: 39.74%, Val PixelAcc: 78.76%
2026-01-04 06:47:45,246 - INFO -   Batch 50/2527: Loss = 0.0519
2026-01-04 06:48:13,930 - INFO -   Batch 100/2527: Loss = 0.0491
2026-01-04 06:48:42,836 - INFO -   Batch 150/2527: Loss = 0.0441
2026-01-04 06:49:11,559 - INFO -   Batch 200/2527: Loss = 0.0505
2026-01-04 06:49:40,413 - INFO -   Batch 250/2527: Loss = 0.0590
2026-01-04 06:50:09,055 - INFO -   Batch 300/2527: Loss = 0.0354
2026-01-04 06:50:37,927 - INFO -   Batch 350/2527: Loss = 0.0670
2026-01-04 06:51:06,582 - INFO -   Batch 400/2527: Loss = 0.0658
2026-01-04 06:51:35,273 - INFO -   Batch 450/2527: Loss = 0.0381
2026-01-04 06:52:04,362 - INFO -   Batch 500/2527: Loss = 0.0674
2026-01-04 06:52:33,055 - INFO -   Batch 550/2527: Loss = 0.0513
2026-01-04 06:53:01,862 - INFO -   Batch 600/2527: Loss = 0.0592
2026-01-04 06:53:30,484 - INFO -   Batch 650/2527: Loss = 0.0549
2026-01-04 06:53:59,057 - INFO -   Batch 700/2527: Loss = 0.0499
2026-01-04 06:54:27,916 - INFO -   Batch 750/2527: Loss = 0.0683
2026-01-04 06:54:56,705 - INFO -   Batch 800/2527: Loss = 0.0393
2026-01-04 06:55:25,626 - INFO -   Batch 850/2527: Loss = 0.0455
2026-01-04 06:55:54,521 - INFO -   Batch 900/2527: Loss = 0.0490
2026-01-04 06:56:23,477 - INFO -   Batch 950/2527: Loss = 0.0502
2026-01-04 06:56:52,080 - INFO -   Batch 1000/2527: Loss = 0.0689
2026-01-04 06:57:20,599 - INFO -   Batch 1050/2527: Loss = 0.0623
2026-01-04 06:57:49,493 - INFO -   Batch 1100/2527: Loss = 0.0683
2026-01-04 06:58:18,073 - INFO -   Batch 1150/2527: Loss = 0.0434
2026-01-04 06:58:47,051 - INFO -   Batch 1200/2527: Loss = 0.0407
2026-01-04 06:59:15,742 - INFO -   Batch 1250/2527: Loss = 0.0727
2026-01-04 06:59:44,816 - INFO -   Batch 1300/2527: Loss = 0.0553
2026-01-04 07:00:13,434 - INFO -   Batch 1350/2527: Loss = 0.0393
2026-01-04 07:00:42,008 - INFO -   Batch 1400/2527: Loss = 0.0667
2026-01-04 07:01:10,830 - INFO -   Batch 1450/2527: Loss = 0.0492
2026-01-04 07:01:39,554 - INFO -   Batch 1500/2527: Loss = 0.0418
2026-01-04 07:02:08,409 - INFO -   Batch 1550/2527: Loss = 0.0784
2026-01-04 07:02:37,165 - INFO -   Batch 1600/2527: Loss = 0.0531
2026-01-04 07:03:05,765 - INFO -   Batch 1650/2527: Loss = 0.0796
2026-01-04 07:03:34,811 - INFO -   Batch 1700/2527: Loss = 0.0591
2026-01-04 07:04:03,494 - INFO -   Batch 1750/2527: Loss = 0.0499
2026-01-04 07:04:32,258 - INFO -   Batch 1800/2527: Loss = 0.0492
2026-01-04 07:05:00,949 - INFO -   Batch 1850/2527: Loss = 0.0452
2026-01-04 07:05:29,884 - INFO -   Batch 1900/2527: Loss = 0.0375
2026-01-04 07:05:58,529 - INFO -   Batch 1950/2527: Loss = 0.0861
2026-01-04 07:06:27,172 - INFO -   Batch 2000/2527: Loss = 0.0632
2026-01-04 07:06:56,106 - INFO -   Batch 2050/2527: Loss = 0.0410
2026-01-04 07:07:25,094 - INFO -   Batch 2100/2527: Loss = 0.1100
2026-01-04 07:07:54,049 - INFO -   Batch 2150/2527: Loss = 0.0301
2026-01-04 07:08:22,611 - INFO -   Batch 2200/2527: Loss = 0.0661
2026-01-04 07:08:51,122 - INFO -   Batch 2250/2527: Loss = 0.0406
2026-01-04 07:09:20,060 - INFO -   Batch 2300/2527: Loss = 0.0559
2026-01-04 07:09:48,683 - INFO -   Batch 2350/2527: Loss = 0.0512
2026-01-04 07:10:17,699 - INFO -   Batch 2400/2527: Loss = 0.0581
2026-01-04 07:10:46,468 - INFO -   Batch 2450/2527: Loss = 0.0473
2026-01-04 07:11:15,556 - INFO -   Batch 2500/2527: Loss = 0.0667
2026-01-04 07:16:01,207 - INFO - Epoch 120/160: Train Loss: 0.0559, Val Loss: 1.4469, Val mIoU: 39.79%, Val PixelAcc: 78.65%
2026-01-04 07:16:04,795 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_120.pth
2026-01-04 07:16:04,798 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_120.pth
2026-01-04 07:16:35,303 - INFO -   Batch 50/2527: Loss = 0.0606
2026-01-04 07:17:04,058 - INFO -   Batch 100/2527: Loss = 0.0366
2026-01-04 07:17:32,673 - INFO -   Batch 150/2527: Loss = 0.0536
2026-01-04 07:18:01,633 - INFO -   Batch 200/2527: Loss = 0.0504
2026-01-04 07:18:30,416 - INFO -   Batch 250/2527: Loss = 0.0410
2026-01-04 07:18:59,021 - INFO -   Batch 300/2527: Loss = 0.0462
2026-01-04 07:19:27,939 - INFO -   Batch 350/2527: Loss = 0.0724
2026-01-04 07:19:56,806 - INFO -   Batch 400/2527: Loss = 0.0441
2026-01-04 07:20:25,367 - INFO -   Batch 450/2527: Loss = 0.0481
2026-01-04 07:20:54,124 - INFO -   Batch 500/2527: Loss = 0.0408
2026-01-04 07:21:22,809 - INFO -   Batch 550/2527: Loss = 0.0693
2026-01-04 07:21:51,535 - INFO -   Batch 600/2527: Loss = 0.0473
2026-01-04 07:22:20,375 - INFO -   Batch 650/2527: Loss = 0.0697
2026-01-04 07:22:49,085 - INFO -   Batch 700/2527: Loss = 0.0429
2026-01-04 07:23:18,187 - INFO -   Batch 750/2527: Loss = 0.0587
2026-01-04 07:23:46,858 - INFO -   Batch 800/2527: Loss = 0.0533
2026-01-04 07:24:15,678 - INFO -   Batch 850/2527: Loss = 0.0816
2026-01-04 07:24:44,196 - INFO -   Batch 900/2527: Loss = 0.0417
2026-01-04 07:25:12,727 - INFO -   Batch 950/2527: Loss = 0.0421
2026-01-04 07:25:41,816 - INFO -   Batch 1000/2527: Loss = 0.0439
2026-01-04 07:26:10,407 - INFO -   Batch 1050/2527: Loss = 0.0602
2026-01-04 07:26:39,369 - INFO -   Batch 1100/2527: Loss = 0.0549
2026-01-04 07:27:08,230 - INFO -   Batch 1150/2527: Loss = 0.0531
2026-01-04 07:27:36,787 - INFO -   Batch 1200/2527: Loss = 0.0843
2026-01-04 07:28:05,576 - INFO -   Batch 1250/2527: Loss = 0.0318
2026-01-04 07:28:34,207 - INFO -   Batch 1300/2527: Loss = 0.0413
2026-01-04 07:29:03,063 - INFO -   Batch 1350/2527: Loss = 0.0492
2026-01-04 07:29:32,094 - INFO -   Batch 1400/2527: Loss = 0.0351
2026-01-04 07:30:00,858 - INFO -   Batch 1450/2527: Loss = 0.0423
2026-01-04 07:30:29,626 - INFO -   Batch 1500/2527: Loss = 0.0647
2026-01-04 07:30:58,407 - INFO -   Batch 1550/2527: Loss = 0.0319
2026-01-04 07:31:27,316 - INFO -   Batch 1600/2527: Loss = 0.0685
2026-01-04 07:31:55,895 - INFO -   Batch 1650/2527: Loss = 0.0512
2026-01-04 07:32:24,690 - INFO -   Batch 1700/2527: Loss = 0.0511
2026-01-04 07:32:53,325 - INFO -   Batch 1750/2527: Loss = 0.0529
2026-01-04 07:33:22,435 - INFO -   Batch 1800/2527: Loss = 0.0462
2026-01-04 07:33:50,986 - INFO -   Batch 1850/2527: Loss = 0.0582
2026-01-04 07:34:19,617 - INFO -   Batch 1900/2527: Loss = 0.0428
2026-01-04 07:34:48,730 - INFO -   Batch 1950/2527: Loss = 0.0517
2026-01-04 07:35:17,400 - INFO -   Batch 2000/2527: Loss = 0.0666
2026-01-04 07:35:46,257 - INFO -   Batch 2050/2527: Loss = 0.0618
2026-01-04 07:36:14,894 - INFO -   Batch 2100/2527: Loss = 0.0975
2026-01-04 07:36:43,436 - INFO -   Batch 2150/2527: Loss = 0.0551
2026-01-04 07:37:12,501 - INFO -   Batch 2200/2527: Loss = 0.0498
2026-01-04 07:37:41,135 - INFO -   Batch 2250/2527: Loss = 0.0712
2026-01-04 07:38:10,080 - INFO -   Batch 2300/2527: Loss = 0.0664
2026-01-04 07:38:38,983 - INFO -   Batch 2350/2527: Loss = 0.0453
2026-01-04 07:39:07,837 - INFO -   Batch 2400/2527: Loss = 0.0481
2026-01-04 07:39:36,495 - INFO -   Batch 2450/2527: Loss = 0.0497
2026-01-04 07:40:04,959 - INFO -   Batch 2500/2527: Loss = 0.0561
2026-01-04 07:44:45,602 - INFO - Epoch 121/160: Train Loss: 0.0556, Val Loss: 1.4383, Val mIoU: 39.87%, Val PixelAcc: 78.78%
2026-01-04 07:45:15,983 - INFO -   Batch 50/2527: Loss = 0.0827
2026-01-04 07:45:44,727 - INFO -   Batch 100/2527: Loss = 0.0593
2026-01-04 07:46:13,601 - INFO -   Batch 150/2527: Loss = 0.0650
2026-01-04 07:46:42,108 - INFO -   Batch 200/2527: Loss = 0.0544
2026-01-04 07:47:10,909 - INFO -   Batch 250/2527: Loss = 0.0504
2026-01-04 07:47:39,546 - INFO -   Batch 300/2527: Loss = 0.0737
2026-01-04 07:48:08,634 - INFO -   Batch 350/2527: Loss = 0.0505
2026-01-04 07:48:37,157 - INFO -   Batch 400/2527: Loss = 0.0547
2026-01-04 07:49:05,772 - INFO -   Batch 450/2527: Loss = 0.0534
2026-01-04 07:49:34,918 - INFO -   Batch 500/2527: Loss = 0.0505
2026-01-04 07:50:03,609 - INFO -   Batch 550/2527: Loss = 0.0574
2026-01-04 07:50:32,372 - INFO -   Batch 600/2527: Loss = 0.0650
2026-01-04 07:51:00,931 - INFO -   Batch 650/2527: Loss = 0.0460
2026-01-04 07:51:29,743 - INFO -   Batch 700/2527: Loss = 0.0573
2026-01-04 07:51:58,499 - INFO -   Batch 750/2527: Loss = 0.0546
2026-01-04 07:52:27,097 - INFO -   Batch 800/2527: Loss = 0.0460
2026-01-04 07:52:56,018 - INFO -   Batch 850/2527: Loss = 0.0650
2026-01-04 07:53:24,903 - INFO -   Batch 900/2527: Loss = 0.0368
2026-01-04 07:53:53,762 - INFO -   Batch 950/2527: Loss = 0.0676
2026-01-04 07:54:22,378 - INFO -   Batch 1000/2527: Loss = 0.0624
2026-01-04 07:54:50,845 - INFO -   Batch 1050/2527: Loss = 0.0526
2026-01-04 07:55:19,717 - INFO -   Batch 1100/2527: Loss = 0.0558
2026-01-04 07:55:48,691 - INFO -   Batch 1150/2527: Loss = 0.0595
2026-01-04 07:56:17,513 - INFO -   Batch 1200/2527: Loss = 0.0815
2026-01-04 07:56:46,262 - INFO -   Batch 1250/2527: Loss = 0.0585
2026-01-04 07:57:15,383 - INFO -   Batch 1300/2527: Loss = 0.0384
2026-01-04 07:57:44,116 - INFO -   Batch 1350/2527: Loss = 0.0430
2026-01-04 07:58:12,573 - INFO -   Batch 1400/2527: Loss = 0.0580
2026-01-04 07:58:41,389 - INFO -   Batch 1450/2527: Loss = 0.0464
2026-01-04 07:59:09,997 - INFO -   Batch 1500/2527: Loss = 0.0509
2026-01-04 07:59:39,080 - INFO -   Batch 1550/2527: Loss = 0.0456
2026-01-04 08:00:07,705 - INFO -   Batch 1600/2527: Loss = 0.0407
2026-01-04 08:00:36,616 - INFO -   Batch 1650/2527: Loss = 0.0548
2026-01-04 08:01:05,375 - INFO -   Batch 1700/2527: Loss = 0.0549
2026-01-04 08:01:34,088 - INFO -   Batch 1750/2527: Loss = 0.0421
2026-01-04 08:02:02,862 - INFO -   Batch 1800/2527: Loss = 0.0609
2026-01-04 08:02:31,541 - INFO -   Batch 1850/2527: Loss = 0.0422
2026-01-04 08:03:00,350 - INFO -   Batch 1900/2527: Loss = 0.0641
2026-01-04 08:03:29,225 - INFO -   Batch 1950/2527: Loss = 0.0479
2026-01-04 08:03:57,713 - INFO -   Batch 2000/2527: Loss = 0.0444
2026-01-04 08:04:26,655 - INFO -   Batch 2050/2527: Loss = 0.0616
2026-01-04 08:04:55,511 - INFO -   Batch 2100/2527: Loss = 0.0522
2026-01-04 08:05:24,389 - INFO -   Batch 2150/2527: Loss = 0.0509
2026-01-04 08:05:52,972 - INFO -   Batch 2200/2527: Loss = 0.0405
2026-01-04 08:06:21,806 - INFO -   Batch 2250/2527: Loss = 0.0474
2026-01-04 08:06:50,397 - INFO -   Batch 2300/2527: Loss = 0.0512
2026-01-04 08:07:19,144 - INFO -   Batch 2350/2527: Loss = 0.0398
2026-01-04 08:07:47,974 - INFO -   Batch 2400/2527: Loss = 0.0646
2026-01-04 08:08:16,713 - INFO -   Batch 2450/2527: Loss = 0.0408
2026-01-04 08:08:45,796 - INFO -   Batch 2500/2527: Loss = 0.0596
2026-01-04 08:13:23,469 - INFO - Epoch 122/160: Train Loss: 0.0553, Val Loss: 1.4684, Val mIoU: 39.70%, Val PixelAcc: 78.78%
2026-01-04 08:13:54,214 - INFO -   Batch 50/2527: Loss = 0.0692
2026-01-04 08:14:23,223 - INFO -   Batch 100/2527: Loss = 0.0544
2026-01-04 08:14:51,885 - INFO -   Batch 150/2527: Loss = 0.0588
2026-01-04 08:15:20,665 - INFO -   Batch 200/2527: Loss = 0.0656
2026-01-04 08:15:49,236 - INFO -   Batch 250/2527: Loss = 0.0783
2026-01-04 08:16:17,813 - INFO -   Batch 300/2527: Loss = 0.0802
2026-01-04 08:16:46,669 - INFO -   Batch 350/2527: Loss = 0.0538
2026-01-04 08:17:15,395 - INFO -   Batch 400/2527: Loss = 0.0516
2026-01-04 08:17:44,357 - INFO -   Batch 450/2527: Loss = 0.0690
2026-01-04 08:18:13,141 - INFO -   Batch 500/2527: Loss = 0.0504
2026-01-04 08:18:42,052 - INFO -   Batch 550/2527: Loss = 0.0370
2026-01-04 08:19:10,512 - INFO -   Batch 600/2527: Loss = 0.0597
2026-01-04 08:19:39,163 - INFO -   Batch 650/2527: Loss = 0.0414
2026-01-04 08:20:08,069 - INFO -   Batch 700/2527: Loss = 0.0424
2026-01-04 08:20:36,872 - INFO -   Batch 750/2527: Loss = 0.0519
2026-01-04 08:21:05,858 - INFO -   Batch 800/2527: Loss = 0.0519
2026-01-04 08:21:34,589 - INFO -   Batch 850/2527: Loss = 0.0441
2026-01-04 08:22:03,301 - INFO -   Batch 900/2527: Loss = 0.0538
2026-01-04 08:22:32,187 - INFO -   Batch 950/2527: Loss = 0.0573
2026-01-04 08:23:00,811 - INFO -   Batch 1000/2527: Loss = 0.0538
2026-01-04 08:23:29,695 - INFO -   Batch 1050/2527: Loss = 0.0498
2026-01-04 08:23:58,345 - INFO -   Batch 1100/2527: Loss = 0.0881
2026-01-04 08:24:27,213 - INFO -   Batch 1150/2527: Loss = 0.0329
2026-01-04 08:24:55,950 - INFO -   Batch 1200/2527: Loss = 0.0404
2026-01-04 08:25:24,545 - INFO -   Batch 1250/2527: Loss = 0.0592
2026-01-04 08:25:53,615 - INFO -   Batch 1300/2527: Loss = 0.0405
2026-01-04 08:26:22,303 - INFO -   Batch 1350/2527: Loss = 0.0496
2026-01-04 08:26:51,128 - INFO -   Batch 1400/2527: Loss = 0.0433
2026-01-04 08:27:19,798 - INFO -   Batch 1450/2527: Loss = 0.0437
2026-01-04 08:27:48,372 - INFO -   Batch 1500/2527: Loss = 0.0575
2026-01-04 08:28:17,284 - INFO -   Batch 1550/2527: Loss = 0.0478
2026-01-04 08:28:46,071 - INFO -   Batch 1600/2527: Loss = 0.0518
2026-01-04 08:29:14,971 - INFO -   Batch 1650/2527: Loss = 0.0586
2026-01-04 08:29:43,771 - INFO -   Batch 1700/2527: Loss = 0.0543
2026-01-04 08:30:12,643 - INFO -   Batch 1750/2527: Loss = 0.0523
2026-01-04 08:30:41,293 - INFO -   Batch 1800/2527: Loss = 0.0595
2026-01-04 08:31:09,818 - INFO -   Batch 1850/2527: Loss = 0.0631
2026-01-04 08:31:38,713 - INFO -   Batch 1900/2527: Loss = 0.0678
2026-01-04 08:32:07,318 - INFO -   Batch 1950/2527: Loss = 0.0476
2026-01-04 08:32:36,281 - INFO -   Batch 2000/2527: Loss = 0.0341
2026-01-04 08:33:04,985 - INFO -   Batch 2050/2527: Loss = 0.0644
2026-01-04 08:33:34,056 - INFO -   Batch 2100/2527: Loss = 0.0419
2026-01-04 08:34:02,658 - INFO -   Batch 2150/2527: Loss = 0.0544
2026-01-04 08:34:31,176 - INFO -   Batch 2200/2527: Loss = 0.0556
2026-01-04 08:35:00,025 - INFO -   Batch 2250/2527: Loss = 0.0611
2026-01-04 08:35:28,651 - INFO -   Batch 2300/2527: Loss = 0.0601
2026-01-04 08:35:57,533 - INFO -   Batch 2350/2527: Loss = 0.0639
2026-01-04 08:36:26,239 - INFO -   Batch 2400/2527: Loss = 0.0465
2026-01-04 08:36:54,834 - INFO -   Batch 2450/2527: Loss = 0.0558
2026-01-04 08:37:23,948 - INFO -   Batch 2500/2527: Loss = 0.0268
2026-01-04 08:42:09,916 - INFO - Epoch 123/160: Train Loss: 0.0549, Val Loss: 1.4646, Val mIoU: 39.75%, Val PixelAcc: 78.67%
2026-01-04 08:42:40,414 - INFO -   Batch 50/2527: Loss = 0.0428
2026-01-04 08:43:08,927 - INFO -   Batch 100/2527: Loss = 0.0727
2026-01-04 08:43:37,429 - INFO -   Batch 150/2527: Loss = 0.0459
2026-01-04 08:44:06,498 - INFO -   Batch 200/2527: Loss = 0.0514
2026-01-04 08:44:35,081 - INFO -   Batch 250/2527: Loss = 0.0429
2026-01-04 08:45:04,021 - INFO -   Batch 300/2527: Loss = 0.0446
2026-01-04 08:45:32,821 - INFO -   Batch 350/2527: Loss = 0.0579
2026-01-04 08:46:01,364 - INFO -   Batch 400/2527: Loss = 0.0546
2026-01-04 08:46:30,160 - INFO -   Batch 450/2527: Loss = 0.0516
2026-01-04 08:46:58,698 - INFO -   Batch 500/2527: Loss = 0.0526
2026-01-04 08:47:27,545 - INFO -   Batch 550/2527: Loss = 0.0681
2026-01-04 08:47:56,360 - INFO -   Batch 600/2527: Loss = 0.0454
2026-01-04 08:48:25,167 - INFO -   Batch 650/2527: Loss = 0.0574
2026-01-04 08:48:53,919 - INFO -   Batch 700/2527: Loss = 0.0465
2026-01-04 08:49:22,686 - INFO -   Batch 750/2527: Loss = 0.0452
2026-01-04 08:49:51,562 - INFO -   Batch 800/2527: Loss = 0.0679
2026-01-04 08:50:20,327 - INFO -   Batch 850/2527: Loss = 0.0367
2026-01-04 08:50:49,096 - INFO -   Batch 900/2527: Loss = 0.0625
2026-01-04 08:51:17,802 - INFO -   Batch 950/2527: Loss = 0.0481
2026-01-04 08:51:46,823 - INFO -   Batch 1000/2527: Loss = 0.0525
2026-01-04 08:52:15,285 - INFO -   Batch 1050/2527: Loss = 0.0420
2026-01-04 08:52:44,039 - INFO -   Batch 1100/2527: Loss = 0.0563
2026-01-04 08:53:13,119 - INFO -   Batch 1150/2527: Loss = 0.0583
2026-01-04 08:53:41,793 - INFO -   Batch 1200/2527: Loss = 0.0384
2026-01-04 08:54:10,588 - INFO -   Batch 1250/2527: Loss = 0.0680
2026-01-04 08:54:39,121 - INFO -   Batch 1300/2527: Loss = 0.0532
2026-01-04 08:55:07,628 - INFO -   Batch 1350/2527: Loss = 0.0463
2026-01-04 08:55:36,690 - INFO -   Batch 1400/2527: Loss = 0.0411
2026-01-04 08:56:05,254 - INFO -   Batch 1450/2527: Loss = 0.0712
2026-01-04 08:56:34,270 - INFO -   Batch 1500/2527: Loss = 0.0447
2026-01-04 08:57:03,187 - INFO -   Batch 1550/2527: Loss = 0.0496
2026-01-04 08:57:32,121 - INFO -   Batch 1600/2527: Loss = 0.0480
2026-01-04 08:58:00,731 - INFO -   Batch 1650/2527: Loss = 0.0977
2026-01-04 08:58:29,211 - INFO -   Batch 1700/2527: Loss = 0.0470
2026-01-04 08:58:58,032 - INFO -   Batch 1750/2527: Loss = 0.0423
2026-01-04 08:59:26,834 - INFO -   Batch 1800/2527: Loss = 0.0714
2026-01-04 08:59:55,661 - INFO -   Batch 1850/2527: Loss = 0.0670
2026-01-04 09:00:24,454 - INFO -   Batch 1900/2527: Loss = 0.0582
2026-01-04 09:00:53,546 - INFO -   Batch 1950/2527: Loss = 0.0618
2026-01-04 09:01:22,131 - INFO -   Batch 2000/2527: Loss = 0.0531
2026-01-04 09:01:50,683 - INFO -   Batch 2050/2527: Loss = 0.0489
2026-01-04 09:02:19,500 - INFO -   Batch 2100/2527: Loss = 0.0469
2026-01-04 09:02:48,144 - INFO -   Batch 2150/2527: Loss = 0.0426
2026-01-04 09:03:17,193 - INFO -   Batch 2200/2527: Loss = 0.0335
2026-01-04 09:03:45,774 - INFO -   Batch 2250/2527: Loss = 0.0446
2026-01-04 09:04:14,385 - INFO -   Batch 2300/2527: Loss = 0.0462
2026-01-04 09:04:43,502 - INFO -   Batch 2350/2527: Loss = 0.0449
2026-01-04 09:05:12,123 - INFO -   Batch 2400/2527: Loss = 0.0762
2026-01-04 09:05:40,948 - INFO -   Batch 2450/2527: Loss = 0.0468
2026-01-04 09:06:09,480 - INFO -   Batch 2500/2527: Loss = 0.0723
2026-01-04 09:10:49,682 - INFO - Epoch 124/160: Train Loss: 0.0543, Val Loss: 1.4491, Val mIoU: 39.65%, Val PixelAcc: 78.73%
2026-01-04 09:11:20,108 - INFO -   Batch 50/2527: Loss = 0.0440
2026-01-04 09:11:48,899 - INFO -   Batch 100/2527: Loss = 0.0541
2026-01-04 09:12:17,728 - INFO -   Batch 150/2527: Loss = 0.0406
2026-01-04 09:12:46,273 - INFO -   Batch 200/2527: Loss = 0.0261
2026-01-04 09:13:14,722 - INFO -   Batch 250/2527: Loss = 0.0518
2026-01-04 09:13:43,540 - INFO -   Batch 300/2527: Loss = 0.0613
2026-01-04 09:14:12,386 - INFO -   Batch 350/2527: Loss = 0.0415
2026-01-04 09:14:41,224 - INFO -   Batch 400/2527: Loss = 0.0651
2026-01-04 09:15:09,998 - INFO -   Batch 450/2527: Loss = 0.0486
2026-01-04 09:15:39,079 - INFO -   Batch 500/2527: Loss = 0.0698
2026-01-04 09:16:07,911 - INFO -   Batch 550/2527: Loss = 0.0518
2026-01-04 09:16:36,405 - INFO -   Batch 600/2527: Loss = 0.0573
2026-01-04 09:17:05,244 - INFO -   Batch 650/2527: Loss = 0.0629
2026-01-04 09:17:33,867 - INFO -   Batch 700/2527: Loss = 0.0506
2026-01-04 09:18:02,949 - INFO -   Batch 750/2527: Loss = 0.0631
2026-01-04 09:18:31,544 - INFO -   Batch 800/2527: Loss = 0.0471
2026-01-04 09:19:00,155 - INFO -   Batch 850/2527: Loss = 0.0539
2026-01-04 09:19:29,282 - INFO -   Batch 900/2527: Loss = 0.0524
2026-01-04 09:19:57,971 - INFO -   Batch 950/2527: Loss = 0.0480
2026-01-04 09:20:26,787 - INFO -   Batch 1000/2527: Loss = 0.0864
2026-01-04 09:20:55,338 - INFO -   Batch 1050/2527: Loss = 0.0653
2026-01-04 09:21:24,169 - INFO -   Batch 1100/2527: Loss = 0.0705
2026-01-04 09:21:53,054 - INFO -   Batch 1150/2527: Loss = 0.0550
2026-01-04 09:22:21,552 - INFO -   Batch 1200/2527: Loss = 0.0498
2026-01-04 09:22:50,523 - INFO -   Batch 1250/2527: Loss = 0.0490
2026-01-04 09:23:19,467 - INFO -   Batch 1300/2527: Loss = 0.0672
2026-01-04 09:23:48,377 - INFO -   Batch 1350/2527: Loss = 0.0606
2026-01-04 09:24:16,990 - INFO -   Batch 1400/2527: Loss = 0.0394
2026-01-04 09:24:45,825 - INFO -   Batch 1450/2527: Loss = 0.0702
2026-01-04 09:25:14,438 - INFO -   Batch 1500/2527: Loss = 0.0473
2026-01-04 09:25:43,159 - INFO -   Batch 1550/2527: Loss = 0.0487
2026-01-04 09:26:11,991 - INFO -   Batch 1600/2527: Loss = 0.0554
2026-01-04 09:26:40,704 - INFO -   Batch 1650/2527: Loss = 0.0649
2026-01-04 09:27:09,800 - INFO -   Batch 1700/2527: Loss = 0.0470
2026-01-04 09:27:38,417 - INFO -   Batch 1750/2527: Loss = 0.0537
2026-01-04 09:28:06,908 - INFO -   Batch 1800/2527: Loss = 0.0530
2026-01-04 09:28:35,726 - INFO -   Batch 1850/2527: Loss = 0.0513
2026-01-04 09:29:04,373 - INFO -   Batch 1900/2527: Loss = 0.0506
2026-01-04 09:29:33,401 - INFO -   Batch 1950/2527: Loss = 0.0424
2026-01-04 09:30:01,971 - INFO -   Batch 2000/2527: Loss = 0.0361
2026-01-04 09:30:30,891 - INFO -   Batch 2050/2527: Loss = 0.0454
2026-01-04 09:30:59,768 - INFO -   Batch 2100/2527: Loss = 0.0362
2026-01-04 09:31:28,329 - INFO -   Batch 2150/2527: Loss = 0.0413
2026-01-04 09:31:57,124 - INFO -   Batch 2200/2527: Loss = 0.0711
2026-01-04 09:32:25,662 - INFO -   Batch 2250/2527: Loss = 0.0366
2026-01-04 09:32:54,497 - INFO -   Batch 2300/2527: Loss = 0.0688
2026-01-04 09:33:23,340 - INFO -   Batch 2350/2527: Loss = 0.0465
2026-01-04 09:33:52,178 - INFO -   Batch 2400/2527: Loss = 0.0467
2026-01-04 09:34:20,819 - INFO -   Batch 2450/2527: Loss = 0.0630
2026-01-04 09:34:49,849 - INFO -   Batch 2500/2527: Loss = 0.0649
2026-01-04 09:39:29,334 - INFO - Epoch 125/160: Train Loss: 0.0539, Val Loss: 1.4628, Val mIoU: 39.65%, Val PixelAcc: 78.60%
2026-01-04 09:40:00,047 - INFO -   Batch 50/2527: Loss = 0.0359
2026-01-04 09:40:28,733 - INFO -   Batch 100/2527: Loss = 0.0319
2026-01-04 09:40:57,625 - INFO -   Batch 150/2527: Loss = 0.0473
2026-01-04 09:41:26,355 - INFO -   Batch 200/2527: Loss = 0.0393
2026-01-04 09:41:55,195 - INFO -   Batch 250/2527: Loss = 0.0287
2026-01-04 09:42:23,895 - INFO -   Batch 300/2527: Loss = 0.0409
2026-01-04 09:42:52,718 - INFO -   Batch 350/2527: Loss = 0.0708
2026-01-04 09:43:21,391 - INFO -   Batch 400/2527: Loss = 0.0519
2026-01-04 09:43:50,112 - INFO -   Batch 450/2527: Loss = 0.0494
2026-01-04 09:44:19,173 - INFO -   Batch 500/2527: Loss = 0.0564
2026-01-04 09:44:47,904 - INFO -   Batch 550/2527: Loss = 0.0454
2026-01-04 09:45:16,696 - INFO -   Batch 600/2527: Loss = 0.0470
2026-01-04 09:45:45,338 - INFO -   Batch 650/2527: Loss = 0.0645
2026-01-04 09:46:13,915 - INFO -   Batch 700/2527: Loss = 0.0446
2026-01-04 09:46:42,762 - INFO -   Batch 750/2527: Loss = 0.0514
2026-01-04 09:47:11,557 - INFO -   Batch 800/2527: Loss = 0.0562
2026-01-04 09:47:40,521 - INFO -   Batch 850/2527: Loss = 0.0320
2026-01-04 09:48:09,372 - INFO -   Batch 900/2527: Loss = 0.0505
2026-01-04 09:48:38,292 - INFO -   Batch 950/2527: Loss = 0.0399
2026-01-04 09:49:06,884 - INFO -   Batch 1000/2527: Loss = 0.0418
2026-01-04 09:49:35,394 - INFO -   Batch 1050/2527: Loss = 0.0501
2026-01-04 09:50:04,301 - INFO -   Batch 1100/2527: Loss = 0.0396
2026-01-04 09:50:32,870 - INFO -   Batch 1150/2527: Loss = 0.0478
2026-01-04 09:51:01,897 - INFO -   Batch 1200/2527: Loss = 0.0561
2026-01-04 09:51:30,619 - INFO -   Batch 1250/2527: Loss = 0.0554
2026-01-04 09:51:59,658 - INFO -   Batch 1300/2527: Loss = 0.0569
2026-01-04 09:52:28,244 - INFO -   Batch 1350/2527: Loss = 0.0616
2026-01-04 09:52:56,783 - INFO -   Batch 1400/2527: Loss = 0.0448
2026-01-04 09:53:25,636 - INFO -   Batch 1450/2527: Loss = 0.0542
2026-01-04 09:53:54,328 - INFO -   Batch 1500/2527: Loss = 0.0550
2026-01-04 09:54:23,164 - INFO -   Batch 1550/2527: Loss = 0.0497
2026-01-04 09:54:51,900 - INFO -   Batch 1600/2527: Loss = 0.0430
2026-01-04 09:55:20,477 - INFO -   Batch 1650/2527: Loss = 0.0508
2026-01-04 09:55:49,488 - INFO -   Batch 1700/2527: Loss = 0.0562
2026-01-04 09:56:18,205 - INFO -   Batch 1750/2527: Loss = 0.0420
2026-01-04 09:56:47,014 - INFO -   Batch 1800/2527: Loss = 0.0697
2026-01-04 09:57:15,609 - INFO -   Batch 1850/2527: Loss = 0.0569
2026-01-04 09:57:44,502 - INFO -   Batch 1900/2527: Loss = 0.0585
2026-01-04 09:58:13,137 - INFO -   Batch 1950/2527: Loss = 0.0488
2026-01-04 09:58:41,801 - INFO -   Batch 2000/2527: Loss = 0.0705
2026-01-04 09:59:10,743 - INFO -   Batch 2050/2527: Loss = 0.0490
2026-01-04 09:59:39,748 - INFO -   Batch 2100/2527: Loss = 0.0446
2026-01-04 10:00:08,695 - INFO -   Batch 2150/2527: Loss = 0.0726
2026-01-04 10:00:37,270 - INFO -   Batch 2200/2527: Loss = 0.0606
2026-01-04 10:01:05,776 - INFO -   Batch 2250/2527: Loss = 0.0564
2026-01-04 10:01:34,679 - INFO -   Batch 2300/2527: Loss = 0.0643
2026-01-04 10:02:03,350 - INFO -   Batch 2350/2527: Loss = 0.0557
2026-01-04 10:02:32,332 - INFO -   Batch 2400/2527: Loss = 0.0407
2026-01-04 10:03:01,042 - INFO -   Batch 2450/2527: Loss = 0.0469
2026-01-04 10:03:30,083 - INFO -   Batch 2500/2527: Loss = 0.0773
2026-01-04 10:08:15,457 - INFO - Epoch 126/160: Train Loss: 0.0535, Val Loss: 1.4760, Val mIoU: 39.66%, Val PixelAcc: 78.75%
2026-01-04 10:08:45,724 - INFO -   Batch 50/2527: Loss = 0.0530
2026-01-04 10:09:14,433 - INFO -   Batch 100/2527: Loss = 0.0436
2026-01-04 10:09:43,064 - INFO -   Batch 150/2527: Loss = 0.0547
2026-01-04 10:10:12,116 - INFO -   Batch 200/2527: Loss = 0.0523
2026-01-04 10:10:40,591 - INFO -   Batch 250/2527: Loss = 0.0484
2026-01-04 10:11:09,494 - INFO -   Batch 300/2527: Loss = 0.0589
2026-01-04 10:11:38,620 - INFO -   Batch 350/2527: Loss = 0.0606
2026-01-04 10:12:07,295 - INFO -   Batch 400/2527: Loss = 0.0546
2026-01-04 10:12:36,115 - INFO -   Batch 450/2527: Loss = 0.0607
2026-01-04 10:13:04,665 - INFO -   Batch 500/2527: Loss = 0.0578
2026-01-04 10:13:33,188 - INFO -   Batch 550/2527: Loss = 0.0678
2026-01-04 10:14:02,253 - INFO -   Batch 600/2527: Loss = 0.0678
2026-01-04 10:14:30,822 - INFO -   Batch 650/2527: Loss = 0.0527
2026-01-04 10:14:59,790 - INFO -   Batch 700/2527: Loss = 0.0568
2026-01-04 10:15:28,628 - INFO -   Batch 750/2527: Loss = 0.0485
2026-01-04 10:15:57,537 - INFO -   Batch 800/2527: Loss = 0.0302
2026-01-04 10:16:26,153 - INFO -   Batch 850/2527: Loss = 0.0509
2026-01-04 10:16:54,609 - INFO -   Batch 900/2527: Loss = 0.0530
2026-01-04 10:17:23,448 - INFO -   Batch 950/2527: Loss = 0.0541
2026-01-04 10:17:52,357 - INFO -   Batch 1000/2527: Loss = 0.0515
2026-01-04 10:18:21,146 - INFO -   Batch 1050/2527: Loss = 0.0379
2026-01-04 10:18:49,899 - INFO -   Batch 1100/2527: Loss = 0.0514
2026-01-04 10:19:18,671 - INFO -   Batch 1150/2527: Loss = 0.0881
2026-01-04 10:19:47,603 - INFO -   Batch 1200/2527: Loss = 0.0435
2026-01-04 10:20:16,127 - INFO -   Batch 1250/2527: Loss = 0.0458
2026-01-04 10:20:44,938 - INFO -   Batch 1300/2527: Loss = 0.0470
2026-01-04 10:21:13,603 - INFO -   Batch 1350/2527: Loss = 0.0356
2026-01-04 10:21:42,704 - INFO -   Batch 1400/2527: Loss = 0.0337
2026-01-04 10:22:11,228 - INFO -   Batch 1450/2527: Loss = 0.0625
2026-01-04 10:22:39,869 - INFO -   Batch 1500/2527: Loss = 0.0763
2026-01-04 10:23:08,996 - INFO -   Batch 1550/2527: Loss = 0.0622
2026-01-04 10:23:37,646 - INFO -   Batch 1600/2527: Loss = 0.0429
2026-01-04 10:24:06,463 - INFO -   Batch 1650/2527: Loss = 0.0344
2026-01-04 10:24:34,997 - INFO -   Batch 1700/2527: Loss = 0.0958
2026-01-04 10:25:03,852 - INFO -   Batch 1750/2527: Loss = 0.0526
2026-01-04 10:25:32,605 - INFO -   Batch 1800/2527: Loss = 0.0604
2026-01-04 10:26:01,207 - INFO -   Batch 1850/2527: Loss = 0.0520
2026-01-04 10:26:30,116 - INFO -   Batch 1900/2527: Loss = 0.0537
2026-01-04 10:26:58,984 - INFO -   Batch 1950/2527: Loss = 0.0765
2026-01-04 10:27:27,821 - INFO -   Batch 2000/2527: Loss = 0.0488
2026-01-04 10:27:56,440 - INFO -   Batch 2050/2527: Loss = 0.0475
2026-01-04 10:28:24,908 - INFO -   Batch 2100/2527: Loss = 0.0742
2026-01-04 10:28:53,755 - INFO -   Batch 2150/2527: Loss = 0.0494
2026-01-04 10:29:22,750 - INFO -   Batch 2200/2527: Loss = 0.0552
2026-01-04 10:29:51,605 - INFO -   Batch 2250/2527: Loss = 0.0370
2026-01-04 10:30:20,334 - INFO -   Batch 2300/2527: Loss = 0.0486
2026-01-04 10:30:49,460 - INFO -   Batch 2350/2527: Loss = 0.0384
2026-01-04 10:31:18,132 - INFO -   Batch 2400/2527: Loss = 0.0368
2026-01-04 10:31:46,594 - INFO -   Batch 2450/2527: Loss = 0.0557
2026-01-04 10:32:15,436 - INFO -   Batch 2500/2527: Loss = 0.0429
2026-01-04 10:36:55,424 - INFO - Epoch 127/160: Train Loss: 0.0533, Val Loss: 1.4433, Val mIoU: 39.63%, Val PixelAcc: 78.71%
2026-01-04 10:37:25,685 - INFO -   Batch 50/2527: Loss = 0.0565
2026-01-04 10:37:54,727 - INFO -   Batch 100/2527: Loss = 0.0437
2026-01-04 10:38:23,408 - INFO -   Batch 150/2527: Loss = 0.0436
2026-01-04 10:38:52,163 - INFO -   Batch 200/2527: Loss = 0.0773
2026-01-04 10:39:20,808 - INFO -   Batch 250/2527: Loss = 0.0452
2026-01-04 10:39:49,632 - INFO -   Batch 300/2527: Loss = 0.0436
2026-01-04 10:40:18,486 - INFO -   Batch 350/2527: Loss = 0.0662
2026-01-04 10:40:46,975 - INFO -   Batch 400/2527: Loss = 0.0559
2026-01-04 10:41:15,920 - INFO -   Batch 450/2527: Loss = 0.0639
2026-01-04 10:41:44,796 - INFO -   Batch 500/2527: Loss = 0.0463
2026-01-04 10:42:13,728 - INFO -   Batch 550/2527: Loss = 0.0355
2026-01-04 10:42:42,340 - INFO -   Batch 600/2527: Loss = 0.0608
2026-01-04 10:43:11,145 - INFO -   Batch 650/2527: Loss = 0.0601
2026-01-04 10:43:39,676 - INFO -   Batch 700/2527: Loss = 0.0562
2026-01-04 10:44:08,549 - INFO -   Batch 750/2527: Loss = 0.0656
2026-01-04 10:44:37,368 - INFO -   Batch 800/2527: Loss = 0.0377
2026-01-04 10:45:06,113 - INFO -   Batch 850/2527: Loss = 0.0404
2026-01-04 10:45:35,214 - INFO -   Batch 900/2527: Loss = 0.0567
2026-01-04 10:46:03,862 - INFO -   Batch 950/2527: Loss = 0.0376
2026-01-04 10:46:32,330 - INFO -   Batch 1000/2527: Loss = 0.0745
2026-01-04 10:47:01,088 - INFO -   Batch 1050/2527: Loss = 0.0577
2026-01-04 10:47:29,763 - INFO -   Batch 1100/2527: Loss = 0.0545
2026-01-04 10:47:58,772 - INFO -   Batch 1150/2527: Loss = 0.0512
2026-01-04 10:48:27,352 - INFO -   Batch 1200/2527: Loss = 0.0348
2026-01-04 10:48:56,306 - INFO -   Batch 1250/2527: Loss = 0.0621
2026-01-04 10:49:25,203 - INFO -   Batch 1300/2527: Loss = 0.0353
2026-01-04 10:49:53,778 - INFO -   Batch 1350/2527: Loss = 0.0481
2026-01-04 10:50:22,568 - INFO -   Batch 1400/2527: Loss = 0.0340
2026-01-04 10:50:51,097 - INFO -   Batch 1450/2527: Loss = 0.0830
2026-01-04 10:51:19,930 - INFO -   Batch 1500/2527: Loss = 0.0565
2026-01-04 10:51:48,708 - INFO -   Batch 1550/2527: Loss = 0.0612
2026-01-04 10:52:17,540 - INFO -   Batch 1600/2527: Loss = 0.0389
2026-01-04 10:52:46,176 - INFO -   Batch 1650/2527: Loss = 0.0509
2026-01-04 10:53:15,003 - INFO -   Batch 1700/2527: Loss = 0.0562
2026-01-04 10:53:43,915 - INFO -   Batch 1750/2527: Loss = 0.0274
2026-01-04 10:54:12,509 - INFO -   Batch 1800/2527: Loss = 0.0482
2026-01-04 10:54:41,308 - INFO -   Batch 1850/2527: Loss = 0.0606
2026-01-04 10:55:10,149 - INFO -   Batch 1900/2527: Loss = 0.0467
2026-01-04 10:55:38,888 - INFO -   Batch 1950/2527: Loss = 0.0545
2026-01-04 10:56:07,717 - INFO -   Batch 2000/2527: Loss = 0.0620
2026-01-04 10:56:36,466 - INFO -   Batch 2050/2527: Loss = 0.0411
2026-01-04 10:57:05,561 - INFO -   Batch 2100/2527: Loss = 0.0466
2026-01-04 10:57:34,282 - INFO -   Batch 2150/2527: Loss = 0.0455
2026-01-04 10:58:03,080 - INFO -   Batch 2200/2527: Loss = 0.0790
2026-01-04 10:58:31,700 - INFO -   Batch 2250/2527: Loss = 0.0469
2026-01-04 10:59:00,206 - INFO -   Batch 2300/2527: Loss = 0.0445
2026-01-04 10:59:29,280 - INFO -   Batch 2350/2527: Loss = 0.0420
2026-01-04 10:59:57,822 - INFO -   Batch 2400/2527: Loss = 0.0402
2026-01-04 11:00:26,777 - INFO -   Batch 2450/2527: Loss = 0.0472
2026-01-04 11:00:55,655 - INFO -   Batch 2500/2527: Loss = 0.0672
2026-01-04 11:05:35,175 - INFO - Epoch 128/160: Train Loss: 0.0530, Val Loss: 1.4609, Val mIoU: 39.44%, Val PixelAcc: 78.59%
2026-01-04 11:06:05,913 - INFO -   Batch 50/2527: Loss = 0.0679
2026-01-04 11:06:34,638 - INFO -   Batch 100/2527: Loss = 0.0344
2026-01-04 11:07:03,512 - INFO -   Batch 150/2527: Loss = 0.0520
2026-01-04 11:07:32,131 - INFO -   Batch 200/2527: Loss = 0.0345
2026-01-04 11:08:00,621 - INFO -   Batch 250/2527: Loss = 0.0460
2026-01-04 11:08:29,533 - INFO -   Batch 300/2527: Loss = 0.0568
2026-01-04 11:08:58,214 - INFO -   Batch 350/2527: Loss = 0.0467
2026-01-04 11:09:27,161 - INFO -   Batch 400/2527: Loss = 0.0683
2026-01-04 11:09:55,912 - INFO -   Batch 450/2527: Loss = 0.0264
2026-01-04 11:10:24,612 - INFO -   Batch 500/2527: Loss = 0.0511
2026-01-04 11:10:53,470 - INFO -   Batch 550/2527: Loss = 0.0577
2026-01-04 11:11:22,043 - INFO -   Batch 600/2527: Loss = 0.0508
2026-01-04 11:11:50,883 - INFO -   Batch 650/2527: Loss = 0.0532
2026-01-04 11:12:19,554 - INFO -   Batch 700/2527: Loss = 0.0463
2026-01-04 11:12:48,412 - INFO -   Batch 750/2527: Loss = 0.0648
2026-01-04 11:13:17,104 - INFO -   Batch 800/2527: Loss = 0.0348
2026-01-04 11:13:45,685 - INFO -   Batch 850/2527: Loss = 0.0404
2026-01-04 11:14:14,746 - INFO -   Batch 900/2527: Loss = 0.0462
2026-01-04 11:14:43,438 - INFO -   Batch 950/2527: Loss = 0.0416
2026-01-04 11:15:12,289 - INFO -   Batch 1000/2527: Loss = 0.0618
2026-01-04 11:15:40,863 - INFO -   Batch 1050/2527: Loss = 0.0501
2026-01-04 11:16:09,764 - INFO -   Batch 1100/2527: Loss = 0.0482
2026-01-04 11:16:38,387 - INFO -   Batch 1150/2527: Loss = 0.0541
2026-01-04 11:17:07,053 - INFO -   Batch 1200/2527: Loss = 0.0627
2026-01-04 11:17:35,963 - INFO -   Batch 1250/2527: Loss = 0.0546
2026-01-04 11:18:04,814 - INFO -   Batch 1300/2527: Loss = 0.0448
2026-01-04 11:18:33,712 - INFO -   Batch 1350/2527: Loss = 0.0426
2026-01-04 11:19:02,331 - INFO -   Batch 1400/2527: Loss = 0.0421
2026-01-04 11:19:30,855 - INFO -   Batch 1450/2527: Loss = 0.0854
2026-01-04 11:19:59,724 - INFO -   Batch 1500/2527: Loss = 0.0411
2026-01-04 11:20:28,561 - INFO -   Batch 1550/2527: Loss = 0.0569
2026-01-04 11:20:57,558 - INFO -   Batch 1600/2527: Loss = 0.0802
2026-01-04 11:21:26,283 - INFO -   Batch 1650/2527: Loss = 0.0546
2026-01-04 11:21:55,338 - INFO -   Batch 1700/2527: Loss = 0.0556
2026-01-04 11:22:24,041 - INFO -   Batch 1750/2527: Loss = 0.0470
2026-01-04 11:22:52,505 - INFO -   Batch 1800/2527: Loss = 0.0482
2026-01-04 11:23:21,353 - INFO -   Batch 1850/2527: Loss = 0.0521
2026-01-04 11:23:50,014 - INFO -   Batch 1900/2527: Loss = 0.0610
2026-01-04 11:24:18,895 - INFO -   Batch 1950/2527: Loss = 0.0449
2026-01-04 11:24:47,609 - INFO -   Batch 2000/2527: Loss = 0.0586
2026-01-04 11:25:16,513 - INFO -   Batch 2050/2527: Loss = 0.0423
2026-01-04 11:25:45,253 - INFO -   Batch 2100/2527: Loss = 0.0669
2026-01-04 11:26:13,992 - INFO -   Batch 2150/2527: Loss = 0.0547
2026-01-04 11:26:42,806 - INFO -   Batch 2200/2527: Loss = 0.0579
2026-01-04 11:27:11,500 - INFO -   Batch 2250/2527: Loss = 0.0437
2026-01-04 11:27:40,389 - INFO -   Batch 2300/2527: Loss = 0.0472
2026-01-04 11:28:09,068 - INFO -   Batch 2350/2527: Loss = 0.0584
2026-01-04 11:28:37,717 - INFO -   Batch 2400/2527: Loss = 0.0723
2026-01-04 11:29:06,634 - INFO -   Batch 2450/2527: Loss = 0.0396
2026-01-04 11:29:35,463 - INFO -   Batch 2500/2527: Loss = 0.0370
2026-01-04 11:34:19,959 - INFO - Epoch 129/160: Train Loss: 0.0525, Val Loss: 1.4804, Val mIoU: 39.72%, Val PixelAcc: 78.78%
2026-01-04 11:34:50,078 - INFO -   Batch 50/2527: Loss = 0.0520
2026-01-04 11:35:18,651 - INFO -   Batch 100/2527: Loss = 0.0482
2026-01-04 11:35:47,526 - INFO -   Batch 150/2527: Loss = 0.0690
2026-01-04 11:36:16,326 - INFO -   Batch 200/2527: Loss = 0.0337
2026-01-04 11:36:45,094 - INFO -   Batch 250/2527: Loss = 0.0578
2026-01-04 11:37:13,849 - INFO -   Batch 300/2527: Loss = 0.0562
2026-01-04 11:37:42,596 - INFO -   Batch 350/2527: Loss = 0.0535
2026-01-04 11:38:11,485 - INFO -   Batch 400/2527: Loss = 0.0467
2026-01-04 11:38:40,168 - INFO -   Batch 450/2527: Loss = 0.0861
2026-01-04 11:39:08,958 - INFO -   Batch 500/2527: Loss = 0.0472
2026-01-04 11:39:37,624 - INFO -   Batch 550/2527: Loss = 0.0482
2026-01-04 11:40:06,674 - INFO -   Batch 600/2527: Loss = 0.0495
2026-01-04 11:40:35,210 - INFO -   Batch 650/2527: Loss = 0.0526
2026-01-04 11:41:03,822 - INFO -   Batch 700/2527: Loss = 0.0426
2026-01-04 11:41:32,937 - INFO -   Batch 750/2527: Loss = 0.0409
2026-01-04 11:42:01,628 - INFO -   Batch 800/2527: Loss = 0.0490
2026-01-04 11:42:30,454 - INFO -   Batch 850/2527: Loss = 0.0350
2026-01-04 11:42:58,994 - INFO -   Batch 900/2527: Loss = 0.0569
2026-01-04 11:43:27,858 - INFO -   Batch 950/2527: Loss = 0.0535
2026-01-04 11:43:56,583 - INFO -   Batch 1000/2527: Loss = 0.0836
2026-01-04 11:44:25,169 - INFO -   Batch 1050/2527: Loss = 0.0534
2026-01-04 11:44:54,127 - INFO -   Batch 1100/2527: Loss = 0.0379
2026-01-04 11:45:22,989 - INFO -   Batch 1150/2527: Loss = 0.0443
2026-01-04 11:45:51,863 - INFO -   Batch 1200/2527: Loss = 0.0375
2026-01-04 11:46:20,446 - INFO -   Batch 1250/2527: Loss = 0.0574
2026-01-04 11:46:48,904 - INFO -   Batch 1300/2527: Loss = 0.0653
2026-01-04 11:47:17,713 - INFO -   Batch 1350/2527: Loss = 0.0529
2026-01-04 11:47:46,539 - INFO -   Batch 1400/2527: Loss = 0.0414
2026-01-04 11:48:15,347 - INFO -   Batch 1450/2527: Loss = 0.0368
2026-01-04 11:48:44,136 - INFO -   Batch 1500/2527: Loss = 0.0607
2026-01-04 11:49:13,266 - INFO -   Batch 1550/2527: Loss = 0.0479
2026-01-04 11:49:42,106 - INFO -   Batch 1600/2527: Loss = 0.0676
2026-01-04 11:50:10,594 - INFO -   Batch 1650/2527: Loss = 0.0686
2026-01-04 11:50:39,402 - INFO -   Batch 1700/2527: Loss = 0.0620
2026-01-04 11:51:08,071 - INFO -   Batch 1750/2527: Loss = 0.0486
2026-01-04 11:51:37,169 - INFO -   Batch 1800/2527: Loss = 0.0600
2026-01-04 11:52:05,755 - INFO -   Batch 1850/2527: Loss = 0.0422
2026-01-04 11:52:34,378 - INFO -   Batch 1900/2527: Loss = 0.0510
2026-01-04 11:53:03,486 - INFO -   Batch 1950/2527: Loss = 0.0538
2026-01-04 11:53:32,168 - INFO -   Batch 2000/2527: Loss = 0.0435
2026-01-04 11:54:00,989 - INFO -   Batch 2050/2527: Loss = 0.0355
2026-01-04 11:54:29,583 - INFO -   Batch 2100/2527: Loss = 0.0445
2026-01-04 11:54:58,410 - INFO -   Batch 2150/2527: Loss = 0.0415
2026-01-04 11:55:27,269 - INFO -   Batch 2200/2527: Loss = 0.0363
2026-01-04 11:55:55,768 - INFO -   Batch 2250/2527: Loss = 0.0532
2026-01-04 11:56:24,689 - INFO -   Batch 2300/2527: Loss = 0.0733
2026-01-04 11:56:53,683 - INFO -   Batch 2350/2527: Loss = 0.0649
2026-01-04 11:57:22,559 - INFO -   Batch 2400/2527: Loss = 0.0651
2026-01-04 11:57:51,193 - INFO -   Batch 2450/2527: Loss = 0.0545
2026-01-04 11:58:20,010 - INFO -   Batch 2500/2527: Loss = 0.0389
2026-01-04 12:02:59,300 - INFO - Epoch 130/160: Train Loss: 0.0522, Val Loss: 1.4783, Val mIoU: 39.43%, Val PixelAcc: 78.51%
2026-01-04 12:03:02,817 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_130.pth
2026-01-04 12:03:02,819 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_130.pth
2026-01-04 12:03:33,234 - INFO -   Batch 50/2527: Loss = 0.0495
2026-01-04 12:04:02,304 - INFO -   Batch 100/2527: Loss = 0.0483
2026-01-04 12:04:30,906 - INFO -   Batch 150/2527: Loss = 0.0428
2026-01-04 12:04:59,676 - INFO -   Batch 200/2527: Loss = 0.0548
2026-01-04 12:05:28,126 - INFO -   Batch 250/2527: Loss = 0.0429
2026-01-04 12:05:56,980 - INFO -   Batch 300/2527: Loss = 0.0556
2026-01-04 12:06:25,769 - INFO -   Batch 350/2527: Loss = 0.0580
2026-01-04 12:06:54,566 - INFO -   Batch 400/2527: Loss = 0.0557
2026-01-04 12:07:23,288 - INFO -   Batch 450/2527: Loss = 0.0635
2026-01-04 12:07:52,405 - INFO -   Batch 500/2527: Loss = 0.0676
2026-01-04 12:08:21,233 - INFO -   Batch 550/2527: Loss = 0.0468
2026-01-04 12:08:49,742 - INFO -   Batch 600/2527: Loss = 0.0508
2026-01-04 12:09:18,563 - INFO -   Batch 650/2527: Loss = 0.0762
2026-01-04 12:09:47,220 - INFO -   Batch 700/2527: Loss = 0.0445
2026-01-04 12:10:16,300 - INFO -   Batch 750/2527: Loss = 0.0385
2026-01-04 12:10:44,893 - INFO -   Batch 800/2527: Loss = 0.0468
2026-01-04 12:11:13,524 - INFO -   Batch 850/2527: Loss = 0.0683
2026-01-04 12:11:42,633 - INFO -   Batch 900/2527: Loss = 0.0529
2026-01-04 12:12:11,353 - INFO -   Batch 950/2527: Loss = 0.0556
2026-01-04 12:12:40,139 - INFO -   Batch 1000/2527: Loss = 0.0420
2026-01-04 12:13:08,728 - INFO -   Batch 1050/2527: Loss = 0.0328
2026-01-04 12:13:37,542 - INFO -   Batch 1100/2527: Loss = 0.0374
2026-01-04 12:14:06,424 - INFO -   Batch 1150/2527: Loss = 0.0469
2026-01-04 12:14:34,921 - INFO -   Batch 1200/2527: Loss = 0.0562
2026-01-04 12:15:03,884 - INFO -   Batch 1250/2527: Loss = 0.0421
2026-01-04 12:15:32,792 - INFO -   Batch 1300/2527: Loss = 0.0316
2026-01-04 12:16:01,720 - INFO -   Batch 1350/2527: Loss = 0.0494
2026-01-04 12:16:30,328 - INFO -   Batch 1400/2527: Loss = 0.0491
2026-01-04 12:16:59,159 - INFO -   Batch 1450/2527: Loss = 0.0409
2026-01-04 12:17:27,736 - INFO -   Batch 1500/2527: Loss = 0.0503
2026-01-04 12:17:56,499 - INFO -   Batch 1550/2527: Loss = 0.0386
2026-01-04 12:18:25,326 - INFO -   Batch 1600/2527: Loss = 0.0542
2026-01-04 12:18:54,028 - INFO -   Batch 1650/2527: Loss = 0.0555
2026-01-04 12:19:23,151 - INFO -   Batch 1700/2527: Loss = 0.0541
2026-01-04 12:19:51,798 - INFO -   Batch 1750/2527: Loss = 0.0411
2026-01-04 12:20:20,284 - INFO -   Batch 1800/2527: Loss = 0.0384
2026-01-04 12:20:49,091 - INFO -   Batch 1850/2527: Loss = 0.0531
2026-01-04 12:21:17,743 - INFO -   Batch 1900/2527: Loss = 0.0457
2026-01-04 12:21:46,771 - INFO -   Batch 1950/2527: Loss = 0.0318
2026-01-04 12:22:15,352 - INFO -   Batch 2000/2527: Loss = 0.0366
2026-01-04 12:22:44,273 - INFO -   Batch 2050/2527: Loss = 0.0450
2026-01-04 12:23:13,161 - INFO -   Batch 2100/2527: Loss = 0.0705
2026-01-04 12:23:41,731 - INFO -   Batch 2150/2527: Loss = 0.0567
2026-01-04 12:24:10,556 - INFO -   Batch 2200/2527: Loss = 0.0696
2026-01-04 12:24:39,089 - INFO -   Batch 2250/2527: Loss = 0.0421
2026-01-04 12:25:07,951 - INFO -   Batch 2300/2527: Loss = 0.0397
2026-01-04 12:25:36,800 - INFO -   Batch 2350/2527: Loss = 0.0551
2026-01-04 12:26:05,615 - INFO -   Batch 2400/2527: Loss = 0.0338
2026-01-04 12:26:34,288 - INFO -   Batch 2450/2527: Loss = 0.0619
2026-01-04 12:27:03,291 - INFO -   Batch 2500/2527: Loss = 0.0593
2026-01-04 12:31:41,707 - INFO - Epoch 131/160: Train Loss: 0.0525, Val Loss: 1.5286, Val mIoU: 39.47%, Val PixelAcc: 78.55%
2026-01-04 12:32:12,283 - INFO -   Batch 50/2527: Loss = 0.0556
2026-01-04 12:32:40,934 - INFO -   Batch 100/2527: Loss = 0.0754
2026-01-04 12:33:09,850 - INFO -   Batch 150/2527: Loss = 0.0304
2026-01-04 12:33:38,551 - INFO -   Batch 200/2527: Loss = 0.0449
2026-01-04 12:34:07,400 - INFO -   Batch 250/2527: Loss = 0.0332
2026-01-04 12:34:36,108 - INFO -   Batch 300/2527: Loss = 0.0648
2026-01-04 12:35:04,960 - INFO -   Batch 350/2527: Loss = 0.0558
2026-01-04 12:35:33,628 - INFO -   Batch 400/2527: Loss = 0.0372
2026-01-04 12:36:02,386 - INFO -   Batch 450/2527: Loss = 0.0495
2026-01-04 12:36:31,405 - INFO -   Batch 500/2527: Loss = 0.0604
2026-01-04 12:37:00,109 - INFO -   Batch 550/2527: Loss = 0.0444
2026-01-04 12:37:28,880 - INFO -   Batch 600/2527: Loss = 0.0466
2026-01-04 12:37:57,512 - INFO -   Batch 650/2527: Loss = 0.0500
2026-01-04 12:38:26,064 - INFO -   Batch 700/2527: Loss = 0.0442
2026-01-04 12:38:54,897 - INFO -   Batch 750/2527: Loss = 0.0790
2026-01-04 12:39:23,702 - INFO -   Batch 800/2527: Loss = 0.0390
2026-01-04 12:39:52,644 - INFO -   Batch 850/2527: Loss = 0.0423
2026-01-04 12:40:21,502 - INFO -   Batch 900/2527: Loss = 0.0644
2026-01-04 12:40:50,441 - INFO -   Batch 950/2527: Loss = 0.0459
2026-01-04 12:41:19,020 - INFO -   Batch 1000/2527: Loss = 0.0393
2026-01-04 12:41:47,553 - INFO -   Batch 1050/2527: Loss = 0.0574
2026-01-04 12:42:16,460 - INFO -   Batch 1100/2527: Loss = 0.0575
2026-01-04 12:42:45,055 - INFO -   Batch 1150/2527: Loss = 0.0511
2026-01-04 12:43:14,045 - INFO -   Batch 1200/2527: Loss = 0.0537
2026-01-04 12:43:42,774 - INFO -   Batch 1250/2527: Loss = 0.0473
2026-01-04 12:44:11,825 - INFO -   Batch 1300/2527: Loss = 0.0498
2026-01-04 12:44:40,398 - INFO -   Batch 1350/2527: Loss = 0.0597
2026-01-04 12:45:08,951 - INFO -   Batch 1400/2527: Loss = 0.0617
2026-01-04 12:45:37,768 - INFO -   Batch 1450/2527: Loss = 0.0810
2026-01-04 12:46:06,465 - INFO -   Batch 1500/2527: Loss = 0.0640
2026-01-04 12:46:35,308 - INFO -   Batch 1550/2527: Loss = 0.0420
2026-01-04 12:47:04,059 - INFO -   Batch 1600/2527: Loss = 0.0713
2026-01-04 12:47:32,653 - INFO -   Batch 1650/2527: Loss = 0.0538
2026-01-04 12:48:01,671 - INFO -   Batch 1700/2527: Loss = 0.0580
2026-01-04 12:48:30,341 - INFO -   Batch 1750/2527: Loss = 0.0441
2026-01-04 12:48:59,171 - INFO -   Batch 1800/2527: Loss = 0.0355
2026-01-04 12:49:27,799 - INFO -   Batch 1850/2527: Loss = 0.0669
2026-01-04 12:49:56,728 - INFO -   Batch 1900/2527: Loss = 0.0669
2026-01-04 12:50:25,378 - INFO -   Batch 1950/2527: Loss = 0.0442
2026-01-04 12:50:54,026 - INFO -   Batch 2000/2527: Loss = 0.0323
2026-01-04 12:51:22,976 - INFO -   Batch 2050/2527: Loss = 0.0410
2026-01-04 12:51:51,942 - INFO -   Batch 2100/2527: Loss = 0.0544
2026-01-04 12:52:20,869 - INFO -   Batch 2150/2527: Loss = 0.0450
2026-01-04 12:52:49,468 - INFO -   Batch 2200/2527: Loss = 0.0618
2026-01-04 12:53:18,001 - INFO -   Batch 2250/2527: Loss = 0.0513
2026-01-04 12:53:46,923 - INFO -   Batch 2300/2527: Loss = 0.0792
2026-01-04 12:54:15,594 - INFO -   Batch 2350/2527: Loss = 0.0417
2026-01-04 12:54:44,550 - INFO -   Batch 2400/2527: Loss = 0.0707
2026-01-04 12:55:13,283 - INFO -   Batch 2450/2527: Loss = 0.0516
2026-01-04 12:55:42,311 - INFO -   Batch 2500/2527: Loss = 0.0598
2026-01-04 13:00:26,899 - INFO - Epoch 132/160: Train Loss: 0.0516, Val Loss: 1.4808, Val mIoU: 39.44%, Val PixelAcc: 78.59%
2026-01-04 13:00:57,255 - INFO -   Batch 50/2527: Loss = 0.0619
2026-01-04 13:01:25,969 - INFO -   Batch 100/2527: Loss = 0.0543
2026-01-04 13:01:54,609 - INFO -   Batch 150/2527: Loss = 0.0604
2026-01-04 13:02:23,675 - INFO -   Batch 200/2527: Loss = 0.0396
2026-01-04 13:02:52,146 - INFO -   Batch 250/2527: Loss = 0.0481
2026-01-04 13:03:21,022 - INFO -   Batch 300/2527: Loss = 0.0430
2026-01-04 13:03:50,122 - INFO -   Batch 350/2527: Loss = 0.0496
2026-01-04 13:04:18,827 - INFO -   Batch 400/2527: Loss = 0.0378
2026-01-04 13:04:47,644 - INFO -   Batch 450/2527: Loss = 0.0690
2026-01-04 13:05:16,233 - INFO -   Batch 500/2527: Loss = 0.0365
2026-01-04 13:05:44,742 - INFO -   Batch 550/2527: Loss = 0.0579
2026-01-04 13:06:13,797 - INFO -   Batch 600/2527: Loss = 0.0560
2026-01-04 13:06:42,407 - INFO -   Batch 650/2527: Loss = 0.0449
2026-01-04 13:07:11,342 - INFO -   Batch 700/2527: Loss = 0.0509
2026-01-04 13:07:40,233 - INFO -   Batch 750/2527: Loss = 0.0486
2026-01-04 13:08:09,099 - INFO -   Batch 800/2527: Loss = 0.0433
2026-01-04 13:08:37,729 - INFO -   Batch 850/2527: Loss = 0.0556
2026-01-04 13:09:06,194 - INFO -   Batch 900/2527: Loss = 0.0466
2026-01-04 13:09:35,017 - INFO -   Batch 950/2527: Loss = 0.0435
2026-01-04 13:10:03,915 - INFO -   Batch 1000/2527: Loss = 0.0463
2026-01-04 13:10:32,744 - INFO -   Batch 1050/2527: Loss = 0.0651
2026-01-04 13:11:01,473 - INFO -   Batch 1100/2527: Loss = 0.0315
2026-01-04 13:11:30,234 - INFO -   Batch 1150/2527: Loss = 0.0344
2026-01-04 13:11:59,131 - INFO -   Batch 1200/2527: Loss = 0.0816
2026-01-04 13:12:27,679 - INFO -   Batch 1250/2527: Loss = 0.0448
2026-01-04 13:12:56,490 - INFO -   Batch 1300/2527: Loss = 0.0439
2026-01-04 13:13:25,101 - INFO -   Batch 1350/2527: Loss = 0.0487
2026-01-04 13:13:54,179 - INFO -   Batch 1400/2527: Loss = 0.0424
2026-01-04 13:14:22,724 - INFO -   Batch 1450/2527: Loss = 0.0335
2026-01-04 13:14:51,372 - INFO -   Batch 1500/2527: Loss = 0.0692
2026-01-04 13:15:20,468 - INFO -   Batch 1550/2527: Loss = 0.0472
2026-01-04 13:15:49,167 - INFO -   Batch 1600/2527: Loss = 0.0454
2026-01-04 13:16:17,939 - INFO -   Batch 1650/2527: Loss = 0.0610
2026-01-04 13:16:46,494 - INFO -   Batch 1700/2527: Loss = 0.0634
2026-01-04 13:17:15,327 - INFO -   Batch 1750/2527: Loss = 0.0391
2026-01-04 13:17:44,063 - INFO -   Batch 1800/2527: Loss = 0.0312
2026-01-04 13:18:12,673 - INFO -   Batch 1850/2527: Loss = 0.0451
2026-01-04 13:18:41,633 - INFO -   Batch 1900/2527: Loss = 0.0332
2026-01-04 13:19:10,521 - INFO -   Batch 1950/2527: Loss = 0.0588
2026-01-04 13:19:39,456 - INFO -   Batch 2000/2527: Loss = 0.0495
2026-01-04 13:20:08,053 - INFO -   Batch 2050/2527: Loss = 0.0470
2026-01-04 13:20:36,529 - INFO -   Batch 2100/2527: Loss = 0.0417
2026-01-04 13:21:05,389 - INFO -   Batch 2150/2527: Loss = 0.0771
2026-01-04 13:21:34,431 - INFO -   Batch 2200/2527: Loss = 0.0383
2026-01-04 13:22:03,268 - INFO -   Batch 2250/2527: Loss = 0.0445
2026-01-04 13:22:32,008 - INFO -   Batch 2300/2527: Loss = 0.0549
2026-01-04 13:23:01,103 - INFO -   Batch 2350/2527: Loss = 0.0376
2026-01-04 13:23:29,819 - INFO -   Batch 2400/2527: Loss = 0.0430
2026-01-04 13:23:58,268 - INFO -   Batch 2450/2527: Loss = 0.0438
2026-01-04 13:24:27,026 - INFO -   Batch 2500/2527: Loss = 0.0563
2026-01-04 13:29:05,377 - INFO - Epoch 133/160: Train Loss: 0.0516, Val Loss: 1.4957, Val mIoU: 39.56%, Val PixelAcc: 78.69%
2026-01-04 13:29:35,727 - INFO -   Batch 50/2527: Loss = 0.0695
2026-01-04 13:30:04,446 - INFO -   Batch 100/2527: Loss = 0.0310
2026-01-04 13:30:33,328 - INFO -   Batch 150/2527: Loss = 0.0518
2026-01-04 13:31:01,885 - INFO -   Batch 200/2527: Loss = 0.0670
2026-01-04 13:31:30,705 - INFO -   Batch 250/2527: Loss = 0.0495
2026-01-04 13:31:59,281 - INFO -   Batch 300/2527: Loss = 0.0383
2026-01-04 13:32:28,366 - INFO -   Batch 350/2527: Loss = 0.0298
2026-01-04 13:32:56,954 - INFO -   Batch 400/2527: Loss = 0.0515
2026-01-04 13:33:25,570 - INFO -   Batch 450/2527: Loss = 0.0501
2026-01-04 13:33:54,648 - INFO -   Batch 500/2527: Loss = 0.0446
2026-01-04 13:34:23,332 - INFO -   Batch 550/2527: Loss = 0.0544
2026-01-04 13:34:52,096 - INFO -   Batch 600/2527: Loss = 0.0480
2026-01-04 13:35:20,722 - INFO -   Batch 650/2527: Loss = 0.0457
2026-01-04 13:35:49,243 - INFO -   Batch 700/2527: Loss = 0.0507
2026-01-04 13:36:18,281 - INFO -   Batch 750/2527: Loss = 0.0494
2026-01-04 13:36:47,083 - INFO -   Batch 800/2527: Loss = 0.0372
2026-01-04 13:37:16,051 - INFO -   Batch 850/2527: Loss = 0.0391
2026-01-04 13:37:44,920 - INFO -   Batch 900/2527: Loss = 0.0444
2026-01-04 13:38:13,840 - INFO -   Batch 950/2527: Loss = 0.0565
2026-01-04 13:38:42,453 - INFO -   Batch 1000/2527: Loss = 0.0533
2026-01-04 13:39:10,923 - INFO -   Batch 1050/2527: Loss = 0.0457
2026-01-04 13:39:39,827 - INFO -   Batch 1100/2527: Loss = 0.0559
2026-01-04 13:40:08,681 - INFO -   Batch 1150/2527: Loss = 0.0724
2026-01-04 13:40:37,566 - INFO -   Batch 1200/2527: Loss = 0.0674
2026-01-04 13:41:06,256 - INFO -   Batch 1250/2527: Loss = 0.0404
2026-01-04 13:41:35,358 - INFO -   Batch 1300/2527: Loss = 0.0353
2026-01-04 13:42:03,926 - INFO -   Batch 1350/2527: Loss = 0.0567
2026-01-04 13:42:32,526 - INFO -   Batch 1400/2527: Loss = 0.0446
2026-01-04 13:43:01,316 - INFO -   Batch 1450/2527: Loss = 0.0387
2026-01-04 13:43:30,013 - INFO -   Batch 1500/2527: Loss = 0.0719
2026-01-04 13:43:59,038 - INFO -   Batch 1550/2527: Loss = 0.0711
2026-01-04 13:44:27,678 - INFO -   Batch 1600/2527: Loss = 0.0481
2026-01-04 13:44:56,319 - INFO -   Batch 1650/2527: Loss = 0.0534
2026-01-04 13:45:25,407 - INFO -   Batch 1700/2527: Loss = 0.0512
2026-01-04 13:45:54,064 - INFO -   Batch 1750/2527: Loss = 0.0525
2026-01-04 13:46:22,893 - INFO -   Batch 1800/2527: Loss = 0.0519
2026-01-04 13:46:51,486 - INFO -   Batch 1850/2527: Loss = 0.0563
2026-01-04 13:47:20,358 - INFO -   Batch 1900/2527: Loss = 0.0426
2026-01-04 13:47:49,135 - INFO -   Batch 1950/2527: Loss = 0.0480
2026-01-04 13:48:17,621 - INFO -   Batch 2000/2527: Loss = 0.0851
2026-01-04 13:48:46,569 - INFO -   Batch 2050/2527: Loss = 0.0596
2026-01-04 13:49:15,455 - INFO -   Batch 2100/2527: Loss = 0.0530
2026-01-04 13:49:44,401 - INFO -   Batch 2150/2527: Loss = 0.0437
2026-01-04 13:50:12,915 - INFO -   Batch 2200/2527: Loss = 0.0614
2026-01-04 13:50:41,728 - INFO -   Batch 2250/2527: Loss = 0.0554
2026-01-04 13:51:10,255 - INFO -   Batch 2300/2527: Loss = 0.0514
2026-01-04 13:51:39,068 - INFO -   Batch 2350/2527: Loss = 0.0635
2026-01-04 13:52:07,861 - INFO -   Batch 2400/2527: Loss = 0.0514
2026-01-04 13:52:36,589 - INFO -   Batch 2450/2527: Loss = 0.0598
2026-01-04 13:53:05,657 - INFO -   Batch 2500/2527: Loss = 0.0496
2026-01-04 13:57:43,335 - INFO - Epoch 134/160: Train Loss: 0.0512, Val Loss: 1.4959, Val mIoU: 39.49%, Val PixelAcc: 78.53%
2026-01-04 13:58:13,789 - INFO -   Batch 50/2527: Loss = 0.0639
2026-01-04 13:58:42,779 - INFO -   Batch 100/2527: Loss = 0.0414
2026-01-04 13:59:11,396 - INFO -   Batch 150/2527: Loss = 0.0360
2026-01-04 13:59:40,185 - INFO -   Batch 200/2527: Loss = 0.0272
2026-01-04 14:00:08,735 - INFO -   Batch 250/2527: Loss = 0.0400
2026-01-04 14:00:37,378 - INFO -   Batch 300/2527: Loss = 0.0433
2026-01-04 14:01:06,256 - INFO -   Batch 350/2527: Loss = 0.0443
2026-01-04 14:01:35,155 - INFO -   Batch 400/2527: Loss = 0.0456
2026-01-04 14:02:04,103 - INFO -   Batch 450/2527: Loss = 0.0528
2026-01-04 14:02:32,965 - INFO -   Batch 500/2527: Loss = 0.0309
2026-01-04 14:03:01,564 - INFO -   Batch 550/2527: Loss = 0.0656
2026-01-04 14:03:30,381 - INFO -   Batch 600/2527: Loss = 0.0557
2026-01-04 14:03:59,047 - INFO -   Batch 650/2527: Loss = 0.0626
2026-01-04 14:04:27,933 - INFO -   Batch 700/2527: Loss = 0.0401
2026-01-04 14:04:56,610 - INFO -   Batch 750/2527: Loss = 0.0345
2026-01-04 14:05:25,553 - INFO -   Batch 800/2527: Loss = 0.0430
2026-01-04 14:05:54,257 - INFO -   Batch 850/2527: Loss = 0.0670
2026-01-04 14:06:22,955 - INFO -   Batch 900/2527: Loss = 0.0563
2026-01-04 14:06:51,856 - INFO -   Batch 950/2527: Loss = 0.0381
2026-01-04 14:07:20,455 - INFO -   Batch 1000/2527: Loss = 0.0394
2026-01-04 14:07:49,313 - INFO -   Batch 1050/2527: Loss = 0.0653
2026-01-04 14:08:18,039 - INFO -   Batch 1100/2527: Loss = 0.0547
2026-01-04 14:08:46,901 - INFO -   Batch 1150/2527: Loss = 0.0624
2026-01-04 14:09:15,563 - INFO -   Batch 1200/2527: Loss = 0.0538
2026-01-04 14:09:44,244 - INFO -   Batch 1250/2527: Loss = 0.0825
2026-01-04 14:10:13,313 - INFO -   Batch 1300/2527: Loss = 0.0461
2026-01-04 14:10:41,969 - INFO -   Batch 1350/2527: Loss = 0.0463
2026-01-04 14:11:10,776 - INFO -   Batch 1400/2527: Loss = 0.0369
2026-01-04 14:11:39,492 - INFO -   Batch 1450/2527: Loss = 0.0386
2026-01-04 14:12:08,075 - INFO -   Batch 1500/2527: Loss = 0.0556
2026-01-04 14:12:36,918 - INFO -   Batch 1550/2527: Loss = 0.0508
2026-01-04 14:13:05,648 - INFO -   Batch 1600/2527: Loss = 0.0434
2026-01-04 14:13:34,537 - INFO -   Batch 1650/2527: Loss = 0.0591
2026-01-04 14:14:03,393 - INFO -   Batch 1700/2527: Loss = 0.0504
2026-01-04 14:14:32,258 - INFO -   Batch 1750/2527: Loss = 0.0551
2026-01-04 14:15:00,821 - INFO -   Batch 1800/2527: Loss = 0.0464
2026-01-04 14:15:29,340 - INFO -   Batch 1850/2527: Loss = 0.0363
2026-01-04 14:15:58,232 - INFO -   Batch 1900/2527: Loss = 0.0454
2026-01-04 14:16:26,865 - INFO -   Batch 1950/2527: Loss = 0.0405
2026-01-04 14:16:55,869 - INFO -   Batch 2000/2527: Loss = 0.0656
2026-01-04 14:17:24,532 - INFO -   Batch 2050/2527: Loss = 0.0732
2026-01-04 14:17:53,265 - INFO -   Batch 2100/2527: Loss = 0.0547
2026-01-04 14:18:22,196 - INFO -   Batch 2150/2527: Loss = 0.0392
2026-01-04 14:18:50,778 - INFO -   Batch 2200/2527: Loss = 0.0542
2026-01-04 14:19:19,658 - INFO -   Batch 2250/2527: Loss = 0.0597
2026-01-04 14:19:48,477 - INFO -   Batch 2300/2527: Loss = 0.0409
2026-01-04 14:20:17,334 - INFO -   Batch 2350/2527: Loss = 0.0416
2026-01-04 14:20:46,111 - INFO -   Batch 2400/2527: Loss = 0.0456
2026-01-04 14:21:14,705 - INFO -   Batch 2450/2527: Loss = 0.0481
2026-01-04 14:21:43,735 - INFO -   Batch 2500/2527: Loss = 0.0482
2026-01-04 14:26:28,347 - INFO - Epoch 135/160: Train Loss: 0.0509, Val Loss: 1.5026, Val mIoU: 38.97%, Val PixelAcc: 78.21%
2026-01-04 14:26:58,491 - INFO -   Batch 50/2527: Loss = 0.0363
2026-01-04 14:27:27,251 - INFO -   Batch 100/2527: Loss = 0.0516
2026-01-04 14:27:55,849 - INFO -   Batch 150/2527: Loss = 0.0375
2026-01-04 14:28:24,878 - INFO -   Batch 200/2527: Loss = 0.0586
2026-01-04 14:28:53,426 - INFO -   Batch 250/2527: Loss = 0.0462
2026-01-04 14:29:22,379 - INFO -   Batch 300/2527: Loss = 0.0579
2026-01-04 14:29:51,255 - INFO -   Batch 350/2527: Loss = 0.0533
2026-01-04 14:30:19,810 - INFO -   Batch 400/2527: Loss = 0.0613
2026-01-04 14:30:48,676 - INFO -   Batch 450/2527: Loss = 0.0628
2026-01-04 14:31:17,404 - INFO -   Batch 500/2527: Loss = 0.0422
2026-01-04 14:31:46,272 - INFO -   Batch 550/2527: Loss = 0.0522
2026-01-04 14:32:15,084 - INFO -   Batch 600/2527: Loss = 0.0507
2026-01-04 14:32:43,931 - INFO -   Batch 650/2527: Loss = 0.0597
2026-01-04 14:33:12,709 - INFO -   Batch 700/2527: Loss = 0.0449
2026-01-04 14:33:41,461 - INFO -   Batch 750/2527: Loss = 0.0389
2026-01-04 14:34:10,337 - INFO -   Batch 800/2527: Loss = 0.0502
2026-01-04 14:34:38,951 - INFO -   Batch 850/2527: Loss = 0.0373
2026-01-04 14:35:07,772 - INFO -   Batch 900/2527: Loss = 0.0521
2026-01-04 14:35:36,412 - INFO -   Batch 950/2527: Loss = 0.0567
2026-01-04 14:36:05,132 - INFO -   Batch 1000/2527: Loss = 0.0581
2026-01-04 14:36:33,909 - INFO -   Batch 1050/2527: Loss = 0.0494
2026-01-04 14:37:02,676 - INFO -   Batch 1100/2527: Loss = 0.0423
2026-01-04 14:37:31,761 - INFO -   Batch 1150/2527: Loss = 0.0430
2026-01-04 14:38:00,474 - INFO -   Batch 1200/2527: Loss = 0.0464
2026-01-04 14:38:29,306 - INFO -   Batch 1250/2527: Loss = 0.0338
2026-01-04 14:38:57,923 - INFO -   Batch 1300/2527: Loss = 0.0431
2026-01-04 14:39:26,504 - INFO -   Batch 1350/2527: Loss = 0.0412
2026-01-04 14:39:55,624 - INFO -   Batch 1400/2527: Loss = 0.0435
2026-01-04 14:40:24,178 - INFO -   Batch 1450/2527: Loss = 0.0537
2026-01-04 14:40:53,138 - INFO -   Batch 1500/2527: Loss = 0.0621
2026-01-04 14:41:22,017 - INFO -   Batch 1550/2527: Loss = 0.0567
2026-01-04 14:41:50,920 - INFO -   Batch 1600/2527: Loss = 0.0444
2026-01-04 14:42:19,383 - INFO -   Batch 1650/2527: Loss = 0.0519
2026-01-04 14:42:47,954 - INFO -   Batch 1700/2527: Loss = 0.0459
2026-01-04 14:43:16,804 - INFO -   Batch 1750/2527: Loss = 0.0923
2026-01-04 14:43:45,655 - INFO -   Batch 1800/2527: Loss = 0.0452
2026-01-04 14:44:14,458 - INFO -   Batch 1850/2527: Loss = 0.0681
2026-01-04 14:44:43,192 - INFO -   Batch 1900/2527: Loss = 0.0518
2026-01-04 14:45:11,935 - INFO -   Batch 1950/2527: Loss = 0.0481
2026-01-04 14:45:40,797 - INFO -   Batch 2000/2527: Loss = 0.0406
2026-01-04 14:46:09,815 - INFO -   Batch 2050/2527: Loss = 0.0603
2026-01-04 14:46:38,597 - INFO -   Batch 2100/2527: Loss = 0.0481
2026-01-04 14:47:07,179 - INFO -   Batch 2150/2527: Loss = 0.0306
2026-01-04 14:47:36,228 - INFO -   Batch 2200/2527: Loss = 0.0449
2026-01-04 14:48:04,826 - INFO -   Batch 2250/2527: Loss = 0.0489
2026-01-04 14:48:33,444 - INFO -   Batch 2300/2527: Loss = 0.0368
2026-01-04 14:49:02,569 - INFO -   Batch 2350/2527: Loss = 0.0473
2026-01-04 14:49:31,381 - INFO -   Batch 2400/2527: Loss = 0.0670
2026-01-04 14:50:00,180 - INFO -   Batch 2450/2527: Loss = 0.0521
2026-01-04 14:50:28,804 - INFO -   Batch 2500/2527: Loss = 0.0881
2026-01-04 14:55:06,450 - INFO - Epoch 136/160: Train Loss: 0.0507, Val Loss: 1.4935, Val mIoU: 39.56%, Val PixelAcc: 78.64%
2026-01-04 14:55:36,817 - INFO -   Batch 50/2527: Loss = 0.0483
2026-01-04 14:56:05,662 - INFO -   Batch 100/2527: Loss = 0.0573
2026-01-04 14:56:34,572 - INFO -   Batch 150/2527: Loss = 0.0572
2026-01-04 14:57:03,273 - INFO -   Batch 200/2527: Loss = 0.0455
2026-01-04 14:57:31,721 - INFO -   Batch 250/2527: Loss = 0.0612
2026-01-04 14:58:00,569 - INFO -   Batch 300/2527: Loss = 0.0515
2026-01-04 14:58:29,431 - INFO -   Batch 350/2527: Loss = 0.0617
2026-01-04 14:58:58,215 - INFO -   Batch 400/2527: Loss = 0.0528
2026-01-04 14:59:26,982 - INFO -   Batch 450/2527: Loss = 0.0530
2026-01-04 14:59:56,047 - INFO -   Batch 500/2527: Loss = 0.0423
2026-01-04 15:00:24,597 - INFO -   Batch 550/2527: Loss = 0.0523
2026-01-04 15:00:53,238 - INFO -   Batch 600/2527: Loss = 0.0387
2026-01-04 15:01:22,030 - INFO -   Batch 650/2527: Loss = 0.0629
2026-01-04 15:01:50,657 - INFO -   Batch 700/2527: Loss = 0.0513
2026-01-04 15:02:19,744 - INFO -   Batch 750/2527: Loss = 0.0644
2026-01-04 15:02:48,360 - INFO -   Batch 800/2527: Loss = 0.0514
2026-01-04 15:03:17,040 - INFO -   Batch 850/2527: Loss = 0.0590
2026-01-04 15:03:46,132 - INFO -   Batch 900/2527: Loss = 0.0474
2026-01-04 15:04:14,861 - INFO -   Batch 950/2527: Loss = 0.0481
2026-01-04 15:04:43,706 - INFO -   Batch 1000/2527: Loss = 0.0690
2026-01-04 15:05:12,318 - INFO -   Batch 1050/2527: Loss = 0.0499
2026-01-04 15:05:41,181 - INFO -   Batch 1100/2527: Loss = 0.0591
2026-01-04 15:06:09,953 - INFO -   Batch 1150/2527: Loss = 0.0409
2026-01-04 15:06:38,424 - INFO -   Batch 1200/2527: Loss = 0.0443
2026-01-04 15:07:07,360 - INFO -   Batch 1250/2527: Loss = 0.0412
2026-01-04 15:07:36,298 - INFO -   Batch 1300/2527: Loss = 0.0422
2026-01-04 15:08:05,169 - INFO -   Batch 1350/2527: Loss = 0.0424
2026-01-04 15:08:33,750 - INFO -   Batch 1400/2527: Loss = 0.0409
2026-01-04 15:09:02,217 - INFO -   Batch 1450/2527: Loss = 0.0486
2026-01-04 15:09:31,027 - INFO -   Batch 1500/2527: Loss = 0.0576
2026-01-04 15:09:59,890 - INFO -   Batch 1550/2527: Loss = 0.0630
2026-01-04 15:10:28,737 - INFO -   Batch 1600/2527: Loss = 0.0399
2026-01-04 15:10:57,396 - INFO -   Batch 1650/2527: Loss = 0.0503
2026-01-04 15:11:26,480 - INFO -   Batch 1700/2527: Loss = 0.0598
2026-01-04 15:11:55,141 - INFO -   Batch 1750/2527: Loss = 0.0498
2026-01-04 15:12:23,611 - INFO -   Batch 1800/2527: Loss = 0.0346
2026-01-04 15:12:52,426 - INFO -   Batch 1850/2527: Loss = 0.0570
2026-01-04 15:13:21,018 - INFO -   Batch 1900/2527: Loss = 0.0503
2026-01-04 15:13:50,083 - INFO -   Batch 1950/2527: Loss = 0.0432
2026-01-04 15:14:18,678 - INFO -   Batch 2000/2527: Loss = 0.0622
2026-01-04 15:14:47,632 - INFO -   Batch 2050/2527: Loss = 0.0532
2026-01-04 15:15:16,667 - INFO -   Batch 2100/2527: Loss = 0.0509
2026-01-04 15:15:45,264 - INFO -   Batch 2150/2527: Loss = 0.0482
2026-01-04 15:16:14,083 - INFO -   Batch 2200/2527: Loss = 0.0651
2026-01-04 15:16:42,683 - INFO -   Batch 2250/2527: Loss = 0.0491
2026-01-04 15:17:11,550 - INFO -   Batch 2300/2527: Loss = 0.0348
2026-01-04 15:17:40,450 - INFO -   Batch 2350/2527: Loss = 0.0510
2026-01-04 15:18:08,919 - INFO -   Batch 2400/2527: Loss = 0.0392
2026-01-04 15:18:37,874 - INFO -   Batch 2450/2527: Loss = 0.0405
2026-01-04 15:19:06,815 - INFO -   Batch 2500/2527: Loss = 0.0721
2026-01-04 15:23:45,279 - INFO - Epoch 137/160: Train Loss: 0.0505, Val Loss: 1.4955, Val mIoU: 39.28%, Val PixelAcc: 78.62%
2026-01-04 15:24:15,781 - INFO -   Batch 50/2527: Loss = 0.0500
2026-01-04 15:24:44,482 - INFO -   Batch 100/2527: Loss = 0.0388
2026-01-04 15:25:13,387 - INFO -   Batch 150/2527: Loss = 0.0632
2026-01-04 15:25:41,980 - INFO -   Batch 200/2527: Loss = 0.0514
2026-01-04 15:26:10,832 - INFO -   Batch 250/2527: Loss = 0.0532
2026-01-04 15:26:39,501 - INFO -   Batch 300/2527: Loss = 0.0667
2026-01-04 15:27:08,123 - INFO -   Batch 350/2527: Loss = 0.0481
2026-01-04 15:27:37,122 - INFO -   Batch 400/2527: Loss = 0.0608
2026-01-04 15:28:05,908 - INFO -   Batch 450/2527: Loss = 0.0424
2026-01-04 15:28:34,935 - INFO -   Batch 500/2527: Loss = 0.0615
2026-01-04 15:29:03,709 - INFO -   Batch 550/2527: Loss = 0.0413
2026-01-04 15:29:32,530 - INFO -   Batch 600/2527: Loss = 0.0542
2026-01-04 15:30:01,169 - INFO -   Batch 650/2527: Loss = 0.0597
2026-01-04 15:30:29,751 - INFO -   Batch 700/2527: Loss = 0.0678
2026-01-04 15:30:58,642 - INFO -   Batch 750/2527: Loss = 0.0513
2026-01-04 15:31:27,367 - INFO -   Batch 800/2527: Loss = 0.0423
2026-01-04 15:31:56,300 - INFO -   Batch 850/2527: Loss = 0.0451
2026-01-04 15:32:25,136 - INFO -   Batch 900/2527: Loss = 0.0425
2026-01-04 15:32:54,089 - INFO -   Batch 950/2527: Loss = 0.0517
2026-01-04 15:33:22,654 - INFO -   Batch 1000/2527: Loss = 0.0651
2026-01-04 15:33:51,384 - INFO -   Batch 1050/2527: Loss = 0.0417
2026-01-04 15:34:20,256 - INFO -   Batch 1100/2527: Loss = 0.0714
2026-01-04 15:34:48,962 - INFO -   Batch 1150/2527: Loss = 0.0437
2026-01-04 15:35:18,006 - INFO -   Batch 1200/2527: Loss = 0.0383
2026-01-04 15:35:46,745 - INFO -   Batch 1250/2527: Loss = 0.0421
2026-01-04 15:36:15,569 - INFO -   Batch 1300/2527: Loss = 0.0285
2026-01-04 15:36:44,518 - INFO -   Batch 1350/2527: Loss = 0.0437
2026-01-04 15:37:13,145 - INFO -   Batch 1400/2527: Loss = 0.0345
2026-01-04 15:37:42,052 - INFO -   Batch 1450/2527: Loss = 0.0514
2026-01-04 15:38:10,716 - INFO -   Batch 1500/2527: Loss = 0.0466
2026-01-04 15:38:39,657 - INFO -   Batch 1550/2527: Loss = 0.0821
2026-01-04 15:39:08,465 - INFO -   Batch 1600/2527: Loss = 0.0648
2026-01-04 15:39:37,438 - INFO -   Batch 1650/2527: Loss = 0.0339
2026-01-04 15:40:06,527 - INFO -   Batch 1700/2527: Loss = 0.0553
2026-01-04 15:40:35,418 - INFO -   Batch 1750/2527: Loss = 0.0442
2026-01-04 15:41:04,226 - INFO -   Batch 1800/2527: Loss = 0.0378
2026-01-04 15:41:32,875 - INFO -   Batch 1850/2527: Loss = 0.0512
2026-01-04 15:42:01,900 - INFO -   Batch 1900/2527: Loss = 0.0466
2026-01-04 15:42:30,744 - INFO -   Batch 1950/2527: Loss = 0.0437
2026-01-04 15:42:59,569 - INFO -   Batch 2000/2527: Loss = 0.0592
2026-01-04 15:43:28,524 - INFO -   Batch 2050/2527: Loss = 0.0499
2026-01-04 15:43:57,492 - INFO -   Batch 2100/2527: Loss = 0.0323
2026-01-04 15:44:26,508 - INFO -   Batch 2150/2527: Loss = 0.0475
2026-01-04 15:44:55,103 - INFO -   Batch 2200/2527: Loss = 0.0511
2026-01-04 15:45:23,627 - INFO -   Batch 2250/2527: Loss = 0.0533
2026-01-04 15:45:52,680 - INFO -   Batch 2300/2527: Loss = 0.0632
2026-01-04 15:46:21,376 - INFO -   Batch 2350/2527: Loss = 0.0762
2026-01-04 15:46:51,073 - INFO -   Batch 2400/2527: Loss = 0.0484
2026-01-04 15:47:19,802 - INFO -   Batch 2450/2527: Loss = 0.0481
2026-01-04 15:47:48,824 - INFO -   Batch 2500/2527: Loss = 0.0434
2026-01-04 15:52:41,306 - INFO - Epoch 138/160: Train Loss: 0.0503, Val Loss: 1.4867, Val mIoU: 39.46%, Val PixelAcc: 78.64%
2026-01-04 15:53:11,679 - INFO -   Batch 50/2527: Loss = 0.0425
2026-01-04 15:53:40,441 - INFO -   Batch 100/2527: Loss = 0.0339
2026-01-04 15:54:09,149 - INFO -   Batch 150/2527: Loss = 0.0460
2026-01-04 15:54:37,878 - INFO -   Batch 200/2527: Loss = 0.0448
2026-01-04 15:55:06,705 - INFO -   Batch 250/2527: Loss = 0.0570
2026-01-04 15:55:35,478 - INFO -   Batch 300/2527: Loss = 0.0437
2026-01-04 15:56:04,610 - INFO -   Batch 350/2527: Loss = 0.0595
2026-01-04 15:56:33,266 - INFO -   Batch 400/2527: Loss = 0.0324
2026-01-04 15:57:02,141 - INFO -   Batch 450/2527: Loss = 0.0612
2026-01-04 15:57:30,730 - INFO -   Batch 500/2527: Loss = 0.0579
2026-01-04 15:57:59,250 - INFO -   Batch 550/2527: Loss = 0.0438
2026-01-04 15:58:28,365 - INFO -   Batch 600/2527: Loss = 0.0562
2026-01-04 15:58:57,006 - INFO -   Batch 650/2527: Loss = 0.0414
2026-01-04 15:59:25,987 - INFO -   Batch 700/2527: Loss = 0.0435
2026-01-04 15:59:54,923 - INFO -   Batch 750/2527: Loss = 0.0565
2026-01-04 16:00:23,801 - INFO -   Batch 800/2527: Loss = 0.0301
2026-01-04 16:00:52,307 - INFO -   Batch 850/2527: Loss = 0.0435
2026-01-04 16:01:20,883 - INFO -   Batch 900/2527: Loss = 0.0485
2026-01-04 16:01:49,739 - INFO -   Batch 950/2527: Loss = 0.0441
2026-01-04 16:02:18,732 - INFO -   Batch 1000/2527: Loss = 0.0468
2026-01-04 16:02:47,512 - INFO -   Batch 1050/2527: Loss = 0.0499
2026-01-04 16:03:16,277 - INFO -   Batch 1100/2527: Loss = 0.1063
2026-01-04 16:03:45,102 - INFO -   Batch 1150/2527: Loss = 0.0566
2026-01-04 16:04:14,048 - INFO -   Batch 1200/2527: Loss = 0.0476
2026-01-04 16:04:42,859 - INFO -   Batch 1250/2527: Loss = 0.0440
2026-01-04 16:05:11,689 - INFO -   Batch 1300/2527: Loss = 0.0720
2026-01-04 16:05:40,288 - INFO -   Batch 1350/2527: Loss = 0.0341
2026-01-04 16:06:09,574 - INFO -   Batch 1400/2527: Loss = 0.0631
2026-01-04 16:06:38,156 - INFO -   Batch 1450/2527: Loss = 0.0391
2026-01-04 16:07:07,310 - INFO -   Batch 1500/2527: Loss = 0.0334
2026-01-04 16:07:36,453 - INFO -   Batch 1550/2527: Loss = 0.0405
2026-01-04 16:08:05,181 - INFO -   Batch 1600/2527: Loss = 0.0489
2026-01-04 16:08:34,024 - INFO -   Batch 1650/2527: Loss = 0.0501
2026-01-04 16:09:02,692 - INFO -   Batch 1700/2527: Loss = 0.0418
2026-01-04 16:09:31,299 - INFO -   Batch 1750/2527: Loss = 0.0517
2026-01-04 16:10:00,413 - INFO -   Batch 1800/2527: Loss = 0.0500
2026-01-04 16:10:29,228 - INFO -   Batch 1850/2527: Loss = 0.0634
2026-01-04 16:10:58,462 - INFO -   Batch 1900/2527: Loss = 0.0487
2026-01-04 16:11:27,421 - INFO -   Batch 1950/2527: Loss = 0.0548
2026-01-04 16:11:56,270 - INFO -   Batch 2000/2527: Loss = 0.0452
2026-01-04 16:12:24,953 - INFO -   Batch 2050/2527: Loss = 0.0452
2026-01-04 16:12:53,517 - INFO -   Batch 2100/2527: Loss = 0.0536
2026-01-04 16:13:22,409 - INFO -   Batch 2150/2527: Loss = 0.0406
2026-01-04 16:13:51,307 - INFO -   Batch 2200/2527: Loss = 0.0550
2026-01-04 16:14:20,371 - INFO -   Batch 2250/2527: Loss = 0.0499
2026-01-04 16:14:49,300 - INFO -   Batch 2300/2527: Loss = 0.0429
2026-01-04 16:15:18,628 - INFO -   Batch 2350/2527: Loss = 0.0360
2026-01-04 16:15:47,320 - INFO -   Batch 2400/2527: Loss = 0.0439
2026-01-04 16:16:15,991 - INFO -   Batch 2450/2527: Loss = 0.0324
2026-01-04 16:16:44,892 - INFO -   Batch 2500/2527: Loss = 0.0644
2026-01-04 16:21:30,447 - INFO - Epoch 139/160: Train Loss: 0.0503, Val Loss: 1.5228, Val mIoU: 39.38%, Val PixelAcc: 78.55%
2026-01-04 16:22:00,967 - INFO -   Batch 50/2527: Loss = 0.0592
2026-01-04 16:22:30,175 - INFO -   Batch 100/2527: Loss = 0.0595
2026-01-04 16:22:58,954 - INFO -   Batch 150/2527: Loss = 0.0552
2026-01-04 16:23:27,887 - INFO -   Batch 200/2527: Loss = 0.0583
2026-01-04 16:23:56,544 - INFO -   Batch 250/2527: Loss = 0.0521
2026-01-04 16:24:25,610 - INFO -   Batch 300/2527: Loss = 0.0353
2026-01-04 16:24:54,657 - INFO -   Batch 350/2527: Loss = 0.0322
2026-01-04 16:25:23,184 - INFO -   Batch 400/2527: Loss = 0.0447
2026-01-04 16:25:52,227 - INFO -   Batch 450/2527: Loss = 0.0513
2026-01-04 16:26:21,298 - INFO -   Batch 500/2527: Loss = 0.0390
2026-01-04 16:26:50,307 - INFO -   Batch 550/2527: Loss = 0.0372
2026-01-04 16:27:18,901 - INFO -   Batch 600/2527: Loss = 0.0457
2026-01-04 16:27:47,353 - INFO -   Batch 650/2527: Loss = 0.0668
2026-01-04 16:28:16,226 - INFO -   Batch 700/2527: Loss = 0.0374
2026-01-04 16:28:45,138 - INFO -   Batch 750/2527: Loss = 0.0635
2026-01-04 16:29:13,961 - INFO -   Batch 800/2527: Loss = 0.0398
2026-01-04 16:29:42,731 - INFO -   Batch 850/2527: Loss = 0.0618
2026-01-04 16:30:11,868 - INFO -   Batch 900/2527: Loss = 0.0670
2026-01-04 16:30:40,756 - INFO -   Batch 950/2527: Loss = 0.0627
2026-01-04 16:31:09,245 - INFO -   Batch 1000/2527: Loss = 0.0650
2026-01-04 16:31:38,092 - INFO -   Batch 1050/2527: Loss = 0.0730
2026-01-04 16:32:06,619 - INFO -   Batch 1100/2527: Loss = 0.0493
2026-01-04 16:32:35,699 - INFO -   Batch 1150/2527: Loss = 0.0480
2026-01-04 16:33:04,301 - INFO -   Batch 1200/2527: Loss = 0.0497
2026-01-04 16:33:33,385 - INFO -   Batch 1250/2527: Loss = 0.0371
2026-01-04 16:34:02,277 - INFO -   Batch 1300/2527: Loss = 0.0468
2026-01-04 16:34:30,958 - INFO -   Batch 1350/2527: Loss = 0.0430
2026-01-04 16:35:00,181 - INFO -   Batch 1400/2527: Loss = 0.0688
2026-01-04 16:35:28,876 - INFO -   Batch 1450/2527: Loss = 0.0522
2026-01-04 16:35:57,860 - INFO -   Batch 1500/2527: Loss = 0.0416
2026-01-04 16:36:26,990 - INFO -   Batch 1550/2527: Loss = 0.0522
2026-01-04 16:36:55,536 - INFO -   Batch 1600/2527: Loss = 0.0529
2026-01-04 16:37:24,383 - INFO -   Batch 1650/2527: Loss = 0.0421
2026-01-04 16:37:53,416 - INFO -   Batch 1700/2527: Loss = 0.0651
2026-01-04 16:38:22,423 - INFO -   Batch 1750/2527: Loss = 0.0650
2026-01-04 16:38:51,089 - INFO -   Batch 1800/2527: Loss = 0.0308
2026-01-04 16:39:19,895 - INFO -   Batch 1850/2527: Loss = 0.0461
2026-01-04 16:39:48,632 - INFO -   Batch 1900/2527: Loss = 0.0705
2026-01-04 16:40:17,352 - INFO -   Batch 1950/2527: Loss = 0.0393
2026-01-04 16:40:46,165 - INFO -   Batch 2000/2527: Loss = 0.0459
2026-01-04 16:41:14,877 - INFO -   Batch 2050/2527: Loss = 0.0477
2026-01-04 16:41:44,005 - INFO -   Batch 2100/2527: Loss = 0.0528
2026-01-04 16:42:12,665 - INFO -   Batch 2150/2527: Loss = 0.0396
2026-01-04 16:42:41,518 - INFO -   Batch 2200/2527: Loss = 0.0590
2026-01-04 16:43:10,024 - INFO -   Batch 2250/2527: Loss = 0.0542
2026-01-04 16:43:39,066 - INFO -   Batch 2300/2527: Loss = 0.0348
2026-01-04 16:44:08,208 - INFO -   Batch 2350/2527: Loss = 0.0445
2026-01-04 16:44:36,897 - INFO -   Batch 2400/2527: Loss = 0.0393
2026-01-04 16:45:06,015 - INFO -   Batch 2450/2527: Loss = 0.0617
2026-01-04 16:45:34,946 - INFO -   Batch 2500/2527: Loss = 0.0515
2026-01-04 16:50:17,518 - INFO - Epoch 140/160: Train Loss: 0.0500, Val Loss: 1.5117, Val mIoU: 39.65%, Val PixelAcc: 78.75%
2026-01-04 16:50:21,360 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_140.pth
2026-01-04 16:50:21,364 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_140.pth
2026-01-04 16:50:52,176 - INFO -   Batch 50/2527: Loss = 0.0560
2026-01-04 16:51:21,344 - INFO -   Batch 100/2527: Loss = 0.0520
2026-01-04 16:51:50,115 - INFO -   Batch 150/2527: Loss = 0.0429
2026-01-04 16:52:18,635 - INFO -   Batch 200/2527: Loss = 0.0551
2026-01-04 16:52:47,507 - INFO -   Batch 250/2527: Loss = 0.0297
2026-01-04 16:53:16,237 - INFO -   Batch 300/2527: Loss = 0.0478
2026-01-04 16:53:45,108 - INFO -   Batch 350/2527: Loss = 0.0480
2026-01-04 16:54:13,864 - INFO -   Batch 400/2527: Loss = 0.0523
2026-01-04 16:54:42,840 - INFO -   Batch 450/2527: Loss = 0.0446
2026-01-04 16:55:11,703 - INFO -   Batch 500/2527: Loss = 0.0500
2026-01-04 16:55:40,338 - INFO -   Batch 550/2527: Loss = 0.0576
2026-01-04 16:56:09,187 - INFO -   Batch 600/2527: Loss = 0.0284
2026-01-04 16:56:37,804 - INFO -   Batch 650/2527: Loss = 0.0613
2026-01-04 16:57:06,731 - INFO -   Batch 700/2527: Loss = 0.0514
2026-01-04 16:57:35,468 - INFO -   Batch 750/2527: Loss = 0.0481
2026-01-04 16:58:04,558 - INFO -   Batch 800/2527: Loss = 0.0557
2026-01-04 16:58:33,166 - INFO -   Batch 850/2527: Loss = 0.0543
2026-01-04 16:59:02,111 - INFO -   Batch 900/2527: Loss = 0.0482
2026-01-04 16:59:31,064 - INFO -   Batch 950/2527: Loss = 0.0549
2026-01-04 16:59:59,731 - INFO -   Batch 1000/2527: Loss = 0.0465
2026-01-04 17:00:28,573 - INFO -   Batch 1050/2527: Loss = 0.0414
2026-01-04 17:00:57,478 - INFO -   Batch 1100/2527: Loss = 0.0516
2026-01-04 17:01:26,093 - INFO -   Batch 1150/2527: Loss = 0.0504
2026-01-04 17:01:55,177 - INFO -   Batch 1200/2527: Loss = 0.0568
2026-01-04 17:02:24,231 - INFO -   Batch 1250/2527: Loss = 0.0486
2026-01-04 17:02:53,762 - INFO -   Batch 1300/2527: Loss = 0.0571
2026-01-04 17:03:22,562 - INFO -   Batch 1350/2527: Loss = 0.0378
2026-01-04 17:03:51,372 - INFO -   Batch 1400/2527: Loss = 0.0429
2026-01-04 17:04:19,997 - INFO -   Batch 1450/2527: Loss = 0.0487
2026-01-04 17:04:48,568 - INFO -   Batch 1500/2527: Loss = 0.0353
2026-01-04 17:05:17,603 - INFO -   Batch 1550/2527: Loss = 0.0402
2026-01-04 17:05:46,405 - INFO -   Batch 1600/2527: Loss = 0.0482
2026-01-04 17:06:15,313 - INFO -   Batch 1650/2527: Loss = 0.0349
2026-01-04 17:06:44,259 - INFO -   Batch 1700/2527: Loss = 0.0427
2026-01-04 17:07:12,786 - INFO -   Batch 1750/2527: Loss = 0.0394
2026-01-04 17:07:41,715 - INFO -   Batch 1800/2527: Loss = 0.0537
2026-01-04 17:08:10,508 - INFO -   Batch 1850/2527: Loss = 0.0328
2026-01-04 17:08:39,504 - INFO -   Batch 1900/2527: Loss = 0.0321
2026-01-04 17:09:09,958 - INFO -   Batch 1950/2527: Loss = 0.0573
2026-01-04 17:09:38,669 - INFO -   Batch 2000/2527: Loss = 0.0534
2026-01-04 17:10:07,771 - INFO -   Batch 2050/2527: Loss = 0.0618
2026-01-04 17:10:36,545 - INFO -   Batch 2100/2527: Loss = 0.0430
2026-01-04 17:11:05,599 - INFO -   Batch 2150/2527: Loss = 0.0614
2026-01-04 17:11:34,483 - INFO -   Batch 2200/2527: Loss = 0.0460
2026-01-04 17:12:03,778 - INFO -   Batch 2250/2527: Loss = 0.0488
2026-01-04 17:12:32,732 - INFO -   Batch 2300/2527: Loss = 0.0367
2026-01-04 17:13:01,800 - INFO -   Batch 2350/2527: Loss = 0.0514
2026-01-04 17:13:30,738 - INFO -   Batch 2400/2527: Loss = 0.0412
2026-01-04 17:13:59,325 - INFO -   Batch 2450/2527: Loss = 0.0754
2026-01-04 17:14:28,338 - INFO -   Batch 2500/2527: Loss = 0.0450
2026-01-04 17:19:21,295 - INFO - Epoch 141/160: Train Loss: 0.0498, Val Loss: 1.5010, Val mIoU: 39.48%, Val PixelAcc: 78.63%
2026-01-04 17:19:51,975 - INFO -   Batch 50/2527: Loss = 0.0616
2026-01-04 17:20:20,418 - INFO -   Batch 100/2527: Loss = 0.0430
2026-01-04 17:20:49,205 - INFO -   Batch 150/2527: Loss = 0.0521
2026-01-04 17:21:18,356 - INFO -   Batch 200/2527: Loss = 0.0464
2026-01-04 17:21:46,976 - INFO -   Batch 250/2527: Loss = 0.0611
2026-01-04 17:22:16,002 - INFO -   Batch 300/2527: Loss = 0.0318
2026-01-04 17:22:44,813 - INFO -   Batch 350/2527: Loss = 0.0443
2026-01-04 17:23:13,427 - INFO -   Batch 400/2527: Loss = 0.0498
2026-01-04 17:23:42,370 - INFO -   Batch 450/2527: Loss = 0.0510
2026-01-04 17:24:11,103 - INFO -   Batch 500/2527: Loss = 0.0730
2026-01-04 17:24:39,912 - INFO -   Batch 550/2527: Loss = 0.0439
2026-01-04 17:25:08,943 - INFO -   Batch 600/2527: Loss = 0.0382
2026-01-04 17:25:38,029 - INFO -   Batch 650/2527: Loss = 0.0577
2026-01-04 17:26:06,990 - INFO -   Batch 700/2527: Loss = 0.0498
2026-01-04 17:26:35,819 - INFO -   Batch 750/2527: Loss = 0.0365
2026-01-04 17:27:04,816 - INFO -   Batch 800/2527: Loss = 0.0472
2026-01-04 17:27:33,377 - INFO -   Batch 850/2527: Loss = 0.0466
2026-01-04 17:28:02,289 - INFO -   Batch 900/2527: Loss = 0.0570
2026-01-04 17:28:30,923 - INFO -   Batch 950/2527: Loss = 0.0814
2026-01-04 17:28:59,689 - INFO -   Batch 1000/2527: Loss = 0.0422
2026-01-04 17:29:28,526 - INFO -   Batch 1050/2527: Loss = 0.0530
2026-01-04 17:29:57,290 - INFO -   Batch 1100/2527: Loss = 0.0296
2026-01-04 17:30:26,764 - INFO -   Batch 1150/2527: Loss = 0.0473
2026-01-04 17:30:55,539 - INFO -   Batch 1200/2527: Loss = 0.0548
2026-01-04 17:31:24,330 - INFO -   Batch 1250/2527: Loss = 0.0486
2026-01-04 17:31:53,223 - INFO -   Batch 1300/2527: Loss = 0.0528
2026-01-04 17:32:22,436 - INFO -   Batch 1350/2527: Loss = 0.0457
2026-01-04 17:32:51,796 - INFO -   Batch 1400/2527: Loss = 0.0435
2026-01-04 17:33:20,901 - INFO -   Batch 1450/2527: Loss = 0.0392
2026-01-04 17:33:50,017 - INFO -   Batch 1500/2527: Loss = 0.0395
2026-01-04 17:34:19,385 - INFO -   Batch 1550/2527: Loss = 0.0423
2026-01-04 17:34:48,293 - INFO -   Batch 1600/2527: Loss = 0.0571
2026-01-04 17:35:17,223 - INFO -   Batch 1650/2527: Loss = 0.0463
2026-01-04 17:35:45,771 - INFO -   Batch 1700/2527: Loss = 0.0394
2026-01-04 17:36:14,577 - INFO -   Batch 1750/2527: Loss = 0.0371
2026-01-04 17:36:43,870 - INFO -   Batch 1800/2527: Loss = 0.0504
2026-01-04 17:37:12,772 - INFO -   Batch 1850/2527: Loss = 0.0531
2026-01-04 17:37:41,717 - INFO -   Batch 1900/2527: Loss = 0.0469
2026-01-04 17:38:10,573 - INFO -   Batch 1950/2527: Loss = 0.0565
2026-01-04 17:38:39,262 - INFO -   Batch 2000/2527: Loss = 0.0437
2026-01-04 17:39:08,095 - INFO -   Batch 2050/2527: Loss = 0.0568
2026-01-04 17:39:36,906 - INFO -   Batch 2100/2527: Loss = 0.0494
2026-01-04 17:40:05,852 - INFO -   Batch 2150/2527: Loss = 0.0441
2026-01-04 17:40:34,876 - INFO -   Batch 2200/2527: Loss = 0.0671
2026-01-04 17:41:03,728 - INFO -   Batch 2250/2527: Loss = 0.0591
2026-01-04 17:41:32,486 - INFO -   Batch 2300/2527: Loss = 0.0372
2026-01-04 17:42:01,664 - INFO -   Batch 2350/2527: Loss = 0.0522
2026-01-04 17:42:30,443 - INFO -   Batch 2400/2527: Loss = 0.0498
2026-01-04 17:42:59,209 - INFO -   Batch 2450/2527: Loss = 0.0517
2026-01-04 17:43:27,893 - INFO -   Batch 2500/2527: Loss = 0.0471
2026-01-04 17:48:14,516 - INFO - Epoch 142/160: Train Loss: 0.0497, Val Loss: 1.5097, Val mIoU: 39.44%, Val PixelAcc: 78.56%
2026-01-04 17:48:45,275 - INFO -   Batch 50/2527: Loss = 0.0304
2026-01-04 17:49:14,135 - INFO -   Batch 100/2527: Loss = 0.0367
2026-01-04 17:49:45,245 - INFO -   Batch 150/2527: Loss = 0.0369
2026-01-04 17:50:13,804 - INFO -   Batch 200/2527: Loss = 0.0469
2026-01-04 17:50:42,335 - INFO -   Batch 250/2527: Loss = 0.0465
2026-01-04 17:51:11,211 - INFO -   Batch 300/2527: Loss = 0.0462
2026-01-04 17:51:40,089 - INFO -   Batch 350/2527: Loss = 0.0430
2026-01-04 17:52:08,943 - INFO -   Batch 400/2527: Loss = 0.0362
2026-01-04 17:52:37,672 - INFO -   Batch 450/2527: Loss = 0.0159
2026-01-04 17:53:06,790 - INFO -   Batch 500/2527: Loss = 0.0672
2026-01-04 17:53:35,555 - INFO -   Batch 550/2527: Loss = 0.0603
2026-01-04 17:54:04,069 - INFO -   Batch 600/2527: Loss = 0.0377
2026-01-04 17:54:33,662 - INFO -   Batch 650/2527: Loss = 0.0406
2026-01-04 17:55:02,811 - INFO -   Batch 700/2527: Loss = 0.0644
2026-01-04 17:55:31,923 - INFO -   Batch 750/2527: Loss = 0.0418
2026-01-04 17:56:00,585 - INFO -   Batch 800/2527: Loss = 0.0454
2026-01-04 17:56:29,427 - INFO -   Batch 850/2527: Loss = 0.0499
2026-01-04 17:56:58,256 - INFO -   Batch 900/2527: Loss = 0.0566
2026-01-04 17:57:27,199 - INFO -   Batch 950/2527: Loss = 0.0566
2026-01-04 17:57:55,840 - INFO -   Batch 1000/2527: Loss = 0.0413
2026-01-04 17:58:24,605 - INFO -   Batch 1050/2527: Loss = 0.0603
2026-01-04 17:58:53,256 - INFO -   Batch 1100/2527: Loss = 0.0487
2026-01-04 17:59:22,311 - INFO -   Batch 1150/2527: Loss = 0.0631
2026-01-04 17:59:50,903 - INFO -   Batch 1200/2527: Loss = 0.0377
2026-01-04 18:00:19,531 - INFO -   Batch 1250/2527: Loss = 0.0442
2026-01-04 18:00:48,647 - INFO -   Batch 1300/2527: Loss = 0.0480
2026-01-04 18:01:17,400 - INFO -   Batch 1350/2527: Loss = 0.0357
2026-01-04 18:01:46,247 - INFO -   Batch 1400/2527: Loss = 0.0445
2026-01-04 18:02:14,842 - INFO -   Batch 1450/2527: Loss = 0.0528
2026-01-04 18:02:43,416 - INFO -   Batch 1500/2527: Loss = 0.0455
2026-01-04 18:03:12,489 - INFO -   Batch 1550/2527: Loss = 0.0647
2026-01-04 18:03:41,245 - INFO -   Batch 1600/2527: Loss = 0.0499
2026-01-04 18:04:10,249 - INFO -   Batch 1650/2527: Loss = 0.0597
2026-01-04 18:04:39,161 - INFO -   Batch 1700/2527: Loss = 0.0436
2026-01-04 18:05:08,101 - INFO -   Batch 1750/2527: Loss = 0.0467
2026-01-04 18:05:36,802 - INFO -   Batch 1800/2527: Loss = 0.0338
2026-01-04 18:06:05,308 - INFO -   Batch 1850/2527: Loss = 0.0516
2026-01-04 18:06:34,161 - INFO -   Batch 1900/2527: Loss = 0.0422
2026-01-04 18:07:03,093 - INFO -   Batch 1950/2527: Loss = 0.0527
2026-01-04 18:07:31,906 - INFO -   Batch 2000/2527: Loss = 0.0713
2026-01-04 18:08:00,697 - INFO -   Batch 2050/2527: Loss = 0.0486
2026-01-04 18:08:29,752 - INFO -   Batch 2100/2527: Loss = 0.0522
2026-01-04 18:08:58,404 - INFO -   Batch 2150/2527: Loss = 0.0539
2026-01-04 18:09:27,012 - INFO -   Batch 2200/2527: Loss = 0.0656
2026-01-04 18:09:55,843 - INFO -   Batch 2250/2527: Loss = 0.0422
2026-01-04 18:10:24,543 - INFO -   Batch 2300/2527: Loss = 0.0310
2026-01-04 18:10:53,638 - INFO -   Batch 2350/2527: Loss = 0.0481
2026-01-04 18:11:22,292 - INFO -   Batch 2400/2527: Loss = 0.0515
2026-01-04 18:11:50,926 - INFO -   Batch 2450/2527: Loss = 0.0455
2026-01-04 18:12:20,029 - INFO -   Batch 2500/2527: Loss = 0.0396
2026-01-04 18:16:56,310 - INFO - Epoch 143/160: Train Loss: 0.0495, Val Loss: 1.4939, Val mIoU: 39.28%, Val PixelAcc: 78.60%
2026-01-04 18:17:27,084 - INFO -   Batch 50/2527: Loss = 0.0477
2026-01-04 18:17:55,894 - INFO -   Batch 100/2527: Loss = 0.0398
2026-01-04 18:18:24,431 - INFO -   Batch 150/2527: Loss = 0.0391
2026-01-04 18:18:53,224 - INFO -   Batch 200/2527: Loss = 0.0623
2026-01-04 18:19:21,809 - INFO -   Batch 250/2527: Loss = 0.0498
2026-01-04 18:19:50,679 - INFO -   Batch 300/2527: Loss = 0.0502
2026-01-04 18:20:19,351 - INFO -   Batch 350/2527: Loss = 0.0453
2026-01-04 18:20:47,992 - INFO -   Batch 400/2527: Loss = 0.0583
2026-01-04 18:21:16,895 - INFO -   Batch 450/2527: Loss = 0.0627
2026-01-04 18:21:45,686 - INFO -   Batch 500/2527: Loss = 0.0399
2026-01-04 18:22:14,544 - INFO -   Batch 550/2527: Loss = 0.0542
2026-01-04 18:22:43,174 - INFO -   Batch 600/2527: Loss = 0.0830
2026-01-04 18:23:12,031 - INFO -   Batch 650/2527: Loss = 0.0591
2026-01-04 18:23:40,691 - INFO -   Batch 700/2527: Loss = 0.0427
2026-01-04 18:24:09,229 - INFO -   Batch 750/2527: Loss = 0.0486
2026-01-04 18:24:38,246 - INFO -   Batch 800/2527: Loss = 0.0368
2026-01-04 18:25:06,922 - INFO -   Batch 850/2527: Loss = 0.0570
2026-01-04 18:25:35,977 - INFO -   Batch 900/2527: Loss = 0.0441
2026-01-04 18:26:04,618 - INFO -   Batch 950/2527: Loss = 0.0533
2026-01-04 18:26:33,412 - INFO -   Batch 1000/2527: Loss = 0.0497
2026-01-04 18:27:01,969 - INFO -   Batch 1050/2527: Loss = 0.0481
2026-01-04 18:27:30,666 - INFO -   Batch 1100/2527: Loss = 0.0340
2026-01-04 18:27:59,600 - INFO -   Batch 1150/2527: Loss = 0.0527
2026-01-04 18:28:28,553 - INFO -   Batch 1200/2527: Loss = 0.0447
2026-01-04 18:28:57,519 - INFO -   Batch 1250/2527: Loss = 0.0486
2026-01-04 18:29:26,434 - INFO -   Batch 1300/2527: Loss = 0.0476
2026-01-04 18:29:55,029 - INFO -   Batch 1350/2527: Loss = 0.0558
2026-01-04 18:30:23,854 - INFO -   Batch 1400/2527: Loss = 0.0517
2026-01-04 18:30:52,549 - INFO -   Batch 1450/2527: Loss = 0.0483
2026-01-04 18:31:21,416 - INFO -   Batch 1500/2527: Loss = 0.0352
2026-01-04 18:31:50,163 - INFO -   Batch 1550/2527: Loss = 0.0450
2026-01-04 18:32:19,185 - INFO -   Batch 1600/2527: Loss = 0.0350
2026-01-04 18:32:47,879 - INFO -   Batch 1650/2527: Loss = 0.0593
2026-01-04 18:33:16,665 - INFO -   Batch 1700/2527: Loss = 0.0591
2026-01-04 18:33:45,610 - INFO -   Batch 1750/2527: Loss = 0.0610
2026-01-04 18:34:14,235 - INFO -   Batch 1800/2527: Loss = 0.0388
2026-01-04 18:34:43,161 - INFO -   Batch 1850/2527: Loss = 0.0393
2026-01-04 18:35:11,900 - INFO -   Batch 1900/2527: Loss = 0.0494
2026-01-04 18:35:40,854 - INFO -   Batch 1950/2527: Loss = 0.0587
2026-01-04 18:36:09,560 - INFO -   Batch 2000/2527: Loss = 0.0511
2026-01-04 18:36:38,352 - INFO -   Batch 2050/2527: Loss = 0.0549
2026-01-04 18:37:07,891 - INFO -   Batch 2100/2527: Loss = 0.0666
2026-01-04 18:37:36,644 - INFO -   Batch 2150/2527: Loss = 0.0427
2026-01-04 18:38:05,455 - INFO -   Batch 2200/2527: Loss = 0.0710
2026-01-04 18:38:34,196 - INFO -   Batch 2250/2527: Loss = 0.0688
2026-01-04 18:39:02,893 - INFO -   Batch 2300/2527: Loss = 0.0311
2026-01-04 18:39:31,767 - INFO -   Batch 2350/2527: Loss = 0.0529
2026-01-04 18:40:00,525 - INFO -   Batch 2400/2527: Loss = 0.0734
2026-01-04 18:40:29,480 - INFO -   Batch 2450/2527: Loss = 0.0563
2026-01-04 18:40:58,315 - INFO -   Batch 2500/2527: Loss = 0.0428
2026-01-04 18:45:43,470 - INFO - Epoch 144/160: Train Loss: 0.0493, Val Loss: 1.5077, Val mIoU: 39.59%, Val PixelAcc: 78.68%
2026-01-04 18:46:13,963 - INFO -   Batch 50/2527: Loss = 0.0340
2026-01-04 18:46:42,517 - INFO -   Batch 100/2527: Loss = 0.0416
2026-01-04 18:47:11,378 - INFO -   Batch 150/2527: Loss = 0.0540
2026-01-04 18:47:40,232 - INFO -   Batch 200/2527: Loss = 0.0449
2026-01-04 18:48:08,749 - INFO -   Batch 250/2527: Loss = 0.0588
2026-01-04 18:48:37,724 - INFO -   Batch 300/2527: Loss = 0.0565
2026-01-04 18:49:06,549 - INFO -   Batch 350/2527: Loss = 0.0315
2026-01-04 18:49:35,419 - INFO -   Batch 400/2527: Loss = 0.0415
2026-01-04 18:50:04,027 - INFO -   Batch 450/2527: Loss = 0.0290
2026-01-04 18:50:32,815 - INFO -   Batch 500/2527: Loss = 0.0383
2026-01-04 18:51:01,427 - INFO -   Batch 550/2527: Loss = 0.0667
2026-01-04 18:51:30,180 - INFO -   Batch 600/2527: Loss = 0.0380
2026-01-04 18:51:59,249 - INFO -   Batch 650/2527: Loss = 0.0372
2026-01-04 18:52:28,176 - INFO -   Batch 700/2527: Loss = 0.0383
2026-01-04 18:52:57,292 - INFO -   Batch 750/2527: Loss = 0.0434
2026-01-04 18:53:25,895 - INFO -   Batch 800/2527: Loss = 0.0816
2026-01-04 18:53:54,345 - INFO -   Batch 850/2527: Loss = 0.0444
2026-01-04 18:54:23,150 - INFO -   Batch 900/2527: Loss = 0.0432
2026-01-04 18:54:51,763 - INFO -   Batch 950/2527: Loss = 0.0610
2026-01-04 18:55:20,831 - INFO -   Batch 1000/2527: Loss = 0.0445
2026-01-04 18:55:49,372 - INFO -   Batch 1050/2527: Loss = 0.0603
2026-01-04 18:56:18,359 - INFO -   Batch 1100/2527: Loss = 0.0365
2026-01-04 18:56:47,236 - INFO -   Batch 1150/2527: Loss = 0.0400
2026-01-04 18:57:15,848 - INFO -   Batch 1200/2527: Loss = 0.0463
2026-01-04 18:57:44,658 - INFO -   Batch 1250/2527: Loss = 0.0450
2026-01-04 18:58:13,431 - INFO -   Batch 1300/2527: Loss = 0.0375
2026-01-04 18:58:42,267 - INFO -   Batch 1350/2527: Loss = 0.0409
2026-01-04 18:59:11,200 - INFO -   Batch 1400/2527: Loss = 0.0659
2026-01-04 18:59:40,026 - INFO -   Batch 1450/2527: Loss = 0.0453
2026-01-04 19:00:08,794 - INFO -   Batch 1500/2527: Loss = 0.0292
2026-01-04 19:00:37,587 - INFO -   Batch 1550/2527: Loss = 0.0619
2026-01-04 19:01:06,461 - INFO -   Batch 1600/2527: Loss = 0.0489
2026-01-04 19:01:35,159 - INFO -   Batch 1650/2527: Loss = 0.0517
2026-01-04 19:02:04,003 - INFO -   Batch 1700/2527: Loss = 0.0553
2026-01-04 19:02:32,663 - INFO -   Batch 1750/2527: Loss = 0.0579
2026-01-04 19:03:01,464 - INFO -   Batch 1800/2527: Loss = 0.0586
2026-01-04 19:03:30,335 - INFO -   Batch 1850/2527: Loss = 0.0427
2026-01-04 19:03:59,103 - INFO -   Batch 1900/2527: Loss = 0.0368
2026-01-04 19:04:28,313 - INFO -   Batch 1950/2527: Loss = 0.0400
2026-01-04 19:04:57,113 - INFO -   Batch 2000/2527: Loss = 0.0392
2026-01-04 19:05:26,008 - INFO -   Batch 2050/2527: Loss = 0.0494
2026-01-04 19:05:54,617 - INFO -   Batch 2100/2527: Loss = 0.0467
2026-01-04 19:06:23,141 - INFO -   Batch 2150/2527: Loss = 0.0428
2026-01-04 19:06:52,245 - INFO -   Batch 2200/2527: Loss = 0.0568
2026-01-04 19:07:20,876 - INFO -   Batch 2250/2527: Loss = 0.0844
2026-01-04 19:07:49,827 - INFO -   Batch 2300/2527: Loss = 0.0469
2026-01-04 19:08:18,767 - INFO -   Batch 2350/2527: Loss = 0.0570
2026-01-04 19:08:47,674 - INFO -   Batch 2400/2527: Loss = 0.0470
2026-01-04 19:09:16,163 - INFO -   Batch 2450/2527: Loss = 0.0487
2026-01-04 19:09:44,763 - INFO -   Batch 2500/2527: Loss = 0.0283
2026-01-04 19:14:24,575 - INFO - Epoch 145/160: Train Loss: 0.0492, Val Loss: 1.4786, Val mIoU: 39.64%, Val PixelAcc: 78.59%
2026-01-04 19:14:54,737 - INFO -   Batch 50/2527: Loss = 0.0477
2026-01-04 19:15:23,452 - INFO -   Batch 100/2527: Loss = 0.0416
2026-01-04 19:15:52,315 - INFO -   Batch 150/2527: Loss = 0.0385
2026-01-04 19:16:20,882 - INFO -   Batch 200/2527: Loss = 0.0698
2026-01-04 19:16:49,669 - INFO -   Batch 250/2527: Loss = 0.0388
2026-01-04 19:17:18,230 - INFO -   Batch 300/2527: Loss = 0.0549
2026-01-04 19:17:47,331 - INFO -   Batch 350/2527: Loss = 0.0642
2026-01-04 19:18:15,833 - INFO -   Batch 400/2527: Loss = 0.0443
2026-01-04 19:18:44,585 - INFO -   Batch 450/2527: Loss = 0.0393
2026-01-04 19:19:13,695 - INFO -   Batch 500/2527: Loss = 0.0774
2026-01-04 19:19:42,386 - INFO -   Batch 550/2527: Loss = 0.0845
2026-01-04 19:20:11,151 - INFO -   Batch 600/2527: Loss = 0.0474
2026-01-04 19:20:39,714 - INFO -   Batch 650/2527: Loss = 0.0562
2026-01-04 19:21:08,218 - INFO -   Batch 700/2527: Loss = 0.0517
2026-01-04 19:21:37,254 - INFO -   Batch 750/2527: Loss = 0.0354
2026-01-04 19:22:05,882 - INFO -   Batch 800/2527: Loss = 0.0502
2026-01-04 19:22:34,838 - INFO -   Batch 850/2527: Loss = 0.0380
2026-01-04 19:23:03,787 - INFO -   Batch 900/2527: Loss = 0.0471
2026-01-04 19:23:32,717 - INFO -   Batch 950/2527: Loss = 0.0381
2026-01-04 19:24:01,454 - INFO -   Batch 1000/2527: Loss = 0.0575
2026-01-04 19:24:29,922 - INFO -   Batch 1050/2527: Loss = 0.0432
2026-01-04 19:24:58,872 - INFO -   Batch 1100/2527: Loss = 0.0558
2026-01-04 19:25:27,751 - INFO -   Batch 1150/2527: Loss = 0.0535
2026-01-04 19:25:56,626 - INFO -   Batch 1200/2527: Loss = 0.0549
2026-01-04 19:26:25,398 - INFO -   Batch 1250/2527: Loss = 0.0397
2026-01-04 19:26:54,545 - INFO -   Batch 1300/2527: Loss = 0.0400
2026-01-04 19:27:23,181 - INFO -   Batch 1350/2527: Loss = 0.0489
2026-01-04 19:27:51,774 - INFO -   Batch 1400/2527: Loss = 0.0589
2026-01-04 19:28:20,657 - INFO -   Batch 1450/2527: Loss = 0.0682
2026-01-04 19:28:49,351 - INFO -   Batch 1500/2527: Loss = 0.0374
2026-01-04 19:29:18,445 - INFO -   Batch 1550/2527: Loss = 0.0412
2026-01-04 19:29:47,124 - INFO -   Batch 1600/2527: Loss = 0.0377
2026-01-04 19:30:15,766 - INFO -   Batch 1650/2527: Loss = 0.0471
2026-01-04 19:30:44,836 - INFO -   Batch 1700/2527: Loss = 0.0539
2026-01-04 19:31:13,627 - INFO -   Batch 1750/2527: Loss = 0.0674
2026-01-04 19:31:42,441 - INFO -   Batch 1800/2527: Loss = 0.0465
2026-01-04 19:32:11,058 - INFO -   Batch 1850/2527: Loss = 0.0636
2026-01-04 19:32:39,898 - INFO -   Batch 1900/2527: Loss = 0.0371
2026-01-04 19:33:08,775 - INFO -   Batch 1950/2527: Loss = 0.0564
2026-01-04 19:33:37,357 - INFO -   Batch 2000/2527: Loss = 0.0467
2026-01-04 19:34:06,334 - INFO -   Batch 2050/2527: Loss = 0.0495
2026-01-04 19:34:35,212 - INFO -   Batch 2100/2527: Loss = 0.0420
2026-01-04 19:35:04,117 - INFO -   Batch 2150/2527: Loss = 0.0486
2026-01-04 19:35:32,781 - INFO -   Batch 2200/2527: Loss = 0.0561
2026-01-04 19:36:01,261 - INFO -   Batch 2250/2527: Loss = 0.0414
2026-01-04 19:36:30,115 - INFO -   Batch 2300/2527: Loss = 0.0508
2026-01-04 19:36:59,016 - INFO -   Batch 2350/2527: Loss = 0.0459
2026-01-04 19:37:27,786 - INFO -   Batch 2400/2527: Loss = 0.0370
2026-01-04 19:37:56,523 - INFO -   Batch 2450/2527: Loss = 0.0549
2026-01-04 19:38:25,621 - INFO -   Batch 2500/2527: Loss = 0.0501
2026-01-04 19:43:04,592 - INFO - Epoch 146/160: Train Loss: 0.0491, Val Loss: 1.5128, Val mIoU: 39.38%, Val PixelAcc: 78.54%
2026-01-04 19:43:35,157 - INFO -   Batch 50/2527: Loss = 0.0337
2026-01-04 19:44:04,168 - INFO -   Batch 100/2527: Loss = 0.0514
2026-01-04 19:44:32,844 - INFO -   Batch 150/2527: Loss = 0.0660
2026-01-04 19:45:01,662 - INFO -   Batch 200/2527: Loss = 0.0639
2026-01-04 19:45:30,258 - INFO -   Batch 250/2527: Loss = 0.0449
2026-01-04 19:45:58,912 - INFO -   Batch 300/2527: Loss = 0.0593
2026-01-04 19:46:27,793 - INFO -   Batch 350/2527: Loss = 0.0547
2026-01-04 19:46:56,523 - INFO -   Batch 400/2527: Loss = 0.0412
2026-01-04 19:47:25,463 - INFO -   Batch 450/2527: Loss = 0.0478
2026-01-04 19:47:54,325 - INFO -   Batch 500/2527: Loss = 0.0410
2026-01-04 19:48:22,912 - INFO -   Batch 550/2527: Loss = 0.0509
2026-01-04 19:48:51,701 - INFO -   Batch 600/2527: Loss = 0.0467
2026-01-04 19:49:20,518 - INFO -   Batch 650/2527: Loss = 0.0529
2026-01-04 19:49:49,417 - INFO -   Batch 700/2527: Loss = 0.0348
2026-01-04 19:50:18,111 - INFO -   Batch 750/2527: Loss = 0.0593
2026-01-04 19:50:47,060 - INFO -   Batch 800/2527: Loss = 0.0556
2026-01-04 19:51:15,798 - INFO -   Batch 850/2527: Loss = 0.0324
2026-01-04 19:51:44,509 - INFO -   Batch 900/2527: Loss = 0.0463
2026-01-04 19:52:13,451 - INFO -   Batch 950/2527: Loss = 0.0640
2026-01-04 19:52:42,074 - INFO -   Batch 1000/2527: Loss = 0.0480
2026-01-04 19:53:10,969 - INFO -   Batch 1050/2527: Loss = 0.0466
2026-01-04 19:53:39,664 - INFO -   Batch 1100/2527: Loss = 0.0410
2026-01-04 19:54:08,211 - INFO -   Batch 1150/2527: Loss = 0.0477
2026-01-04 19:54:37,250 - INFO -   Batch 1200/2527: Loss = 0.0464
2026-01-04 19:55:06,009 - INFO -   Batch 1250/2527: Loss = 0.0391
2026-01-04 19:55:35,111 - INFO -   Batch 1300/2527: Loss = 0.0389
2026-01-04 19:56:03,872 - INFO -   Batch 1350/2527: Loss = 0.0507
2026-01-04 19:56:32,660 - INFO -   Batch 1400/2527: Loss = 0.0378
2026-01-04 19:57:01,335 - INFO -   Batch 1450/2527: Loss = 0.0641
2026-01-04 19:57:30,450 - INFO -   Batch 1500/2527: Loss = 0.0441
2026-01-04 19:57:59,293 - INFO -   Batch 1550/2527: Loss = 0.0496
2026-01-04 19:58:28,108 - INFO -   Batch 1600/2527: Loss = 0.0566
2026-01-04 19:58:57,052 - INFO -   Batch 1650/2527: Loss = 0.0502
2026-01-04 19:59:26,021 - INFO -   Batch 1700/2527: Loss = 0.0457
2026-01-04 19:59:54,964 - INFO -   Batch 1750/2527: Loss = 0.0667
2026-01-04 20:00:23,483 - INFO -   Batch 1800/2527: Loss = 0.0669
2026-01-04 20:00:52,276 - INFO -   Batch 1850/2527: Loss = 0.0318
2026-01-04 20:01:21,209 - INFO -   Batch 1900/2527: Loss = 0.0439
2026-01-04 20:01:49,865 - INFO -   Batch 1950/2527: Loss = 0.0320
2026-01-04 20:02:18,897 - INFO -   Batch 2000/2527: Loss = 0.0541
2026-01-04 20:02:47,595 - INFO -   Batch 2050/2527: Loss = 0.0397
2026-01-04 20:03:16,347 - INFO -   Batch 2100/2527: Loss = 0.0604
2026-01-04 20:03:45,319 - INFO -   Batch 2150/2527: Loss = 0.0447
2026-01-04 20:04:13,926 - INFO -   Batch 2200/2527: Loss = 0.0493
2026-01-04 20:04:42,825 - INFO -   Batch 2250/2527: Loss = 0.0488
2026-01-04 20:05:11,572 - INFO -   Batch 2300/2527: Loss = 0.0400
2026-01-04 20:05:40,424 - INFO -   Batch 2350/2527: Loss = 0.0661
2026-01-04 20:06:09,250 - INFO -   Batch 2400/2527: Loss = 0.0480
2026-01-04 20:06:37,868 - INFO -   Batch 2450/2527: Loss = 0.0399
2026-01-04 20:07:06,926 - INFO -   Batch 2500/2527: Loss = 0.0438
2026-01-04 20:11:52,101 - INFO - Epoch 147/160: Train Loss: 0.0491, Val Loss: 1.5097, Val mIoU: 39.41%, Val PixelAcc: 78.64%
2026-01-04 20:12:22,263 - INFO -   Batch 50/2527: Loss = 0.0668
2026-01-04 20:12:51,013 - INFO -   Batch 100/2527: Loss = 0.0317
2026-01-04 20:13:19,645 - INFO -   Batch 150/2527: Loss = 0.0465
2026-01-04 20:13:48,666 - INFO -   Batch 200/2527: Loss = 0.0687
2026-01-04 20:14:17,237 - INFO -   Batch 250/2527: Loss = 0.0305
2026-01-04 20:14:46,173 - INFO -   Batch 300/2527: Loss = 0.0472
2026-01-04 20:15:14,997 - INFO -   Batch 350/2527: Loss = 0.0361
2026-01-04 20:15:43,596 - INFO -   Batch 400/2527: Loss = 0.0561
2026-01-04 20:16:12,409 - INFO -   Batch 450/2527: Loss = 0.0586
2026-01-04 20:16:40,948 - INFO -   Batch 500/2527: Loss = 0.0454
2026-01-04 20:17:09,838 - INFO -   Batch 550/2527: Loss = 0.0627
2026-01-04 20:17:38,681 - INFO -   Batch 600/2527: Loss = 0.0374
2026-01-04 20:18:07,536 - INFO -   Batch 650/2527: Loss = 0.0449
2026-01-04 20:18:36,177 - INFO -   Batch 700/2527: Loss = 0.0513
2026-01-04 20:19:05,189 - INFO -   Batch 750/2527: Loss = 0.0529
2026-01-04 20:19:34,092 - INFO -   Batch 800/2527: Loss = 0.0388
2026-01-04 20:20:02,720 - INFO -   Batch 850/2527: Loss = 0.0334
2026-01-04 20:20:31,580 - INFO -   Batch 900/2527: Loss = 0.0442
2026-01-04 20:21:00,928 - INFO -   Batch 950/2527: Loss = 0.0571
2026-01-04 20:21:30,088 - INFO -   Batch 1000/2527: Loss = 0.0411
2026-01-04 20:21:58,595 - INFO -   Batch 1050/2527: Loss = 0.0416
2026-01-04 20:22:27,389 - INFO -   Batch 1100/2527: Loss = 0.0451
2026-01-04 20:22:56,424 - INFO -   Batch 1150/2527: Loss = 0.0397
2026-01-04 20:23:25,344 - INFO -   Batch 1200/2527: Loss = 0.0368
2026-01-04 20:23:53,946 - INFO -   Batch 1250/2527: Loss = 0.0579
2026-01-04 20:24:22,859 - INFO -   Batch 1300/2527: Loss = 0.0473
2026-01-04 20:24:51,478 - INFO -   Batch 1350/2527: Loss = 0.0438
2026-01-04 20:25:20,319 - INFO -   Batch 1400/2527: Loss = 0.0733
2026-01-04 20:25:49,164 - INFO -   Batch 1450/2527: Loss = 0.0360
2026-01-04 20:26:17,905 - INFO -   Batch 1500/2527: Loss = 0.0924
2026-01-04 20:26:46,988 - INFO -   Batch 1550/2527: Loss = 0.0671
2026-01-04 20:27:15,700 - INFO -   Batch 1600/2527: Loss = 0.0418
2026-01-04 20:27:44,183 - INFO -   Batch 1650/2527: Loss = 0.0358
2026-01-04 20:28:13,994 - INFO -   Batch 1700/2527: Loss = 0.0526
2026-01-04 20:28:42,731 - INFO -   Batch 1750/2527: Loss = 0.0629
2026-01-04 20:29:11,830 - INFO -   Batch 1800/2527: Loss = 0.0667
2026-01-04 20:29:40,461 - INFO -   Batch 1850/2527: Loss = 0.0437
2026-01-04 20:30:09,443 - INFO -   Batch 1900/2527: Loss = 0.0437
2026-01-04 20:30:38,335 - INFO -   Batch 1950/2527: Loss = 0.0712
2026-01-04 20:31:06,917 - INFO -   Batch 2000/2527: Loss = 0.0410
2026-01-04 20:31:35,761 - INFO -   Batch 2050/2527: Loss = 0.0508
2026-01-04 20:32:04,332 - INFO -   Batch 2100/2527: Loss = 0.0299
2026-01-04 20:32:33,215 - INFO -   Batch 2150/2527: Loss = 0.0534
2026-01-04 20:33:02,102 - INFO -   Batch 2200/2527: Loss = 0.0412
2026-01-04 20:33:30,990 - INFO -   Batch 2250/2527: Loss = 0.0577
2026-01-04 20:33:59,701 - INFO -   Batch 2300/2527: Loss = 0.0659
2026-01-04 20:34:28,801 - INFO -   Batch 2350/2527: Loss = 0.0530
2026-01-04 20:34:57,689 - INFO -   Batch 2400/2527: Loss = 0.0525
2026-01-04 20:35:26,350 - INFO -   Batch 2450/2527: Loss = 0.0433
2026-01-04 20:35:55,151 - INFO -   Batch 2500/2527: Loss = 0.0474
2026-01-04 20:40:34,517 - INFO - Epoch 148/160: Train Loss: 0.0489, Val Loss: 1.5220, Val mIoU: 39.49%, Val PixelAcc: 78.72%
2026-01-04 20:41:04,752 - INFO -   Batch 50/2527: Loss = 0.0523
2026-01-04 20:41:33,826 - INFO -   Batch 100/2527: Loss = 0.0517
2026-01-04 20:42:02,642 - INFO -   Batch 150/2527: Loss = 0.0430
2026-01-04 20:42:31,423 - INFO -   Batch 200/2527: Loss = 0.0403
2026-01-04 20:42:59,903 - INFO -   Batch 250/2527: Loss = 0.0501
2026-01-04 20:43:28,488 - INFO -   Batch 300/2527: Loss = 0.0494
2026-01-04 20:43:57,566 - INFO -   Batch 350/2527: Loss = 0.0319
2026-01-04 20:44:26,268 - INFO -   Batch 400/2527: Loss = 0.0436
2026-01-04 20:44:55,186 - INFO -   Batch 450/2527: Loss = 0.0364
2026-01-04 20:45:24,068 - INFO -   Batch 500/2527: Loss = 0.0674
2026-01-04 20:45:52,618 - INFO -   Batch 550/2527: Loss = 0.0464
2026-01-04 20:46:21,470 - INFO -   Batch 600/2527: Loss = 0.0694
2026-01-04 20:46:50,043 - INFO -   Batch 650/2527: Loss = 0.0447
2026-01-04 20:47:18,938 - INFO -   Batch 700/2527: Loss = 0.0331
2026-01-04 20:47:47,827 - INFO -   Batch 750/2527: Loss = 0.0554
2026-01-04 20:48:16,612 - INFO -   Batch 800/2527: Loss = 0.0571
2026-01-04 20:48:45,441 - INFO -   Batch 850/2527: Loss = 0.0411
2026-01-04 20:49:14,209 - INFO -   Batch 900/2527: Loss = 0.0863
2026-01-04 20:49:43,075 - INFO -   Batch 950/2527: Loss = 0.0602
2026-01-04 20:50:11,723 - INFO -   Batch 1000/2527: Loss = 0.0732
2026-01-04 20:50:40,565 - INFO -   Batch 1050/2527: Loss = 0.0404
2026-01-04 20:51:09,145 - INFO -   Batch 1100/2527: Loss = 0.0399
2026-01-04 20:51:38,246 - INFO -   Batch 1150/2527: Loss = 0.0491
2026-01-04 20:52:06,776 - INFO -   Batch 1200/2527: Loss = 0.0541
2026-01-04 20:52:35,534 - INFO -   Batch 1250/2527: Loss = 0.0626
2026-01-04 20:53:04,718 - INFO -   Batch 1300/2527: Loss = 0.0585
2026-01-04 20:53:33,326 - INFO -   Batch 1350/2527: Loss = 0.0477
2026-01-04 20:54:02,179 - INFO -   Batch 1400/2527: Loss = 0.0548
2026-01-04 20:54:30,849 - INFO -   Batch 1450/2527: Loss = 0.0591
2026-01-04 20:54:59,418 - INFO -   Batch 1500/2527: Loss = 0.0449
2026-01-04 20:55:28,495 - INFO -   Batch 1550/2527: Loss = 0.0571
2026-01-04 20:55:57,094 - INFO -   Batch 1600/2527: Loss = 0.0490
2026-01-04 20:56:26,106 - INFO -   Batch 1650/2527: Loss = 0.0423
2026-01-04 20:56:54,961 - INFO -   Batch 1700/2527: Loss = 0.0461
2026-01-04 20:57:23,837 - INFO -   Batch 1750/2527: Loss = 0.0305
2026-01-04 20:57:52,467 - INFO -   Batch 1800/2527: Loss = 0.0728
2026-01-04 20:58:20,965 - INFO -   Batch 1850/2527: Loss = 0.0618
2026-01-04 20:58:49,889 - INFO -   Batch 1900/2527: Loss = 0.0483
2026-01-04 20:59:18,822 - INFO -   Batch 1950/2527: Loss = 0.0651
2026-01-04 20:59:47,741 - INFO -   Batch 2000/2527: Loss = 0.0800
2026-01-04 21:00:16,685 - INFO -   Batch 2050/2527: Loss = 0.0434
2026-01-04 21:00:45,476 - INFO -   Batch 2100/2527: Loss = 0.0416
2026-01-04 21:01:14,419 - INFO -   Batch 2150/2527: Loss = 0.0524
2026-01-04 21:01:43,039 - INFO -   Batch 2200/2527: Loss = 0.0437
2026-01-04 21:02:11,921 - INFO -   Batch 2250/2527: Loss = 0.0668
2026-01-04 21:02:40,599 - INFO -   Batch 2300/2527: Loss = 0.0362
2026-01-04 21:03:09,705 - INFO -   Batch 2350/2527: Loss = 0.0630
2026-01-04 21:03:38,315 - INFO -   Batch 2400/2527: Loss = 0.0554
2026-01-04 21:04:06,983 - INFO -   Batch 2450/2527: Loss = 0.0678
2026-01-04 21:04:36,297 - INFO -   Batch 2500/2527: Loss = 0.0551
2026-01-04 21:09:17,752 - INFO - Epoch 149/160: Train Loss: 0.0488, Val Loss: 1.5087, Val mIoU: 39.55%, Val PixelAcc: 78.69%
2026-01-04 21:09:48,451 - INFO -   Batch 50/2527: Loss = 0.0469
2026-01-04 21:10:17,120 - INFO -   Batch 100/2527: Loss = 0.0442
2026-01-04 21:10:45,828 - INFO -   Batch 150/2527: Loss = 0.0527
2026-01-04 21:11:14,579 - INFO -   Batch 200/2527: Loss = 0.0639
2026-01-04 21:11:43,216 - INFO -   Batch 250/2527: Loss = 0.0650
2026-01-04 21:12:12,091 - INFO -   Batch 300/2527: Loss = 0.0394
2026-01-04 21:12:40,785 - INFO -   Batch 350/2527: Loss = 0.0358
2026-01-04 21:13:09,406 - INFO -   Batch 400/2527: Loss = 0.0523
2026-01-04 21:13:38,300 - INFO -   Batch 450/2527: Loss = 0.0393
2026-01-04 21:14:07,228 - INFO -   Batch 500/2527: Loss = 0.0485
2026-01-04 21:14:36,164 - INFO -   Batch 550/2527: Loss = 0.0468
2026-01-04 21:15:04,770 - INFO -   Batch 600/2527: Loss = 0.0450
2026-01-04 21:15:33,635 - INFO -   Batch 650/2527: Loss = 0.0491
2026-01-04 21:16:02,239 - INFO -   Batch 700/2527: Loss = 0.0385
2026-01-04 21:16:30,765 - INFO -   Batch 750/2527: Loss = 0.0458
2026-01-04 21:16:59,780 - INFO -   Batch 800/2527: Loss = 0.0376
2026-01-04 21:17:28,483 - INFO -   Batch 850/2527: Loss = 0.0573
2026-01-04 21:17:57,578 - INFO -   Batch 900/2527: Loss = 0.0249
2026-01-04 21:18:26,207 - INFO -   Batch 950/2527: Loss = 0.0473
2026-01-04 21:18:54,664 - INFO -   Batch 1000/2527: Loss = 0.0536
2026-01-04 21:19:23,538 - INFO -   Batch 1050/2527: Loss = 0.0422
2026-01-04 21:19:52,251 - INFO -   Batch 1100/2527: Loss = 0.0340
2026-01-04 21:20:21,110 - INFO -   Batch 1150/2527: Loss = 0.0535
2026-01-04 21:20:49,842 - INFO -   Batch 1200/2527: Loss = 0.0371
2026-01-04 21:21:18,729 - INFO -   Batch 1250/2527: Loss = 0.0599
2026-01-04 21:21:47,560 - INFO -   Batch 1300/2527: Loss = 0.0553
2026-01-04 21:22:16,127 - INFO -   Batch 1350/2527: Loss = 0.0762
2026-01-04 21:22:44,937 - INFO -   Batch 1400/2527: Loss = 0.0682
2026-01-04 21:23:13,577 - INFO -   Batch 1450/2527: Loss = 0.0614
2026-01-04 21:23:42,478 - INFO -   Batch 1500/2527: Loss = 0.0617
2026-01-04 21:24:11,171 - INFO -   Batch 1550/2527: Loss = 0.0345
2026-01-04 21:24:40,153 - INFO -   Batch 1600/2527: Loss = 0.0474
2026-01-04 21:25:08,739 - INFO -   Batch 1650/2527: Loss = 0.0536
2026-01-04 21:25:37,725 - INFO -   Batch 1700/2527: Loss = 0.0499
2026-01-04 21:26:06,693 - INFO -   Batch 1750/2527: Loss = 0.0748
2026-01-04 21:26:35,251 - INFO -   Batch 1800/2527: Loss = 0.0476
2026-01-04 21:27:04,182 - INFO -   Batch 1850/2527: Loss = 0.0572
2026-01-04 21:27:32,877 - INFO -   Batch 1900/2527: Loss = 0.0496
2026-01-04 21:28:01,410 - INFO -   Batch 1950/2527: Loss = 0.0304
2026-01-04 21:28:30,410 - INFO -   Batch 2000/2527: Loss = 0.0418
2026-01-04 21:28:59,142 - INFO -   Batch 2050/2527: Loss = 0.0419
2026-01-04 21:29:28,203 - INFO -   Batch 2100/2527: Loss = 0.0459
2026-01-04 21:29:56,886 - INFO -   Batch 2150/2527: Loss = 0.0507
2026-01-04 21:30:25,705 - INFO -   Batch 2200/2527: Loss = 0.0567
2026-01-04 21:30:54,401 - INFO -   Batch 2250/2527: Loss = 0.0507
2026-01-04 21:31:22,976 - INFO -   Batch 2300/2527: Loss = 0.0451
2026-01-04 21:31:51,805 - INFO -   Batch 2350/2527: Loss = 0.0502
2026-01-04 21:32:20,636 - INFO -   Batch 2400/2527: Loss = 0.0558
2026-01-04 21:32:49,598 - INFO -   Batch 2450/2527: Loss = 0.0414
2026-01-04 21:33:18,445 - INFO -   Batch 2500/2527: Loss = 0.0692
2026-01-04 21:38:03,153 - INFO - Epoch 150/160: Train Loss: 0.0488, Val Loss: 1.5122, Val mIoU: 39.58%, Val PixelAcc: 78.53%
2026-01-04 21:38:06,713 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_150.pth
2026-01-04 21:38:06,715 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_150.pth
2026-01-04 21:38:37,248 - INFO -   Batch 50/2527: Loss = 0.0469
2026-01-04 21:39:05,945 - INFO -   Batch 100/2527: Loss = 0.0516
2026-01-04 21:39:34,668 - INFO -   Batch 150/2527: Loss = 0.0403
2026-01-04 21:40:03,718 - INFO -   Batch 200/2527: Loss = 0.0457
2026-01-04 21:40:32,281 - INFO -   Batch 250/2527: Loss = 0.0408
2026-01-04 21:41:00,917 - INFO -   Batch 300/2527: Loss = 0.0468
2026-01-04 21:41:30,045 - INFO -   Batch 350/2527: Loss = 0.0297
2026-01-04 21:41:58,772 - INFO -   Batch 400/2527: Loss = 0.0433
2026-01-04 21:42:27,641 - INFO -   Batch 450/2527: Loss = 0.0517
2026-01-04 21:42:56,241 - INFO -   Batch 500/2527: Loss = 0.0500
2026-01-04 21:43:24,746 - INFO -   Batch 550/2527: Loss = 0.0463
2026-01-04 21:43:53,821 - INFO -   Batch 600/2527: Loss = 0.0845
2026-01-04 21:44:22,444 - INFO -   Batch 650/2527: Loss = 0.0540
2026-01-04 21:44:51,344 - INFO -   Batch 700/2527: Loss = 0.0462
2026-01-04 21:45:20,176 - INFO -   Batch 750/2527: Loss = 0.0636
2026-01-04 21:45:49,047 - INFO -   Batch 800/2527: Loss = 0.0489
2026-01-04 21:46:17,664 - INFO -   Batch 850/2527: Loss = 0.0404
2026-01-04 21:46:46,138 - INFO -   Batch 900/2527: Loss = 0.0540
2026-01-04 21:47:14,950 - INFO -   Batch 950/2527: Loss = 0.0562
2026-01-04 21:47:43,752 - INFO -   Batch 1000/2527: Loss = 0.0483
2026-01-04 21:48:12,585 - INFO -   Batch 1050/2527: Loss = 0.0454
2026-01-04 21:48:41,299 - INFO -   Batch 1100/2527: Loss = 0.0527
2026-01-04 21:49:10,432 - INFO -   Batch 1150/2527: Loss = 0.0503
2026-01-04 21:49:38,984 - INFO -   Batch 1200/2527: Loss = 0.0634
2026-01-04 21:50:07,507 - INFO -   Batch 1250/2527: Loss = 0.0514
2026-01-04 21:50:36,349 - INFO -   Batch 1300/2527: Loss = 0.0669
2026-01-04 21:51:04,936 - INFO -   Batch 1350/2527: Loss = 0.0476
2026-01-04 21:51:34,010 - INFO -   Batch 1400/2527: Loss = 0.0431
2026-01-04 21:52:02,580 - INFO -   Batch 1450/2527: Loss = 0.0524
2026-01-04 21:52:31,187 - INFO -   Batch 1500/2527: Loss = 0.0378
2026-01-04 21:53:00,280 - INFO -   Batch 1550/2527: Loss = 0.0427
2026-01-04 21:53:28,982 - INFO -   Batch 1600/2527: Loss = 0.0533
2026-01-04 21:53:57,768 - INFO -   Batch 1650/2527: Loss = 0.0674
2026-01-04 21:54:26,517 - INFO -   Batch 1700/2527: Loss = 0.0624
2026-01-04 21:54:55,333 - INFO -   Batch 1750/2527: Loss = 0.0569
2026-01-04 21:55:24,193 - INFO -   Batch 1800/2527: Loss = 0.0504
2026-01-04 21:55:52,691 - INFO -   Batch 1850/2527: Loss = 0.0593
2026-01-04 21:56:21,637 - INFO -   Batch 1900/2527: Loss = 0.0424
2026-01-04 21:56:50,558 - INFO -   Batch 1950/2527: Loss = 0.0592
2026-01-04 21:57:19,461 - INFO -   Batch 2000/2527: Loss = 0.0674
2026-01-04 21:57:48,042 - INFO -   Batch 2050/2527: Loss = 0.0473
2026-01-04 21:58:16,870 - INFO -   Batch 2100/2527: Loss = 0.0521
2026-01-04 21:58:45,389 - INFO -   Batch 2150/2527: Loss = 0.0438
2026-01-04 21:59:14,211 - INFO -   Batch 2200/2527: Loss = 0.0575
2026-01-04 21:59:43,022 - INFO -   Batch 2250/2527: Loss = 0.0468
2026-01-04 22:00:11,754 - INFO -   Batch 2300/2527: Loss = 0.0519
2026-01-04 22:00:40,861 - INFO -   Batch 2350/2527: Loss = 0.0750
2026-01-04 22:01:09,583 - INFO -   Batch 2400/2527: Loss = 0.0435
2026-01-04 22:01:38,032 - INFO -   Batch 2450/2527: Loss = 0.0255
2026-01-04 22:02:06,851 - INFO -   Batch 2500/2527: Loss = 0.0792
2026-01-04 22:06:45,702 - INFO - Epoch 151/160: Train Loss: 0.0486, Val Loss: 1.5012, Val mIoU: 39.49%, Val PixelAcc: 78.71%
2026-01-04 22:07:16,214 - INFO -   Batch 50/2527: Loss = 0.0467
2026-01-04 22:07:44,916 - INFO -   Batch 100/2527: Loss = 0.0307
2026-01-04 22:08:13,555 - INFO -   Batch 150/2527: Loss = 0.0464
2026-01-04 22:08:42,235 - INFO -   Batch 200/2527: Loss = 0.0512
2026-01-04 22:09:11,036 - INFO -   Batch 250/2527: Loss = 0.0400
2026-01-04 22:09:39,803 - INFO -   Batch 300/2527: Loss = 0.0430
2026-01-04 22:10:08,862 - INFO -   Batch 350/2527: Loss = 0.0465
2026-01-04 22:10:37,474 - INFO -   Batch 400/2527: Loss = 0.0434
2026-01-04 22:11:06,104 - INFO -   Batch 450/2527: Loss = 0.0872
2026-01-04 22:11:35,176 - INFO -   Batch 500/2527: Loss = 0.0477
2026-01-04 22:12:03,884 - INFO -   Batch 550/2527: Loss = 0.0556
2026-01-04 22:12:32,659 - INFO -   Batch 600/2527: Loss = 0.0399
2026-01-04 22:13:01,263 - INFO -   Batch 650/2527: Loss = 0.0462
2026-01-04 22:13:30,099 - INFO -   Batch 700/2527: Loss = 0.0481
2026-01-04 22:13:58,805 - INFO -   Batch 750/2527: Loss = 0.0513
2026-01-04 22:14:27,434 - INFO -   Batch 800/2527: Loss = 0.0412
2026-01-04 22:14:56,370 - INFO -   Batch 850/2527: Loss = 0.0229
2026-01-04 22:15:25,243 - INFO -   Batch 900/2527: Loss = 0.0481
2026-01-04 22:15:54,143 - INFO -   Batch 950/2527: Loss = 0.0784
2026-01-04 22:16:22,752 - INFO -   Batch 1000/2527: Loss = 0.0415
2026-01-04 22:16:51,198 - INFO -   Batch 1050/2527: Loss = 0.0464
2026-01-04 22:17:20,087 - INFO -   Batch 1100/2527: Loss = 0.0898
2026-01-04 22:17:48,915 - INFO -   Batch 1150/2527: Loss = 0.0324
2026-01-04 22:18:17,739 - INFO -   Batch 1200/2527: Loss = 0.0523
2026-01-04 22:18:46,401 - INFO -   Batch 1250/2527: Loss = 0.0529
2026-01-04 22:19:15,487 - INFO -   Batch 1300/2527: Loss = 0.0475
2026-01-04 22:19:44,212 - INFO -   Batch 1350/2527: Loss = 0.0415
2026-01-04 22:20:12,672 - INFO -   Batch 1400/2527: Loss = 0.0583
2026-01-04 22:20:41,444 - INFO -   Batch 1450/2527: Loss = 0.0348
2026-01-04 22:21:10,056 - INFO -   Batch 1500/2527: Loss = 0.0559
2026-01-04 22:21:39,086 - INFO -   Batch 1550/2527: Loss = 0.0502
2026-01-04 22:22:07,747 - INFO -   Batch 1600/2527: Loss = 0.0378
2026-01-04 22:22:36,636 - INFO -   Batch 1650/2527: Loss = 0.0192
2026-01-04 22:23:05,385 - INFO -   Batch 1700/2527: Loss = 0.0526
2026-01-04 22:23:34,027 - INFO -   Batch 1750/2527: Loss = 0.0408
2026-01-04 22:24:02,810 - INFO -   Batch 1800/2527: Loss = 0.0502
2026-01-04 22:24:31,323 - INFO -   Batch 1850/2527: Loss = 0.0483
2026-01-04 22:25:00,169 - INFO -   Batch 1900/2527: Loss = 0.0396
2026-01-04 22:25:28,939 - INFO -   Batch 1950/2527: Loss = 0.0511
2026-01-04 22:25:57,445 - INFO -   Batch 2000/2527: Loss = 0.0537
2026-01-04 22:26:26,410 - INFO -   Batch 2050/2527: Loss = 0.0429
2026-01-04 22:26:55,268 - INFO -   Batch 2100/2527: Loss = 0.0473
2026-01-04 22:27:24,199 - INFO -   Batch 2150/2527: Loss = 0.0424
2026-01-04 22:27:52,927 - INFO -   Batch 2200/2527: Loss = 0.0410
2026-01-04 22:28:21,741 - INFO -   Batch 2250/2527: Loss = 0.0492
2026-01-04 22:28:50,403 - INFO -   Batch 2300/2527: Loss = 0.0325
2026-01-04 22:29:19,163 - INFO -   Batch 2350/2527: Loss = 0.0475
2026-01-04 22:29:47,950 - INFO -   Batch 2400/2527: Loss = 0.0315
2026-01-04 22:30:16,691 - INFO -   Batch 2450/2527: Loss = 0.0390
2026-01-04 22:30:45,751 - INFO -   Batch 2500/2527: Loss = 0.0506
2026-01-04 22:35:21,335 - INFO - Epoch 152/160: Train Loss: 0.0488, Val Loss: 1.5151, Val mIoU: 39.80%, Val PixelAcc: 78.72%
2026-01-04 22:35:51,916 - INFO -   Batch 50/2527: Loss = 0.0572
2026-01-04 22:36:20,872 - INFO -   Batch 100/2527: Loss = 0.0562
2026-01-04 22:36:49,606 - INFO -   Batch 150/2527: Loss = 0.0343
2026-01-04 22:37:18,382 - INFO -   Batch 200/2527: Loss = 0.0238
2026-01-04 22:37:46,988 - INFO -   Batch 250/2527: Loss = 0.0334
2026-01-04 22:38:15,541 - INFO -   Batch 300/2527: Loss = 0.0457
2026-01-04 22:38:44,428 - INFO -   Batch 350/2527: Loss = 0.0402
2026-01-04 22:39:13,128 - INFO -   Batch 400/2527: Loss = 0.0385
2026-01-04 22:39:42,052 - INFO -   Batch 450/2527: Loss = 0.0311
2026-01-04 22:40:10,900 - INFO -   Batch 500/2527: Loss = 0.0424
2026-01-04 22:40:39,459 - INFO -   Batch 550/2527: Loss = 0.0667
2026-01-04 22:41:08,293 - INFO -   Batch 600/2527: Loss = 0.0442
2026-01-04 22:41:36,966 - INFO -   Batch 650/2527: Loss = 0.0529
2026-01-04 22:42:05,902 - INFO -   Batch 700/2527: Loss = 0.0529
2026-01-04 22:42:34,582 - INFO -   Batch 750/2527: Loss = 0.0603
2026-01-04 22:43:03,564 - INFO -   Batch 800/2527: Loss = 0.0506
2026-01-04 22:43:32,232 - INFO -   Batch 850/2527: Loss = 0.0600
2026-01-04 22:44:00,944 - INFO -   Batch 900/2527: Loss = 0.0426
2026-01-04 22:44:29,818 - INFO -   Batch 950/2527: Loss = 0.0491
2026-01-04 22:44:58,427 - INFO -   Batch 1000/2527: Loss = 0.0455
2026-01-04 22:45:27,219 - INFO -   Batch 1050/2527: Loss = 0.0543
2026-01-04 22:45:55,883 - INFO -   Batch 1100/2527: Loss = 0.0395
2026-01-04 22:46:24,745 - INFO -   Batch 1150/2527: Loss = 0.0617
2026-01-04 22:46:53,499 - INFO -   Batch 1200/2527: Loss = 0.0487
2026-01-04 22:47:22,099 - INFO -   Batch 1250/2527: Loss = 0.0787
2026-01-04 22:47:51,161 - INFO -   Batch 1300/2527: Loss = 0.0254
2026-01-04 22:48:19,783 - INFO -   Batch 1350/2527: Loss = 0.0541
2026-01-04 22:48:48,601 - INFO -   Batch 1400/2527: Loss = 0.0390
2026-01-04 22:49:17,183 - INFO -   Batch 1450/2527: Loss = 0.0480
2026-01-04 22:49:45,741 - INFO -   Batch 1500/2527: Loss = 0.0573
2026-01-04 22:50:14,608 - INFO -   Batch 1550/2527: Loss = 0.0466
2026-01-04 22:50:43,329 - INFO -   Batch 1600/2527: Loss = 0.0557
2026-01-04 22:51:12,252 - INFO -   Batch 1650/2527: Loss = 0.0693
2026-01-04 22:51:41,103 - INFO -   Batch 1700/2527: Loss = 0.0647
2026-01-04 22:52:09,998 - INFO -   Batch 1750/2527: Loss = 0.0502
2026-01-04 22:52:38,795 - INFO -   Batch 1800/2527: Loss = 0.0477
2026-01-04 22:53:07,303 - INFO -   Batch 1850/2527: Loss = 0.0383
2026-01-04 22:53:36,183 - INFO -   Batch 1900/2527: Loss = 0.0412
2026-01-04 22:54:04,881 - INFO -   Batch 1950/2527: Loss = 0.0805
2026-01-04 22:54:33,819 - INFO -   Batch 2000/2527: Loss = 0.0365
2026-01-04 22:55:02,587 - INFO -   Batch 2050/2527: Loss = 0.0517
2026-01-04 22:55:31,618 - INFO -   Batch 2100/2527: Loss = 0.0370
2026-01-04 22:56:00,190 - INFO -   Batch 2150/2527: Loss = 0.0648
2026-01-04 22:56:28,772 - INFO -   Batch 2200/2527: Loss = 0.0264
2026-01-04 22:56:57,630 - INFO -   Batch 2250/2527: Loss = 0.0362
2026-01-04 22:57:26,290 - INFO -   Batch 2300/2527: Loss = 0.0417
2026-01-04 22:57:55,150 - INFO -   Batch 2350/2527: Loss = 0.0570
2026-01-04 22:58:23,919 - INFO -   Batch 2400/2527: Loss = 0.0387
2026-01-04 22:58:52,522 - INFO -   Batch 2450/2527: Loss = 0.0576
2026-01-04 22:59:21,587 - INFO -   Batch 2500/2527: Loss = 0.0497
2026-01-04 23:04:06,354 - INFO - Epoch 153/160: Train Loss: 0.0485, Val Loss: 1.5085, Val mIoU: 39.43%, Val PixelAcc: 78.52%
2026-01-04 23:04:36,844 - INFO -   Batch 50/2527: Loss = 0.0439
2026-01-04 23:05:05,282 - INFO -   Batch 100/2527: Loss = 0.0366
2026-01-04 23:05:33,917 - INFO -   Batch 150/2527: Loss = 0.0374
2026-01-04 23:06:02,936 - INFO -   Batch 200/2527: Loss = 0.0362
2026-01-04 23:06:31,551 - INFO -   Batch 250/2527: Loss = 0.0472
2026-01-04 23:07:00,483 - INFO -   Batch 300/2527: Loss = 0.0425
2026-01-04 23:07:29,344 - INFO -   Batch 350/2527: Loss = 0.0485
2026-01-04 23:07:57,902 - INFO -   Batch 400/2527: Loss = 0.0444
2026-01-04 23:08:26,762 - INFO -   Batch 450/2527: Loss = 0.0515
2026-01-04 23:08:55,280 - INFO -   Batch 500/2527: Loss = 0.0618
2026-01-04 23:09:24,164 - INFO -   Batch 550/2527: Loss = 0.0469
2026-01-04 23:09:52,997 - INFO -   Batch 600/2527: Loss = 0.0488
2026-01-04 23:10:21,854 - INFO -   Batch 650/2527: Loss = 0.0267
2026-01-04 23:10:50,620 - INFO -   Batch 700/2527: Loss = 0.0535
2026-01-04 23:11:19,367 - INFO -   Batch 750/2527: Loss = 0.0364
2026-01-04 23:11:48,271 - INFO -   Batch 800/2527: Loss = 0.0479
2026-01-04 23:12:16,913 - INFO -   Batch 850/2527: Loss = 0.0451
2026-01-04 23:12:45,705 - INFO -   Batch 900/2527: Loss = 0.0549
2026-01-04 23:13:14,291 - INFO -   Batch 950/2527: Loss = 0.0629
2026-01-04 23:13:43,318 - INFO -   Batch 1000/2527: Loss = 0.0500
2026-01-04 23:14:11,810 - INFO -   Batch 1050/2527: Loss = 0.0613
2026-01-04 23:14:40,581 - INFO -   Batch 1100/2527: Loss = 0.0489
2026-01-04 23:15:09,629 - INFO -   Batch 1150/2527: Loss = 0.0458
2026-01-04 23:15:38,262 - INFO -   Batch 1200/2527: Loss = 0.0477
2026-01-04 23:16:07,077 - INFO -   Batch 1250/2527: Loss = 0.0403
2026-01-04 23:16:35,645 - INFO -   Batch 1300/2527: Loss = 0.0591
2026-01-04 23:17:04,184 - INFO -   Batch 1350/2527: Loss = 0.0427
2026-01-04 23:17:33,244 - INFO -   Batch 1400/2527: Loss = 0.0575
2026-01-04 23:18:01,789 - INFO -   Batch 1450/2527: Loss = 0.0290
2026-01-04 23:18:30,778 - INFO -   Batch 1500/2527: Loss = 0.0402
2026-01-04 23:18:59,602 - INFO -   Batch 1550/2527: Loss = 0.0451
2026-01-04 23:19:28,516 - INFO -   Batch 1600/2527: Loss = 0.0640
2026-01-04 23:19:57,044 - INFO -   Batch 1650/2527: Loss = 0.0778
2026-01-04 23:20:25,488 - INFO -   Batch 1700/2527: Loss = 0.0356
2026-01-04 23:20:54,336 - INFO -   Batch 1750/2527: Loss = 0.0473
2026-01-04 23:21:23,223 - INFO -   Batch 1800/2527: Loss = 0.0390
2026-01-04 23:21:52,009 - INFO -   Batch 1850/2527: Loss = 0.0603
2026-01-04 23:22:20,964 - INFO -   Batch 1900/2527: Loss = 0.0473
2026-01-04 23:22:49,714 - INFO -   Batch 1950/2527: Loss = 0.0523
2026-01-04 23:23:18,602 - INFO -   Batch 2000/2527: Loss = 0.0443
2026-01-04 23:23:47,200 - INFO -   Batch 2050/2527: Loss = 0.0587
2026-01-04 23:24:16,017 - INFO -   Batch 2100/2527: Loss = 0.0385
2026-01-04 23:24:44,652 - INFO -   Batch 2150/2527: Loss = 0.0566
2026-01-04 23:25:13,707 - INFO -   Batch 2200/2527: Loss = 0.0778
2026-01-04 23:25:42,316 - INFO -   Batch 2250/2527: Loss = 0.0595
2026-01-04 23:26:10,945 - INFO -   Batch 2300/2527: Loss = 0.0677
2026-01-04 23:26:40,079 - INFO -   Batch 2350/2527: Loss = 0.0435
2026-01-04 23:27:08,692 - INFO -   Batch 2400/2527: Loss = 0.0237
2026-01-04 23:27:37,512 - INFO -   Batch 2450/2527: Loss = 0.0480
2026-01-04 23:28:06,085 - INFO -   Batch 2500/2527: Loss = 0.0418
2026-01-04 23:32:44,891 - INFO - Epoch 154/160: Train Loss: 0.0484, Val Loss: 1.5159, Val mIoU: 39.64%, Val PixelAcc: 78.62%
2026-01-04 23:33:15,434 - INFO -   Batch 50/2527: Loss = 0.0593
2026-01-04 23:33:44,257 - INFO -   Batch 100/2527: Loss = 0.0675
2026-01-04 23:34:13,097 - INFO -   Batch 150/2527: Loss = 0.0434
2026-01-04 23:34:41,660 - INFO -   Batch 200/2527: Loss = 0.0675
2026-01-04 23:35:10,107 - INFO -   Batch 250/2527: Loss = 0.0330
2026-01-04 23:35:38,929 - INFO -   Batch 300/2527: Loss = 0.0366
2026-01-04 23:36:07,814 - INFO -   Batch 350/2527: Loss = 0.0497
2026-01-04 23:36:36,671 - INFO -   Batch 400/2527: Loss = 0.0444
2026-01-04 23:37:05,448 - INFO -   Batch 450/2527: Loss = 0.0339
2026-01-04 23:37:34,585 - INFO -   Batch 500/2527: Loss = 0.0524
2026-01-04 23:38:03,341 - INFO -   Batch 550/2527: Loss = 0.0462
2026-01-04 23:38:31,801 - INFO -   Batch 600/2527: Loss = 0.0386
2026-01-04 23:39:00,605 - INFO -   Batch 650/2527: Loss = 0.0700
2026-01-04 23:39:29,165 - INFO -   Batch 700/2527: Loss = 0.0495
2026-01-04 23:39:58,210 - INFO -   Batch 750/2527: Loss = 0.0506
2026-01-04 23:40:26,805 - INFO -   Batch 800/2527: Loss = 0.0637
2026-01-04 23:40:55,419 - INFO -   Batch 850/2527: Loss = 0.0401
2026-01-04 23:41:25,090 - INFO -   Batch 900/2527: Loss = 0.0401
2026-01-04 23:41:53,725 - INFO -   Batch 950/2527: Loss = 0.0580
2026-01-04 23:42:22,530 - INFO -   Batch 1000/2527: Loss = 0.0351
2026-01-04 23:42:51,141 - INFO -   Batch 1050/2527: Loss = 0.0631
2026-01-04 23:43:19,961 - INFO -   Batch 1100/2527: Loss = 0.0482
2026-01-04 23:43:48,762 - INFO -   Batch 1150/2527: Loss = 0.0460
2026-01-04 23:44:17,254 - INFO -   Batch 1200/2527: Loss = 0.0322
2026-01-04 23:44:46,252 - INFO -   Batch 1250/2527: Loss = 0.0654
2026-01-04 23:45:15,123 - INFO -   Batch 1300/2527: Loss = 0.0571
2026-01-04 23:45:44,054 - INFO -   Batch 1350/2527: Loss = 0.0606
2026-01-04 23:46:12,644 - INFO -   Batch 1400/2527: Loss = 0.0261
2026-01-04 23:46:41,478 - INFO -   Batch 1450/2527: Loss = 0.0513
2026-01-04 23:47:10,002 - INFO -   Batch 1500/2527: Loss = 0.0423
2026-01-04 23:47:38,867 - INFO -   Batch 1550/2527: Loss = 0.0565
2026-01-04 23:48:07,731 - INFO -   Batch 1600/2527: Loss = 0.0517
2026-01-04 23:48:36,628 - INFO -   Batch 1650/2527: Loss = 0.0609
2026-01-04 23:49:05,756 - INFO -   Batch 1700/2527: Loss = 0.0467
2026-01-04 23:49:34,447 - INFO -   Batch 1750/2527: Loss = 0.0415
2026-01-04 23:50:02,918 - INFO -   Batch 1800/2527: Loss = 0.0443
2026-01-04 23:50:31,753 - INFO -   Batch 1850/2527: Loss = 0.0502
2026-01-04 23:51:00,422 - INFO -   Batch 1900/2527: Loss = 0.0537
2026-01-04 23:51:29,472 - INFO -   Batch 1950/2527: Loss = 0.0399
2026-01-04 23:51:58,099 - INFO -   Batch 2000/2527: Loss = 0.0483
2026-01-04 23:52:27,024 - INFO -   Batch 2050/2527: Loss = 0.0449
2026-01-04 23:52:55,892 - INFO -   Batch 2100/2527: Loss = 0.0331
2026-01-04 23:53:24,499 - INFO -   Batch 2150/2527: Loss = 0.0459
2026-01-04 23:53:53,341 - INFO -   Batch 2200/2527: Loss = 0.0349
2026-01-04 23:54:21,978 - INFO -   Batch 2250/2527: Loss = 0.0493
2026-01-04 23:54:50,858 - INFO -   Batch 2300/2527: Loss = 0.0540
2026-01-04 23:55:19,762 - INFO -   Batch 2350/2527: Loss = 0.0255
2026-01-04 23:55:48,618 - INFO -   Batch 2400/2527: Loss = 0.0538
2026-01-04 23:56:17,249 - INFO -   Batch 2450/2527: Loss = 0.0616
2026-01-04 23:56:46,143 - INFO -   Batch 2500/2527: Loss = 0.0481
2026-01-05 00:01:23,101 - INFO - Epoch 155/160: Train Loss: 0.0484, Val Loss: 1.5222, Val mIoU: 39.40%, Val PixelAcc: 78.57%
2026-01-05 00:01:53,756 - INFO -   Batch 50/2527: Loss = 0.0367
2026-01-05 00:02:22,425 - INFO -   Batch 100/2527: Loss = 0.0523
2026-01-05 00:02:51,377 - INFO -   Batch 150/2527: Loss = 0.0527
2026-01-05 00:03:20,018 - INFO -   Batch 200/2527: Loss = 0.0648
2026-01-05 00:03:48,901 - INFO -   Batch 250/2527: Loss = 0.0599
2026-01-05 00:04:17,551 - INFO -   Batch 300/2527: Loss = 0.0456
2026-01-05 00:04:46,413 - INFO -   Batch 350/2527: Loss = 0.0440
2026-01-05 00:05:15,106 - INFO -   Batch 400/2527: Loss = 0.0554
2026-01-05 00:05:43,912 - INFO -   Batch 450/2527: Loss = 0.0273
2026-01-05 00:06:12,946 - INFO -   Batch 500/2527: Loss = 0.0751
2026-01-05 00:06:41,648 - INFO -   Batch 550/2527: Loss = 0.0507
2026-01-05 00:07:10,501 - INFO -   Batch 600/2527: Loss = 0.0261
2026-01-05 00:07:39,144 - INFO -   Batch 650/2527: Loss = 0.0552
2026-01-05 00:08:07,728 - INFO -   Batch 700/2527: Loss = 0.0652
2026-01-05 00:08:36,638 - INFO -   Batch 750/2527: Loss = 0.0370
2026-01-05 00:09:05,377 - INFO -   Batch 800/2527: Loss = 0.0361
2026-01-05 00:09:34,330 - INFO -   Batch 850/2527: Loss = 0.0630
2026-01-05 00:10:03,138 - INFO -   Batch 900/2527: Loss = 0.0495
2026-01-05 00:10:32,093 - INFO -   Batch 950/2527: Loss = 0.0655
2026-01-05 00:11:00,650 - INFO -   Batch 1000/2527: Loss = 0.0463
2026-01-05 00:11:29,191 - INFO -   Batch 1050/2527: Loss = 0.1123
2026-01-05 00:11:58,087 - INFO -   Batch 1100/2527: Loss = 0.0657
2026-01-05 00:12:26,784 - INFO -   Batch 1150/2527: Loss = 0.0737
2026-01-05 00:12:55,782 - INFO -   Batch 1200/2527: Loss = 0.0483
2026-01-05 00:13:24,710 - INFO -   Batch 1250/2527: Loss = 0.0375
2026-01-05 00:13:53,777 - INFO -   Batch 1300/2527: Loss = 0.0366
2026-01-05 00:14:22,371 - INFO -   Batch 1350/2527: Loss = 0.0641
2026-01-05 00:14:51,066 - INFO -   Batch 1400/2527: Loss = 0.0648
2026-01-05 00:15:19,975 - INFO -   Batch 1450/2527: Loss = 0.0437
2026-01-05 00:15:48,700 - INFO -   Batch 1500/2527: Loss = 0.0434
2026-01-05 00:16:17,621 - INFO -   Batch 1550/2527: Loss = 0.0374
2026-01-05 00:16:46,429 - INFO -   Batch 1600/2527: Loss = 0.0344
2026-01-05 00:17:15,081 - INFO -   Batch 1650/2527: Loss = 0.0730
2026-01-05 00:17:44,185 - INFO -   Batch 1700/2527: Loss = 0.0800
2026-01-05 00:18:12,932 - INFO -   Batch 1750/2527: Loss = 0.0529
2026-01-05 00:18:41,781 - INFO -   Batch 1800/2527: Loss = 0.0593
2026-01-05 00:19:10,459 - INFO -   Batch 1850/2527: Loss = 0.0502
2026-01-05 00:19:39,430 - INFO -   Batch 1900/2527: Loss = 0.0276
2026-01-05 00:20:08,149 - INFO -   Batch 1950/2527: Loss = 0.0563
2026-01-05 00:20:36,802 - INFO -   Batch 2000/2527: Loss = 0.0486
2026-01-05 00:21:05,739 - INFO -   Batch 2050/2527: Loss = 0.0504
2026-01-05 00:21:34,636 - INFO -   Batch 2100/2527: Loss = 0.0347
2026-01-05 00:22:03,871 - INFO -   Batch 2150/2527: Loss = 0.0502
2026-01-05 00:22:32,470 - INFO -   Batch 2200/2527: Loss = 0.0294
2026-01-05 00:23:01,033 - INFO -   Batch 2250/2527: Loss = 0.0651
2026-01-05 00:23:29,955 - INFO -   Batch 2300/2527: Loss = 0.0635
2026-01-05 00:23:58,672 - INFO -   Batch 2350/2527: Loss = 0.0672
2026-01-05 00:24:27,652 - INFO -   Batch 2400/2527: Loss = 0.0505
2026-01-05 00:24:56,381 - INFO -   Batch 2450/2527: Loss = 0.0419
2026-01-05 00:25:25,469 - INFO -   Batch 2500/2527: Loss = 0.0452
2026-01-05 00:30:10,420 - INFO - Epoch 156/160: Train Loss: 0.0483, Val Loss: 1.5323, Val mIoU: 39.48%, Val PixelAcc: 78.60%
2026-01-05 00:30:40,890 - INFO -   Batch 50/2527: Loss = 0.0282
2026-01-05 00:31:09,690 - INFO -   Batch 100/2527: Loss = 0.0439
2026-01-05 00:31:38,311 - INFO -   Batch 150/2527: Loss = 0.0561
2026-01-05 00:32:07,014 - INFO -   Batch 200/2527: Loss = 0.0519
2026-01-05 00:32:35,856 - INFO -   Batch 250/2527: Loss = 0.0403
2026-01-05 00:33:04,564 - INFO -   Batch 300/2527: Loss = 0.0396
2026-01-05 00:33:33,697 - INFO -   Batch 350/2527: Loss = 0.0479
2026-01-05 00:34:02,351 - INFO -   Batch 400/2527: Loss = 0.0741
2026-01-05 00:34:31,107 - INFO -   Batch 450/2527: Loss = 0.0601
2026-01-05 00:34:59,739 - INFO -   Batch 500/2527: Loss = 0.0365
2026-01-05 00:35:28,287 - INFO -   Batch 550/2527: Loss = 0.0529
2026-01-05 00:35:57,302 - INFO -   Batch 600/2527: Loss = 0.0381
2026-01-05 00:36:25,890 - INFO -   Batch 650/2527: Loss = 0.0523
2026-01-05 00:36:54,806 - INFO -   Batch 700/2527: Loss = 0.0430
2026-01-05 00:37:23,714 - INFO -   Batch 750/2527: Loss = 0.0431
2026-01-05 00:37:52,605 - INFO -   Batch 800/2527: Loss = 0.0551
2026-01-05 00:38:21,110 - INFO -   Batch 850/2527: Loss = 0.0518
2026-01-05 00:38:49,568 - INFO -   Batch 900/2527: Loss = 0.0497
2026-01-05 00:39:18,439 - INFO -   Batch 950/2527: Loss = 0.0302
2026-01-05 00:39:47,214 - INFO -   Batch 1000/2527: Loss = 0.0358
2026-01-05 00:40:16,063 - INFO -   Batch 1050/2527: Loss = 0.0353
2026-01-05 00:40:44,741 - INFO -   Batch 1100/2527: Loss = 0.0542
2026-01-05 00:41:13,506 - INFO -   Batch 1150/2527: Loss = 0.0497
2026-01-05 00:41:42,399 - INFO -   Batch 1200/2527: Loss = 0.0575
2026-01-05 00:42:10,963 - INFO -   Batch 1250/2527: Loss = 0.0631
2026-01-05 00:42:39,785 - INFO -   Batch 1300/2527: Loss = 0.0557
2026-01-05 00:43:08,589 - INFO -   Batch 1350/2527: Loss = 0.0659
2026-01-05 00:43:37,628 - INFO -   Batch 1400/2527: Loss = 0.0630
2026-01-05 00:44:06,251 - INFO -   Batch 1450/2527: Loss = 0.0783
2026-01-05 00:44:34,867 - INFO -   Batch 1500/2527: Loss = 0.0391
2026-01-05 00:45:03,943 - INFO -   Batch 1550/2527: Loss = 0.0362
2026-01-05 00:45:32,659 - INFO -   Batch 1600/2527: Loss = 0.0513
2026-01-05 00:46:01,451 - INFO -   Batch 1650/2527: Loss = 0.0384
2026-01-05 00:46:30,058 - INFO -   Batch 1700/2527: Loss = 0.0440
2026-01-05 00:46:58,896 - INFO -   Batch 1750/2527: Loss = 0.0651
2026-01-05 00:47:27,646 - INFO -   Batch 1800/2527: Loss = 0.0338
2026-01-05 00:47:56,204 - INFO -   Batch 1850/2527: Loss = 0.0349
2026-01-05 00:48:25,144 - INFO -   Batch 1900/2527: Loss = 0.0466
2026-01-05 00:48:54,013 - INFO -   Batch 1950/2527: Loss = 0.0570
2026-01-05 00:49:22,926 - INFO -   Batch 2000/2527: Loss = 0.0378
2026-01-05 00:49:51,564 - INFO -   Batch 2050/2527: Loss = 0.0663
2026-01-05 00:50:20,009 - INFO -   Batch 2100/2527: Loss = 0.0691
2026-01-05 00:50:48,894 - INFO -   Batch 2150/2527: Loss = 0.0403
2026-01-05 00:51:17,741 - INFO -   Batch 2200/2527: Loss = 0.0473
2026-01-05 00:51:46,568 - INFO -   Batch 2250/2527: Loss = 0.0366
2026-01-05 00:52:15,293 - INFO -   Batch 2300/2527: Loss = 0.0734
2026-01-05 00:52:44,378 - INFO -   Batch 2350/2527: Loss = 0.0327
2026-01-05 00:53:13,108 - INFO -   Batch 2400/2527: Loss = 0.0387
2026-01-05 00:53:41,574 - INFO -   Batch 2450/2527: Loss = 0.0432
2026-01-05 00:54:10,355 - INFO -   Batch 2500/2527: Loss = 0.0534
2026-01-05 00:58:49,394 - INFO - Epoch 157/160: Train Loss: 0.0483, Val Loss: 1.5223, Val mIoU: 39.49%, Val PixelAcc: 78.65%
2026-01-05 00:59:19,614 - INFO -   Batch 50/2527: Loss = 0.0393
2026-01-05 00:59:48,650 - INFO -   Batch 100/2527: Loss = 0.0426
2026-01-05 01:00:17,268 - INFO -   Batch 150/2527: Loss = 0.0500
2026-01-05 01:00:46,076 - INFO -   Batch 200/2527: Loss = 0.0556
2026-01-05 01:01:14,662 - INFO -   Batch 250/2527: Loss = 0.0330
2026-01-05 01:01:43,495 - INFO -   Batch 300/2527: Loss = 0.0512
2026-01-05 01:02:12,285 - INFO -   Batch 350/2527: Loss = 0.0511
2026-01-05 01:02:40,754 - INFO -   Batch 400/2527: Loss = 0.0565
2026-01-05 01:03:09,770 - INFO -   Batch 450/2527: Loss = 0.0566
2026-01-05 01:03:38,654 - INFO -   Batch 500/2527: Loss = 0.0454
2026-01-05 01:04:07,588 - INFO -   Batch 550/2527: Loss = 0.0363
2026-01-05 01:04:36,126 - INFO -   Batch 600/2527: Loss = 0.0520
2026-01-05 01:05:04,945 - INFO -   Batch 650/2527: Loss = 0.0525
2026-01-05 01:05:33,477 - INFO -   Batch 700/2527: Loss = 0.0355
2026-01-05 01:06:02,292 - INFO -   Batch 750/2527: Loss = 0.0688
2026-01-05 01:06:31,071 - INFO -   Batch 800/2527: Loss = 0.0452
2026-01-05 01:06:59,792 - INFO -   Batch 850/2527: Loss = 0.0527
2026-01-05 01:07:28,867 - INFO -   Batch 900/2527: Loss = 0.0591
2026-01-05 01:07:57,609 - INFO -   Batch 950/2527: Loss = 0.0560
2026-01-05 01:08:26,093 - INFO -   Batch 1000/2527: Loss = 0.0528
2026-01-05 01:08:54,852 - INFO -   Batch 1050/2527: Loss = 0.0545
2026-01-05 01:09:23,668 - INFO -   Batch 1100/2527: Loss = 0.0615
2026-01-05 01:09:52,714 - INFO -   Batch 1150/2527: Loss = 0.0428
2026-01-05 01:10:21,289 - INFO -   Batch 1200/2527: Loss = 0.0453
2026-01-05 01:10:50,230 - INFO -   Batch 1250/2527: Loss = 0.0457
2026-01-05 01:11:19,113 - INFO -   Batch 1300/2527: Loss = 0.0483
2026-01-05 01:11:47,712 - INFO -   Batch 1350/2527: Loss = 0.0477
2026-01-05 01:12:16,539 - INFO -   Batch 1400/2527: Loss = 0.0397
2026-01-05 01:12:45,120 - INFO -   Batch 1450/2527: Loss = 0.0360
2026-01-05 01:13:13,988 - INFO -   Batch 1500/2527: Loss = 0.0527
2026-01-05 01:13:42,794 - INFO -   Batch 1550/2527: Loss = 0.0379
2026-01-05 01:14:11,304 - INFO -   Batch 1600/2527: Loss = 0.0459
2026-01-05 01:14:40,290 - INFO -   Batch 1650/2527: Loss = 0.0621
2026-01-05 01:15:09,211 - INFO -   Batch 1700/2527: Loss = 0.0399
2026-01-05 01:15:38,082 - INFO -   Batch 1750/2527: Loss = 0.0346
2026-01-05 01:16:06,769 - INFO -   Batch 1800/2527: Loss = 0.0326
2026-01-05 01:16:35,589 - INFO -   Batch 1850/2527: Loss = 0.0545
2026-01-05 01:17:04,301 - INFO -   Batch 1900/2527: Loss = 0.0415
2026-01-05 01:17:33,021 - INFO -   Batch 1950/2527: Loss = 0.0527
2026-01-05 01:18:01,816 - INFO -   Batch 2000/2527: Loss = 0.0494
2026-01-05 01:18:30,518 - INFO -   Batch 2050/2527: Loss = 0.0690
2026-01-05 01:18:59,600 - INFO -   Batch 2100/2527: Loss = 0.0572
2026-01-05 01:19:28,298 - INFO -   Batch 2150/2527: Loss = 0.0573
2026-01-05 01:19:57,112 - INFO -   Batch 2200/2527: Loss = 0.0571
2026-01-05 01:20:25,686 - INFO -   Batch 2250/2527: Loss = 0.0593
2026-01-05 01:20:54,231 - INFO -   Batch 2300/2527: Loss = 0.0444
2026-01-05 01:21:23,316 - INFO -   Batch 2350/2527: Loss = 0.0555
2026-01-05 01:21:51,921 - INFO -   Batch 2400/2527: Loss = 0.0401
2026-01-05 01:22:20,877 - INFO -   Batch 2450/2527: Loss = 0.0399
2026-01-05 01:22:49,683 - INFO -   Batch 2500/2527: Loss = 0.0434
2026-01-05 01:27:27,983 - INFO - Epoch 158/160: Train Loss: 0.0483, Val Loss: 1.5162, Val mIoU: 39.31%, Val PixelAcc: 78.56%
2026-01-05 01:27:58,765 - INFO -   Batch 50/2527: Loss = 0.0499
2026-01-05 01:28:27,581 - INFO -   Batch 100/2527: Loss = 0.0508
2026-01-05 01:28:56,434 - INFO -   Batch 150/2527: Loss = 0.0478
2026-01-05 01:29:24,976 - INFO -   Batch 200/2527: Loss = 0.0742
2026-01-05 01:29:53,474 - INFO -   Batch 250/2527: Loss = 0.0444
2026-01-05 01:30:22,344 - INFO -   Batch 300/2527: Loss = 0.0478
2026-01-05 01:30:50,970 - INFO -   Batch 350/2527: Loss = 0.0473
2026-01-05 01:31:19,937 - INFO -   Batch 400/2527: Loss = 0.0426
2026-01-05 01:31:48,660 - INFO -   Batch 450/2527: Loss = 0.0447
2026-01-05 01:32:17,367 - INFO -   Batch 500/2527: Loss = 0.0332
2026-01-05 01:32:46,316 - INFO -   Batch 550/2527: Loss = 0.0506
2026-01-05 01:33:14,935 - INFO -   Batch 600/2527: Loss = 0.0764
2026-01-05 01:33:43,784 - INFO -   Batch 650/2527: Loss = 0.0427
2026-01-05 01:34:12,627 - INFO -   Batch 700/2527: Loss = 0.0456
2026-01-05 01:34:41,521 - INFO -   Batch 750/2527: Loss = 0.0840
2026-01-05 01:35:10,295 - INFO -   Batch 800/2527: Loss = 0.0514
2026-01-05 01:35:38,875 - INFO -   Batch 850/2527: Loss = 0.0763
2026-01-05 01:36:07,983 - INFO -   Batch 900/2527: Loss = 0.0332
2026-01-05 01:36:36,728 - INFO -   Batch 950/2527: Loss = 0.0510
2026-01-05 01:37:05,504 - INFO -   Batch 1000/2527: Loss = 0.0527
2026-01-05 01:37:34,146 - INFO -   Batch 1050/2527: Loss = 0.0380
2026-01-05 01:38:03,035 - INFO -   Batch 1100/2527: Loss = 0.0397
2026-01-05 01:38:31,607 - INFO -   Batch 1150/2527: Loss = 0.0577
2026-01-05 01:39:00,393 - INFO -   Batch 1200/2527: Loss = 0.0463
2026-01-05 01:39:29,307 - INFO -   Batch 1250/2527: Loss = 0.0483
2026-01-05 01:39:58,203 - INFO -   Batch 1300/2527: Loss = 0.0427
2026-01-05 01:40:27,155 - INFO -   Batch 1350/2527: Loss = 0.0521
2026-01-05 01:40:55,769 - INFO -   Batch 1400/2527: Loss = 0.0439
2026-01-05 01:41:24,288 - INFO -   Batch 1450/2527: Loss = 0.0732
2026-01-05 01:41:53,216 - INFO -   Batch 1500/2527: Loss = 0.0475
2026-01-05 01:42:21,879 - INFO -   Batch 1550/2527: Loss = 0.0461
2026-01-05 01:42:50,925 - INFO -   Batch 1600/2527: Loss = 0.0472
2026-01-05 01:43:19,619 - INFO -   Batch 1650/2527: Loss = 0.0497
2026-01-05 01:43:48,983 - INFO -   Batch 1700/2527: Loss = 0.0442
2026-01-05 01:44:17,764 - INFO -   Batch 1750/2527: Loss = 0.0534
2026-01-05 01:44:46,264 - INFO -   Batch 1800/2527: Loss = 0.0423
2026-01-05 01:45:15,161 - INFO -   Batch 1850/2527: Loss = 0.0463
2026-01-05 01:45:43,823 - INFO -   Batch 1900/2527: Loss = 0.0670
2026-01-05 01:46:12,700 - INFO -   Batch 1950/2527: Loss = 0.0807
2026-01-05 01:46:41,519 - INFO -   Batch 2000/2527: Loss = 0.0631
2026-01-05 01:47:10,446 - INFO -   Batch 2050/2527: Loss = 0.0415
2026-01-05 01:47:39,185 - INFO -   Batch 2100/2527: Loss = 0.0506
2026-01-05 01:48:07,888 - INFO -   Batch 2150/2527: Loss = 0.0365
2026-01-05 01:48:36,796 - INFO -   Batch 2200/2527: Loss = 0.0371
2026-01-05 01:49:05,425 - INFO -   Batch 2250/2527: Loss = 0.0379
2026-01-05 01:49:34,328 - INFO -   Batch 2300/2527: Loss = 0.0723
2026-01-05 01:50:03,012 - INFO -   Batch 2350/2527: Loss = 0.0665
2026-01-05 01:50:31,668 - INFO -   Batch 2400/2527: Loss = 0.0459
2026-01-05 01:51:00,621 - INFO -   Batch 2450/2527: Loss = 0.0545
2026-01-05 01:51:29,468 - INFO -   Batch 2500/2527: Loss = 0.0548
2026-01-05 01:56:13,216 - INFO - Epoch 159/160: Train Loss: 0.0482, Val Loss: 1.5273, Val mIoU: 39.54%, Val PixelAcc: 78.61%
2026-01-05 01:56:43,391 - INFO -   Batch 50/2527: Loss = 0.0574
2026-01-05 01:57:11,912 - INFO -   Batch 100/2527: Loss = 0.0934
2026-01-05 01:57:40,785 - INFO -   Batch 150/2527: Loss = 0.0493
2026-01-05 01:58:09,621 - INFO -   Batch 200/2527: Loss = 0.0459
2026-01-05 01:58:38,388 - INFO -   Batch 250/2527: Loss = 0.0244
2026-01-05 01:59:07,107 - INFO -   Batch 300/2527: Loss = 0.0454
2026-01-05 01:59:35,877 - INFO -   Batch 350/2527: Loss = 0.0617
2026-01-05 02:00:04,747 - INFO -   Batch 400/2527: Loss = 0.0452
2026-01-05 02:00:33,312 - INFO -   Batch 450/2527: Loss = 0.0538
2026-01-05 02:01:02,071 - INFO -   Batch 500/2527: Loss = 0.0487
2026-01-05 02:01:30,721 - INFO -   Batch 550/2527: Loss = 0.0453
2026-01-05 02:01:59,780 - INFO -   Batch 600/2527: Loss = 0.0549
2026-01-05 02:02:28,376 - INFO -   Batch 650/2527: Loss = 0.0442
2026-01-05 02:02:56,993 - INFO -   Batch 700/2527: Loss = 0.0404
2026-01-05 02:03:26,089 - INFO -   Batch 750/2527: Loss = 0.0622
2026-01-05 02:03:54,912 - INFO -   Batch 800/2527: Loss = 0.0409
2026-01-05 02:04:23,728 - INFO -   Batch 850/2527: Loss = 0.0371
2026-01-05 02:04:52,310 - INFO -   Batch 900/2527: Loss = 0.0488
2026-01-05 02:05:21,190 - INFO -   Batch 950/2527: Loss = 0.0422
2026-01-05 02:05:49,928 - INFO -   Batch 1000/2527: Loss = 0.0583
2026-01-05 02:06:18,534 - INFO -   Batch 1050/2527: Loss = 0.0376
2026-01-05 02:06:47,481 - INFO -   Batch 1100/2527: Loss = 0.0411
2026-01-05 02:07:16,357 - INFO -   Batch 1150/2527: Loss = 0.0364
2026-01-05 02:07:45,266 - INFO -   Batch 1200/2527: Loss = 0.0500
2026-01-05 02:08:13,841 - INFO -   Batch 1250/2527: Loss = 0.0275
2026-01-05 02:08:42,325 - INFO -   Batch 1300/2527: Loss = 0.0345
2026-01-05 02:09:11,182 - INFO -   Batch 1350/2527: Loss = 0.0539
2026-01-05 02:09:40,081 - INFO -   Batch 1400/2527: Loss = 0.0404
2026-01-05 02:10:08,916 - INFO -   Batch 1450/2527: Loss = 0.0362
2026-01-05 02:10:37,697 - INFO -   Batch 1500/2527: Loss = 0.0371
2026-01-05 02:11:06,808 - INFO -   Batch 1550/2527: Loss = 0.0554
2026-01-05 02:11:35,526 - INFO -   Batch 1600/2527: Loss = 0.0553
2026-01-05 02:12:03,989 - INFO -   Batch 1650/2527: Loss = 0.0559
2026-01-05 02:12:32,834 - INFO -   Batch 1700/2527: Loss = 0.0576
2026-01-05 02:13:01,459 - INFO -   Batch 1750/2527: Loss = 0.0445
2026-01-05 02:13:30,522 - INFO -   Batch 1800/2527: Loss = 0.0392
2026-01-05 02:13:59,204 - INFO -   Batch 1850/2527: Loss = 0.0447
2026-01-05 02:14:27,949 - INFO -   Batch 1900/2527: Loss = 0.0434
2026-01-05 02:14:57,026 - INFO -   Batch 1950/2527: Loss = 0.0460
2026-01-05 02:15:25,737 - INFO -   Batch 2000/2527: Loss = 0.0583
2026-01-05 02:15:54,550 - INFO -   Batch 2050/2527: Loss = 0.0531
2026-01-05 02:16:23,203 - INFO -   Batch 2100/2527: Loss = 0.0509
2026-01-05 02:16:52,155 - INFO -   Batch 2150/2527: Loss = 0.0418
2026-01-05 02:17:21,000 - INFO -   Batch 2200/2527: Loss = 0.0501
2026-01-05 02:17:49,548 - INFO -   Batch 2250/2527: Loss = 0.0542
2026-01-05 02:18:18,535 - INFO -   Batch 2300/2527: Loss = 0.0478
2026-01-05 02:18:47,401 - INFO -   Batch 2350/2527: Loss = 0.0414
2026-01-05 02:19:16,325 - INFO -   Batch 2400/2527: Loss = 0.0520
2026-01-05 02:19:44,922 - INFO -   Batch 2450/2527: Loss = 0.0490
2026-01-05 02:20:13,805 - INFO -   Batch 2500/2527: Loss = 0.0444
2026-01-05 02:24:50,818 - INFO - Epoch 160/160: Train Loss: 0.0482, Val Loss: 1.5121, Val mIoU: 39.44%, Val PixelAcc: 78.65%
2026-01-05 02:24:57,905 - INFO - ✅ Checkpoint saved: runs/run_259/checkpoint_epoch_160.pth
2026-01-05 02:24:57,908 - INFO - Checkpoint saved: runs/run_259/checkpoint_epoch_160.pth
2026-01-05 02:24:57,909 - INFO - Training complete! Best mIoU: 40.39%
2026-01-05 02:24:57,911 - INFO - Training complete!
2026-01-05 02:24:57,912 - INFO - Generating reports...
2026-01-05 02:24:57,913 - INFO - Generating segmentation training curves...
2026-01-05 02:25:00,303 - INFO - Segmentation training curves saved with base name 'runs/run_259/segmentation_curves_resnet101_upernet'
2026-01-05 02:25:00,304 - INFO - Performing final evaluation on validation set...
2026-01-05 02:29:21,139 - INFO - Generating per-class IoU plot...
2026-01-05 02:29:22,714 - INFO - Per-class IoU plot saved to 'runs/run_259/iou_per_class_resnet101_upernet.png'
2026-01-05 02:29:22,716 - INFO - Generating LR schedule plot...
2026-01-05 02:29:23,553 - INFO - LR schedule plot saved to 'runs/run_259/lr_schedule_resnet101_upernet.png'
2026-01-05 02:29:23,555 - INFO - 
==================================================
2026-01-05 02:29:23,556 - INFO - Final Segmentation Results (resnet101_upernet)
2026-01-05 02:29:23,557 - INFO - ==================================================
2026-01-05 02:29:23,559 - INFO - Loss: 1.5121
2026-01-05 02:29:23,560 - INFO - mIoU: 39.44%
2026-01-05 02:29:23,562 - INFO - Pixel Accuracy: 78.65%
2026-01-05 02:29:23,563 - INFO - Classes Present: 150/150
2026-01-05 02:29:23,879 - INFO - Final metrics saved to runs/run_259/final_metrics_resnet101_upernet.json
2026-01-05 02:29:25,126 - INFO - ✅ Model weights saved: runs/run_259/final_model_resnet101_upernet_weights.pth
2026-01-05 02:29:25,132 - INFO - Final model saved: runs/run_259/final_model_resnet101_upernet_weights.pth
2026-01-05 02:29:25,134 - INFO - Final model metadata saved: runs/run_259/final_model_resnet101_upernet_metadata.json
2026-01-05 02:29:25,142 - INFO - Experiment completed. Results saved to runs/run_259/results_resnet101_upernet.json
2026-01-05 02:29:25,168 - INFO - Experiment completed. Metadata saved to runs/run_259/metadata_resnet101_upernet.json
2026-01-05 02:29:25,169 - INFO - ============================================================
2026-01-05 02:29:25,171 - INFO - SEGMENTATION PIPELINE COMPLETE
2026-01-05 02:29:25,172 - INFO - Final mIoU: 39.44%
2026-01-05 02:29:25,174 - INFO - Final Pixel Acc: 78.65%
2026-01-05 02:29:25,175 - INFO - ============================================================
2026-01-05 02:29:25,188 - INFO - Segmentation training completed successfully!
2026-01-05 02:29:25,190 - INFO - Final mIoU: 39.44%
