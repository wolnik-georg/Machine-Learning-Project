2025-12-25 23:17:04,955 - INFO - Logging initialized. Log file: runs/run_221/training.log
2025-12-25 23:17:05,267 - INFO - Experiment directory: runs/run_221
2025-12-25 23:17:05,269 - INFO - Setting random seeds (seed: 42)...
2025-12-25 23:17:05,283 - INFO - ✅ All seeds set to 42 (deterministic=False)
2025-12-25 23:17:05,673 - INFO - Using GPU: NVIDIA A100 80GB PCIe MIG 3g.40gb
2025-12-25 23:17:05,675 - INFO - GPU memory: 39.2GB
2025-12-25 23:17:05,676 - INFO - CuDNN benchmark mode enabled
2025-12-25 23:17:05,678 - INFO - Data config: {'dataset': 'ADE20K', 'use_batch_for_val': False, 'val_batch': 5, 'batch_size': 16, 'num_workers': 8, 'root': './datasets', 'img_size': 512, 'n_train': None, 'n_test': None, 'stratified': False}
2025-12-25 23:17:05,679 - INFO - Swin config: {'img_size': 512, 'variant': 'tiny', 'patch_size': 4, 'embed_dim': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24], 'window_size': 7, 'mlp_ratio': 4.0, 'dropout': 0.0, 'attention_dropout': 0.0, 'projection_dropout': 0.0, 'drop_path_rate': 0.2}
2025-12-25 23:17:05,681 - INFO - Downstream config: {'mode': 'from_scratch', 'head_type': 'upernet', 'num_classes': 150, 'hidden_dim': None, 'freeze_encoder': False, 'use_pretrained': True}
2025-12-25 23:17:05,682 - INFO - Training config: {'learning_rate': 6e-05, 'num_epochs': 160, 'warmup_epochs': 2, 'warmup_start_factor': 0.1, 'weight_decay': 0.01, 'mixed_precision': True, 'compile': False}
2025-12-25 23:17:05,684 - INFO - Loading ADE20K dataset...
2025-12-25 23:17:05,686 - INFO - Using user ADE20K dataset from /home/pml20/datasets/ade20k
2025-12-25 23:17:06,451 - INFO - Loaded ADE20K data from /home/pml20/datasets/ade20k: train=20210, val=2000, test=2000
2025-12-25 23:17:06,452 - INFO - Dataset limits applied
2025-12-25 23:17:06,456 - INFO - Dataset loaded: train=20210 (1264 batches), val=2000 (125 batches)
2025-12-25 23:17:06,458 - INFO - ============================================================
2025-12-25 23:17:06,459 - INFO - Segmentation Pipeline: Swin-T + UperNet on ADE20K
2025-12-25 23:17:06,460 - INFO - ============================================================
2025-12-25 23:17:06,461 - INFO - Creating Swin-T + UperNet segmentation model...
2025-12-25 23:17:07,313 - INFO - Loading pretrained weights from Hugging Face hub (timm/swin_tiny_patch4_window7_224.ms_in1k)
2025-12-25 23:17:07,526 - INFO - HTTP Request: HEAD https://huggingface.co/timm/swin_tiny_patch4_window7_224.ms_in1k/resolve/main/model.safetensors "HTTP/1.1 302 Found"
2025-12-25 23:17:07,538 - INFO - [timm/swin_tiny_patch4_window7_224.ms_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-12-25 23:17:07,575 - INFO - Loaded pretrained model: swin_tiny_patch4_window7_224 from timm.
2025-12-25 23:17:07,625 - INFO - Weight transfer: 169 layers transferred.
2025-12-25 23:17:07,638 - INFO - Model parameters:
2025-12-25 23:17:07,639 - INFO -   Encoder: 27,517,818
2025-12-25 23:17:07,641 - INFO -   Head: 31,497,366
2025-12-25 23:17:07,643 - INFO -   Total: 59,015,184
2025-12-25 23:17:07,644 - INFO -   Trainable: 59,015,184
2025-12-25 23:17:07,704 - INFO - Mixed precision: CUDA bf16 selected (hardware supported)
2025-12-25 23:17:07,706 - INFO - Optimizer: training all model parameters (encoder + head)
2025-12-25 23:17:07,709 - INFO - LR Scheduler: cosine with 2 warmup epochs
2025-12-25 23:17:07,710 - INFO - Learning rate: 6e-05, min_lr: 6.000000000000001e-07
2025-12-25 23:17:07,712 - INFO - Training configuration:
2025-12-25 23:17:07,713 - INFO -   Epochs: 160
2025-12-25 23:17:07,715 - INFO -   Warmup epochs: 2
2025-12-25 23:17:07,716 - INFO -   Learning rate: 6e-05
2025-12-25 23:17:07,717 - INFO -   Weight decay: 0.01
2025-12-25 23:17:07,719 - INFO -   Freeze encoder: False
2025-12-25 23:17:07,721 - INFO -   Num classes: 150
2025-12-25 23:17:07,722 - INFO -   Mixed precision: torch.bfloat16
2025-12-25 23:17:07,724 - INFO - Starting training...
2025-12-25 23:17:07,725 - INFO - Starting segmentation training...
2025-12-25 23:17:07,727 - INFO - Training for 160 epochs, 1264 batches/epoch
2025-12-25 23:18:00,931 - INFO -   Batch 50/1264: Loss = 2.8976
2025-12-25 23:18:33,583 - INFO -   Batch 100/1264: Loss = 2.4661
2025-12-25 23:19:06,234 - INFO -   Batch 150/1264: Loss = 2.2644
2025-12-25 23:19:38,907 - INFO -   Batch 200/1264: Loss = 2.5702
2025-12-25 23:20:11,554 - INFO -   Batch 250/1264: Loss = 1.9140
2025-12-25 23:20:44,243 - INFO -   Batch 300/1264: Loss = 1.9310
2025-12-25 23:21:16,884 - INFO -   Batch 350/1264: Loss = 2.1669
2025-12-25 23:21:49,526 - INFO -   Batch 400/1264: Loss = 1.7388
2025-12-25 23:22:22,177 - INFO -   Batch 450/1264: Loss = 1.8357
2025-12-25 23:22:54,817 - INFO -   Batch 500/1264: Loss = 1.5035
2025-12-25 23:23:27,457 - INFO -   Batch 550/1264: Loss = 1.3183
2025-12-25 23:24:00,093 - INFO -   Batch 600/1264: Loss = 1.9521
2025-12-25 23:24:32,803 - INFO -   Batch 650/1264: Loss = 1.5907
2025-12-25 23:25:05,446 - INFO -   Batch 700/1264: Loss = 1.6489
2025-12-25 23:25:38,087 - INFO -   Batch 750/1264: Loss = 1.3963
2025-12-25 23:26:10,780 - INFO -   Batch 800/1264: Loss = 1.4360
2025-12-25 23:26:43,470 - INFO -   Batch 850/1264: Loss = 1.4827
2025-12-25 23:27:16,114 - INFO -   Batch 900/1264: Loss = 1.1174
2025-12-25 23:27:48,752 - INFO -   Batch 950/1264: Loss = 1.2477
2025-12-25 23:28:21,395 - INFO -   Batch 1000/1264: Loss = 1.6201
2025-12-25 23:28:54,036 - INFO -   Batch 1050/1264: Loss = 1.3296
2025-12-25 23:29:26,678 - INFO -   Batch 1100/1264: Loss = 1.2393
2025-12-25 23:29:59,319 - INFO -   Batch 1150/1264: Loss = 1.6031
2025-12-25 23:30:31,958 - INFO -   Batch 1200/1264: Loss = 1.7360
2025-12-25 23:31:04,598 - INFO -   Batch 1250/1264: Loss = 1.3631
2025-12-25 23:34:23,883 - INFO - Epoch 1/160: Train Loss: 1.8914, Val Loss: 1.2818, Val mIoU: 13.65%, Val PixelAcc: 70.39%
2025-12-25 23:34:23,885 - INFO -   → New best mIoU: 13.65%
2025-12-25 23:34:25,377 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-25 23:34:25,379 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-25 23:34:59,679 - INFO -   Batch 50/1264: Loss = 1.3001
2025-12-25 23:35:32,317 - INFO -   Batch 100/1264: Loss = 1.3458
2025-12-25 23:36:04,956 - INFO -   Batch 150/1264: Loss = 1.2123
2025-12-25 23:36:37,596 - INFO -   Batch 200/1264: Loss = 1.4176
2025-12-25 23:37:10,235 - INFO -   Batch 250/1264: Loss = 1.3457
2025-12-25 23:37:42,874 - INFO -   Batch 300/1264: Loss = 1.1699
2025-12-25 23:38:15,516 - INFO -   Batch 350/1264: Loss = 1.3432
2025-12-25 23:38:48,153 - INFO -   Batch 400/1264: Loss = 1.0652
2025-12-25 23:39:20,795 - INFO -   Batch 450/1264: Loss = 1.3043
2025-12-25 23:39:53,437 - INFO -   Batch 500/1264: Loss = 0.9352
2025-12-25 23:40:26,080 - INFO -   Batch 550/1264: Loss = 1.2789
2025-12-25 23:40:58,724 - INFO -   Batch 600/1264: Loss = 1.2018
2025-12-25 23:41:31,362 - INFO -   Batch 650/1264: Loss = 1.2468
2025-12-25 23:42:04,001 - INFO -   Batch 700/1264: Loss = 1.4979
2025-12-25 23:42:36,643 - INFO -   Batch 750/1264: Loss = 0.8794
2025-12-25 23:43:09,287 - INFO -   Batch 800/1264: Loss = 1.5113
2025-12-25 23:43:41,932 - INFO -   Batch 850/1264: Loss = 1.2294
2025-12-25 23:44:14,571 - INFO -   Batch 900/1264: Loss = 1.2202
2025-12-25 23:44:47,213 - INFO -   Batch 950/1264: Loss = 0.9004
2025-12-25 23:45:19,858 - INFO -   Batch 1000/1264: Loss = 1.5015
2025-12-25 23:45:52,497 - INFO -   Batch 1050/1264: Loss = 1.2958
2025-12-25 23:46:25,135 - INFO -   Batch 1100/1264: Loss = 1.4465
2025-12-25 23:46:57,776 - INFO -   Batch 1150/1264: Loss = 1.0731
2025-12-25 23:47:30,417 - INFO -   Batch 1200/1264: Loss = 0.8512
2025-12-25 23:48:03,060 - INFO -   Batch 1250/1264: Loss = 1.2742
2025-12-25 23:51:03,054 - INFO - Epoch 2/160: Train Loss: 1.1896, Val Loss: 0.9470, Val mIoU: 24.77%, Val PixelAcc: 73.94%
2025-12-25 23:51:03,055 - INFO -   → New best mIoU: 24.77%
2025-12-25 23:51:04,562 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-25 23:51:04,563 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-25 23:51:39,578 - INFO -   Batch 50/1264: Loss = 1.3469
2025-12-25 23:52:12,233 - INFO -   Batch 100/1264: Loss = 1.0300
2025-12-25 23:52:44,879 - INFO -   Batch 150/1264: Loss = 0.8489
2025-12-25 23:53:17,521 - INFO -   Batch 200/1264: Loss = 1.1739
2025-12-25 23:53:50,162 - INFO -   Batch 250/1264: Loss = 1.1929
2025-12-25 23:54:22,801 - INFO -   Batch 300/1264: Loss = 1.1699
2025-12-25 23:54:55,441 - INFO -   Batch 350/1264: Loss = 0.8550
2025-12-25 23:55:28,078 - INFO -   Batch 400/1264: Loss = 1.0503
2025-12-25 23:56:00,718 - INFO -   Batch 450/1264: Loss = 0.8150
2025-12-25 23:56:33,360 - INFO -   Batch 500/1264: Loss = 0.9013
2025-12-25 23:57:06,005 - INFO -   Batch 550/1264: Loss = 1.0635
2025-12-25 23:57:38,648 - INFO -   Batch 600/1264: Loss = 0.7424
2025-12-25 23:58:11,291 - INFO -   Batch 650/1264: Loss = 0.7937
2025-12-25 23:58:43,933 - INFO -   Batch 700/1264: Loss = 1.1083
2025-12-25 23:59:16,574 - INFO -   Batch 750/1264: Loss = 1.0589
2025-12-25 23:59:49,213 - INFO -   Batch 800/1264: Loss = 0.7228
2025-12-26 00:00:21,851 - INFO -   Batch 850/1264: Loss = 0.9248
2025-12-26 00:00:54,488 - INFO -   Batch 900/1264: Loss = 0.7694
2025-12-26 00:01:27,124 - INFO -   Batch 950/1264: Loss = 0.9416
2025-12-26 00:01:59,757 - INFO -   Batch 1000/1264: Loss = 1.3522
2025-12-26 00:02:32,395 - INFO -   Batch 1050/1264: Loss = 1.0975
2025-12-26 00:03:05,034 - INFO -   Batch 1100/1264: Loss = 1.0468
2025-12-26 00:03:37,668 - INFO -   Batch 1150/1264: Loss = 0.8530
2025-12-26 00:04:10,304 - INFO -   Batch 1200/1264: Loss = 0.9353
2025-12-26 00:04:42,941 - INFO -   Batch 1250/1264: Loss = 1.3228
2025-12-26 00:07:42,714 - INFO - Epoch 3/160: Train Loss: 0.9816, Val Loss: 0.8867, Val mIoU: 29.86%, Val PixelAcc: 74.31%
2025-12-26 00:07:42,716 - INFO -   → New best mIoU: 29.86%
2025-12-26 00:07:44,265 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 00:07:44,267 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 00:08:18,688 - INFO -   Batch 50/1264: Loss = 0.7275
2025-12-26 00:08:51,329 - INFO -   Batch 100/1264: Loss = 0.6825
2025-12-26 00:09:23,970 - INFO -   Batch 150/1264: Loss = 0.7321
2025-12-26 00:09:56,610 - INFO -   Batch 200/1264: Loss = 0.7343
2025-12-26 00:10:29,251 - INFO -   Batch 250/1264: Loss = 0.9383
2025-12-26 00:11:01,900 - INFO -   Batch 300/1264: Loss = 1.2218
2025-12-26 00:11:34,545 - INFO -   Batch 350/1264: Loss = 1.0823
2025-12-26 00:12:07,186 - INFO -   Batch 400/1264: Loss = 0.6287
2025-12-26 00:12:39,829 - INFO -   Batch 450/1264: Loss = 0.7618
2025-12-26 00:13:12,474 - INFO -   Batch 500/1264: Loss = 1.0043
2025-12-26 00:13:45,117 - INFO -   Batch 550/1264: Loss = 1.0454
2025-12-26 00:14:17,757 - INFO -   Batch 600/1264: Loss = 0.6671
2025-12-26 00:14:50,402 - INFO -   Batch 650/1264: Loss = 1.1947
2025-12-26 00:15:23,047 - INFO -   Batch 700/1264: Loss = 0.8473
2025-12-26 00:15:55,690 - INFO -   Batch 750/1264: Loss = 0.7822
2025-12-26 00:16:28,333 - INFO -   Batch 800/1264: Loss = 0.7123
2025-12-26 00:17:00,977 - INFO -   Batch 850/1264: Loss = 0.9020
2025-12-26 00:17:33,619 - INFO -   Batch 900/1264: Loss = 1.0053
2025-12-26 00:18:06,265 - INFO -   Batch 950/1264: Loss = 0.7966
2025-12-26 00:18:38,909 - INFO -   Batch 1000/1264: Loss = 0.8503
2025-12-26 00:19:11,548 - INFO -   Batch 1050/1264: Loss = 0.6958
2025-12-26 00:19:44,188 - INFO -   Batch 1100/1264: Loss = 0.8543
2025-12-26 00:20:16,832 - INFO -   Batch 1150/1264: Loss = 0.9072
2025-12-26 00:20:49,479 - INFO -   Batch 1200/1264: Loss = 0.7206
2025-12-26 00:21:22,124 - INFO -   Batch 1250/1264: Loss = 0.7215
2025-12-26 00:24:21,631 - INFO - Epoch 4/160: Train Loss: 0.8414, Val Loss: 0.8221, Val mIoU: 32.21%, Val PixelAcc: 75.78%
2025-12-26 00:24:21,633 - INFO -   → New best mIoU: 32.21%
2025-12-26 00:24:24,654 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 00:24:24,655 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 00:24:59,371 - INFO -   Batch 50/1264: Loss = 0.7070
2025-12-26 00:25:32,015 - INFO -   Batch 100/1264: Loss = 0.7776
2025-12-26 00:26:04,659 - INFO -   Batch 150/1264: Loss = 1.1224
2025-12-26 00:26:37,305 - INFO -   Batch 200/1264: Loss = 0.8909
2025-12-26 00:27:09,949 - INFO -   Batch 250/1264: Loss = 0.6067
2025-12-26 00:27:42,590 - INFO -   Batch 300/1264: Loss = 0.6927
2025-12-26 00:28:15,228 - INFO -   Batch 350/1264: Loss = 0.7392
2025-12-26 00:28:47,863 - INFO -   Batch 400/1264: Loss = 0.7942
2025-12-26 00:29:20,500 - INFO -   Batch 450/1264: Loss = 0.6602
2025-12-26 00:29:53,138 - INFO -   Batch 500/1264: Loss = 0.9910
2025-12-26 00:30:25,778 - INFO -   Batch 550/1264: Loss = 0.7345
2025-12-26 00:30:58,417 - INFO -   Batch 600/1264: Loss = 0.7016
2025-12-26 00:31:31,053 - INFO -   Batch 650/1264: Loss = 0.7084
2025-12-26 00:32:03,689 - INFO -   Batch 700/1264: Loss = 0.6872
2025-12-26 00:32:36,325 - INFO -   Batch 750/1264: Loss = 0.7537
2025-12-26 00:33:08,958 - INFO -   Batch 800/1264: Loss = 0.8530
2025-12-26 00:33:41,596 - INFO -   Batch 850/1264: Loss = 0.7995
2025-12-26 00:34:14,231 - INFO -   Batch 900/1264: Loss = 0.8054
2025-12-26 00:34:46,870 - INFO -   Batch 950/1264: Loss = 0.6963
2025-12-26 00:35:19,509 - INFO -   Batch 1000/1264: Loss = 0.7806
2025-12-26 00:35:52,151 - INFO -   Batch 1050/1264: Loss = 0.8416
2025-12-26 00:36:24,790 - INFO -   Batch 1100/1264: Loss = 0.9187
2025-12-26 00:36:57,431 - INFO -   Batch 1150/1264: Loss = 0.7941
2025-12-26 00:37:30,069 - INFO -   Batch 1200/1264: Loss = 0.8927
2025-12-26 00:38:02,735 - INFO -   Batch 1250/1264: Loss = 0.6650
2025-12-26 00:41:02,089 - INFO - Epoch 5/160: Train Loss: 0.7637, Val Loss: 0.7980, Val mIoU: 34.98%, Val PixelAcc: 76.35%
2025-12-26 00:41:02,100 - INFO -   → New best mIoU: 34.98%
2025-12-26 00:41:04,029 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 00:41:04,031 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 00:41:38,924 - INFO -   Batch 50/1264: Loss = 0.6511
2025-12-26 00:42:11,558 - INFO -   Batch 100/1264: Loss = 0.6773
2025-12-26 00:42:44,195 - INFO -   Batch 150/1264: Loss = 0.5617
2025-12-26 00:43:16,833 - INFO -   Batch 200/1264: Loss = 0.6469
2025-12-26 00:43:49,475 - INFO -   Batch 250/1264: Loss = 0.5777
2025-12-26 00:44:22,110 - INFO -   Batch 300/1264: Loss = 0.8121
2025-12-26 00:44:54,745 - INFO -   Batch 350/1264: Loss = 0.6387
2025-12-26 00:45:27,386 - INFO -   Batch 400/1264: Loss = 0.4926
2025-12-26 00:46:00,027 - INFO -   Batch 450/1264: Loss = 0.6496
2025-12-26 00:46:32,667 - INFO -   Batch 500/1264: Loss = 0.7802
2025-12-26 00:47:05,306 - INFO -   Batch 550/1264: Loss = 0.6803
2025-12-26 00:47:37,949 - INFO -   Batch 600/1264: Loss = 0.8004
2025-12-26 00:48:10,590 - INFO -   Batch 650/1264: Loss = 0.6131
2025-12-26 00:48:43,233 - INFO -   Batch 700/1264: Loss = 0.8616
2025-12-26 00:49:15,877 - INFO -   Batch 750/1264: Loss = 0.6982
2025-12-26 00:49:48,522 - INFO -   Batch 800/1264: Loss = 0.6122
2025-12-26 00:50:21,162 - INFO -   Batch 850/1264: Loss = 0.6098
2025-12-26 00:50:53,803 - INFO -   Batch 900/1264: Loss = 0.6900
2025-12-26 00:51:26,444 - INFO -   Batch 950/1264: Loss = 0.6218
2025-12-26 00:51:59,086 - INFO -   Batch 1000/1264: Loss = 0.7392
2025-12-26 00:52:31,728 - INFO -   Batch 1050/1264: Loss = 0.6949
2025-12-26 00:53:04,373 - INFO -   Batch 1100/1264: Loss = 0.7885
2025-12-26 00:53:37,020 - INFO -   Batch 1150/1264: Loss = 0.7592
2025-12-26 00:54:09,664 - INFO -   Batch 1200/1264: Loss = 0.6632
2025-12-26 00:54:42,311 - INFO -   Batch 1250/1264: Loss = 0.8370
2025-12-26 00:57:42,039 - INFO - Epoch 6/160: Train Loss: 0.7020, Val Loss: 0.7803, Val mIoU: 37.72%, Val PixelAcc: 76.96%
2025-12-26 00:57:42,041 - INFO -   → New best mIoU: 37.72%
2025-12-26 00:57:44,352 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 00:57:44,354 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 00:58:19,284 - INFO -   Batch 50/1264: Loss = 0.6263
2025-12-26 00:58:51,927 - INFO -   Batch 100/1264: Loss = 0.7400
2025-12-26 00:59:24,572 - INFO -   Batch 150/1264: Loss = 0.7536
2025-12-26 00:59:57,211 - INFO -   Batch 200/1264: Loss = 0.5496
2025-12-26 01:00:29,851 - INFO -   Batch 250/1264: Loss = 0.6872
2025-12-26 01:01:02,490 - INFO -   Batch 300/1264: Loss = 0.6118
2025-12-26 01:01:35,129 - INFO -   Batch 350/1264: Loss = 0.6583
2025-12-26 01:02:07,772 - INFO -   Batch 400/1264: Loss = 0.6017
2025-12-26 01:02:40,416 - INFO -   Batch 450/1264: Loss = 0.7332
2025-12-26 01:03:13,056 - INFO -   Batch 500/1264: Loss = 0.8239
2025-12-26 01:03:45,699 - INFO -   Batch 550/1264: Loss = 0.5023
2025-12-26 01:04:18,341 - INFO -   Batch 600/1264: Loss = 0.7135
2025-12-26 01:04:50,985 - INFO -   Batch 650/1264: Loss = 0.6786
2025-12-26 01:05:23,630 - INFO -   Batch 700/1264: Loss = 0.7062
2025-12-26 01:05:56,276 - INFO -   Batch 750/1264: Loss = 0.8584
2025-12-26 01:06:28,919 - INFO -   Batch 800/1264: Loss = 0.6385
2025-12-26 01:07:01,564 - INFO -   Batch 850/1264: Loss = 0.5826
2025-12-26 01:07:34,207 - INFO -   Batch 900/1264: Loss = 0.5545
2025-12-26 01:08:06,852 - INFO -   Batch 950/1264: Loss = 0.8218
2025-12-26 01:08:39,496 - INFO -   Batch 1000/1264: Loss = 0.6011
2025-12-26 01:09:12,140 - INFO -   Batch 1050/1264: Loss = 0.5690
2025-12-26 01:09:44,780 - INFO -   Batch 1100/1264: Loss = 0.6824
2025-12-26 01:10:17,423 - INFO -   Batch 1150/1264: Loss = 0.6233
2025-12-26 01:10:50,065 - INFO -   Batch 1200/1264: Loss = 0.4654
2025-12-26 01:11:22,707 - INFO -   Batch 1250/1264: Loss = 0.7176
2025-12-26 01:14:22,421 - INFO - Epoch 7/160: Train Loss: 0.6495, Val Loss: 0.7824, Val mIoU: 37.74%, Val PixelAcc: 76.95%
2025-12-26 01:14:22,422 - INFO -   → New best mIoU: 37.74%
2025-12-26 01:14:23,981 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 01:14:23,983 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 01:14:58,932 - INFO -   Batch 50/1264: Loss = 0.7215
2025-12-26 01:15:31,575 - INFO -   Batch 100/1264: Loss = 0.5581
2025-12-26 01:16:04,221 - INFO -   Batch 150/1264: Loss = 0.5782
2025-12-26 01:16:36,867 - INFO -   Batch 200/1264: Loss = 0.5171
2025-12-26 01:17:09,512 - INFO -   Batch 250/1264: Loss = 0.7730
2025-12-26 01:17:42,157 - INFO -   Batch 300/1264: Loss = 0.6521
2025-12-26 01:18:14,799 - INFO -   Batch 350/1264: Loss = 0.9579
2025-12-26 01:18:47,444 - INFO -   Batch 400/1264: Loss = 0.3557
2025-12-26 01:19:20,088 - INFO -   Batch 450/1264: Loss = 0.5201
2025-12-26 01:19:52,730 - INFO -   Batch 500/1264: Loss = 0.6818
2025-12-26 01:20:25,374 - INFO -   Batch 550/1264: Loss = 0.6842
2025-12-26 01:20:58,020 - INFO -   Batch 600/1264: Loss = 0.5177
2025-12-26 01:21:30,663 - INFO -   Batch 650/1264: Loss = 0.7754
2025-12-26 01:22:03,305 - INFO -   Batch 700/1264: Loss = 0.4934
2025-12-26 01:22:35,946 - INFO -   Batch 750/1264: Loss = 0.6455
2025-12-26 01:23:08,587 - INFO -   Batch 800/1264: Loss = 0.6995
2025-12-26 01:23:41,226 - INFO -   Batch 850/1264: Loss = 0.6374
2025-12-26 01:24:13,866 - INFO -   Batch 900/1264: Loss = 0.6938
2025-12-26 01:24:46,508 - INFO -   Batch 950/1264: Loss = 0.5548
2025-12-26 01:25:19,150 - INFO -   Batch 1000/1264: Loss = 0.5265
2025-12-26 01:25:51,794 - INFO -   Batch 1050/1264: Loss = 0.5382
2025-12-26 01:26:24,437 - INFO -   Batch 1100/1264: Loss = 0.5748
2025-12-26 01:26:57,074 - INFO -   Batch 1150/1264: Loss = 0.6531
2025-12-26 01:27:29,712 - INFO -   Batch 1200/1264: Loss = 0.5928
2025-12-26 01:28:02,348 - INFO -   Batch 1250/1264: Loss = 0.8570
2025-12-26 01:31:01,723 - INFO - Epoch 8/160: Train Loss: 0.6051, Val Loss: 0.7569, Val mIoU: 39.06%, Val PixelAcc: 77.68%
2025-12-26 01:31:01,725 - INFO -   → New best mIoU: 39.06%
2025-12-26 01:31:03,268 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 01:31:03,270 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 01:31:37,825 - INFO -   Batch 50/1264: Loss = 0.6084
2025-12-26 01:32:10,462 - INFO -   Batch 100/1264: Loss = 0.4135
2025-12-26 01:32:43,101 - INFO -   Batch 150/1264: Loss = 0.5221
2025-12-26 01:33:15,740 - INFO -   Batch 200/1264: Loss = 0.6107
2025-12-26 01:33:48,378 - INFO -   Batch 250/1264: Loss = 0.4896
2025-12-26 01:34:21,016 - INFO -   Batch 300/1264: Loss = 0.4933
2025-12-26 01:34:53,654 - INFO -   Batch 350/1264: Loss = 0.5458
2025-12-26 01:35:26,295 - INFO -   Batch 400/1264: Loss = 0.4876
2025-12-26 01:35:58,938 - INFO -   Batch 450/1264: Loss = 0.6911
2025-12-26 01:36:31,576 - INFO -   Batch 500/1264: Loss = 0.6437
2025-12-26 01:37:04,216 - INFO -   Batch 550/1264: Loss = 0.5351
2025-12-26 01:37:36,858 - INFO -   Batch 600/1264: Loss = 0.5521
2025-12-26 01:38:09,499 - INFO -   Batch 650/1264: Loss = 0.6268
2025-12-26 01:38:42,142 - INFO -   Batch 700/1264: Loss = 0.6206
2025-12-26 01:39:14,784 - INFO -   Batch 750/1264: Loss = 0.6819
2025-12-26 01:39:47,424 - INFO -   Batch 800/1264: Loss = 0.7584
2025-12-26 01:40:20,061 - INFO -   Batch 850/1264: Loss = 0.5750
2025-12-26 01:40:52,701 - INFO -   Batch 900/1264: Loss = 0.4529
2025-12-26 01:41:25,341 - INFO -   Batch 950/1264: Loss = 0.6955
2025-12-26 01:41:57,980 - INFO -   Batch 1000/1264: Loss = 0.5963
2025-12-26 01:42:30,621 - INFO -   Batch 1050/1264: Loss = 0.5656
2025-12-26 01:43:03,260 - INFO -   Batch 1100/1264: Loss = 0.6533
2025-12-26 01:43:35,902 - INFO -   Batch 1150/1264: Loss = 0.6028
2025-12-26 01:44:08,546 - INFO -   Batch 1200/1264: Loss = 0.6814
2025-12-26 01:44:41,188 - INFO -   Batch 1250/1264: Loss = 0.4852
2025-12-26 01:47:40,376 - INFO - Epoch 9/160: Train Loss: 0.5722, Val Loss: 0.7607, Val mIoU: 38.94%, Val PixelAcc: 77.83%
2025-12-26 01:48:15,067 - INFO -   Batch 50/1264: Loss = 0.4680
2025-12-26 01:48:47,712 - INFO -   Batch 100/1264: Loss = 0.5741
2025-12-26 01:49:20,358 - INFO -   Batch 150/1264: Loss = 0.4950
2025-12-26 01:49:53,004 - INFO -   Batch 200/1264: Loss = 0.4516
2025-12-26 01:50:25,650 - INFO -   Batch 250/1264: Loss = 0.5811
2025-12-26 01:50:58,290 - INFO -   Batch 300/1264: Loss = 0.7684
2025-12-26 01:51:30,930 - INFO -   Batch 350/1264: Loss = 0.4413
2025-12-26 01:52:03,570 - INFO -   Batch 400/1264: Loss = 0.6198
2025-12-26 01:52:36,209 - INFO -   Batch 450/1264: Loss = 0.5109
2025-12-26 01:53:08,847 - INFO -   Batch 500/1264: Loss = 0.6489
2025-12-26 01:53:41,486 - INFO -   Batch 550/1264: Loss = 0.6308
2025-12-26 01:54:14,125 - INFO -   Batch 600/1264: Loss = 0.5090
2025-12-26 01:54:46,764 - INFO -   Batch 650/1264: Loss = 0.5881
2025-12-26 01:55:19,405 - INFO -   Batch 700/1264: Loss = 0.5397
2025-12-26 01:55:52,044 - INFO -   Batch 750/1264: Loss = 0.5184
2025-12-26 01:56:24,684 - INFO -   Batch 800/1264: Loss = 0.5792
2025-12-26 01:56:57,320 - INFO -   Batch 850/1264: Loss = 0.5356
2025-12-26 01:57:29,954 - INFO -   Batch 900/1264: Loss = 0.6084
2025-12-26 01:58:02,586 - INFO -   Batch 950/1264: Loss = 0.4965
2025-12-26 01:58:35,222 - INFO -   Batch 1000/1264: Loss = 0.5917
2025-12-26 01:59:07,859 - INFO -   Batch 1050/1264: Loss = 0.6086
2025-12-26 01:59:40,498 - INFO -   Batch 1100/1264: Loss = 0.6015
2025-12-26 02:00:13,137 - INFO -   Batch 1150/1264: Loss = 0.5970
2025-12-26 02:00:45,780 - INFO -   Batch 1200/1264: Loss = 0.6577
2025-12-26 02:01:18,418 - INFO -   Batch 1250/1264: Loss = 0.6716
2025-12-26 02:04:18,272 - INFO - Epoch 10/160: Train Loss: 0.5362, Val Loss: 0.7566, Val mIoU: 39.92%, Val PixelAcc: 78.17%
2025-12-26 02:04:18,274 - INFO -   → New best mIoU: 39.92%
2025-12-26 02:04:19,840 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 02:04:19,842 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 02:04:21,333 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_10.pth
2025-12-26 02:04:21,335 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_10.pth
2025-12-26 02:04:56,362 - INFO -   Batch 50/1264: Loss = 0.4468
2025-12-26 02:05:28,997 - INFO -   Batch 100/1264: Loss = 0.8460
2025-12-26 02:06:01,632 - INFO -   Batch 150/1264: Loss = 0.3992
2025-12-26 02:06:34,271 - INFO -   Batch 200/1264: Loss = 0.5845
2025-12-26 02:07:06,911 - INFO -   Batch 250/1264: Loss = 0.5100
2025-12-26 02:07:39,550 - INFO -   Batch 300/1264: Loss = 0.4240
2025-12-26 02:08:12,188 - INFO -   Batch 350/1264: Loss = 0.5073
2025-12-26 02:08:44,828 - INFO -   Batch 400/1264: Loss = 0.5142
2025-12-26 02:09:17,473 - INFO -   Batch 450/1264: Loss = 0.5108
2025-12-26 02:09:50,114 - INFO -   Batch 500/1264: Loss = 0.4372
2025-12-26 02:10:22,752 - INFO -   Batch 550/1264: Loss = 0.6539
2025-12-26 02:10:55,393 - INFO -   Batch 600/1264: Loss = 0.4616
2025-12-26 02:11:28,034 - INFO -   Batch 650/1264: Loss = 0.5035
2025-12-26 02:12:00,678 - INFO -   Batch 700/1264: Loss = 0.3999
2025-12-26 02:12:33,319 - INFO -   Batch 750/1264: Loss = 0.4091
2025-12-26 02:13:05,960 - INFO -   Batch 800/1264: Loss = 0.4424
2025-12-26 02:13:38,603 - INFO -   Batch 850/1264: Loss = 0.4451
2025-12-26 02:14:11,245 - INFO -   Batch 900/1264: Loss = 0.5750
2025-12-26 02:14:43,887 - INFO -   Batch 950/1264: Loss = 0.3239
2025-12-26 02:15:16,526 - INFO -   Batch 1000/1264: Loss = 0.4096
2025-12-26 02:15:49,169 - INFO -   Batch 1050/1264: Loss = 0.5584
2025-12-26 02:16:21,810 - INFO -   Batch 1100/1264: Loss = 0.6437
2025-12-26 02:16:54,450 - INFO -   Batch 1150/1264: Loss = 0.5451
2025-12-26 02:17:27,090 - INFO -   Batch 1200/1264: Loss = 0.3851
2025-12-26 02:17:59,730 - INFO -   Batch 1250/1264: Loss = 0.5704
2025-12-26 02:20:59,660 - INFO - Epoch 11/160: Train Loss: 0.5109, Val Loss: 0.7483, Val mIoU: 39.72%, Val PixelAcc: 78.38%
2025-12-26 02:21:34,373 - INFO -   Batch 50/1264: Loss = 0.3330
2025-12-26 02:22:07,013 - INFO -   Batch 100/1264: Loss = 0.3155
2025-12-26 02:22:39,655 - INFO -   Batch 150/1264: Loss = 0.4429
2025-12-26 02:23:12,298 - INFO -   Batch 200/1264: Loss = 0.3967
2025-12-26 02:23:44,937 - INFO -   Batch 250/1264: Loss = 0.5760
2025-12-26 02:24:17,576 - INFO -   Batch 300/1264: Loss = 0.5279
2025-12-26 02:24:50,229 - INFO -   Batch 350/1264: Loss = 0.4481
2025-12-26 02:25:22,870 - INFO -   Batch 400/1264: Loss = 0.4164
2025-12-26 02:25:55,509 - INFO -   Batch 450/1264: Loss = 0.5479
2025-12-26 02:26:28,152 - INFO -   Batch 500/1264: Loss = 0.4853
2025-12-26 02:27:00,793 - INFO -   Batch 550/1264: Loss = 0.5923
2025-12-26 02:27:33,437 - INFO -   Batch 600/1264: Loss = 0.5017
2025-12-26 02:28:06,078 - INFO -   Batch 650/1264: Loss = 0.2823
2025-12-26 02:28:38,723 - INFO -   Batch 700/1264: Loss = 0.5269
2025-12-26 02:29:11,367 - INFO -   Batch 750/1264: Loss = 0.4299
2025-12-26 02:29:44,009 - INFO -   Batch 800/1264: Loss = 0.6153
2025-12-26 02:30:16,649 - INFO -   Batch 850/1264: Loss = 0.5071
2025-12-26 02:30:49,286 - INFO -   Batch 900/1264: Loss = 0.5105
2025-12-26 02:31:21,923 - INFO -   Batch 950/1264: Loss = 0.4270
2025-12-26 02:31:54,564 - INFO -   Batch 1000/1264: Loss = 0.4749
2025-12-26 02:32:27,206 - INFO -   Batch 1050/1264: Loss = 0.4395
2025-12-26 02:32:59,849 - INFO -   Batch 1100/1264: Loss = 0.4301
2025-12-26 02:33:32,492 - INFO -   Batch 1150/1264: Loss = 0.4949
2025-12-26 02:34:05,135 - INFO -   Batch 1200/1264: Loss = 0.4524
2025-12-26 02:34:37,774 - INFO -   Batch 1250/1264: Loss = 0.4238
2025-12-26 02:37:37,575 - INFO - Epoch 12/160: Train Loss: 0.4789, Val Loss: 0.7520, Val mIoU: 39.99%, Val PixelAcc: 78.36%
2025-12-26 02:37:37,577 - INFO -   → New best mIoU: 39.99%
2025-12-26 02:37:39,489 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 02:37:39,491 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 02:38:13,916 - INFO -   Batch 50/1264: Loss = 0.4605
2025-12-26 02:38:46,556 - INFO -   Batch 100/1264: Loss = 0.4838
2025-12-26 02:39:19,196 - INFO -   Batch 150/1264: Loss = 0.3443
2025-12-26 02:39:51,838 - INFO -   Batch 200/1264: Loss = 0.4778
2025-12-26 02:40:24,479 - INFO -   Batch 250/1264: Loss = 0.4248
2025-12-26 02:40:57,122 - INFO -   Batch 300/1264: Loss = 0.5077
2025-12-26 02:41:29,763 - INFO -   Batch 350/1264: Loss = 0.2654
2025-12-26 02:42:02,402 - INFO -   Batch 400/1264: Loss = 0.5197
2025-12-26 02:42:35,040 - INFO -   Batch 450/1264: Loss = 0.4053
2025-12-26 02:43:07,680 - INFO -   Batch 500/1264: Loss = 0.4328
2025-12-26 02:43:40,323 - INFO -   Batch 550/1264: Loss = 0.4430
2025-12-26 02:44:12,966 - INFO -   Batch 600/1264: Loss = 0.3746
2025-12-26 02:44:45,610 - INFO -   Batch 650/1264: Loss = 0.3962
2025-12-26 02:45:18,249 - INFO -   Batch 700/1264: Loss = 0.5759
2025-12-26 02:45:50,891 - INFO -   Batch 750/1264: Loss = 0.3704
2025-12-26 02:46:23,534 - INFO -   Batch 800/1264: Loss = 0.4699
2025-12-26 02:46:56,178 - INFO -   Batch 850/1264: Loss = 0.4611
2025-12-26 02:47:28,819 - INFO -   Batch 900/1264: Loss = 0.5421
2025-12-26 02:48:01,464 - INFO -   Batch 950/1264: Loss = 0.4613
2025-12-26 02:48:34,103 - INFO -   Batch 1000/1264: Loss = 0.5302
2025-12-26 02:49:06,746 - INFO -   Batch 1050/1264: Loss = 0.5684
2025-12-26 02:49:39,392 - INFO -   Batch 1100/1264: Loss = 0.3560
2025-12-26 02:50:12,034 - INFO -   Batch 1150/1264: Loss = 0.5567
2025-12-26 02:50:44,677 - INFO -   Batch 1200/1264: Loss = 0.6218
2025-12-26 02:51:17,325 - INFO -   Batch 1250/1264: Loss = 0.4380
2025-12-26 02:54:16,726 - INFO - Epoch 13/160: Train Loss: 0.4633, Val Loss: 0.7808, Val mIoU: 39.96%, Val PixelAcc: 77.85%
2025-12-26 02:54:50,964 - INFO -   Batch 50/1264: Loss = 0.4702
2025-12-26 02:55:23,609 - INFO -   Batch 100/1264: Loss = 0.4921
2025-12-26 02:55:56,253 - INFO -   Batch 150/1264: Loss = 0.3473
2025-12-26 02:56:28,896 - INFO -   Batch 200/1264: Loss = 0.5121
2025-12-26 02:57:01,539 - INFO -   Batch 250/1264: Loss = 0.6109
2025-12-26 02:57:34,185 - INFO -   Batch 300/1264: Loss = 0.4113
2025-12-26 02:58:06,830 - INFO -   Batch 350/1264: Loss = 0.3934
2025-12-26 02:58:39,476 - INFO -   Batch 400/1264: Loss = 0.3193
2025-12-26 02:59:12,124 - INFO -   Batch 450/1264: Loss = 0.5025
2025-12-26 02:59:44,773 - INFO -   Batch 500/1264: Loss = 0.3072
2025-12-26 03:00:17,421 - INFO -   Batch 550/1264: Loss = 0.3677
2025-12-26 03:00:50,066 - INFO -   Batch 600/1264: Loss = 0.6906
2025-12-26 03:01:22,715 - INFO -   Batch 650/1264: Loss = 0.5299
2025-12-26 03:01:55,363 - INFO -   Batch 700/1264: Loss = 0.4182
2025-12-26 03:02:28,009 - INFO -   Batch 750/1264: Loss = 0.4699
2025-12-26 03:03:00,653 - INFO -   Batch 800/1264: Loss = 0.4175
2025-12-26 03:03:33,297 - INFO -   Batch 850/1264: Loss = 0.3916
2025-12-26 03:04:05,940 - INFO -   Batch 900/1264: Loss = 0.3217
2025-12-26 03:04:38,584 - INFO -   Batch 950/1264: Loss = 0.5550
2025-12-26 03:05:11,227 - INFO -   Batch 1000/1264: Loss = 0.5570
2025-12-26 03:05:43,870 - INFO -   Batch 1050/1264: Loss = 0.4089
2025-12-26 03:06:16,512 - INFO -   Batch 1100/1264: Loss = 0.5221
2025-12-26 03:06:49,156 - INFO -   Batch 1150/1264: Loss = 0.5369
2025-12-26 03:07:21,797 - INFO -   Batch 1200/1264: Loss = 0.3813
2025-12-26 03:07:54,440 - INFO -   Batch 1250/1264: Loss = 0.5891
2025-12-26 03:10:54,026 - INFO - Epoch 14/160: Train Loss: 0.4430, Val Loss: 0.7643, Val mIoU: 40.82%, Val PixelAcc: 78.31%
2025-12-26 03:10:54,028 - INFO -   → New best mIoU: 40.82%
2025-12-26 03:10:56,062 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 03:10:56,063 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 03:11:30,728 - INFO -   Batch 50/1264: Loss = 0.3502
2025-12-26 03:12:03,367 - INFO -   Batch 100/1264: Loss = 0.4030
2025-12-26 03:12:36,010 - INFO -   Batch 150/1264: Loss = 0.3597
2025-12-26 03:13:08,652 - INFO -   Batch 200/1264: Loss = 0.3950
2025-12-26 03:13:41,291 - INFO -   Batch 250/1264: Loss = 0.3517
2025-12-26 03:14:13,930 - INFO -   Batch 300/1264: Loss = 0.4193
2025-12-26 03:14:46,571 - INFO -   Batch 350/1264: Loss = 0.3807
2025-12-26 03:15:19,215 - INFO -   Batch 400/1264: Loss = 0.5451
2025-12-26 03:15:51,861 - INFO -   Batch 450/1264: Loss = 0.5103
2025-12-26 03:16:24,505 - INFO -   Batch 500/1264: Loss = 0.5976
2025-12-26 03:16:57,145 - INFO -   Batch 550/1264: Loss = 0.3500
2025-12-26 03:17:29,780 - INFO -   Batch 600/1264: Loss = 0.5644
2025-12-26 03:18:02,420 - INFO -   Batch 650/1264: Loss = 0.3613
2025-12-26 03:18:35,058 - INFO -   Batch 700/1264: Loss = 0.4923
2025-12-26 03:19:07,696 - INFO -   Batch 750/1264: Loss = 0.4105
2025-12-26 03:19:40,336 - INFO -   Batch 800/1264: Loss = 0.5501
2025-12-26 03:20:12,974 - INFO -   Batch 850/1264: Loss = 0.4845
2025-12-26 03:20:45,609 - INFO -   Batch 900/1264: Loss = 0.3273
2025-12-26 03:21:18,250 - INFO -   Batch 950/1264: Loss = 0.4364
2025-12-26 03:21:50,891 - INFO -   Batch 1000/1264: Loss = 0.4493
2025-12-26 03:22:23,530 - INFO -   Batch 1050/1264: Loss = 0.4201
2025-12-26 03:22:56,169 - INFO -   Batch 1100/1264: Loss = 0.4032
2025-12-26 03:23:28,807 - INFO -   Batch 1150/1264: Loss = 0.3629
2025-12-26 03:24:01,442 - INFO -   Batch 1200/1264: Loss = 0.4860
2025-12-26 03:24:34,077 - INFO -   Batch 1250/1264: Loss = 0.3921
2025-12-26 03:27:33,918 - INFO - Epoch 15/160: Train Loss: 0.4244, Val Loss: 0.7790, Val mIoU: 40.80%, Val PixelAcc: 78.48%
2025-12-26 03:28:08,029 - INFO -   Batch 50/1264: Loss = 0.5064
2025-12-26 03:28:40,669 - INFO -   Batch 100/1264: Loss = 0.3535
2025-12-26 03:29:13,310 - INFO -   Batch 150/1264: Loss = 0.5312
2025-12-26 03:29:45,947 - INFO -   Batch 200/1264: Loss = 0.3617
2025-12-26 03:30:18,587 - INFO -   Batch 250/1264: Loss = 0.3291
2025-12-26 03:30:51,222 - INFO -   Batch 300/1264: Loss = 0.3462
2025-12-26 03:31:23,861 - INFO -   Batch 350/1264: Loss = 0.3771
2025-12-26 03:31:56,498 - INFO -   Batch 400/1264: Loss = 0.4822
2025-12-26 03:32:29,138 - INFO -   Batch 450/1264: Loss = 0.3618
2025-12-26 03:33:01,779 - INFO -   Batch 500/1264: Loss = 0.4023
2025-12-26 03:33:34,421 - INFO -   Batch 550/1264: Loss = 0.3134
2025-12-26 03:34:07,057 - INFO -   Batch 600/1264: Loss = 0.3916
2025-12-26 03:34:39,690 - INFO -   Batch 650/1264: Loss = 0.4268
2025-12-26 03:35:12,323 - INFO -   Batch 700/1264: Loss = 0.4444
2025-12-26 03:35:44,961 - INFO -   Batch 750/1264: Loss = 0.3128
2025-12-26 03:36:17,597 - INFO -   Batch 800/1264: Loss = 0.5109
2025-12-26 03:36:50,236 - INFO -   Batch 850/1264: Loss = 0.3290
2025-12-26 03:37:22,873 - INFO -   Batch 900/1264: Loss = 0.3438
2025-12-26 03:37:55,511 - INFO -   Batch 950/1264: Loss = 0.4215
2025-12-26 03:38:28,148 - INFO -   Batch 1000/1264: Loss = 0.3497
2025-12-26 03:39:00,787 - INFO -   Batch 1050/1264: Loss = 0.3614
2025-12-26 03:39:33,419 - INFO -   Batch 1100/1264: Loss = 0.6222
2025-12-26 03:40:06,052 - INFO -   Batch 1150/1264: Loss = 0.5487
2025-12-26 03:40:38,688 - INFO -   Batch 1200/1264: Loss = 0.3005
2025-12-26 03:41:11,324 - INFO -   Batch 1250/1264: Loss = 0.5542
2025-12-26 03:44:11,179 - INFO - Epoch 16/160: Train Loss: 0.4055, Val Loss: 0.7813, Val mIoU: 40.21%, Val PixelAcc: 78.61%
2025-12-26 03:44:45,326 - INFO -   Batch 50/1264: Loss = 0.3669
2025-12-26 03:45:17,956 - INFO -   Batch 100/1264: Loss = 0.3408
2025-12-26 03:45:50,593 - INFO -   Batch 150/1264: Loss = 0.4448
2025-12-26 03:46:23,228 - INFO -   Batch 200/1264: Loss = 0.3319
2025-12-26 03:46:55,861 - INFO -   Batch 250/1264: Loss = 0.4213
2025-12-26 03:47:28,496 - INFO -   Batch 300/1264: Loss = 0.2542
2025-12-26 03:48:01,128 - INFO -   Batch 350/1264: Loss = 0.3788
2025-12-26 03:48:33,766 - INFO -   Batch 400/1264: Loss = 0.4331
2025-12-26 03:49:06,403 - INFO -   Batch 450/1264: Loss = 0.2903
2025-12-26 03:49:39,041 - INFO -   Batch 500/1264: Loss = 0.4701
2025-12-26 03:50:11,679 - INFO -   Batch 550/1264: Loss = 0.3888
2025-12-26 03:50:44,318 - INFO -   Batch 600/1264: Loss = 0.4226
2025-12-26 03:51:16,959 - INFO -   Batch 650/1264: Loss = 0.3522
2025-12-26 03:51:49,602 - INFO -   Batch 700/1264: Loss = 0.4183
2025-12-26 03:52:22,241 - INFO -   Batch 750/1264: Loss = 0.4660
2025-12-26 03:52:54,878 - INFO -   Batch 800/1264: Loss = 0.4817
2025-12-26 03:53:27,514 - INFO -   Batch 850/1264: Loss = 0.4123
2025-12-26 03:54:00,147 - INFO -   Batch 900/1264: Loss = 0.2824
2025-12-26 03:54:32,782 - INFO -   Batch 950/1264: Loss = 0.3383
2025-12-26 03:55:05,419 - INFO -   Batch 1000/1264: Loss = 0.4697
2025-12-26 03:55:38,062 - INFO -   Batch 1050/1264: Loss = 0.5180
2025-12-26 03:56:10,705 - INFO -   Batch 1100/1264: Loss = 0.3680
2025-12-26 03:56:43,345 - INFO -   Batch 1150/1264: Loss = 0.4409
2025-12-26 03:57:15,982 - INFO -   Batch 1200/1264: Loss = 0.3284
2025-12-26 03:57:48,622 - INFO -   Batch 1250/1264: Loss = 0.3376
2025-12-26 04:00:48,536 - INFO - Epoch 17/160: Train Loss: 0.3960, Val Loss: 0.7890, Val mIoU: 41.64%, Val PixelAcc: 78.65%
2025-12-26 04:00:48,538 - INFO -   → New best mIoU: 41.64%
2025-12-26 04:00:50,103 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 04:00:50,105 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 04:01:24,224 - INFO -   Batch 50/1264: Loss = 0.3215
2025-12-26 04:01:56,866 - INFO -   Batch 100/1264: Loss = 0.3480
2025-12-26 04:02:29,505 - INFO -   Batch 150/1264: Loss = 0.4259
2025-12-26 04:03:02,141 - INFO -   Batch 200/1264: Loss = 0.3541
2025-12-26 04:03:34,777 - INFO -   Batch 250/1264: Loss = 0.4070
2025-12-26 04:04:07,417 - INFO -   Batch 300/1264: Loss = 0.4303
2025-12-26 04:04:40,056 - INFO -   Batch 350/1264: Loss = 0.5228
2025-12-26 04:05:12,697 - INFO -   Batch 400/1264: Loss = 0.3589
2025-12-26 04:05:45,342 - INFO -   Batch 450/1264: Loss = 0.3801
2025-12-26 04:06:17,988 - INFO -   Batch 500/1264: Loss = 0.3928
2025-12-26 04:06:50,628 - INFO -   Batch 550/1264: Loss = 0.3791
2025-12-26 04:07:23,265 - INFO -   Batch 600/1264: Loss = 0.4017
2025-12-26 04:07:55,902 - INFO -   Batch 650/1264: Loss = 0.5528
2025-12-26 04:08:28,540 - INFO -   Batch 700/1264: Loss = 0.3764
2025-12-26 04:09:01,177 - INFO -   Batch 750/1264: Loss = 0.3708
2025-12-26 04:09:33,814 - INFO -   Batch 800/1264: Loss = 0.4609
2025-12-26 04:10:06,450 - INFO -   Batch 850/1264: Loss = 0.3731
2025-12-26 04:10:39,085 - INFO -   Batch 900/1264: Loss = 0.2821
2025-12-26 04:11:11,720 - INFO -   Batch 950/1264: Loss = 0.3601
2025-12-26 04:11:44,358 - INFO -   Batch 1000/1264: Loss = 0.4578
2025-12-26 04:12:16,996 - INFO -   Batch 1050/1264: Loss = 0.4575
2025-12-26 04:12:49,636 - INFO -   Batch 1100/1264: Loss = 0.4677
2025-12-26 04:13:22,282 - INFO -   Batch 1150/1264: Loss = 0.4287
2025-12-26 04:13:54,923 - INFO -   Batch 1200/1264: Loss = 0.3253
2025-12-26 04:14:27,564 - INFO -   Batch 1250/1264: Loss = 0.3128
2025-12-26 04:17:27,615 - INFO - Epoch 18/160: Train Loss: 0.3802, Val Loss: 0.7913, Val mIoU: 41.54%, Val PixelAcc: 78.60%
2025-12-26 04:18:01,730 - INFO -   Batch 50/1264: Loss = 0.4437
2025-12-26 04:18:34,367 - INFO -   Batch 100/1264: Loss = 0.3687
2025-12-26 04:19:07,007 - INFO -   Batch 150/1264: Loss = 0.4267
2025-12-26 04:19:39,649 - INFO -   Batch 200/1264: Loss = 0.2953
2025-12-26 04:20:12,293 - INFO -   Batch 250/1264: Loss = 0.3650
2025-12-26 04:20:44,936 - INFO -   Batch 300/1264: Loss = 0.3239
2025-12-26 04:21:17,576 - INFO -   Batch 350/1264: Loss = 0.2839
2025-12-26 04:21:50,213 - INFO -   Batch 400/1264: Loss = 0.3654
2025-12-26 04:22:22,849 - INFO -   Batch 450/1264: Loss = 0.3517
2025-12-26 04:22:55,482 - INFO -   Batch 500/1264: Loss = 0.5268
2025-12-26 04:23:28,118 - INFO -   Batch 550/1264: Loss = 0.4625
2025-12-26 04:24:00,756 - INFO -   Batch 600/1264: Loss = 0.3282
2025-12-26 04:24:33,393 - INFO -   Batch 650/1264: Loss = 0.3514
2025-12-26 04:25:06,031 - INFO -   Batch 700/1264: Loss = 0.3283
2025-12-26 04:25:38,666 - INFO -   Batch 750/1264: Loss = 0.2594
2025-12-26 04:26:11,303 - INFO -   Batch 800/1264: Loss = 0.4274
2025-12-26 04:26:43,937 - INFO -   Batch 850/1264: Loss = 0.3303
2025-12-26 04:27:16,572 - INFO -   Batch 900/1264: Loss = 0.4580
2025-12-26 04:27:49,205 - INFO -   Batch 950/1264: Loss = 0.3146
2025-12-26 04:28:21,840 - INFO -   Batch 1000/1264: Loss = 0.4040
2025-12-26 04:28:54,479 - INFO -   Batch 1050/1264: Loss = 0.3578
2025-12-26 04:29:27,122 - INFO -   Batch 1100/1264: Loss = 0.3895
2025-12-26 04:29:59,767 - INFO -   Batch 1150/1264: Loss = 0.2992
2025-12-26 04:30:32,409 - INFO -   Batch 1200/1264: Loss = 0.2553
2025-12-26 04:31:05,049 - INFO -   Batch 1250/1264: Loss = 0.3467
2025-12-26 04:34:05,295 - INFO - Epoch 19/160: Train Loss: 0.3729, Val Loss: 0.7779, Val mIoU: 41.64%, Val PixelAcc: 79.01%
2025-12-26 04:34:05,297 - INFO -   → New best mIoU: 41.64%
2025-12-26 04:34:07,300 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 04:34:07,302 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 04:34:41,561 - INFO -   Batch 50/1264: Loss = 0.4291
2025-12-26 04:35:14,217 - INFO -   Batch 100/1264: Loss = 0.4305
2025-12-26 04:35:46,872 - INFO -   Batch 150/1264: Loss = 0.3415
2025-12-26 04:36:19,524 - INFO -   Batch 200/1264: Loss = 0.2649
2025-12-26 04:36:52,172 - INFO -   Batch 250/1264: Loss = 0.3723
2025-12-26 04:37:24,816 - INFO -   Batch 300/1264: Loss = 0.3712
2025-12-26 04:37:57,457 - INFO -   Batch 350/1264: Loss = 0.3056
2025-12-26 04:38:30,101 - INFO -   Batch 400/1264: Loss = 0.4924
2025-12-26 04:39:02,743 - INFO -   Batch 450/1264: Loss = 0.3151
2025-12-26 04:39:35,382 - INFO -   Batch 500/1264: Loss = 0.6025
2025-12-26 04:40:08,022 - INFO -   Batch 550/1264: Loss = 0.2830
2025-12-26 04:40:40,662 - INFO -   Batch 600/1264: Loss = 0.4176
2025-12-26 04:41:13,300 - INFO -   Batch 650/1264: Loss = 0.4191
2025-12-26 04:41:45,940 - INFO -   Batch 700/1264: Loss = 0.3332
2025-12-26 04:42:18,583 - INFO -   Batch 750/1264: Loss = 0.3966
2025-12-26 04:42:51,225 - INFO -   Batch 800/1264: Loss = 0.3530
2025-12-26 04:43:23,868 - INFO -   Batch 850/1264: Loss = 0.3051
2025-12-26 04:43:56,511 - INFO -   Batch 900/1264: Loss = 0.3348
2025-12-26 04:44:29,154 - INFO -   Batch 950/1264: Loss = 0.3855
2025-12-26 04:45:01,799 - INFO -   Batch 1000/1264: Loss = 0.3687
2025-12-26 04:45:34,442 - INFO -   Batch 1050/1264: Loss = 0.3196
2025-12-26 04:46:07,088 - INFO -   Batch 1100/1264: Loss = 0.3203
2025-12-26 04:46:39,731 - INFO -   Batch 1150/1264: Loss = 0.3263
2025-12-26 04:47:12,371 - INFO -   Batch 1200/1264: Loss = 0.3157
2025-12-26 04:47:45,011 - INFO -   Batch 1250/1264: Loss = 0.4364
2025-12-26 04:50:44,850 - INFO - Epoch 20/160: Train Loss: 0.3533, Val Loss: 0.8119, Val mIoU: 41.29%, Val PixelAcc: 78.51%
2025-12-26 04:50:46,387 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_20.pth
2025-12-26 04:50:46,389 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_20.pth
2025-12-26 04:51:20,571 - INFO -   Batch 50/1264: Loss = 0.3747
2025-12-26 04:51:53,216 - INFO -   Batch 100/1264: Loss = 0.2314
2025-12-26 04:52:25,854 - INFO -   Batch 150/1264: Loss = 0.3001
2025-12-26 04:52:58,494 - INFO -   Batch 200/1264: Loss = 0.3621
2025-12-26 04:53:31,133 - INFO -   Batch 250/1264: Loss = 0.3941
2025-12-26 04:54:03,774 - INFO -   Batch 300/1264: Loss = 0.2736
2025-12-26 04:54:36,415 - INFO -   Batch 350/1264: Loss = 0.2730
2025-12-26 04:55:09,053 - INFO -   Batch 400/1264: Loss = 0.2796
2025-12-26 04:55:41,695 - INFO -   Batch 450/1264: Loss = 0.3004
2025-12-26 04:56:14,333 - INFO -   Batch 500/1264: Loss = 0.3279
2025-12-26 04:56:46,967 - INFO -   Batch 550/1264: Loss = 0.4564
2025-12-26 04:57:19,600 - INFO -   Batch 600/1264: Loss = 0.3061
2025-12-26 04:57:52,233 - INFO -   Batch 650/1264: Loss = 0.2935
2025-12-26 04:58:24,864 - INFO -   Batch 700/1264: Loss = 0.4171
2025-12-26 04:58:57,496 - INFO -   Batch 750/1264: Loss = 0.3351
2025-12-26 04:59:30,128 - INFO -   Batch 800/1264: Loss = 0.3792
2025-12-26 05:00:02,759 - INFO -   Batch 850/1264: Loss = 0.4545
2025-12-26 05:00:35,388 - INFO -   Batch 900/1264: Loss = 0.4497
2025-12-26 05:01:08,019 - INFO -   Batch 950/1264: Loss = 0.3183
2025-12-26 05:01:40,648 - INFO -   Batch 1000/1264: Loss = 0.3651
2025-12-26 05:02:13,277 - INFO -   Batch 1050/1264: Loss = 0.2220
2025-12-26 05:02:45,911 - INFO -   Batch 1100/1264: Loss = 0.3154
2025-12-26 05:03:18,541 - INFO -   Batch 1150/1264: Loss = 0.3112
2025-12-26 05:03:51,174 - INFO -   Batch 1200/1264: Loss = 0.4519
2025-12-26 05:04:23,810 - INFO -   Batch 1250/1264: Loss = 0.4148
2025-12-26 05:07:23,508 - INFO - Epoch 21/160: Train Loss: 0.3453, Val Loss: 0.7942, Val mIoU: 41.85%, Val PixelAcc: 78.85%
2025-12-26 05:07:23,509 - INFO -   → New best mIoU: 41.85%
2025-12-26 05:07:25,073 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 05:07:25,075 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 05:07:59,048 - INFO -   Batch 50/1264: Loss = 0.4861
2025-12-26 05:08:31,685 - INFO -   Batch 100/1264: Loss = 0.2947
2025-12-26 05:09:04,321 - INFO -   Batch 150/1264: Loss = 0.4484
2025-12-26 05:09:36,957 - INFO -   Batch 200/1264: Loss = 0.3448
2025-12-26 05:10:09,592 - INFO -   Batch 250/1264: Loss = 0.2811
2025-12-26 05:10:42,224 - INFO -   Batch 300/1264: Loss = 0.2723
2025-12-26 05:11:14,854 - INFO -   Batch 350/1264: Loss = 0.3109
2025-12-26 05:11:47,487 - INFO -   Batch 400/1264: Loss = 0.4235
2025-12-26 05:12:20,120 - INFO -   Batch 450/1264: Loss = 0.4352
2025-12-26 05:12:52,750 - INFO -   Batch 500/1264: Loss = 0.2986
2025-12-26 05:13:25,389 - INFO -   Batch 550/1264: Loss = 0.2670
2025-12-26 05:13:58,026 - INFO -   Batch 600/1264: Loss = 0.3380
2025-12-26 05:14:30,664 - INFO -   Batch 650/1264: Loss = 0.3408
2025-12-26 05:15:03,303 - INFO -   Batch 700/1264: Loss = 0.4304
2025-12-26 05:15:35,943 - INFO -   Batch 750/1264: Loss = 0.3919
2025-12-26 05:16:08,580 - INFO -   Batch 800/1264: Loss = 0.3057
2025-12-26 05:16:41,218 - INFO -   Batch 850/1264: Loss = 0.3164
2025-12-26 05:17:13,861 - INFO -   Batch 900/1264: Loss = 0.2360
2025-12-26 05:17:46,502 - INFO -   Batch 950/1264: Loss = 0.3731
2025-12-26 05:18:19,142 - INFO -   Batch 1000/1264: Loss = 0.2754
2025-12-26 05:18:51,781 - INFO -   Batch 1050/1264: Loss = 0.3712
2025-12-26 05:19:24,421 - INFO -   Batch 1100/1264: Loss = 0.3637
2025-12-26 05:19:57,063 - INFO -   Batch 1150/1264: Loss = 0.2968
2025-12-26 05:20:29,706 - INFO -   Batch 1200/1264: Loss = 0.2827
2025-12-26 05:21:02,345 - INFO -   Batch 1250/1264: Loss = 0.3960
2025-12-26 05:24:02,241 - INFO - Epoch 22/160: Train Loss: 0.3372, Val Loss: 0.8014, Val mIoU: 41.94%, Val PixelAcc: 79.06%
2025-12-26 05:24:02,243 - INFO -   → New best mIoU: 41.94%
2025-12-26 05:24:03,813 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 05:24:03,815 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 05:24:37,986 - INFO -   Batch 50/1264: Loss = 0.3618
2025-12-26 05:25:10,630 - INFO -   Batch 100/1264: Loss = 0.3726
2025-12-26 05:25:43,270 - INFO -   Batch 150/1264: Loss = 0.3023
2025-12-26 05:26:15,911 - INFO -   Batch 200/1264: Loss = 0.2147
2025-12-26 05:26:48,553 - INFO -   Batch 250/1264: Loss = 0.2619
2025-12-26 05:27:21,191 - INFO -   Batch 300/1264: Loss = 0.2293
2025-12-26 05:27:53,831 - INFO -   Batch 350/1264: Loss = 0.2483
2025-12-26 05:28:26,469 - INFO -   Batch 400/1264: Loss = 0.2923
2025-12-26 05:28:59,111 - INFO -   Batch 450/1264: Loss = 0.3252
2025-12-26 05:29:31,747 - INFO -   Batch 500/1264: Loss = 0.3415
2025-12-26 05:30:04,384 - INFO -   Batch 550/1264: Loss = 0.3435
2025-12-26 05:30:37,019 - INFO -   Batch 600/1264: Loss = 0.2176
2025-12-26 05:31:09,656 - INFO -   Batch 650/1264: Loss = 0.3788
2025-12-26 05:31:42,292 - INFO -   Batch 700/1264: Loss = 0.4807
2025-12-26 05:32:14,932 - INFO -   Batch 750/1264: Loss = 0.3668
2025-12-26 05:32:47,571 - INFO -   Batch 800/1264: Loss = 0.2909
2025-12-26 05:33:20,212 - INFO -   Batch 850/1264: Loss = 0.3713
2025-12-26 05:33:52,852 - INFO -   Batch 900/1264: Loss = 0.2860
2025-12-26 05:34:25,490 - INFO -   Batch 950/1264: Loss = 0.4395
2025-12-26 05:34:58,122 - INFO -   Batch 1000/1264: Loss = 0.3290
2025-12-26 05:35:30,753 - INFO -   Batch 1050/1264: Loss = 0.3052
2025-12-26 05:36:03,390 - INFO -   Batch 1100/1264: Loss = 0.3330
2025-12-26 05:36:36,031 - INFO -   Batch 1150/1264: Loss = 0.3197
2025-12-26 05:37:08,670 - INFO -   Batch 1200/1264: Loss = 0.4013
2025-12-26 05:37:41,307 - INFO -   Batch 1250/1264: Loss = 0.3387
2025-12-26 05:40:41,415 - INFO - Epoch 23/160: Train Loss: 0.3278, Val Loss: 0.8012, Val mIoU: 42.00%, Val PixelAcc: 79.21%
2025-12-26 05:40:41,417 - INFO -   → New best mIoU: 42.00%
2025-12-26 05:40:43,001 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 05:40:43,003 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 05:41:17,238 - INFO -   Batch 50/1264: Loss = 0.3310
2025-12-26 05:41:49,876 - INFO -   Batch 100/1264: Loss = 0.3539
2025-12-26 05:42:22,514 - INFO -   Batch 150/1264: Loss = 0.2233
2025-12-26 05:42:55,151 - INFO -   Batch 200/1264: Loss = 0.3197
2025-12-26 05:43:27,789 - INFO -   Batch 250/1264: Loss = 0.3258
2025-12-26 05:44:00,425 - INFO -   Batch 300/1264: Loss = 0.3676
2025-12-26 05:44:33,061 - INFO -   Batch 350/1264: Loss = 0.3753
2025-12-26 05:45:05,699 - INFO -   Batch 400/1264: Loss = 0.2355
2025-12-26 05:45:38,335 - INFO -   Batch 450/1264: Loss = 0.2739
2025-12-26 05:46:10,973 - INFO -   Batch 500/1264: Loss = 0.3557
2025-12-26 05:46:43,612 - INFO -   Batch 550/1264: Loss = 0.3420
2025-12-26 05:47:16,248 - INFO -   Batch 600/1264: Loss = 0.3011
2025-12-26 05:47:48,882 - INFO -   Batch 650/1264: Loss = 0.4016
2025-12-26 05:48:21,516 - INFO -   Batch 700/1264: Loss = 0.2823
2025-12-26 05:48:54,152 - INFO -   Batch 750/1264: Loss = 0.1989
2025-12-26 05:49:26,791 - INFO -   Batch 800/1264: Loss = 0.3258
2025-12-26 05:49:59,428 - INFO -   Batch 850/1264: Loss = 0.2766
2025-12-26 05:50:32,061 - INFO -   Batch 900/1264: Loss = 0.4606
2025-12-26 05:51:04,695 - INFO -   Batch 950/1264: Loss = 0.2646
2025-12-26 05:51:37,329 - INFO -   Batch 1000/1264: Loss = 0.3467
2025-12-26 05:52:09,964 - INFO -   Batch 1050/1264: Loss = 0.2734
2025-12-26 05:52:42,595 - INFO -   Batch 1100/1264: Loss = 0.2313
2025-12-26 05:53:15,228 - INFO -   Batch 1150/1264: Loss = 0.3783
2025-12-26 05:53:47,861 - INFO -   Batch 1200/1264: Loss = 0.3555
2025-12-26 05:54:20,494 - INFO -   Batch 1250/1264: Loss = 0.3181
2025-12-26 05:57:20,778 - INFO - Epoch 24/160: Train Loss: 0.3164, Val Loss: 0.8215, Val mIoU: 42.18%, Val PixelAcc: 78.83%
2025-12-26 05:57:20,780 - INFO -   → New best mIoU: 42.18%
2025-12-26 05:57:22,378 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 05:57:22,379 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 05:57:56,278 - INFO -   Batch 50/1264: Loss = 0.3943
2025-12-26 05:58:28,915 - INFO -   Batch 100/1264: Loss = 0.2811
2025-12-26 05:59:01,553 - INFO -   Batch 150/1264: Loss = 0.3026
2025-12-26 05:59:34,189 - INFO -   Batch 200/1264: Loss = 0.2897
2025-12-26 06:00:06,822 - INFO -   Batch 250/1264: Loss = 0.3146
2025-12-26 06:00:39,457 - INFO -   Batch 300/1264: Loss = 0.3618
2025-12-26 06:01:12,091 - INFO -   Batch 350/1264: Loss = 0.3638
2025-12-26 06:01:44,726 - INFO -   Batch 400/1264: Loss = 0.2690
2025-12-26 06:02:17,360 - INFO -   Batch 450/1264: Loss = 0.3811
2025-12-26 06:02:49,993 - INFO -   Batch 500/1264: Loss = 0.2747
2025-12-26 06:03:22,628 - INFO -   Batch 550/1264: Loss = 0.2969
2025-12-26 06:03:55,265 - INFO -   Batch 600/1264: Loss = 0.5088
2025-12-26 06:04:27,904 - INFO -   Batch 650/1264: Loss = 0.2958
2025-12-26 06:05:00,542 - INFO -   Batch 700/1264: Loss = 0.2259
2025-12-26 06:05:33,181 - INFO -   Batch 750/1264: Loss = 0.3313
2025-12-26 06:06:05,817 - INFO -   Batch 800/1264: Loss = 0.2771
2025-12-26 06:06:38,457 - INFO -   Batch 850/1264: Loss = 0.2730
2025-12-26 06:07:11,098 - INFO -   Batch 900/1264: Loss = 0.4582
2025-12-26 06:07:43,734 - INFO -   Batch 950/1264: Loss = 0.4314
2025-12-26 06:08:16,372 - INFO -   Batch 1000/1264: Loss = 0.3681
2025-12-26 06:08:49,010 - INFO -   Batch 1050/1264: Loss = 0.3436
2025-12-26 06:09:21,647 - INFO -   Batch 1100/1264: Loss = 0.3301
2025-12-26 06:09:54,280 - INFO -   Batch 1150/1264: Loss = 0.3415
2025-12-26 06:10:26,914 - INFO -   Batch 1200/1264: Loss = 0.3596
2025-12-26 06:10:59,545 - INFO -   Batch 1250/1264: Loss = 0.3067
2025-12-26 06:13:59,583 - INFO - Epoch 25/160: Train Loss: 0.3132, Val Loss: 0.8264, Val mIoU: 41.68%, Val PixelAcc: 78.89%
2025-12-26 06:14:34,317 - INFO -   Batch 50/1264: Loss = 0.3333
2025-12-26 06:15:06,949 - INFO -   Batch 100/1264: Loss = 0.3006
2025-12-26 06:15:39,585 - INFO -   Batch 150/1264: Loss = 0.3423
2025-12-26 06:16:12,226 - INFO -   Batch 200/1264: Loss = 0.3175
2025-12-26 06:16:44,863 - INFO -   Batch 250/1264: Loss = 0.3804
2025-12-26 06:17:17,498 - INFO -   Batch 300/1264: Loss = 0.2888
2025-12-26 06:17:50,132 - INFO -   Batch 350/1264: Loss = 0.3947
2025-12-26 06:18:22,767 - INFO -   Batch 400/1264: Loss = 0.2427
2025-12-26 06:18:55,402 - INFO -   Batch 450/1264: Loss = 0.2157
2025-12-26 06:19:28,035 - INFO -   Batch 500/1264: Loss = 0.3442
2025-12-26 06:20:00,672 - INFO -   Batch 550/1264: Loss = 0.2465
2025-12-26 06:20:33,308 - INFO -   Batch 600/1264: Loss = 0.3468
2025-12-26 06:21:05,941 - INFO -   Batch 650/1264: Loss = 0.3501
2025-12-26 06:21:38,576 - INFO -   Batch 700/1264: Loss = 0.1997
2025-12-26 06:22:11,209 - INFO -   Batch 750/1264: Loss = 0.2747
2025-12-26 06:22:43,841 - INFO -   Batch 800/1264: Loss = 0.2666
2025-12-26 06:23:16,478 - INFO -   Batch 850/1264: Loss = 0.3126
2025-12-26 06:23:49,112 - INFO -   Batch 900/1264: Loss = 0.2907
2025-12-26 06:24:21,746 - INFO -   Batch 950/1264: Loss = 0.2455
2025-12-26 06:24:54,377 - INFO -   Batch 1000/1264: Loss = 0.2290
2025-12-26 06:25:27,011 - INFO -   Batch 1050/1264: Loss = 0.3067
2025-12-26 06:25:59,647 - INFO -   Batch 1100/1264: Loss = 0.3461
2025-12-26 06:26:32,280 - INFO -   Batch 1150/1264: Loss = 0.3768
2025-12-26 06:27:04,916 - INFO -   Batch 1200/1264: Loss = 0.3170
2025-12-26 06:27:37,552 - INFO -   Batch 1250/1264: Loss = 0.3168
2025-12-26 06:30:37,499 - INFO - Epoch 26/160: Train Loss: 0.3067, Val Loss: 0.8402, Val mIoU: 41.32%, Val PixelAcc: 78.88%
2025-12-26 06:31:11,885 - INFO -   Batch 50/1264: Loss = 0.4354
2025-12-26 06:31:44,520 - INFO -   Batch 100/1264: Loss = 0.2736
2025-12-26 06:32:17,157 - INFO -   Batch 150/1264: Loss = 0.2878
2025-12-26 06:32:49,792 - INFO -   Batch 200/1264: Loss = 0.3047
2025-12-26 06:33:22,427 - INFO -   Batch 250/1264: Loss = 0.3917
2025-12-26 06:33:55,064 - INFO -   Batch 300/1264: Loss = 0.3081
2025-12-26 06:34:27,700 - INFO -   Batch 350/1264: Loss = 0.3339
2025-12-26 06:35:00,337 - INFO -   Batch 400/1264: Loss = 0.3557
2025-12-26 06:35:32,975 - INFO -   Batch 450/1264: Loss = 0.3000
2025-12-26 06:36:05,612 - INFO -   Batch 500/1264: Loss = 0.2600
2025-12-26 06:36:38,248 - INFO -   Batch 550/1264: Loss = 0.2884
2025-12-26 06:37:10,885 - INFO -   Batch 600/1264: Loss = 0.3493
2025-12-26 06:37:43,524 - INFO -   Batch 650/1264: Loss = 0.2576
2025-12-26 06:38:16,164 - INFO -   Batch 700/1264: Loss = 0.2364
2025-12-26 06:38:48,805 - INFO -   Batch 750/1264: Loss = 0.2882
2025-12-26 06:39:21,444 - INFO -   Batch 800/1264: Loss = 0.2895
2025-12-26 06:39:54,084 - INFO -   Batch 850/1264: Loss = 0.3479
2025-12-26 06:40:26,719 - INFO -   Batch 900/1264: Loss = 0.2787
2025-12-26 06:40:59,356 - INFO -   Batch 950/1264: Loss = 0.2811
2025-12-26 06:41:31,995 - INFO -   Batch 1000/1264: Loss = 0.2565
2025-12-26 06:42:04,633 - INFO -   Batch 1050/1264: Loss = 0.2468
2025-12-26 06:42:37,271 - INFO -   Batch 1100/1264: Loss = 0.2620
2025-12-26 06:43:09,906 - INFO -   Batch 1150/1264: Loss = 0.2955
2025-12-26 06:43:42,543 - INFO -   Batch 1200/1264: Loss = 0.3358
2025-12-26 06:44:15,185 - INFO -   Batch 1250/1264: Loss = 0.2859
2025-12-26 06:47:15,130 - INFO - Epoch 27/160: Train Loss: 0.2957, Val Loss: 0.8193, Val mIoU: 41.98%, Val PixelAcc: 79.33%
2025-12-26 06:47:49,511 - INFO -   Batch 50/1264: Loss = 0.2476
2025-12-26 06:48:22,149 - INFO -   Batch 100/1264: Loss = 0.2919
2025-12-26 06:48:54,790 - INFO -   Batch 150/1264: Loss = 0.3588
2025-12-26 06:49:27,426 - INFO -   Batch 200/1264: Loss = 0.2963
2025-12-26 06:50:00,063 - INFO -   Batch 250/1264: Loss = 0.2341
2025-12-26 06:50:32,698 - INFO -   Batch 300/1264: Loss = 0.2716
2025-12-26 06:51:05,335 - INFO -   Batch 350/1264: Loss = 0.3199
2025-12-26 06:51:37,972 - INFO -   Batch 400/1264: Loss = 0.2827
2025-12-26 06:52:10,608 - INFO -   Batch 450/1264: Loss = 0.3345
2025-12-26 06:52:43,245 - INFO -   Batch 500/1264: Loss = 0.2271
2025-12-26 06:53:15,886 - INFO -   Batch 550/1264: Loss = 0.2559
2025-12-26 06:53:48,525 - INFO -   Batch 600/1264: Loss = 0.3557
2025-12-26 06:54:21,160 - INFO -   Batch 650/1264: Loss = 0.6813
2025-12-26 06:54:53,798 - INFO -   Batch 700/1264: Loss = 0.2438
2025-12-26 06:55:26,435 - INFO -   Batch 750/1264: Loss = 0.4515
2025-12-26 06:55:59,065 - INFO -   Batch 800/1264: Loss = 0.2639
2025-12-26 06:56:31,700 - INFO -   Batch 850/1264: Loss = 0.2502
2025-12-26 06:57:04,333 - INFO -   Batch 900/1264: Loss = 0.4308
2025-12-26 06:57:36,964 - INFO -   Batch 950/1264: Loss = 0.3249
2025-12-26 06:58:09,598 - INFO -   Batch 1000/1264: Loss = 0.4182
2025-12-26 06:58:42,232 - INFO -   Batch 1050/1264: Loss = 0.2173
2025-12-26 06:59:14,866 - INFO -   Batch 1100/1264: Loss = 0.2711
2025-12-26 06:59:47,497 - INFO -   Batch 1150/1264: Loss = 0.3016
2025-12-26 07:00:20,129 - INFO -   Batch 1200/1264: Loss = 0.2774
2025-12-26 07:00:52,762 - INFO -   Batch 1250/1264: Loss = 0.2877
2025-12-26 07:03:52,624 - INFO - Epoch 28/160: Train Loss: 0.2916, Val Loss: 0.8104, Val mIoU: 42.67%, Val PixelAcc: 79.40%
2025-12-26 07:03:52,626 - INFO -   → New best mIoU: 42.67%
2025-12-26 07:03:54,213 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 07:03:54,215 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 07:04:28,286 - INFO -   Batch 50/1264: Loss = 0.3488
2025-12-26 07:05:00,920 - INFO -   Batch 100/1264: Loss = 0.2536
2025-12-26 07:05:33,554 - INFO -   Batch 150/1264: Loss = 0.3317
2025-12-26 07:06:06,187 - INFO -   Batch 200/1264: Loss = 0.2455
2025-12-26 07:06:38,817 - INFO -   Batch 250/1264: Loss = 0.2334
2025-12-26 07:07:11,448 - INFO -   Batch 300/1264: Loss = 0.2461
2025-12-26 07:07:44,080 - INFO -   Batch 350/1264: Loss = 0.3516
2025-12-26 07:08:16,711 - INFO -   Batch 400/1264: Loss = 0.3462
2025-12-26 07:08:49,341 - INFO -   Batch 450/1264: Loss = 0.2311
2025-12-26 07:09:21,972 - INFO -   Batch 500/1264: Loss = 0.2694
2025-12-26 07:09:54,607 - INFO -   Batch 550/1264: Loss = 0.2364
2025-12-26 07:10:27,241 - INFO -   Batch 600/1264: Loss = 0.2865
2025-12-26 07:10:59,876 - INFO -   Batch 650/1264: Loss = 0.3166
2025-12-26 07:11:32,512 - INFO -   Batch 700/1264: Loss = 0.3055
2025-12-26 07:12:05,151 - INFO -   Batch 750/1264: Loss = 0.2787
2025-12-26 07:12:37,790 - INFO -   Batch 800/1264: Loss = 0.2764
2025-12-26 07:13:10,427 - INFO -   Batch 850/1264: Loss = 0.2915
2025-12-26 07:13:43,064 - INFO -   Batch 900/1264: Loss = 0.2424
2025-12-26 07:14:15,702 - INFO -   Batch 950/1264: Loss = 0.3027
2025-12-26 07:14:48,335 - INFO -   Batch 1000/1264: Loss = 0.3027
2025-12-26 07:15:20,967 - INFO -   Batch 1050/1264: Loss = 0.2509
2025-12-26 07:15:53,600 - INFO -   Batch 1100/1264: Loss = 0.2551
2025-12-26 07:16:26,235 - INFO -   Batch 1150/1264: Loss = 0.1967
2025-12-26 07:16:58,865 - INFO -   Batch 1200/1264: Loss = 0.2142
2025-12-26 07:17:31,497 - INFO -   Batch 1250/1264: Loss = 0.2787
2025-12-26 07:20:30,888 - INFO - Epoch 29/160: Train Loss: 0.2844, Val Loss: 0.8168, Val mIoU: 42.85%, Val PixelAcc: 79.48%
2025-12-26 07:20:30,890 - INFO -   → New best mIoU: 42.85%
2025-12-26 07:20:32,458 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 07:20:32,459 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 07:21:06,742 - INFO -   Batch 50/1264: Loss = 0.2563
2025-12-26 07:21:39,378 - INFO -   Batch 100/1264: Loss = 0.2472
2025-12-26 07:22:12,008 - INFO -   Batch 150/1264: Loss = 0.1862
2025-12-26 07:22:44,639 - INFO -   Batch 200/1264: Loss = 0.2891
2025-12-26 07:23:17,272 - INFO -   Batch 250/1264: Loss = 0.2009
2025-12-26 07:23:49,906 - INFO -   Batch 300/1264: Loss = 0.2760
2025-12-26 07:24:22,544 - INFO -   Batch 350/1264: Loss = 0.2693
2025-12-26 07:24:55,181 - INFO -   Batch 400/1264: Loss = 0.2210
2025-12-26 07:25:27,817 - INFO -   Batch 450/1264: Loss = 0.3151
2025-12-26 07:26:00,453 - INFO -   Batch 500/1264: Loss = 0.2679
2025-12-26 07:26:33,089 - INFO -   Batch 550/1264: Loss = 0.2241
2025-12-26 07:27:05,725 - INFO -   Batch 600/1264: Loss = 0.2786
2025-12-26 07:27:38,365 - INFO -   Batch 650/1264: Loss = 0.2441
2025-12-26 07:28:11,007 - INFO -   Batch 700/1264: Loss = 0.2398
2025-12-26 07:28:43,645 - INFO -   Batch 750/1264: Loss = 0.2917
2025-12-26 07:29:16,281 - INFO -   Batch 800/1264: Loss = 0.3358
2025-12-26 07:29:48,919 - INFO -   Batch 850/1264: Loss = 0.2545
2025-12-26 07:30:21,558 - INFO -   Batch 900/1264: Loss = 0.2991
2025-12-26 07:30:54,202 - INFO -   Batch 950/1264: Loss = 0.2695
2025-12-26 07:31:26,841 - INFO -   Batch 1000/1264: Loss = 0.2561
2025-12-26 07:31:59,478 - INFO -   Batch 1050/1264: Loss = 0.3101
2025-12-26 07:32:32,111 - INFO -   Batch 1100/1264: Loss = 0.2233
2025-12-26 07:33:04,744 - INFO -   Batch 1150/1264: Loss = 0.2409
2025-12-26 07:33:37,375 - INFO -   Batch 1200/1264: Loss = 0.2635
2025-12-26 07:34:10,007 - INFO -   Batch 1250/1264: Loss = 0.3316
2025-12-26 07:37:09,505 - INFO - Epoch 30/160: Train Loss: 0.2764, Val Loss: 0.8367, Val mIoU: 42.19%, Val PixelAcc: 79.54%
2025-12-26 07:37:11,013 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_30.pth
2025-12-26 07:37:11,014 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_30.pth
2025-12-26 07:37:44,920 - INFO -   Batch 50/1264: Loss = 0.1948
2025-12-26 07:38:17,567 - INFO -   Batch 100/1264: Loss = 0.2341
2025-12-26 07:38:50,209 - INFO -   Batch 150/1264: Loss = 0.3400
2025-12-26 07:39:22,853 - INFO -   Batch 200/1264: Loss = 0.2926
2025-12-26 07:39:55,494 - INFO -   Batch 250/1264: Loss = 0.2754
2025-12-26 07:40:28,133 - INFO -   Batch 300/1264: Loss = 0.3440
2025-12-26 07:41:00,773 - INFO -   Batch 350/1264: Loss = 0.3693
2025-12-26 07:41:33,414 - INFO -   Batch 400/1264: Loss = 0.2285
2025-12-26 07:42:06,058 - INFO -   Batch 450/1264: Loss = 0.2133
2025-12-26 07:42:38,703 - INFO -   Batch 500/1264: Loss = 0.2732
2025-12-26 07:43:11,349 - INFO -   Batch 550/1264: Loss = 0.2957
2025-12-26 07:43:43,994 - INFO -   Batch 600/1264: Loss = 0.2006
2025-12-26 07:44:16,639 - INFO -   Batch 650/1264: Loss = 0.2907
2025-12-26 07:44:49,286 - INFO -   Batch 700/1264: Loss = 0.2337
2025-12-26 07:45:21,930 - INFO -   Batch 750/1264: Loss = 0.3000
2025-12-26 07:45:54,575 - INFO -   Batch 800/1264: Loss = 0.2457
2025-12-26 07:46:27,225 - INFO -   Batch 850/1264: Loss = 0.2761
2025-12-26 07:46:59,874 - INFO -   Batch 900/1264: Loss = 0.3092
2025-12-26 07:47:32,518 - INFO -   Batch 950/1264: Loss = 0.3410
2025-12-26 07:48:05,165 - INFO -   Batch 1000/1264: Loss = 0.3090
2025-12-26 07:48:37,809 - INFO -   Batch 1050/1264: Loss = 0.2758
2025-12-26 07:49:10,450 - INFO -   Batch 1100/1264: Loss = 0.2469
2025-12-26 07:49:43,091 - INFO -   Batch 1150/1264: Loss = 0.3486
2025-12-26 07:50:15,730 - INFO -   Batch 1200/1264: Loss = 0.3149
2025-12-26 07:50:48,370 - INFO -   Batch 1250/1264: Loss = 0.1953
2025-12-26 07:53:47,993 - INFO - Epoch 31/160: Train Loss: 0.2724, Val Loss: 0.8264, Val mIoU: 42.79%, Val PixelAcc: 79.53%
2025-12-26 07:54:22,931 - INFO -   Batch 50/1264: Loss = 0.2668
2025-12-26 07:54:55,570 - INFO -   Batch 100/1264: Loss = 0.2290
2025-12-26 07:55:28,213 - INFO -   Batch 150/1264: Loss = 0.2406
2025-12-26 07:56:00,854 - INFO -   Batch 200/1264: Loss = 0.2851
2025-12-26 07:56:33,500 - INFO -   Batch 250/1264: Loss = 0.3728
2025-12-26 07:57:06,140 - INFO -   Batch 300/1264: Loss = 0.2660
2025-12-26 07:57:38,786 - INFO -   Batch 350/1264: Loss = 0.2716
2025-12-26 07:58:11,432 - INFO -   Batch 400/1264: Loss = 0.2863
2025-12-26 07:58:44,079 - INFO -   Batch 450/1264: Loss = 0.2625
2025-12-26 07:59:16,730 - INFO -   Batch 500/1264: Loss = 0.2696
2025-12-26 07:59:49,378 - INFO -   Batch 550/1264: Loss = 0.2578
2025-12-26 08:00:22,026 - INFO -   Batch 600/1264: Loss = 0.2404
2025-12-26 08:00:54,674 - INFO -   Batch 650/1264: Loss = 0.2870
2025-12-26 08:01:27,324 - INFO -   Batch 700/1264: Loss = 0.2747
2025-12-26 08:01:59,977 - INFO -   Batch 750/1264: Loss = 0.2438
2025-12-26 08:02:32,624 - INFO -   Batch 800/1264: Loss = 0.2243
2025-12-26 08:03:05,267 - INFO -   Batch 850/1264: Loss = 0.3161
2025-12-26 08:03:37,903 - INFO -   Batch 900/1264: Loss = 0.2277
2025-12-26 08:04:10,541 - INFO -   Batch 950/1264: Loss = 0.3044
2025-12-26 08:04:43,179 - INFO -   Batch 1000/1264: Loss = 0.3126
2025-12-26 08:05:15,817 - INFO -   Batch 1050/1264: Loss = 0.2215
2025-12-26 08:05:48,455 - INFO -   Batch 1100/1264: Loss = 0.3163
2025-12-26 08:06:21,092 - INFO -   Batch 1150/1264: Loss = 0.2381
2025-12-26 08:06:53,730 - INFO -   Batch 1200/1264: Loss = 0.3263
2025-12-26 08:07:26,369 - INFO -   Batch 1250/1264: Loss = 0.3163
2025-12-26 08:10:25,812 - INFO - Epoch 32/160: Train Loss: 0.2700, Val Loss: 0.8626, Val mIoU: 41.53%, Val PixelAcc: 78.97%
2025-12-26 08:11:00,453 - INFO -   Batch 50/1264: Loss = 0.3339
2025-12-26 08:11:33,096 - INFO -   Batch 100/1264: Loss = 0.2350
2025-12-26 08:12:05,738 - INFO -   Batch 150/1264: Loss = 0.2425
2025-12-26 08:12:38,384 - INFO -   Batch 200/1264: Loss = 0.2120
2025-12-26 08:13:11,030 - INFO -   Batch 250/1264: Loss = 0.2723
2025-12-26 08:13:43,676 - INFO -   Batch 300/1264: Loss = 0.2136
2025-12-26 08:14:16,316 - INFO -   Batch 350/1264: Loss = 0.3288
2025-12-26 08:14:48,958 - INFO -   Batch 400/1264: Loss = 0.2571
2025-12-26 08:15:21,598 - INFO -   Batch 450/1264: Loss = 0.2181
2025-12-26 08:15:54,238 - INFO -   Batch 500/1264: Loss = 0.2903
2025-12-26 08:16:26,881 - INFO -   Batch 550/1264: Loss = 0.2483
2025-12-26 08:16:59,523 - INFO -   Batch 600/1264: Loss = 0.2399
2025-12-26 08:17:32,164 - INFO -   Batch 650/1264: Loss = 0.2801
2025-12-26 08:18:04,805 - INFO -   Batch 700/1264: Loss = 0.2494
2025-12-26 08:18:37,446 - INFO -   Batch 750/1264: Loss = 0.2518
2025-12-26 08:19:10,087 - INFO -   Batch 800/1264: Loss = 0.2837
2025-12-26 08:19:42,728 - INFO -   Batch 850/1264: Loss = 0.2183
2025-12-26 08:20:15,370 - INFO -   Batch 900/1264: Loss = 0.1898
2025-12-26 08:20:48,008 - INFO -   Batch 950/1264: Loss = 0.2906
2025-12-26 08:21:20,645 - INFO -   Batch 1000/1264: Loss = 0.3040
2025-12-26 08:21:53,285 - INFO -   Batch 1050/1264: Loss = 0.3171
2025-12-26 08:22:25,926 - INFO -   Batch 1100/1264: Loss = 0.2910
2025-12-26 08:22:58,568 - INFO -   Batch 1150/1264: Loss = 0.3539
2025-12-26 08:23:31,213 - INFO -   Batch 1200/1264: Loss = 0.2656
2025-12-26 08:24:03,854 - INFO -   Batch 1250/1264: Loss = 0.3061
2025-12-26 08:27:03,144 - INFO - Epoch 33/160: Train Loss: 0.2598, Val Loss: 0.8650, Val mIoU: 42.03%, Val PixelAcc: 79.31%
2025-12-26 08:27:37,280 - INFO -   Batch 50/1264: Loss = 0.2454
2025-12-26 08:28:09,924 - INFO -   Batch 100/1264: Loss = 0.2392
2025-12-26 08:28:42,568 - INFO -   Batch 150/1264: Loss = 0.2319
2025-12-26 08:29:15,214 - INFO -   Batch 200/1264: Loss = 0.2380
2025-12-26 08:29:47,857 - INFO -   Batch 250/1264: Loss = 0.2965
2025-12-26 08:30:20,503 - INFO -   Batch 300/1264: Loss = 0.3163
2025-12-26 08:30:53,144 - INFO -   Batch 350/1264: Loss = 0.2062
2025-12-26 08:31:25,786 - INFO -   Batch 400/1264: Loss = 0.2213
2025-12-26 08:31:58,428 - INFO -   Batch 450/1264: Loss = 0.2061
2025-12-26 08:32:31,068 - INFO -   Batch 500/1264: Loss = 0.2450
2025-12-26 08:33:03,711 - INFO -   Batch 550/1264: Loss = 0.1845
2025-12-26 08:33:36,349 - INFO -   Batch 600/1264: Loss = 0.1812
2025-12-26 08:34:08,994 - INFO -   Batch 650/1264: Loss = 0.2445
2025-12-26 08:34:41,638 - INFO -   Batch 700/1264: Loss = 0.2556
2025-12-26 08:35:14,280 - INFO -   Batch 750/1264: Loss = 0.3322
2025-12-26 08:35:46,923 - INFO -   Batch 800/1264: Loss = 0.2018
2025-12-26 08:36:19,566 - INFO -   Batch 850/1264: Loss = 0.2177
2025-12-26 08:36:52,209 - INFO -   Batch 900/1264: Loss = 0.2343
2025-12-26 08:37:24,852 - INFO -   Batch 950/1264: Loss = 0.2545
2025-12-26 08:37:57,494 - INFO -   Batch 1000/1264: Loss = 0.2341
2025-12-26 08:38:30,139 - INFO -   Batch 1050/1264: Loss = 0.2505
2025-12-26 08:39:02,785 - INFO -   Batch 1100/1264: Loss = 0.2380
2025-12-26 08:39:35,428 - INFO -   Batch 1150/1264: Loss = 0.3246
2025-12-26 08:40:08,066 - INFO -   Batch 1200/1264: Loss = 0.2037
2025-12-26 08:40:40,708 - INFO -   Batch 1250/1264: Loss = 0.2272
2025-12-26 08:43:40,412 - INFO - Epoch 34/160: Train Loss: 0.2557, Val Loss: 0.8487, Val mIoU: 42.51%, Val PixelAcc: 79.59%
2025-12-26 08:44:14,376 - INFO -   Batch 50/1264: Loss = 0.1942
2025-12-26 08:44:47,015 - INFO -   Batch 100/1264: Loss = 0.2147
2025-12-26 08:45:19,653 - INFO -   Batch 150/1264: Loss = 0.2240
2025-12-26 08:45:52,293 - INFO -   Batch 200/1264: Loss = 0.2319
2025-12-26 08:46:24,932 - INFO -   Batch 250/1264: Loss = 0.2937
2025-12-26 08:46:57,578 - INFO -   Batch 300/1264: Loss = 0.2457
2025-12-26 08:47:30,222 - INFO -   Batch 350/1264: Loss = 0.3390
2025-12-26 08:48:02,869 - INFO -   Batch 400/1264: Loss = 0.2899
2025-12-26 08:48:35,514 - INFO -   Batch 450/1264: Loss = 0.2190
2025-12-26 08:49:08,160 - INFO -   Batch 500/1264: Loss = 0.2195
2025-12-26 08:49:40,804 - INFO -   Batch 550/1264: Loss = 0.2160
2025-12-26 08:50:13,448 - INFO -   Batch 600/1264: Loss = 0.2069
2025-12-26 08:50:46,088 - INFO -   Batch 650/1264: Loss = 0.2506
2025-12-26 08:51:18,726 - INFO -   Batch 700/1264: Loss = 0.1995
2025-12-26 08:51:51,368 - INFO -   Batch 750/1264: Loss = 0.2527
2025-12-26 08:52:24,009 - INFO -   Batch 800/1264: Loss = 0.2207
2025-12-26 08:52:56,651 - INFO -   Batch 850/1264: Loss = 0.1939
2025-12-26 08:53:29,287 - INFO -   Batch 900/1264: Loss = 0.2286
2025-12-26 08:54:01,928 - INFO -   Batch 950/1264: Loss = 0.2791
2025-12-26 08:54:34,571 - INFO -   Batch 1000/1264: Loss = 0.2726
2025-12-26 08:55:07,217 - INFO -   Batch 1050/1264: Loss = 0.2381
2025-12-26 08:55:39,865 - INFO -   Batch 1100/1264: Loss = 0.2037
2025-12-26 08:56:12,511 - INFO -   Batch 1150/1264: Loss = 0.2160
2025-12-26 08:56:45,155 - INFO -   Batch 1200/1264: Loss = 0.2088
2025-12-26 08:57:17,800 - INFO -   Batch 1250/1264: Loss = 0.1753
2025-12-26 09:00:16,911 - INFO - Epoch 35/160: Train Loss: 0.2490, Val Loss: 0.8535, Val mIoU: 42.76%, Val PixelAcc: 79.76%
2025-12-26 09:00:50,977 - INFO -   Batch 50/1264: Loss = 0.2835
2025-12-26 09:01:23,622 - INFO -   Batch 100/1264: Loss = 0.2692
2025-12-26 09:01:56,268 - INFO -   Batch 150/1264: Loss = 0.2480
2025-12-26 09:02:28,910 - INFO -   Batch 200/1264: Loss = 0.2911
2025-12-26 09:03:01,553 - INFO -   Batch 250/1264: Loss = 0.2851
2025-12-26 09:03:34,192 - INFO -   Batch 300/1264: Loss = 0.2197
2025-12-26 09:04:06,831 - INFO -   Batch 350/1264: Loss = 0.2904
2025-12-26 09:04:39,475 - INFO -   Batch 400/1264: Loss = 0.3051
2025-12-26 09:05:12,121 - INFO -   Batch 450/1264: Loss = 0.2036
2025-12-26 09:05:44,767 - INFO -   Batch 500/1264: Loss = 0.3134
2025-12-26 09:06:17,412 - INFO -   Batch 550/1264: Loss = 0.2502
2025-12-26 09:06:50,058 - INFO -   Batch 600/1264: Loss = 0.2861
2025-12-26 09:07:22,706 - INFO -   Batch 650/1264: Loss = 0.3185
2025-12-26 09:07:55,352 - INFO -   Batch 700/1264: Loss = 0.3016
2025-12-26 09:08:27,998 - INFO -   Batch 750/1264: Loss = 0.2535
2025-12-26 09:09:00,642 - INFO -   Batch 800/1264: Loss = 0.2467
2025-12-26 09:09:33,284 - INFO -   Batch 850/1264: Loss = 0.2995
2025-12-26 09:10:05,930 - INFO -   Batch 900/1264: Loss = 0.2907
2025-12-26 09:10:38,570 - INFO -   Batch 950/1264: Loss = 0.3135
2025-12-26 09:11:11,212 - INFO -   Batch 1000/1264: Loss = 0.2317
2025-12-26 09:11:43,854 - INFO -   Batch 1050/1264: Loss = 0.2127
2025-12-26 09:12:16,500 - INFO -   Batch 1100/1264: Loss = 0.2165
2025-12-26 09:12:49,143 - INFO -   Batch 1150/1264: Loss = 0.2555
2025-12-26 09:13:21,785 - INFO -   Batch 1200/1264: Loss = 0.3365
2025-12-26 09:13:54,430 - INFO -   Batch 1250/1264: Loss = 0.2342
2025-12-26 09:16:54,205 - INFO - Epoch 36/160: Train Loss: 0.2488, Val Loss: 0.8607, Val mIoU: 42.49%, Val PixelAcc: 79.36%
2025-12-26 09:17:28,755 - INFO -   Batch 50/1264: Loss = 0.2461
2025-12-26 09:18:01,396 - INFO -   Batch 100/1264: Loss = 0.2352
2025-12-26 09:18:34,038 - INFO -   Batch 150/1264: Loss = 0.2480
2025-12-26 09:19:06,679 - INFO -   Batch 200/1264: Loss = 0.2336
2025-12-26 09:19:39,322 - INFO -   Batch 250/1264: Loss = 0.2002
2025-12-26 09:20:11,962 - INFO -   Batch 300/1264: Loss = 0.2075
2025-12-26 09:20:44,600 - INFO -   Batch 350/1264: Loss = 0.2675
2025-12-26 09:21:17,243 - INFO -   Batch 400/1264: Loss = 0.2590
2025-12-26 09:21:49,887 - INFO -   Batch 450/1264: Loss = 0.2482
2025-12-26 09:22:22,527 - INFO -   Batch 500/1264: Loss = 0.2972
2025-12-26 09:22:55,167 - INFO -   Batch 550/1264: Loss = 0.2884
2025-12-26 09:23:27,809 - INFO -   Batch 600/1264: Loss = 0.2323
2025-12-26 09:24:00,452 - INFO -   Batch 650/1264: Loss = 0.2280
2025-12-26 09:24:33,092 - INFO -   Batch 700/1264: Loss = 0.2469
2025-12-26 09:25:05,728 - INFO -   Batch 750/1264: Loss = 0.2415
2025-12-26 09:25:38,366 - INFO -   Batch 800/1264: Loss = 0.1826
2025-12-26 09:26:11,004 - INFO -   Batch 850/1264: Loss = 0.2719
2025-12-26 09:26:43,646 - INFO -   Batch 900/1264: Loss = 0.2544
2025-12-26 09:27:16,292 - INFO -   Batch 950/1264: Loss = 0.3134
2025-12-26 09:27:48,936 - INFO -   Batch 1000/1264: Loss = 0.2867
2025-12-26 09:28:21,579 - INFO -   Batch 1050/1264: Loss = 0.1988
2025-12-26 09:28:54,217 - INFO -   Batch 1100/1264: Loss = 0.2153
2025-12-26 09:29:26,855 - INFO -   Batch 1150/1264: Loss = 0.3095
2025-12-26 09:29:59,494 - INFO -   Batch 1200/1264: Loss = 0.2974
2025-12-26 09:30:32,134 - INFO -   Batch 1250/1264: Loss = 0.2821
2025-12-26 09:33:31,320 - INFO - Epoch 37/160: Train Loss: 0.2408, Val Loss: 0.8864, Val mIoU: 42.52%, Val PixelAcc: 79.56%
2025-12-26 09:34:05,384 - INFO -   Batch 50/1264: Loss = 0.2926
2025-12-26 09:34:38,024 - INFO -   Batch 100/1264: Loss = 0.2355
2025-12-26 09:35:10,667 - INFO -   Batch 150/1264: Loss = 0.3394
2025-12-26 09:35:43,310 - INFO -   Batch 200/1264: Loss = 0.2619
2025-12-26 09:36:15,955 - INFO -   Batch 250/1264: Loss = 0.2420
2025-12-26 09:36:48,598 - INFO -   Batch 300/1264: Loss = 0.2460
2025-12-26 09:37:21,240 - INFO -   Batch 350/1264: Loss = 0.2115
2025-12-26 09:37:53,881 - INFO -   Batch 400/1264: Loss = 0.2790
2025-12-26 09:38:26,522 - INFO -   Batch 450/1264: Loss = 0.2536
2025-12-26 09:38:59,164 - INFO -   Batch 500/1264: Loss = 0.2656
2025-12-26 09:39:31,808 - INFO -   Batch 550/1264: Loss = 0.2724
2025-12-26 09:40:04,448 - INFO -   Batch 600/1264: Loss = 0.2082
2025-12-26 09:40:37,091 - INFO -   Batch 650/1264: Loss = 0.2097
2025-12-26 09:41:09,740 - INFO -   Batch 700/1264: Loss = 0.2094
2025-12-26 09:41:42,383 - INFO -   Batch 750/1264: Loss = 0.2815
2025-12-26 09:42:15,031 - INFO -   Batch 800/1264: Loss = 0.2010
2025-12-26 09:42:47,675 - INFO -   Batch 850/1264: Loss = 0.2584
2025-12-26 09:43:20,319 - INFO -   Batch 900/1264: Loss = 0.2338
2025-12-26 09:43:52,967 - INFO -   Batch 950/1264: Loss = 0.2673
2025-12-26 09:44:25,613 - INFO -   Batch 1000/1264: Loss = 0.2480
2025-12-26 09:44:58,259 - INFO -   Batch 1050/1264: Loss = 0.2840
2025-12-26 09:45:30,908 - INFO -   Batch 1100/1264: Loss = 0.2173
2025-12-26 09:46:03,557 - INFO -   Batch 1150/1264: Loss = 0.2235
2025-12-26 09:46:36,203 - INFO -   Batch 1200/1264: Loss = 0.2575
2025-12-26 09:47:08,847 - INFO -   Batch 1250/1264: Loss = 0.2495
2025-12-26 09:50:08,342 - INFO - Epoch 38/160: Train Loss: 0.2404, Val Loss: 0.8633, Val mIoU: 43.07%, Val PixelAcc: 79.72%
2025-12-26 09:50:08,344 - INFO -   → New best mIoU: 43.07%
2025-12-26 09:50:10,237 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 09:50:10,238 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 09:50:44,249 - INFO -   Batch 50/1264: Loss = 0.2138
2025-12-26 09:51:16,894 - INFO -   Batch 100/1264: Loss = 0.2224
2025-12-26 09:51:49,536 - INFO -   Batch 150/1264: Loss = 0.1746
2025-12-26 09:52:22,179 - INFO -   Batch 200/1264: Loss = 0.2130
2025-12-26 09:52:54,823 - INFO -   Batch 250/1264: Loss = 0.2084
2025-12-26 09:53:27,470 - INFO -   Batch 300/1264: Loss = 0.1517
2025-12-26 09:54:00,118 - INFO -   Batch 350/1264: Loss = 0.1747
2025-12-26 09:54:32,765 - INFO -   Batch 400/1264: Loss = 0.1942
2025-12-26 09:55:05,415 - INFO -   Batch 450/1264: Loss = 0.2367
2025-12-26 09:55:38,065 - INFO -   Batch 500/1264: Loss = 0.1813
2025-12-26 09:56:10,715 - INFO -   Batch 550/1264: Loss = 0.2440
2025-12-26 09:56:43,360 - INFO -   Batch 600/1264: Loss = 0.2390
2025-12-26 09:57:16,007 - INFO -   Batch 650/1264: Loss = 0.2242
2025-12-26 09:57:48,653 - INFO -   Batch 700/1264: Loss = 0.2391
2025-12-26 09:58:21,297 - INFO -   Batch 750/1264: Loss = 0.2669
2025-12-26 09:58:53,940 - INFO -   Batch 800/1264: Loss = 0.1703
2025-12-26 09:59:26,586 - INFO -   Batch 850/1264: Loss = 0.1901
2025-12-26 09:59:59,230 - INFO -   Batch 900/1264: Loss = 0.2541
2025-12-26 10:00:31,870 - INFO -   Batch 950/1264: Loss = 0.1713
2025-12-26 10:01:04,508 - INFO -   Batch 1000/1264: Loss = 0.2174
2025-12-26 10:01:37,146 - INFO -   Batch 1050/1264: Loss = 0.2826
2025-12-26 10:02:09,782 - INFO -   Batch 1100/1264: Loss = 0.3610
2025-12-26 10:02:42,420 - INFO -   Batch 1150/1264: Loss = 0.2963
2025-12-26 10:03:15,061 - INFO -   Batch 1200/1264: Loss = 0.1840
2025-12-26 10:03:47,704 - INFO -   Batch 1250/1264: Loss = 0.1782
2025-12-26 10:06:46,972 - INFO - Epoch 39/160: Train Loss: 0.2318, Val Loss: 0.8850, Val mIoU: 42.50%, Val PixelAcc: 79.37%
2025-12-26 10:07:21,017 - INFO -   Batch 50/1264: Loss = 0.3308
2025-12-26 10:07:53,659 - INFO -   Batch 100/1264: Loss = 0.2855
2025-12-26 10:08:26,299 - INFO -   Batch 150/1264: Loss = 0.2128
2025-12-26 10:08:58,941 - INFO -   Batch 200/1264: Loss = 0.2553
2025-12-26 10:09:31,576 - INFO -   Batch 250/1264: Loss = 0.1778
2025-12-26 10:10:04,212 - INFO -   Batch 300/1264: Loss = 0.2520
2025-12-26 10:10:36,850 - INFO -   Batch 350/1264: Loss = 0.2341
2025-12-26 10:11:09,488 - INFO -   Batch 400/1264: Loss = 0.2025
2025-12-26 10:11:42,130 - INFO -   Batch 450/1264: Loss = 0.2608
2025-12-26 10:12:14,773 - INFO -   Batch 500/1264: Loss = 0.1873
2025-12-26 10:12:47,415 - INFO -   Batch 550/1264: Loss = 0.2089
2025-12-26 10:13:20,055 - INFO -   Batch 600/1264: Loss = 0.1880
2025-12-26 10:13:52,695 - INFO -   Batch 650/1264: Loss = 0.2804
2025-12-26 10:14:25,337 - INFO -   Batch 700/1264: Loss = 0.2397
2025-12-26 10:14:57,978 - INFO -   Batch 750/1264: Loss = 0.1924
2025-12-26 10:15:30,617 - INFO -   Batch 800/1264: Loss = 0.2818
2025-12-26 10:16:03,255 - INFO -   Batch 850/1264: Loss = 0.2204
2025-12-26 10:16:35,892 - INFO -   Batch 900/1264: Loss = 0.1747
2025-12-26 10:17:08,532 - INFO -   Batch 950/1264: Loss = 0.1818
2025-12-26 10:17:41,173 - INFO -   Batch 1000/1264: Loss = 0.1757
2025-12-26 10:18:13,815 - INFO -   Batch 1050/1264: Loss = 0.2225
2025-12-26 10:18:46,455 - INFO -   Batch 1100/1264: Loss = 0.2142
2025-12-26 10:19:19,092 - INFO -   Batch 1150/1264: Loss = 0.2496
2025-12-26 10:19:51,727 - INFO -   Batch 1200/1264: Loss = 0.2481
2025-12-26 10:20:24,367 - INFO -   Batch 1250/1264: Loss = 0.2501
2025-12-26 10:23:23,664 - INFO - Epoch 40/160: Train Loss: 0.2310, Val Loss: 0.8781, Val mIoU: 42.79%, Val PixelAcc: 79.65%
2025-12-26 10:23:25,159 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_40.pth
2025-12-26 10:23:25,160 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_40.pth
2025-12-26 10:23:59,660 - INFO -   Batch 50/1264: Loss = 0.3642
2025-12-26 10:24:32,302 - INFO -   Batch 100/1264: Loss = 0.2172
2025-12-26 10:25:04,943 - INFO -   Batch 150/1264: Loss = 0.2319
2025-12-26 10:25:37,585 - INFO -   Batch 200/1264: Loss = 0.1691
2025-12-26 10:26:10,227 - INFO -   Batch 250/1264: Loss = 0.2691
2025-12-26 10:26:42,868 - INFO -   Batch 300/1264: Loss = 0.2638
2025-12-26 10:27:15,511 - INFO -   Batch 350/1264: Loss = 0.1962
2025-12-26 10:27:48,152 - INFO -   Batch 400/1264: Loss = 0.2238
2025-12-26 10:28:20,793 - INFO -   Batch 450/1264: Loss = 0.2585
2025-12-26 10:28:53,432 - INFO -   Batch 500/1264: Loss = 0.2075
2025-12-26 10:29:26,068 - INFO -   Batch 550/1264: Loss = 0.2137
2025-12-26 10:29:58,707 - INFO -   Batch 600/1264: Loss = 0.1955
2025-12-26 10:30:31,348 - INFO -   Batch 650/1264: Loss = 0.2363
2025-12-26 10:31:03,987 - INFO -   Batch 700/1264: Loss = 0.2135
2025-12-26 10:31:36,623 - INFO -   Batch 750/1264: Loss = 0.2172
2025-12-26 10:32:09,265 - INFO -   Batch 800/1264: Loss = 0.2365
2025-12-26 10:32:41,905 - INFO -   Batch 850/1264: Loss = 0.1672
2025-12-26 10:33:14,547 - INFO -   Batch 900/1264: Loss = 0.3456
2025-12-26 10:33:47,188 - INFO -   Batch 950/1264: Loss = 0.1654
2025-12-26 10:34:19,830 - INFO -   Batch 1000/1264: Loss = 0.3082
2025-12-26 10:34:52,473 - INFO -   Batch 1050/1264: Loss = 0.2142
2025-12-26 10:35:25,109 - INFO -   Batch 1100/1264: Loss = 0.2627
2025-12-26 10:35:57,747 - INFO -   Batch 1150/1264: Loss = 0.2219
2025-12-26 10:36:30,387 - INFO -   Batch 1200/1264: Loss = 0.2237
2025-12-26 10:37:03,028 - INFO -   Batch 1250/1264: Loss = 0.2183
2025-12-26 10:40:01,792 - INFO - Epoch 41/160: Train Loss: 0.2276, Val Loss: 0.8955, Val mIoU: 41.97%, Val PixelAcc: 79.15%
2025-12-26 10:40:36,312 - INFO -   Batch 50/1264: Loss = 0.1483
2025-12-26 10:41:08,962 - INFO -   Batch 100/1264: Loss = 0.2703
2025-12-26 10:41:41,604 - INFO -   Batch 150/1264: Loss = 0.1781
2025-12-26 10:42:14,245 - INFO -   Batch 200/1264: Loss = 0.1725
2025-12-26 10:42:46,888 - INFO -   Batch 250/1264: Loss = 0.1483
2025-12-26 10:43:19,530 - INFO -   Batch 300/1264: Loss = 0.2034
2025-12-26 10:43:52,171 - INFO -   Batch 350/1264: Loss = 0.2038
2025-12-26 10:44:24,809 - INFO -   Batch 400/1264: Loss = 0.1894
2025-12-26 10:44:57,447 - INFO -   Batch 450/1264: Loss = 0.2242
2025-12-26 10:45:30,085 - INFO -   Batch 500/1264: Loss = 0.2867
2025-12-26 10:46:02,720 - INFO -   Batch 550/1264: Loss = 0.2180
2025-12-26 10:46:35,355 - INFO -   Batch 600/1264: Loss = 0.2057
2025-12-26 10:47:07,992 - INFO -   Batch 650/1264: Loss = 0.1855
2025-12-26 10:47:40,628 - INFO -   Batch 700/1264: Loss = 0.1741
2025-12-26 10:48:13,268 - INFO -   Batch 750/1264: Loss = 0.1767
2025-12-26 10:48:45,906 - INFO -   Batch 800/1264: Loss = 0.2038
2025-12-26 10:49:18,542 - INFO -   Batch 850/1264: Loss = 0.1940
2025-12-26 10:49:51,180 - INFO -   Batch 900/1264: Loss = 0.2448
2025-12-26 10:50:23,813 - INFO -   Batch 950/1264: Loss = 0.2346
2025-12-26 10:50:56,447 - INFO -   Batch 1000/1264: Loss = 0.2507
2025-12-26 10:51:29,083 - INFO -   Batch 1050/1264: Loss = 0.2503
2025-12-26 10:52:01,723 - INFO -   Batch 1100/1264: Loss = 0.2728
2025-12-26 10:52:34,363 - INFO -   Batch 1150/1264: Loss = 0.2094
2025-12-26 10:53:07,002 - INFO -   Batch 1200/1264: Loss = 0.2965
2025-12-26 10:53:39,639 - INFO -   Batch 1250/1264: Loss = 0.1812
2025-12-26 10:56:38,490 - INFO - Epoch 42/160: Train Loss: 0.2197, Val Loss: 0.8806, Val mIoU: 42.78%, Val PixelAcc: 79.72%
2025-12-26 10:57:13,429 - INFO -   Batch 50/1264: Loss = 0.1900
2025-12-26 10:57:46,070 - INFO -   Batch 100/1264: Loss = 0.1623
2025-12-26 10:58:18,715 - INFO -   Batch 150/1264: Loss = 0.2278
2025-12-26 10:58:51,358 - INFO -   Batch 200/1264: Loss = 0.1937
2025-12-26 10:59:24,000 - INFO -   Batch 250/1264: Loss = 0.2069
2025-12-26 10:59:56,639 - INFO -   Batch 300/1264: Loss = 0.2444
2025-12-26 11:00:29,275 - INFO -   Batch 350/1264: Loss = 0.2107
2025-12-26 11:01:01,913 - INFO -   Batch 400/1264: Loss = 0.2369
2025-12-26 11:01:34,549 - INFO -   Batch 450/1264: Loss = 0.2344
2025-12-26 11:02:07,189 - INFO -   Batch 500/1264: Loss = 0.1813
2025-12-26 11:02:39,833 - INFO -   Batch 550/1264: Loss = 0.2037
2025-12-26 11:03:12,474 - INFO -   Batch 600/1264: Loss = 0.2086
2025-12-26 11:03:45,117 - INFO -   Batch 650/1264: Loss = 0.1938
2025-12-26 11:04:17,828 - INFO -   Batch 700/1264: Loss = 0.2574
2025-12-26 11:04:50,472 - INFO -   Batch 750/1264: Loss = 0.2297
2025-12-26 11:05:23,151 - INFO -   Batch 800/1264: Loss = 0.1813
2025-12-26 11:05:55,797 - INFO -   Batch 850/1264: Loss = 0.2440
2025-12-26 11:06:28,447 - INFO -   Batch 900/1264: Loss = 0.1723
2025-12-26 11:07:01,096 - INFO -   Batch 950/1264: Loss = 0.3079
2025-12-26 11:07:33,739 - INFO -   Batch 1000/1264: Loss = 0.2286
2025-12-26 11:08:06,383 - INFO -   Batch 1050/1264: Loss = 0.2028
2025-12-26 11:08:39,030 - INFO -   Batch 1100/1264: Loss = 0.1446
2025-12-26 11:09:11,680 - INFO -   Batch 1150/1264: Loss = 0.2417
2025-12-26 11:09:44,349 - INFO -   Batch 1200/1264: Loss = 0.2084
2025-12-26 11:10:16,998 - INFO -   Batch 1250/1264: Loss = 0.3100
2025-12-26 11:13:16,171 - INFO - Epoch 43/160: Train Loss: 0.2227, Val Loss: 0.8795, Val mIoU: 42.46%, Val PixelAcc: 79.43%
2025-12-26 11:13:50,586 - INFO -   Batch 50/1264: Loss = 0.2748
2025-12-26 11:14:23,233 - INFO -   Batch 100/1264: Loss = 0.2197
2025-12-26 11:14:55,877 - INFO -   Batch 150/1264: Loss = 0.1839
2025-12-26 11:15:28,521 - INFO -   Batch 200/1264: Loss = 0.2001
2025-12-26 11:16:01,163 - INFO -   Batch 250/1264: Loss = 0.1849
2025-12-26 11:16:33,806 - INFO -   Batch 300/1264: Loss = 0.2252
2025-12-26 11:17:06,447 - INFO -   Batch 350/1264: Loss = 0.1734
2025-12-26 11:17:39,088 - INFO -   Batch 400/1264: Loss = 0.1454
2025-12-26 11:18:11,730 - INFO -   Batch 450/1264: Loss = 0.3085
2025-12-26 11:18:44,371 - INFO -   Batch 500/1264: Loss = 0.2122
2025-12-26 11:19:17,009 - INFO -   Batch 550/1264: Loss = 0.2850
2025-12-26 11:19:49,649 - INFO -   Batch 600/1264: Loss = 0.1287
2025-12-26 11:20:22,289 - INFO -   Batch 650/1264: Loss = 0.2552
2025-12-26 11:20:54,930 - INFO -   Batch 700/1264: Loss = 0.2566
2025-12-26 11:21:27,570 - INFO -   Batch 750/1264: Loss = 0.2324
2025-12-26 11:22:00,213 - INFO -   Batch 800/1264: Loss = 0.2138
2025-12-26 11:22:32,857 - INFO -   Batch 850/1264: Loss = 0.2006
2025-12-26 11:23:05,501 - INFO -   Batch 900/1264: Loss = 0.2486
2025-12-26 11:23:38,144 - INFO -   Batch 950/1264: Loss = 0.2378
2025-12-26 11:24:10,786 - INFO -   Batch 1000/1264: Loss = 0.1892
2025-12-26 11:24:43,424 - INFO -   Batch 1050/1264: Loss = 0.1620
2025-12-26 11:25:16,065 - INFO -   Batch 1100/1264: Loss = 0.1870
2025-12-26 11:25:48,708 - INFO -   Batch 1150/1264: Loss = 0.2514
2025-12-26 11:26:21,353 - INFO -   Batch 1200/1264: Loss = 0.1582
2025-12-26 11:26:53,997 - INFO -   Batch 1250/1264: Loss = 0.1689
2025-12-26 11:29:54,918 - INFO - Epoch 44/160: Train Loss: 0.2151, Val Loss: 0.8875, Val mIoU: 42.65%, Val PixelAcc: 79.59%
2025-12-26 11:30:29,545 - INFO -   Batch 50/1264: Loss = 0.1700
2025-12-26 11:31:02,192 - INFO -   Batch 100/1264: Loss = 0.1949
2025-12-26 11:31:34,838 - INFO -   Batch 150/1264: Loss = 0.2733
2025-12-26 11:32:07,482 - INFO -   Batch 200/1264: Loss = 0.2195
2025-12-26 11:32:40,121 - INFO -   Batch 250/1264: Loss = 0.1758
2025-12-26 11:33:12,761 - INFO -   Batch 300/1264: Loss = 0.2229
2025-12-26 11:33:45,396 - INFO -   Batch 350/1264: Loss = 0.2141
2025-12-26 11:34:18,036 - INFO -   Batch 400/1264: Loss = 0.1816
2025-12-26 11:34:50,676 - INFO -   Batch 450/1264: Loss = 0.2179
2025-12-26 11:35:23,316 - INFO -   Batch 500/1264: Loss = 0.2345
2025-12-26 11:35:55,956 - INFO -   Batch 550/1264: Loss = 0.1592
2025-12-26 11:36:28,594 - INFO -   Batch 600/1264: Loss = 0.2363
2025-12-26 11:37:01,232 - INFO -   Batch 650/1264: Loss = 0.2007
2025-12-26 11:37:33,872 - INFO -   Batch 700/1264: Loss = 0.1851
2025-12-26 11:38:06,515 - INFO -   Batch 750/1264: Loss = 0.2213
2025-12-26 11:38:39,157 - INFO -   Batch 800/1264: Loss = 0.2013
2025-12-26 11:39:11,800 - INFO -   Batch 850/1264: Loss = 0.3201
2025-12-26 11:39:44,440 - INFO -   Batch 900/1264: Loss = 0.1834
2025-12-26 11:40:17,081 - INFO -   Batch 950/1264: Loss = 0.1846
2025-12-26 11:40:49,718 - INFO -   Batch 1000/1264: Loss = 0.1643
2025-12-26 11:41:22,358 - INFO -   Batch 1050/1264: Loss = 0.1774
2025-12-26 11:41:54,996 - INFO -   Batch 1100/1264: Loss = 0.2542
2025-12-26 11:42:27,644 - INFO -   Batch 1150/1264: Loss = 0.2028
2025-12-26 11:43:00,288 - INFO -   Batch 1200/1264: Loss = 0.2161
2025-12-26 11:43:32,933 - INFO -   Batch 1250/1264: Loss = 0.2113
2025-12-26 11:46:31,965 - INFO - Epoch 45/160: Train Loss: 0.2073, Val Loss: 0.8850, Val mIoU: 43.11%, Val PixelAcc: 79.77%
2025-12-26 11:46:31,990 - INFO -   → New best mIoU: 43.11%
2025-12-26 11:46:34,010 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 11:46:34,012 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 11:47:08,380 - INFO -   Batch 50/1264: Loss = 0.1601
2025-12-26 11:47:41,030 - INFO -   Batch 100/1264: Loss = 0.2014
2025-12-26 11:48:13,676 - INFO -   Batch 150/1264: Loss = 0.1759
2025-12-26 11:48:46,320 - INFO -   Batch 200/1264: Loss = 0.2025
2025-12-26 11:49:18,959 - INFO -   Batch 250/1264: Loss = 0.1861
2025-12-26 11:49:51,597 - INFO -   Batch 300/1264: Loss = 0.2291
2025-12-26 11:50:24,233 - INFO -   Batch 350/1264: Loss = 0.2795
2025-12-26 11:50:56,870 - INFO -   Batch 400/1264: Loss = 0.2191
2025-12-26 11:51:29,509 - INFO -   Batch 450/1264: Loss = 0.2174
2025-12-26 11:52:02,149 - INFO -   Batch 500/1264: Loss = 0.2448
2025-12-26 11:52:34,787 - INFO -   Batch 550/1264: Loss = 0.1728
2025-12-26 11:53:07,428 - INFO -   Batch 600/1264: Loss = 0.2002
2025-12-26 11:53:40,066 - INFO -   Batch 650/1264: Loss = 0.2237
2025-12-26 11:54:12,699 - INFO -   Batch 700/1264: Loss = 0.2514
2025-12-26 11:54:45,336 - INFO -   Batch 750/1264: Loss = 0.2913
2025-12-26 11:55:17,976 - INFO -   Batch 800/1264: Loss = 0.2162
2025-12-26 11:55:50,615 - INFO -   Batch 850/1264: Loss = 0.2272
2025-12-26 11:56:23,257 - INFO -   Batch 900/1264: Loss = 0.2234
2025-12-26 11:56:55,900 - INFO -   Batch 950/1264: Loss = 0.2082
2025-12-26 11:57:28,544 - INFO -   Batch 1000/1264: Loss = 0.1996
2025-12-26 11:58:01,187 - INFO -   Batch 1050/1264: Loss = 0.2454
2025-12-26 11:58:33,830 - INFO -   Batch 1100/1264: Loss = 0.2281
2025-12-26 11:59:06,473 - INFO -   Batch 1150/1264: Loss = 0.2584
2025-12-26 11:59:39,117 - INFO -   Batch 1200/1264: Loss = 0.2391
2025-12-26 12:00:11,762 - INFO -   Batch 1250/1264: Loss = 0.2097
2025-12-26 12:03:10,949 - INFO - Epoch 46/160: Train Loss: 0.2063, Val Loss: 0.8969, Val mIoU: 43.71%, Val PixelAcc: 79.64%
2025-12-26 12:03:10,985 - INFO -   → New best mIoU: 43.71%
2025-12-26 12:03:13,589 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 12:03:13,591 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 12:03:47,857 - INFO -   Batch 50/1264: Loss = 0.1984
2025-12-26 12:04:20,500 - INFO -   Batch 100/1264: Loss = 0.1613
2025-12-26 12:04:53,141 - INFO -   Batch 150/1264: Loss = 0.1955
2025-12-26 12:05:25,788 - INFO -   Batch 200/1264: Loss = 0.1981
2025-12-26 12:05:58,540 - INFO -   Batch 250/1264: Loss = 0.2011
2025-12-26 12:06:31,185 - INFO -   Batch 300/1264: Loss = 0.1696
2025-12-26 12:07:03,824 - INFO -   Batch 350/1264: Loss = 0.1897
2025-12-26 12:07:36,461 - INFO -   Batch 400/1264: Loss = 0.1761
2025-12-26 12:08:09,101 - INFO -   Batch 450/1264: Loss = 0.2032
2025-12-26 12:08:41,743 - INFO -   Batch 500/1264: Loss = 0.2966
2025-12-26 12:09:14,385 - INFO -   Batch 550/1264: Loss = 0.1202
2025-12-26 12:09:47,025 - INFO -   Batch 600/1264: Loss = 0.1502
2025-12-26 12:10:19,669 - INFO -   Batch 650/1264: Loss = 0.2025
2025-12-26 12:10:52,310 - INFO -   Batch 700/1264: Loss = 0.2073
2025-12-26 12:11:25,005 - INFO -   Batch 750/1264: Loss = 0.2556
2025-12-26 12:11:57,649 - INFO -   Batch 800/1264: Loss = 0.2259
2025-12-26 12:12:30,292 - INFO -   Batch 850/1264: Loss = 0.1730
2025-12-26 12:13:02,936 - INFO -   Batch 900/1264: Loss = 0.1701
2025-12-26 12:13:35,579 - INFO -   Batch 950/1264: Loss = 0.1852
2025-12-26 12:14:08,218 - INFO -   Batch 1000/1264: Loss = 0.1991
2025-12-26 12:14:40,860 - INFO -   Batch 1050/1264: Loss = 0.2735
2025-12-26 12:15:13,500 - INFO -   Batch 1100/1264: Loss = 0.1468
2025-12-26 12:15:46,138 - INFO -   Batch 1150/1264: Loss = 0.1953
2025-12-26 12:16:18,777 - INFO -   Batch 1200/1264: Loss = 0.2056
2025-12-26 12:16:51,416 - INFO -   Batch 1250/1264: Loss = 0.2312
2025-12-26 12:19:51,412 - INFO - Epoch 47/160: Train Loss: 0.2037, Val Loss: 0.9168, Val mIoU: 42.58%, Val PixelAcc: 79.42%
2025-12-26 12:20:25,666 - INFO -   Batch 50/1264: Loss = 0.4611
2025-12-26 12:20:58,307 - INFO -   Batch 100/1264: Loss = 0.1847
2025-12-26 12:21:30,951 - INFO -   Batch 150/1264: Loss = 0.1975
2025-12-26 12:22:03,598 - INFO -   Batch 200/1264: Loss = 0.1721
2025-12-26 12:22:36,243 - INFO -   Batch 250/1264: Loss = 0.1378
2025-12-26 12:23:08,888 - INFO -   Batch 300/1264: Loss = 0.2659
2025-12-26 12:23:41,529 - INFO -   Batch 350/1264: Loss = 0.2005
2025-12-26 12:24:14,171 - INFO -   Batch 400/1264: Loss = 0.1828
2025-12-26 12:24:46,812 - INFO -   Batch 450/1264: Loss = 0.2338
2025-12-26 12:25:19,452 - INFO -   Batch 500/1264: Loss = 0.1806
2025-12-26 12:25:52,093 - INFO -   Batch 550/1264: Loss = 0.1337
2025-12-26 12:26:24,734 - INFO -   Batch 600/1264: Loss = 0.1891
2025-12-26 12:26:57,378 - INFO -   Batch 650/1264: Loss = 0.1868
2025-12-26 12:27:30,021 - INFO -   Batch 700/1264: Loss = 0.1940
2025-12-26 12:28:02,663 - INFO -   Batch 750/1264: Loss = 0.1878
2025-12-26 12:28:35,307 - INFO -   Batch 800/1264: Loss = 0.2746
2025-12-26 12:29:07,951 - INFO -   Batch 850/1264: Loss = 0.1968
2025-12-26 12:29:40,596 - INFO -   Batch 900/1264: Loss = 0.1762
2025-12-26 12:30:13,237 - INFO -   Batch 950/1264: Loss = 0.2196
2025-12-26 12:30:45,881 - INFO -   Batch 1000/1264: Loss = 0.2317
2025-12-26 12:31:18,526 - INFO -   Batch 1050/1264: Loss = 0.2676
2025-12-26 12:31:51,169 - INFO -   Batch 1100/1264: Loss = 0.2214
2025-12-26 12:32:23,811 - INFO -   Batch 1150/1264: Loss = 0.2138
2025-12-26 12:32:56,454 - INFO -   Batch 1200/1264: Loss = 0.2472
2025-12-26 12:33:29,094 - INFO -   Batch 1250/1264: Loss = 0.2067
2025-12-26 12:36:28,992 - INFO - Epoch 48/160: Train Loss: 0.2112, Val Loss: 0.9060, Val mIoU: 43.27%, Val PixelAcc: 79.76%
2025-12-26 12:37:03,409 - INFO -   Batch 50/1264: Loss = 0.1992
2025-12-26 12:37:36,052 - INFO -   Batch 100/1264: Loss = 0.1834
2025-12-26 12:38:08,730 - INFO -   Batch 150/1264: Loss = 0.1703
2025-12-26 12:38:41,380 - INFO -   Batch 200/1264: Loss = 0.2190
2025-12-26 12:39:14,033 - INFO -   Batch 250/1264: Loss = 0.1951
2025-12-26 12:39:46,681 - INFO -   Batch 300/1264: Loss = 0.1957
2025-12-26 12:40:19,325 - INFO -   Batch 350/1264: Loss = 0.1444
2025-12-26 12:40:51,967 - INFO -   Batch 400/1264: Loss = 0.1819
2025-12-26 12:41:24,609 - INFO -   Batch 450/1264: Loss = 0.1508
2025-12-26 12:41:57,539 - INFO -   Batch 500/1264: Loss = 0.1912
2025-12-26 12:42:30,603 - INFO -   Batch 550/1264: Loss = 0.1863
2025-12-26 12:43:03,753 - INFO -   Batch 600/1264: Loss = 0.1665
2025-12-26 12:43:36,924 - INFO -   Batch 650/1264: Loss = 0.1368
2025-12-26 12:44:10,206 - INFO -   Batch 700/1264: Loss = 0.2344
2025-12-26 12:44:43,507 - INFO -   Batch 750/1264: Loss = 0.2308
2025-12-26 12:45:16,875 - INFO -   Batch 800/1264: Loss = 0.2438
2025-12-26 12:45:50,269 - INFO -   Batch 850/1264: Loss = 0.1649
2025-12-26 12:46:23,693 - INFO -   Batch 900/1264: Loss = 0.2007
2025-12-26 12:46:57,153 - INFO -   Batch 950/1264: Loss = 0.1782
2025-12-26 12:47:30,657 - INFO -   Batch 1000/1264: Loss = 0.2094
2025-12-26 12:48:04,217 - INFO -   Batch 1050/1264: Loss = 0.2824
2025-12-26 12:48:37,744 - INFO -   Batch 1100/1264: Loss = 0.1706
2025-12-26 12:49:11,226 - INFO -   Batch 1150/1264: Loss = 0.1878
2025-12-26 12:49:44,482 - INFO -   Batch 1200/1264: Loss = 0.1481
2025-12-26 12:50:17,671 - INFO -   Batch 1250/1264: Loss = 0.1346
2025-12-26 12:53:18,341 - INFO - Epoch 49/160: Train Loss: 0.1971, Val Loss: 0.9072, Val mIoU: 43.79%, Val PixelAcc: 79.73%
2025-12-26 12:53:18,342 - INFO -   → New best mIoU: 43.79%
2025-12-26 12:53:21,973 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-26 12:53:21,974 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-26 12:53:56,644 - INFO -   Batch 50/1264: Loss = 0.1466
2025-12-26 12:54:29,728 - INFO -   Batch 100/1264: Loss = 0.1760
2025-12-26 12:55:02,912 - INFO -   Batch 150/1264: Loss = 0.2312
2025-12-26 12:55:36,093 - INFO -   Batch 200/1264: Loss = 0.2307
2025-12-26 12:56:09,363 - INFO -   Batch 250/1264: Loss = 0.2174
2025-12-26 12:56:42,708 - INFO -   Batch 300/1264: Loss = 0.1889
2025-12-26 12:57:15,992 - INFO -   Batch 350/1264: Loss = 0.2447
2025-12-26 12:57:49,358 - INFO -   Batch 400/1264: Loss = 0.2395
2025-12-26 12:58:22,750 - INFO -   Batch 450/1264: Loss = 0.1828
2025-12-26 12:58:56,177 - INFO -   Batch 500/1264: Loss = 0.1037
2025-12-26 12:59:29,676 - INFO -   Batch 550/1264: Loss = 0.2063
2025-12-26 13:00:03,240 - INFO -   Batch 600/1264: Loss = 0.1876
2025-12-26 13:00:36,812 - INFO -   Batch 650/1264: Loss = 0.1876
2025-12-26 13:01:10,442 - INFO -   Batch 700/1264: Loss = 0.1992
2025-12-26 13:01:43,800 - INFO -   Batch 750/1264: Loss = 0.1704
2025-12-26 13:02:17,152 - INFO -   Batch 800/1264: Loss = 0.1854
2025-12-26 13:02:50,579 - INFO -   Batch 850/1264: Loss = 0.2051
2025-12-26 13:03:24,098 - INFO -   Batch 900/1264: Loss = 0.1212
2025-12-26 13:03:57,557 - INFO -   Batch 950/1264: Loss = 0.1782
2025-12-26 13:04:30,761 - INFO -   Batch 1000/1264: Loss = 0.2075
2025-12-26 13:05:03,937 - INFO -   Batch 1050/1264: Loss = 0.1887
2025-12-26 13:05:37,134 - INFO -   Batch 1100/1264: Loss = 0.1796
2025-12-26 13:06:10,395 - INFO -   Batch 1150/1264: Loss = 0.1785
2025-12-26 13:06:43,675 - INFO -   Batch 1200/1264: Loss = 0.1368
2025-12-26 13:07:17,048 - INFO -   Batch 1250/1264: Loss = 0.1906
2025-12-26 13:10:17,843 - INFO - Epoch 50/160: Train Loss: 0.1929, Val Loss: 0.9123, Val mIoU: 42.92%, Val PixelAcc: 79.83%
2025-12-26 13:10:19,365 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_50.pth
2025-12-26 13:10:19,366 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_50.pth
2025-12-26 13:10:53,802 - INFO -   Batch 50/1264: Loss = 0.1758
2025-12-26 13:11:26,922 - INFO -   Batch 100/1264: Loss = 0.2294
2025-12-26 13:12:00,095 - INFO -   Batch 150/1264: Loss = 0.2105
2025-12-26 13:12:33,318 - INFO -   Batch 200/1264: Loss = 0.2128
2025-12-26 13:13:06,620 - INFO -   Batch 250/1264: Loss = 0.2435
2025-12-26 13:13:39,922 - INFO -   Batch 300/1264: Loss = 0.2418
2025-12-26 13:14:13,296 - INFO -   Batch 350/1264: Loss = 0.1782
2025-12-26 13:14:46,694 - INFO -   Batch 400/1264: Loss = 0.1933
2025-12-26 13:15:20,170 - INFO -   Batch 450/1264: Loss = 0.1791
2025-12-26 13:15:53,632 - INFO -   Batch 500/1264: Loss = 0.1915
2025-12-26 13:16:27,109 - INFO -   Batch 550/1264: Loss = 0.1625
2025-12-26 13:17:00,620 - INFO -   Batch 600/1264: Loss = 0.2753
2025-12-26 13:17:34,154 - INFO -   Batch 650/1264: Loss = 0.1946
2025-12-26 13:18:07,661 - INFO -   Batch 700/1264: Loss = 0.1781
2025-12-26 13:18:41,231 - INFO -   Batch 750/1264: Loss = 0.1937
2025-12-26 13:19:14,771 - INFO -   Batch 800/1264: Loss = 0.1892
2025-12-26 13:19:48,356 - INFO -   Batch 850/1264: Loss = 0.2780
2025-12-26 13:20:21,966 - INFO -   Batch 900/1264: Loss = 0.1355
2025-12-26 13:20:55,604 - INFO -   Batch 950/1264: Loss = 0.2115
2025-12-26 13:21:29,235 - INFO -   Batch 1000/1264: Loss = 0.1948
2025-12-26 13:22:02,915 - INFO -   Batch 1050/1264: Loss = 0.2279
2025-12-26 13:22:36,626 - INFO -   Batch 1100/1264: Loss = 0.2298
2025-12-26 13:23:10,294 - INFO -   Batch 1150/1264: Loss = 0.1839
2025-12-26 13:23:44,000 - INFO -   Batch 1200/1264: Loss = 0.2165
2025-12-26 13:24:17,709 - INFO -   Batch 1250/1264: Loss = 0.1747
2025-12-26 13:27:18,011 - INFO - Epoch 51/160: Train Loss: 0.2039, Val Loss: 0.9057, Val mIoU: 43.54%, Val PixelAcc: 79.90%
2025-12-26 13:27:53,329 - INFO -   Batch 50/1264: Loss = 0.3583
2025-12-26 13:28:26,886 - INFO -   Batch 100/1264: Loss = 0.1847
2025-12-26 13:29:00,325 - INFO -   Batch 150/1264: Loss = 0.1820
2025-12-26 13:29:33,612 - INFO -   Batch 200/1264: Loss = 0.2307
2025-12-26 13:30:06,968 - INFO -   Batch 250/1264: Loss = 0.1991
2025-12-26 13:30:40,345 - INFO -   Batch 300/1264: Loss = 0.1622
2025-12-26 13:31:13,727 - INFO -   Batch 350/1264: Loss = 0.1939
2025-12-26 13:31:47,193 - INFO -   Batch 400/1264: Loss = 0.1848
2025-12-26 13:32:20,644 - INFO -   Batch 450/1264: Loss = 0.2035
2025-12-26 13:32:54,199 - INFO -   Batch 500/1264: Loss = 0.1424
2025-12-26 13:33:27,808 - INFO -   Batch 550/1264: Loss = 0.1712
2025-12-26 13:34:01,404 - INFO -   Batch 600/1264: Loss = 0.1798
2025-12-26 13:34:34,951 - INFO -   Batch 650/1264: Loss = 0.2293
2025-12-26 13:35:08,307 - INFO -   Batch 700/1264: Loss = 0.2359
2025-12-26 13:35:41,510 - INFO -   Batch 750/1264: Loss = 0.2423
2025-12-26 13:36:14,733 - INFO -   Batch 800/1264: Loss = 0.1701
2025-12-26 13:36:47,916 - INFO -   Batch 850/1264: Loss = 0.2963
2025-12-26 13:37:21,139 - INFO -   Batch 900/1264: Loss = 0.2040
2025-12-26 13:37:54,303 - INFO -   Batch 950/1264: Loss = 0.1499
2025-12-26 13:38:27,457 - INFO -   Batch 1000/1264: Loss = 0.2052
2025-12-26 13:39:00,687 - INFO -   Batch 1050/1264: Loss = 0.1645
2025-12-26 13:39:33,910 - INFO -   Batch 1100/1264: Loss = 0.1963
2025-12-26 13:40:07,117 - INFO -   Batch 1150/1264: Loss = 0.2286
2025-12-26 13:40:40,391 - INFO -   Batch 1200/1264: Loss = 0.1750
2025-12-26 13:41:13,675 - INFO -   Batch 1250/1264: Loss = 0.1751
2025-12-26 13:44:13,114 - INFO - Epoch 52/160: Train Loss: 0.1937, Val Loss: 0.9228, Val mIoU: 43.45%, Val PixelAcc: 79.86%
2025-12-26 13:44:48,265 - INFO -   Batch 50/1264: Loss = 0.2239
2025-12-26 13:45:21,736 - INFO -   Batch 100/1264: Loss = 0.2050
2025-12-26 13:45:55,249 - INFO -   Batch 150/1264: Loss = 0.1854
2025-12-26 13:46:28,819 - INFO -   Batch 200/1264: Loss = 0.1559
2025-12-26 13:47:02,397 - INFO -   Batch 250/1264: Loss = 0.1551
2025-12-26 13:47:35,953 - INFO -   Batch 300/1264: Loss = 0.1456
2025-12-26 13:48:09,609 - INFO -   Batch 350/1264: Loss = 0.1570
2025-12-26 13:48:43,297 - INFO -   Batch 400/1264: Loss = 0.1874
2025-12-26 13:49:17,025 - INFO -   Batch 450/1264: Loss = 0.2171
2025-12-26 13:49:50,797 - INFO -   Batch 500/1264: Loss = 0.1533
2025-12-26 13:50:24,470 - INFO -   Batch 550/1264: Loss = 0.1760
2025-12-26 13:50:58,186 - INFO -   Batch 600/1264: Loss = 0.1502
2025-12-26 13:51:31,925 - INFO -   Batch 650/1264: Loss = 0.2138
2025-12-26 13:52:05,644 - INFO -   Batch 700/1264: Loss = 0.1579
2025-12-26 13:52:39,374 - INFO -   Batch 750/1264: Loss = 0.2302
2025-12-26 13:53:13,128 - INFO -   Batch 800/1264: Loss = 0.1878
2025-12-26 13:53:46,913 - INFO -   Batch 850/1264: Loss = 0.1685
2025-12-26 13:54:20,717 - INFO -   Batch 900/1264: Loss = 0.2506
2025-12-26 13:54:54,540 - INFO -   Batch 950/1264: Loss = 0.1887
2025-12-26 13:55:28,345 - INFO -   Batch 1000/1264: Loss = 0.2046
2025-12-26 13:56:02,151 - INFO -   Batch 1050/1264: Loss = 0.2210
2025-12-26 13:56:36,005 - INFO -   Batch 1100/1264: Loss = 0.2382
2025-12-26 13:57:09,892 - INFO -   Batch 1150/1264: Loss = 0.2122
2025-12-26 13:57:43,870 - INFO -   Batch 1200/1264: Loss = 0.2307
2025-12-26 13:58:17,731 - INFO -   Batch 1250/1264: Loss = 0.1606
2025-12-26 14:01:17,912 - INFO - Epoch 53/160: Train Loss: 0.1856, Val Loss: 0.9251, Val mIoU: 42.76%, Val PixelAcc: 79.78%
2025-12-26 14:01:53,981 - INFO -   Batch 50/1264: Loss = 0.2162
2025-12-26 14:02:27,856 - INFO -   Batch 100/1264: Loss = 0.1570
2025-12-26 14:03:01,803 - INFO -   Batch 150/1264: Loss = 0.1722
2025-12-26 14:03:35,667 - INFO -   Batch 200/1264: Loss = 0.1683
2025-12-26 14:04:09,528 - INFO -   Batch 250/1264: Loss = 0.1733
2025-12-26 14:04:43,441 - INFO -   Batch 300/1264: Loss = 0.1650
2025-12-26 14:05:17,357 - INFO -   Batch 350/1264: Loss = 0.1791
2025-12-26 14:05:51,320 - INFO -   Batch 400/1264: Loss = 0.1989
2025-12-26 14:06:25,278 - INFO -   Batch 450/1264: Loss = 0.1063
2025-12-26 14:06:59,260 - INFO -   Batch 500/1264: Loss = 0.1952
2025-12-26 14:07:33,203 - INFO -   Batch 550/1264: Loss = 0.1243
2025-12-26 14:08:07,088 - INFO -   Batch 600/1264: Loss = 0.1449
2025-12-26 14:08:40,912 - INFO -   Batch 650/1264: Loss = 0.1896
2025-12-26 14:09:14,731 - INFO -   Batch 700/1264: Loss = 0.2119
2025-12-26 14:09:48,589 - INFO -   Batch 750/1264: Loss = 0.1863
2025-12-26 14:10:22,139 - INFO -   Batch 800/1264: Loss = 0.2423
2025-12-26 14:10:55,363 - INFO -   Batch 850/1264: Loss = 0.2172
2025-12-26 14:11:28,642 - INFO -   Batch 900/1264: Loss = 0.1657
2025-12-26 14:12:01,917 - INFO -   Batch 950/1264: Loss = 0.1626
2025-12-26 14:12:35,276 - INFO -   Batch 1000/1264: Loss = 0.2248
2025-12-26 14:13:08,650 - INFO -   Batch 1050/1264: Loss = 0.2040
2025-12-26 14:13:42,069 - INFO -   Batch 1100/1264: Loss = 0.1444
2025-12-26 14:14:15,506 - INFO -   Batch 1150/1264: Loss = 0.1859
2025-12-26 14:14:49,015 - INFO -   Batch 1200/1264: Loss = 0.1824
2025-12-26 14:15:22,530 - INFO -   Batch 1250/1264: Loss = 0.2003
2025-12-26 14:18:22,954 - INFO - Epoch 54/160: Train Loss: 0.1812, Val Loss: 0.9474, Val mIoU: 43.41%, Val PixelAcc: 79.71%
2025-12-26 14:18:58,036 - INFO -   Batch 50/1264: Loss = 0.1668
2025-12-26 14:19:31,188 - INFO -   Batch 100/1264: Loss = 0.1386
2025-12-26 14:20:04,267 - INFO -   Batch 150/1264: Loss = 0.1630
2025-12-26 14:20:37,349 - INFO -   Batch 200/1264: Loss = 0.1565
2025-12-26 14:21:10,543 - INFO -   Batch 250/1264: Loss = 0.1168
2025-12-26 14:21:43,736 - INFO -   Batch 300/1264: Loss = 0.1943
2025-12-26 14:22:16,994 - INFO -   Batch 350/1264: Loss = 0.1624
2025-12-26 14:22:50,275 - INFO -   Batch 400/1264: Loss = 0.2231
2025-12-26 14:23:23,584 - INFO -   Batch 450/1264: Loss = 0.1499
2025-12-26 14:23:56,939 - INFO -   Batch 500/1264: Loss = 0.1448
2025-12-26 14:24:30,338 - INFO -   Batch 550/1264: Loss = 0.1869
2025-12-26 14:25:03,760 - INFO -   Batch 600/1264: Loss = 0.1460
2025-12-26 14:25:37,239 - INFO -   Batch 650/1264: Loss = 0.1833
2025-12-26 14:26:10,704 - INFO -   Batch 700/1264: Loss = 0.2320
2025-12-26 14:26:44,175 - INFO -   Batch 750/1264: Loss = 0.1713
2025-12-26 14:27:17,633 - INFO -   Batch 800/1264: Loss = 0.1894
2025-12-26 14:27:51,171 - INFO -   Batch 850/1264: Loss = 0.1904
2025-12-26 14:28:24,706 - INFO -   Batch 900/1264: Loss = 0.2994
2025-12-26 14:28:58,241 - INFO -   Batch 950/1264: Loss = 0.1958
2025-12-26 14:29:31,830 - INFO -   Batch 1000/1264: Loss = 0.1854
2025-12-26 14:30:05,467 - INFO -   Batch 1050/1264: Loss = 0.2434
2025-12-26 14:30:39,049 - INFO -   Batch 1100/1264: Loss = 0.1489
2025-12-26 14:31:12,656 - INFO -   Batch 1150/1264: Loss = 0.1803
2025-12-26 14:31:46,249 - INFO -   Batch 1200/1264: Loss = 0.1420
2025-12-26 14:32:19,884 - INFO -   Batch 1250/1264: Loss = 0.1535
2025-12-26 14:35:20,016 - INFO - Epoch 55/160: Train Loss: 0.1807, Val Loss: 0.9372, Val mIoU: 43.14%, Val PixelAcc: 79.77%
2025-12-26 14:35:55,195 - INFO -   Batch 50/1264: Loss = 0.2111
2025-12-26 14:36:28,803 - INFO -   Batch 100/1264: Loss = 0.1441
2025-12-26 14:37:02,450 - INFO -   Batch 150/1264: Loss = 0.1787
2025-12-26 14:37:36,081 - INFO -   Batch 200/1264: Loss = 0.2078
2025-12-26 14:38:09,731 - INFO -   Batch 250/1264: Loss = 0.1978
2025-12-26 14:38:43,382 - INFO -   Batch 300/1264: Loss = 0.1299
2025-12-26 14:39:17,072 - INFO -   Batch 350/1264: Loss = 0.1508
2025-12-26 14:39:50,794 - INFO -   Batch 400/1264: Loss = 0.1956
2025-12-26 14:40:24,509 - INFO -   Batch 450/1264: Loss = 0.1883
2025-12-26 14:40:58,233 - INFO -   Batch 500/1264: Loss = 0.1670
2025-12-26 14:41:31,936 - INFO -   Batch 550/1264: Loss = 0.2017
2025-12-26 14:42:05,632 - INFO -   Batch 600/1264: Loss = 0.1751
2025-12-26 14:42:39,343 - INFO -   Batch 650/1264: Loss = 0.2498
2025-12-26 14:43:13,076 - INFO -   Batch 700/1264: Loss = 0.1261
2025-12-26 14:43:46,837 - INFO -   Batch 750/1264: Loss = 0.1677
2025-12-26 14:44:20,622 - INFO -   Batch 800/1264: Loss = 0.1984
2025-12-26 14:44:54,422 - INFO -   Batch 850/1264: Loss = 0.1497
2025-12-26 14:45:28,222 - INFO -   Batch 900/1264: Loss = 0.1982
2025-12-26 14:46:02,012 - INFO -   Batch 950/1264: Loss = 0.1869
2025-12-26 14:46:35,799 - INFO -   Batch 1000/1264: Loss = 0.1401
2025-12-26 14:47:09,599 - INFO -   Batch 1050/1264: Loss = 0.1874
2025-12-26 14:47:43,410 - INFO -   Batch 1100/1264: Loss = 0.1855
2025-12-26 14:48:17,203 - INFO -   Batch 1150/1264: Loss = 0.1557
2025-12-26 14:48:51,016 - INFO -   Batch 1200/1264: Loss = 0.1923
2025-12-26 14:49:24,842 - INFO -   Batch 1250/1264: Loss = 0.1341
2025-12-26 14:52:25,222 - INFO - Epoch 56/160: Train Loss: 0.1824, Val Loss: 0.9414, Val mIoU: 43.49%, Val PixelAcc: 79.99%
2025-12-26 14:53:00,742 - INFO -   Batch 50/1264: Loss = 0.1730
2025-12-26 14:53:34,460 - INFO -   Batch 100/1264: Loss = 0.1991
2025-12-26 14:54:08,233 - INFO -   Batch 150/1264: Loss = 0.1653
2025-12-26 14:54:42,015 - INFO -   Batch 200/1264: Loss = 0.1670
2025-12-26 14:55:15,794 - INFO -   Batch 250/1264: Loss = 0.1897
2025-12-26 14:55:49,558 - INFO -   Batch 300/1264: Loss = 0.2032
2025-12-26 14:56:23,360 - INFO -   Batch 350/1264: Loss = 0.1876
2025-12-26 14:56:57,151 - INFO -   Batch 400/1264: Loss = 0.1923
2025-12-26 14:57:30,963 - INFO -   Batch 450/1264: Loss = 0.1779
2025-12-26 14:58:04,793 - INFO -   Batch 500/1264: Loss = 0.1665
2025-12-26 14:58:38,609 - INFO -   Batch 550/1264: Loss = 0.2142
2025-12-26 14:59:12,327 - INFO -   Batch 600/1264: Loss = 0.1456
2025-12-26 14:59:45,798 - INFO -   Batch 650/1264: Loss = 0.1507
2025-12-26 15:00:19,149 - INFO -   Batch 700/1264: Loss = 0.1813
2025-12-26 15:00:52,306 - INFO -   Batch 750/1264: Loss = 0.1911
2025-12-26 15:01:25,419 - INFO -   Batch 800/1264: Loss = 0.1551
2025-12-26 15:01:58,554 - INFO -   Batch 850/1264: Loss = 0.1775
2025-12-26 15:02:31,739 - INFO -   Batch 900/1264: Loss = 0.2524
2025-12-26 15:03:05,003 - INFO -   Batch 950/1264: Loss = 0.2670
2025-12-26 15:03:38,236 - INFO -   Batch 1000/1264: Loss = 0.1879
2025-12-26 15:04:11,502 - INFO -   Batch 1050/1264: Loss = 0.1144
2025-12-26 15:04:44,833 - INFO -   Batch 1100/1264: Loss = 0.1998
2025-12-26 15:05:18,183 - INFO -   Batch 1150/1264: Loss = 0.2314
2025-12-26 15:05:51,548 - INFO -   Batch 1200/1264: Loss = 0.1833
2025-12-26 15:06:24,957 - INFO -   Batch 1250/1264: Loss = 0.1079
2025-12-26 15:09:24,667 - INFO - Epoch 57/160: Train Loss: 0.1787, Val Loss: 0.9551, Val mIoU: 43.09%, Val PixelAcc: 79.76%
2025-12-26 15:09:59,503 - INFO -   Batch 50/1264: Loss = 0.3103
2025-12-26 15:10:32,982 - INFO -   Batch 100/1264: Loss = 0.1807
2025-12-26 15:11:06,531 - INFO -   Batch 150/1264: Loss = 0.1610
2025-12-26 15:11:40,096 - INFO -   Batch 200/1264: Loss = 0.2082
2025-12-26 15:12:13,685 - INFO -   Batch 250/1264: Loss = 0.1771
2025-12-26 15:12:47,308 - INFO -   Batch 300/1264: Loss = 0.1804
2025-12-26 15:13:20,942 - INFO -   Batch 350/1264: Loss = 0.1995
2025-12-26 15:13:54,613 - INFO -   Batch 400/1264: Loss = 0.1848
2025-12-26 15:14:28,287 - INFO -   Batch 450/1264: Loss = 0.1613
2025-12-26 15:15:01,960 - INFO -   Batch 500/1264: Loss = 0.1967
2025-12-26 15:15:35,671 - INFO -   Batch 550/1264: Loss = 0.2068
2025-12-26 15:16:09,394 - INFO -   Batch 600/1264: Loss = 0.2378
2025-12-26 15:16:43,115 - INFO -   Batch 650/1264: Loss = 0.2160
2025-12-26 15:17:16,851 - INFO -   Batch 700/1264: Loss = 0.1636
2025-12-26 15:17:50,589 - INFO -   Batch 750/1264: Loss = 0.1821
2025-12-26 15:18:24,314 - INFO -   Batch 800/1264: Loss = 0.1563
2025-12-26 15:18:58,075 - INFO -   Batch 850/1264: Loss = 0.1979
2025-12-26 15:19:31,811 - INFO -   Batch 900/1264: Loss = 0.1897
2025-12-26 15:20:05,564 - INFO -   Batch 950/1264: Loss = 0.2070
2025-12-26 15:20:39,345 - INFO -   Batch 1000/1264: Loss = 0.1539
2025-12-26 15:21:13,150 - INFO -   Batch 1050/1264: Loss = 0.1772
2025-12-26 15:21:46,892 - INFO -   Batch 1100/1264: Loss = 0.1924
2025-12-26 15:22:20,591 - INFO -   Batch 1150/1264: Loss = 0.1831
2025-12-26 15:22:54,275 - INFO -   Batch 1200/1264: Loss = 0.2179
2025-12-26 15:23:27,923 - INFO -   Batch 1250/1264: Loss = 0.2110
2025-12-26 15:26:27,994 - INFO - Epoch 58/160: Train Loss: 0.1759, Val Loss: 0.9269, Val mIoU: 43.71%, Val PixelAcc: 79.96%
2025-12-26 15:27:03,113 - INFO -   Batch 50/1264: Loss = 0.1641
2025-12-26 15:27:36,795 - INFO -   Batch 100/1264: Loss = 0.1641
2025-12-26 15:28:10,524 - INFO -   Batch 150/1264: Loss = 0.1531
2025-12-26 15:28:44,303 - INFO -   Batch 200/1264: Loss = 0.1456
2025-12-26 15:29:18,101 - INFO -   Batch 250/1264: Loss = 0.2377
2025-12-26 15:29:51,915 - INFO -   Batch 300/1264: Loss = 0.1116
2025-12-26 15:30:25,310 - INFO -   Batch 350/1264: Loss = 0.1259
2025-12-26 15:30:58,608 - INFO -   Batch 400/1264: Loss = 0.1846
2025-12-26 15:31:31,737 - INFO -   Batch 450/1264: Loss = 0.1848
2025-12-26 15:32:04,818 - INFO -   Batch 500/1264: Loss = 0.2149
2025-12-26 15:32:37,924 - INFO -   Batch 550/1264: Loss = 0.1898
2025-12-26 15:33:11,071 - INFO -   Batch 600/1264: Loss = 0.1705
2025-12-26 15:33:44,256 - INFO -   Batch 650/1264: Loss = 0.1965
2025-12-26 15:34:17,513 - INFO -   Batch 700/1264: Loss = 0.1608
2025-12-26 15:34:50,786 - INFO -   Batch 750/1264: Loss = 0.2197
2025-12-26 15:35:24,132 - INFO -   Batch 800/1264: Loss = 0.1709
2025-12-26 15:35:57,482 - INFO -   Batch 850/1264: Loss = 0.1309
2025-12-26 15:36:30,902 - INFO -   Batch 900/1264: Loss = 0.1456
2025-12-26 15:37:04,310 - INFO -   Batch 950/1264: Loss = 0.1911
2025-12-26 15:37:37,754 - INFO -   Batch 1000/1264: Loss = 0.1571
2025-12-26 15:38:11,235 - INFO -   Batch 1050/1264: Loss = 0.1580
2025-12-26 15:38:44,727 - INFO -   Batch 1100/1264: Loss = 0.1780
2025-12-26 15:39:18,213 - INFO -   Batch 1150/1264: Loss = 0.1621
2025-12-26 15:39:51,753 - INFO -   Batch 1200/1264: Loss = 0.2001
2025-12-26 15:40:25,284 - INFO -   Batch 1250/1264: Loss = 0.2566
2025-12-26 15:43:26,734 - INFO - Epoch 59/160: Train Loss: 0.1716, Val Loss: 0.9619, Val mIoU: 43.09%, Val PixelAcc: 79.72%
2025-12-26 15:44:02,087 - INFO -   Batch 50/1264: Loss = 0.2764
2025-12-26 15:44:35,652 - INFO -   Batch 100/1264: Loss = 0.1562
2025-12-26 15:45:09,412 - INFO -   Batch 150/1264: Loss = 0.1632
2025-12-26 15:45:42,210 - INFO -   Batch 200/1264: Loss = 0.1772
2025-12-26 15:46:14,856 - INFO -   Batch 250/1264: Loss = 0.1577
2025-12-26 15:46:47,500 - INFO -   Batch 300/1264: Loss = 0.1831
2025-12-26 15:47:20,143 - INFO -   Batch 350/1264: Loss = 0.1531
2025-12-26 15:47:52,785 - INFO -   Batch 400/1264: Loss = 0.1355
2025-12-26 15:48:25,426 - INFO -   Batch 450/1264: Loss = 0.1655
2025-12-26 15:48:58,068 - INFO -   Batch 500/1264: Loss = 0.1267
2025-12-26 15:49:30,715 - INFO -   Batch 550/1264: Loss = 0.1456
2025-12-26 15:50:03,360 - INFO -   Batch 600/1264: Loss = 0.1224
2025-12-26 15:50:36,388 - INFO -   Batch 650/1264: Loss = 0.1512
2025-12-26 15:51:09,972 - INFO -   Batch 700/1264: Loss = 0.1641
2025-12-26 15:51:43,587 - INFO -   Batch 750/1264: Loss = 0.1930
2025-12-26 15:52:17,254 - INFO -   Batch 800/1264: Loss = 0.1535
2025-12-26 15:52:50,859 - INFO -   Batch 850/1264: Loss = 0.1717
2025-12-26 15:53:24,672 - INFO -   Batch 900/1264: Loss = 0.1464
2025-12-26 15:53:58,587 - INFO -   Batch 950/1264: Loss = 0.1681
2025-12-26 15:54:32,604 - INFO -   Batch 1000/1264: Loss = 0.1609
2025-12-26 15:55:06,577 - INFO -   Batch 1050/1264: Loss = 0.1818
2025-12-26 15:55:40,494 - INFO -   Batch 1100/1264: Loss = 0.1301
2025-12-26 15:56:14,266 - INFO -   Batch 1150/1264: Loss = 0.1348
2025-12-26 15:56:48,028 - INFO -   Batch 1200/1264: Loss = 0.1494
2025-12-26 15:57:21,803 - INFO -   Batch 1250/1264: Loss = 0.1354
2025-12-26 16:00:21,556 - INFO - Epoch 60/160: Train Loss: 0.1742, Val Loss: 0.9476, Val mIoU: 43.59%, Val PixelAcc: 79.96%
2025-12-26 16:00:23,848 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_60.pth
2025-12-26 16:00:23,849 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_60.pth
2025-12-26 16:00:59,536 - INFO -   Batch 50/1264: Loss = 0.1790
2025-12-26 16:01:33,220 - INFO -   Batch 100/1264: Loss = 0.1670
2025-12-26 16:02:06,901 - INFO -   Batch 150/1264: Loss = 0.1942
2025-12-26 16:02:40,594 - INFO -   Batch 200/1264: Loss = 0.1424
2025-12-26 16:03:14,316 - INFO -   Batch 250/1264: Loss = 0.1709
2025-12-26 16:03:47,997 - INFO -   Batch 300/1264: Loss = 0.1700
2025-12-26 16:04:21,681 - INFO -   Batch 350/1264: Loss = 0.1492
2025-12-26 16:04:55,362 - INFO -   Batch 400/1264: Loss = 0.1694
2025-12-26 16:05:29,045 - INFO -   Batch 450/1264: Loss = 0.1567
2025-12-26 16:06:02,759 - INFO -   Batch 500/1264: Loss = 0.1411
2025-12-26 16:06:36,567 - INFO -   Batch 550/1264: Loss = 0.1617
2025-12-26 16:07:10,474 - INFO -   Batch 600/1264: Loss = 0.1383
2025-12-26 16:07:44,305 - INFO -   Batch 650/1264: Loss = 0.2026
2025-12-26 16:08:18,063 - INFO -   Batch 700/1264: Loss = 0.1631
2025-12-26 16:08:51,814 - INFO -   Batch 750/1264: Loss = 0.1432
2025-12-26 16:09:25,594 - INFO -   Batch 800/1264: Loss = 0.1320
2025-12-26 16:09:59,370 - INFO -   Batch 850/1264: Loss = 0.1845
2025-12-26 16:10:33,093 - INFO -   Batch 900/1264: Loss = 0.1879
2025-12-26 16:11:06,835 - INFO -   Batch 950/1264: Loss = 0.1465
2025-12-26 16:11:40,588 - INFO -   Batch 1000/1264: Loss = 0.1453
2025-12-26 16:12:14,397 - INFO -   Batch 1050/1264: Loss = 0.1597
2025-12-26 16:12:48,261 - INFO -   Batch 1100/1264: Loss = 0.2025
2025-12-26 16:13:22,162 - INFO -   Batch 1150/1264: Loss = 0.1678
2025-12-26 16:13:56,009 - INFO -   Batch 1200/1264: Loss = 0.1937
2025-12-26 16:14:29,779 - INFO -   Batch 1250/1264: Loss = 0.1536
2025-12-26 16:17:28,924 - INFO - Epoch 61/160: Train Loss: 0.1669, Val Loss: 0.9495, Val mIoU: 43.50%, Val PixelAcc: 79.99%
2025-12-26 16:18:03,819 - INFO -   Batch 50/1264: Loss = 0.1658
2025-12-26 16:18:37,482 - INFO -   Batch 100/1264: Loss = 0.1351
2025-12-26 16:19:11,162 - INFO -   Batch 150/1264: Loss = 0.1581
2025-12-26 16:19:44,823 - INFO -   Batch 200/1264: Loss = 0.1450
2025-12-26 16:20:18,488 - INFO -   Batch 250/1264: Loss = 0.1446
2025-12-26 16:20:52,138 - INFO -   Batch 300/1264: Loss = 0.1929
2025-12-26 16:21:25,850 - INFO -   Batch 350/1264: Loss = 0.1582
2025-12-26 16:21:59,550 - INFO -   Batch 400/1264: Loss = 0.1627
2025-12-26 16:22:33,263 - INFO -   Batch 450/1264: Loss = 0.1345
2025-12-26 16:23:06,932 - INFO -   Batch 500/1264: Loss = 0.1461
2025-12-26 16:23:40,617 - INFO -   Batch 550/1264: Loss = 0.1359
2025-12-26 16:24:14,278 - INFO -   Batch 600/1264: Loss = 0.1285
2025-12-26 16:24:47,985 - INFO -   Batch 650/1264: Loss = 0.2168
2025-12-26 16:25:21,703 - INFO -   Batch 700/1264: Loss = 0.1586
2025-12-26 16:25:55,447 - INFO -   Batch 750/1264: Loss = 0.1719
2025-12-26 16:26:29,180 - INFO -   Batch 800/1264: Loss = 0.2093
2025-12-26 16:27:02,935 - INFO -   Batch 850/1264: Loss = 0.1269
2025-12-26 16:27:36,687 - INFO -   Batch 900/1264: Loss = 0.1280
2025-12-26 16:28:10,440 - INFO -   Batch 950/1264: Loss = 0.1458
2025-12-26 16:28:44,198 - INFO -   Batch 1000/1264: Loss = 0.1585
2025-12-26 16:29:18,047 - INFO -   Batch 1050/1264: Loss = 0.1306
2025-12-26 16:29:51,960 - INFO -   Batch 1100/1264: Loss = 0.2021
2025-12-26 16:30:25,930 - INFO -   Batch 1150/1264: Loss = 0.1875
2025-12-26 16:30:59,853 - INFO -   Batch 1200/1264: Loss = 0.1342
2025-12-26 16:31:33,689 - INFO -   Batch 1250/1264: Loss = 0.2072
2025-12-26 16:34:33,193 - INFO - Epoch 62/160: Train Loss: 0.1652, Val Loss: 0.9521, Val mIoU: 43.49%, Val PixelAcc: 80.00%
2025-12-26 16:35:08,349 - INFO -   Batch 50/1264: Loss = 0.1291
2025-12-26 16:35:42,091 - INFO -   Batch 100/1264: Loss = 0.1175
2025-12-26 16:36:15,803 - INFO -   Batch 150/1264: Loss = 0.1528
2025-12-26 16:36:49,506 - INFO -   Batch 200/1264: Loss = 0.1001
2025-12-26 16:37:23,208 - INFO -   Batch 250/1264: Loss = 0.1526
2025-12-26 16:37:56,896 - INFO -   Batch 300/1264: Loss = 0.1159
2025-12-26 16:38:30,578 - INFO -   Batch 350/1264: Loss = 0.2465
2025-12-26 16:39:04,250 - INFO -   Batch 400/1264: Loss = 0.1332
2025-12-26 16:39:37,944 - INFO -   Batch 450/1264: Loss = 0.2444
2025-12-26 16:40:11,654 - INFO -   Batch 500/1264: Loss = 0.2216
2025-12-26 16:40:45,443 - INFO -   Batch 550/1264: Loss = 0.1374
2025-12-26 16:41:19,317 - INFO -   Batch 600/1264: Loss = 0.1381
2025-12-26 16:41:53,188 - INFO -   Batch 650/1264: Loss = 0.1539
2025-12-26 16:42:26,996 - INFO -   Batch 700/1264: Loss = 0.1606
2025-12-26 16:43:00,786 - INFO -   Batch 750/1264: Loss = 0.2270
2025-12-26 16:43:34,554 - INFO -   Batch 800/1264: Loss = 0.1915
2025-12-26 16:44:08,301 - INFO -   Batch 850/1264: Loss = 0.2003
2025-12-26 16:44:41,994 - INFO -   Batch 900/1264: Loss = 0.1603
2025-12-26 16:45:15,683 - INFO -   Batch 950/1264: Loss = 0.1435
2025-12-26 16:45:49,390 - INFO -   Batch 1000/1264: Loss = 0.1720
2025-12-26 16:46:23,164 - INFO -   Batch 1050/1264: Loss = 0.1819
2025-12-26 16:46:56,967 - INFO -   Batch 1100/1264: Loss = 0.1822
2025-12-26 16:47:30,740 - INFO -   Batch 1150/1264: Loss = 0.1569
2025-12-26 16:48:04,463 - INFO -   Batch 1200/1264: Loss = 0.1285
2025-12-26 16:48:38,185 - INFO -   Batch 1250/1264: Loss = 0.1928
2025-12-26 16:51:37,736 - INFO - Epoch 63/160: Train Loss: 0.1654, Val Loss: 0.9555, Val mIoU: 43.68%, Val PixelAcc: 79.87%
2025-12-26 16:52:13,025 - INFO -   Batch 50/1264: Loss = 0.1433
2025-12-26 16:52:46,698 - INFO -   Batch 100/1264: Loss = 0.1766
2025-12-26 16:53:20,351 - INFO -   Batch 150/1264: Loss = 0.2152
2025-12-26 16:53:54,036 - INFO -   Batch 200/1264: Loss = 0.1217
2025-12-26 16:54:27,711 - INFO -   Batch 250/1264: Loss = 0.1977
2025-12-26 16:55:01,388 - INFO -   Batch 300/1264: Loss = 0.1958
2025-12-26 16:55:35,039 - INFO -   Batch 350/1264: Loss = 0.1432
2025-12-26 16:56:08,709 - INFO -   Batch 400/1264: Loss = 0.1749
2025-12-26 16:56:42,360 - INFO -   Batch 450/1264: Loss = 0.1404
2025-12-26 16:57:16,064 - INFO -   Batch 500/1264: Loss = 0.1459
2025-12-26 16:57:49,781 - INFO -   Batch 550/1264: Loss = 0.1852
2025-12-26 16:58:23,493 - INFO -   Batch 600/1264: Loss = 0.1866
2025-12-26 16:58:57,248 - INFO -   Batch 650/1264: Loss = 0.1418
2025-12-26 16:59:31,053 - INFO -   Batch 700/1264: Loss = 0.1631
2025-12-26 17:00:04,851 - INFO -   Batch 750/1264: Loss = 0.1216
2025-12-26 17:00:38,652 - INFO -   Batch 800/1264: Loss = 0.1934
2025-12-26 17:01:12,422 - INFO -   Batch 850/1264: Loss = 0.1598
2025-12-26 17:01:46,161 - INFO -   Batch 900/1264: Loss = 0.1587
2025-12-26 17:02:19,889 - INFO -   Batch 950/1264: Loss = 0.1610
2025-12-26 17:02:53,592 - INFO -   Batch 1000/1264: Loss = 0.1357
2025-12-26 17:03:27,275 - INFO -   Batch 1050/1264: Loss = 0.1709
2025-12-26 17:04:00,954 - INFO -   Batch 1100/1264: Loss = 0.1820
2025-12-26 17:04:34,615 - INFO -   Batch 1150/1264: Loss = 0.1603
2025-12-26 17:05:08,300 - INFO -   Batch 1200/1264: Loss = 0.1694
2025-12-26 17:05:41,986 - INFO -   Batch 1250/1264: Loss = 0.1552
2025-12-26 17:08:41,226 - INFO - Epoch 64/160: Train Loss: 0.1654, Val Loss: 0.9731, Val mIoU: 43.35%, Val PixelAcc: 79.54%
2025-12-26 17:09:16,553 - INFO -   Batch 50/1264: Loss = 0.1493
2025-12-26 17:09:50,171 - INFO -   Batch 100/1264: Loss = 0.1643
2025-12-26 17:10:23,874 - INFO -   Batch 150/1264: Loss = 0.1329
2025-12-26 17:10:57,617 - INFO -   Batch 200/1264: Loss = 0.1686
2025-12-26 17:11:31,333 - INFO -   Batch 250/1264: Loss = 0.1469
2025-12-26 17:12:05,063 - INFO -   Batch 300/1264: Loss = 0.1496
2025-12-26 17:12:38,756 - INFO -   Batch 350/1264: Loss = 0.1681
2025-12-26 17:13:12,446 - INFO -   Batch 400/1264: Loss = 0.1266
2025-12-26 17:13:46,102 - INFO -   Batch 450/1264: Loss = 0.1094
2025-12-26 17:14:19,778 - INFO -   Batch 500/1264: Loss = 0.1569
2025-12-26 17:14:53,419 - INFO -   Batch 550/1264: Loss = 0.1520
2025-12-26 17:15:27,102 - INFO -   Batch 600/1264: Loss = 0.1312
2025-12-26 17:16:00,796 - INFO -   Batch 650/1264: Loss = 0.1274
2025-12-26 17:16:34,505 - INFO -   Batch 700/1264: Loss = 0.1568
2025-12-26 17:17:08,181 - INFO -   Batch 750/1264: Loss = 0.1214
2025-12-26 17:17:41,869 - INFO -   Batch 800/1264: Loss = 0.1690
2025-12-26 17:18:15,533 - INFO -   Batch 850/1264: Loss = 0.1447
2025-12-26 17:18:49,205 - INFO -   Batch 900/1264: Loss = 0.1743
2025-12-26 17:19:22,884 - INFO -   Batch 950/1264: Loss = 0.1810
2025-12-26 17:19:56,588 - INFO -   Batch 1000/1264: Loss = 0.1328
2025-12-26 17:20:30,280 - INFO -   Batch 1050/1264: Loss = 0.1320
2025-12-26 17:21:03,966 - INFO -   Batch 1100/1264: Loss = 0.1635
2025-12-26 17:21:37,641 - INFO -   Batch 1150/1264: Loss = 0.1598
2025-12-26 17:22:11,371 - INFO -   Batch 1200/1264: Loss = 0.1396
2025-12-26 17:22:45,116 - INFO -   Batch 1250/1264: Loss = 0.1476
2025-12-26 17:25:44,112 - INFO - Epoch 65/160: Train Loss: 0.1597, Val Loss: 0.9592, Val mIoU: 43.69%, Val PixelAcc: 79.99%
2025-12-26 17:26:19,232 - INFO -   Batch 50/1264: Loss = 0.1984
2025-12-26 17:26:52,911 - INFO -   Batch 100/1264: Loss = 0.1496
2025-12-26 17:27:26,588 - INFO -   Batch 150/1264: Loss = 0.1559
2025-12-26 17:28:00,283 - INFO -   Batch 200/1264: Loss = 0.1273
2025-12-26 17:28:33,985 - INFO -   Batch 250/1264: Loss = 0.1303
2025-12-26 17:29:07,651 - INFO -   Batch 300/1264: Loss = 0.1476
2025-12-26 17:29:41,333 - INFO -   Batch 350/1264: Loss = 0.1211
2025-12-26 17:30:15,026 - INFO -   Batch 400/1264: Loss = 0.1697
2025-12-26 17:30:48,709 - INFO -   Batch 450/1264: Loss = 0.1577
2025-12-26 17:31:22,380 - INFO -   Batch 500/1264: Loss = 0.1376
2025-12-26 17:31:56,066 - INFO -   Batch 550/1264: Loss = 0.1644
2025-12-26 17:32:29,754 - INFO -   Batch 600/1264: Loss = 0.1763
2025-12-26 17:33:03,485 - INFO -   Batch 650/1264: Loss = 0.1721
2025-12-26 17:33:37,203 - INFO -   Batch 700/1264: Loss = 0.1313
2025-12-26 17:34:10,890 - INFO -   Batch 750/1264: Loss = 0.1417
2025-12-26 17:34:44,571 - INFO -   Batch 800/1264: Loss = 0.1442
2025-12-26 17:35:18,239 - INFO -   Batch 850/1264: Loss = 0.1470
2025-12-26 17:35:51,912 - INFO -   Batch 900/1264: Loss = 0.1341
2025-12-26 17:36:25,599 - INFO -   Batch 950/1264: Loss = 0.1123
2025-12-26 17:36:59,284 - INFO -   Batch 1000/1264: Loss = 0.2121
2025-12-26 17:37:32,997 - INFO -   Batch 1050/1264: Loss = 0.1363
2025-12-26 17:38:06,727 - INFO -   Batch 1100/1264: Loss = 0.1806
2025-12-26 17:38:40,490 - INFO -   Batch 1150/1264: Loss = 0.1655
2025-12-26 17:39:14,225 - INFO -   Batch 1200/1264: Loss = 0.1385
2025-12-26 17:39:47,920 - INFO -   Batch 1250/1264: Loss = 0.1472
2025-12-26 17:42:47,568 - INFO - Epoch 66/160: Train Loss: 0.1566, Val Loss: 0.9656, Val mIoU: 43.58%, Val PixelAcc: 79.95%
2025-12-26 17:43:23,157 - INFO -   Batch 50/1264: Loss = 0.1930
2025-12-26 17:43:56,810 - INFO -   Batch 100/1264: Loss = 0.1733
2025-12-26 17:44:30,463 - INFO -   Batch 150/1264: Loss = 0.1694
2025-12-26 17:45:04,130 - INFO -   Batch 200/1264: Loss = 0.1465
2025-12-26 17:45:37,791 - INFO -   Batch 250/1264: Loss = 0.1527
2025-12-26 17:46:11,447 - INFO -   Batch 300/1264: Loss = 0.1507
2025-12-26 17:46:45,102 - INFO -   Batch 350/1264: Loss = 0.1452
2025-12-26 17:47:17,945 - INFO -   Batch 400/1264: Loss = 0.1816
2025-12-26 17:47:50,584 - INFO -   Batch 450/1264: Loss = 0.1243
2025-12-26 17:48:23,220 - INFO -   Batch 500/1264: Loss = 0.1357
2025-12-26 17:48:55,853 - INFO -   Batch 550/1264: Loss = 0.1321
2025-12-26 17:49:28,484 - INFO -   Batch 600/1264: Loss = 0.1482
2025-12-26 17:50:01,120 - INFO -   Batch 650/1264: Loss = 0.1877
2025-12-26 17:50:33,758 - INFO -   Batch 700/1264: Loss = 0.1747
2025-12-26 17:51:06,398 - INFO -   Batch 750/1264: Loss = 0.2101
2025-12-26 17:51:39,037 - INFO -   Batch 800/1264: Loss = 0.1676
2025-12-26 17:52:11,680 - INFO -   Batch 850/1264: Loss = 0.1779
2025-12-26 17:52:44,322 - INFO -   Batch 900/1264: Loss = 0.1677
2025-12-26 17:53:16,963 - INFO -   Batch 950/1264: Loss = 0.1426
2025-12-26 17:53:49,606 - INFO -   Batch 1000/1264: Loss = 0.1426
2025-12-26 17:54:22,246 - INFO -   Batch 1050/1264: Loss = 0.1271
2025-12-26 17:54:54,886 - INFO -   Batch 1100/1264: Loss = 0.1435
2025-12-26 17:55:27,527 - INFO -   Batch 1150/1264: Loss = 0.1343
2025-12-26 17:56:00,170 - INFO -   Batch 1200/1264: Loss = 0.1686
2025-12-26 17:56:32,811 - INFO -   Batch 1250/1264: Loss = 0.1492
2025-12-26 17:59:31,879 - INFO - Epoch 67/160: Train Loss: 0.1570, Val Loss: 0.9874, Val mIoU: 43.11%, Val PixelAcc: 79.78%
2025-12-26 18:00:06,090 - INFO -   Batch 50/1264: Loss = 0.1430
2025-12-26 18:00:38,734 - INFO -   Batch 100/1264: Loss = 0.1390
2025-12-26 18:01:11,378 - INFO -   Batch 150/1264: Loss = 0.1337
2025-12-26 18:01:44,023 - INFO -   Batch 200/1264: Loss = 0.2051
2025-12-26 18:02:16,668 - INFO -   Batch 250/1264: Loss = 0.1375
2025-12-26 18:02:49,315 - INFO -   Batch 300/1264: Loss = 0.1618
2025-12-26 18:03:21,961 - INFO -   Batch 350/1264: Loss = 0.1568
2025-12-26 18:03:54,606 - INFO -   Batch 400/1264: Loss = 0.1538
2025-12-26 18:04:27,252 - INFO -   Batch 450/1264: Loss = 0.1552
2025-12-26 18:04:59,896 - INFO -   Batch 500/1264: Loss = 0.1270
2025-12-26 18:05:32,538 - INFO -   Batch 550/1264: Loss = 0.1160
2025-12-26 18:06:05,175 - INFO -   Batch 600/1264: Loss = 0.1386
2025-12-26 18:06:37,815 - INFO -   Batch 650/1264: Loss = 0.1866
2025-12-26 18:07:10,456 - INFO -   Batch 700/1264: Loss = 0.1653
2025-12-26 18:07:43,098 - INFO -   Batch 750/1264: Loss = 0.1852
2025-12-26 18:08:15,744 - INFO -   Batch 800/1264: Loss = 0.1403
2025-12-26 18:08:48,386 - INFO -   Batch 850/1264: Loss = 0.1377
2025-12-26 18:09:21,027 - INFO -   Batch 900/1264: Loss = 0.1823
2025-12-26 18:09:53,670 - INFO -   Batch 950/1264: Loss = 0.1830
2025-12-26 18:10:26,316 - INFO -   Batch 1000/1264: Loss = 0.1136
2025-12-26 18:10:58,964 - INFO -   Batch 1050/1264: Loss = 0.1087
2025-12-26 18:11:31,608 - INFO -   Batch 1100/1264: Loss = 0.1674
2025-12-26 18:12:04,251 - INFO -   Batch 1150/1264: Loss = 0.1411
2025-12-26 18:12:36,893 - INFO -   Batch 1200/1264: Loss = 0.1245
2025-12-26 18:13:09,531 - INFO -   Batch 1250/1264: Loss = 0.1225
2025-12-26 18:16:08,382 - INFO - Epoch 68/160: Train Loss: 0.1537, Val Loss: 0.9584, Val mIoU: 43.28%, Val PixelAcc: 79.83%
2025-12-26 18:16:43,061 - INFO -   Batch 50/1264: Loss = 0.2262
2025-12-26 18:17:15,707 - INFO -   Batch 100/1264: Loss = 0.1644
2025-12-26 18:17:48,358 - INFO -   Batch 150/1264: Loss = 0.0952
2025-12-26 18:18:21,014 - INFO -   Batch 200/1264: Loss = 0.2066
2025-12-26 18:18:53,664 - INFO -   Batch 250/1264: Loss = 0.1119
2025-12-26 18:19:26,316 - INFO -   Batch 300/1264: Loss = 0.2013
2025-12-26 18:19:58,967 - INFO -   Batch 350/1264: Loss = 0.1686
2025-12-26 18:20:31,616 - INFO -   Batch 400/1264: Loss = 0.1346
2025-12-26 18:21:04,263 - INFO -   Batch 450/1264: Loss = 0.1458
2025-12-26 18:21:36,909 - INFO -   Batch 500/1264: Loss = 0.1490
2025-12-26 18:22:09,557 - INFO -   Batch 550/1264: Loss = 0.1804
2025-12-26 18:22:42,201 - INFO -   Batch 600/1264: Loss = 0.1751
2025-12-26 18:23:14,842 - INFO -   Batch 650/1264: Loss = 0.1242
2025-12-26 18:23:47,483 - INFO -   Batch 700/1264: Loss = 0.1146
2025-12-26 18:24:20,125 - INFO -   Batch 750/1264: Loss = 0.1294
2025-12-26 18:24:52,766 - INFO -   Batch 800/1264: Loss = 0.1348
2025-12-26 18:25:25,405 - INFO -   Batch 850/1264: Loss = 0.1697
2025-12-26 18:25:58,047 - INFO -   Batch 900/1264: Loss = 0.1741
2025-12-26 18:26:30,691 - INFO -   Batch 950/1264: Loss = 0.1263
2025-12-26 18:27:03,349 - INFO -   Batch 1000/1264: Loss = 0.1353
2025-12-26 18:27:35,998 - INFO -   Batch 1050/1264: Loss = 0.1536
2025-12-26 18:28:08,644 - INFO -   Batch 1100/1264: Loss = 0.1804
2025-12-26 18:28:41,290 - INFO -   Batch 1150/1264: Loss = 0.1517
2025-12-26 18:29:13,931 - INFO -   Batch 1200/1264: Loss = 0.1356
2025-12-26 18:29:46,579 - INFO -   Batch 1250/1264: Loss = 0.1560
2025-12-26 18:32:45,153 - INFO - Epoch 69/160: Train Loss: 0.1556, Val Loss: 0.9781, Val mIoU: 43.11%, Val PixelAcc: 79.86%
2025-12-26 18:33:19,060 - INFO -   Batch 50/1264: Loss = 0.1876
2025-12-26 18:33:51,704 - INFO -   Batch 100/1264: Loss = 0.1364
2025-12-26 18:34:24,353 - INFO -   Batch 150/1264: Loss = 0.1404
2025-12-26 18:34:57,002 - INFO -   Batch 200/1264: Loss = 0.1074
2025-12-26 18:35:29,647 - INFO -   Batch 250/1264: Loss = 0.1331
2025-12-26 18:36:02,292 - INFO -   Batch 300/1264: Loss = 0.1512
2025-12-26 18:36:34,934 - INFO -   Batch 350/1264: Loss = 0.0964
2025-12-26 18:37:07,580 - INFO -   Batch 400/1264: Loss = 0.1564
2025-12-26 18:37:40,224 - INFO -   Batch 450/1264: Loss = 0.1655
2025-12-26 18:38:12,868 - INFO -   Batch 500/1264: Loss = 0.1614
2025-12-26 18:38:45,510 - INFO -   Batch 550/1264: Loss = 0.1222
2025-12-26 18:39:18,150 - INFO -   Batch 600/1264: Loss = 0.1372
2025-12-26 18:39:50,790 - INFO -   Batch 650/1264: Loss = 0.1314
2025-12-26 18:40:23,434 - INFO -   Batch 700/1264: Loss = 0.1815
2025-12-26 18:40:56,079 - INFO -   Batch 750/1264: Loss = 0.1500
2025-12-26 18:41:28,718 - INFO -   Batch 800/1264: Loss = 0.1413
2025-12-26 18:42:01,358 - INFO -   Batch 850/1264: Loss = 0.1303
2025-12-26 18:42:33,997 - INFO -   Batch 900/1264: Loss = 0.1531
2025-12-26 18:43:06,639 - INFO -   Batch 950/1264: Loss = 0.1847
2025-12-26 18:43:39,280 - INFO -   Batch 1000/1264: Loss = 0.1300
2025-12-26 18:44:11,948 - INFO -   Batch 1050/1264: Loss = 0.1615
2025-12-26 18:44:44,589 - INFO -   Batch 1100/1264: Loss = 0.1960
2025-12-26 18:45:17,228 - INFO -   Batch 1150/1264: Loss = 0.1527
2025-12-26 18:45:49,869 - INFO -   Batch 1200/1264: Loss = 0.1292
2025-12-26 18:46:22,510 - INFO -   Batch 1250/1264: Loss = 0.1911
2025-12-26 18:49:21,579 - INFO - Epoch 70/160: Train Loss: 0.1498, Val Loss: 0.9814, Val mIoU: 43.12%, Val PixelAcc: 80.16%
2025-12-26 18:49:23,107 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_70.pth
2025-12-26 18:49:23,109 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_70.pth
2025-12-26 18:49:57,773 - INFO -   Batch 50/1264: Loss = 0.1296
2025-12-26 18:50:30,424 - INFO -   Batch 100/1264: Loss = 0.1297
2025-12-26 18:51:03,064 - INFO -   Batch 150/1264: Loss = 0.1397
2025-12-26 18:51:35,702 - INFO -   Batch 200/1264: Loss = 0.1470
2025-12-26 18:52:08,342 - INFO -   Batch 250/1264: Loss = 0.1369
2025-12-26 18:52:40,982 - INFO -   Batch 300/1264: Loss = 0.1639
2025-12-26 18:53:13,625 - INFO -   Batch 350/1264: Loss = 0.1206
2025-12-26 18:53:46,268 - INFO -   Batch 400/1264: Loss = 0.1687
2025-12-26 18:54:18,908 - INFO -   Batch 450/1264: Loss = 0.1887
2025-12-26 18:54:51,550 - INFO -   Batch 500/1264: Loss = 0.1608
2025-12-26 18:55:24,191 - INFO -   Batch 550/1264: Loss = 0.1251
2025-12-26 18:55:56,850 - INFO -   Batch 600/1264: Loss = 0.2101
2025-12-26 18:56:29,819 - INFO -   Batch 650/1264: Loss = 0.1835
2025-12-26 18:57:02,561 - INFO -   Batch 700/1264: Loss = 0.1192
2025-12-26 18:57:35,203 - INFO -   Batch 750/1264: Loss = 0.1922
2025-12-26 18:58:07,844 - INFO -   Batch 800/1264: Loss = 0.1440
2025-12-26 18:58:40,483 - INFO -   Batch 850/1264: Loss = 0.1135
2025-12-26 18:59:13,132 - INFO -   Batch 900/1264: Loss = 0.1376
2025-12-26 18:59:45,774 - INFO -   Batch 950/1264: Loss = 0.1257
2025-12-26 19:00:18,412 - INFO -   Batch 1000/1264: Loss = 0.1311
2025-12-26 19:00:51,050 - INFO -   Batch 1050/1264: Loss = 0.1180
2025-12-26 19:01:23,687 - INFO -   Batch 1100/1264: Loss = 0.1634
2025-12-26 19:01:56,329 - INFO -   Batch 1150/1264: Loss = 0.1727
2025-12-26 19:02:28,980 - INFO -   Batch 1200/1264: Loss = 0.1727
2025-12-26 19:03:01,624 - INFO -   Batch 1250/1264: Loss = 0.1119
2025-12-26 19:06:00,520 - INFO - Epoch 71/160: Train Loss: 0.1492, Val Loss: 0.9891, Val mIoU: 42.99%, Val PixelAcc: 80.06%
2025-12-26 19:06:34,578 - INFO -   Batch 50/1264: Loss = 0.1580
2025-12-26 19:07:07,223 - INFO -   Batch 100/1264: Loss = 0.1506
2025-12-26 19:07:39,871 - INFO -   Batch 150/1264: Loss = 0.1714
2025-12-26 19:08:12,517 - INFO -   Batch 200/1264: Loss = 0.1287
2025-12-26 19:08:45,164 - INFO -   Batch 250/1264: Loss = 0.1557
2025-12-26 19:09:17,810 - INFO -   Batch 300/1264: Loss = 0.1439
2025-12-26 19:09:50,453 - INFO -   Batch 350/1264: Loss = 0.1684
2025-12-26 19:10:23,098 - INFO -   Batch 400/1264: Loss = 0.1769
2025-12-26 19:10:55,746 - INFO -   Batch 450/1264: Loss = 0.1730
2025-12-26 19:11:28,397 - INFO -   Batch 500/1264: Loss = 0.1264
2025-12-26 19:12:01,047 - INFO -   Batch 550/1264: Loss = 0.1765
2025-12-26 19:12:33,697 - INFO -   Batch 600/1264: Loss = 0.1563
2025-12-26 19:13:06,347 - INFO -   Batch 650/1264: Loss = 0.1468
2025-12-26 19:13:38,992 - INFO -   Batch 700/1264: Loss = 0.1454
2025-12-26 19:14:11,632 - INFO -   Batch 750/1264: Loss = 0.1704
2025-12-26 19:14:44,275 - INFO -   Batch 800/1264: Loss = 0.1071
2025-12-26 19:15:16,919 - INFO -   Batch 850/1264: Loss = 0.1494
2025-12-26 19:15:49,559 - INFO -   Batch 900/1264: Loss = 0.1778
2025-12-26 19:16:22,200 - INFO -   Batch 950/1264: Loss = 0.1196
2025-12-26 19:16:54,841 - INFO -   Batch 1000/1264: Loss = 0.1605
2025-12-26 19:17:27,481 - INFO -   Batch 1050/1264: Loss = 0.1494
2025-12-26 19:18:00,120 - INFO -   Batch 1100/1264: Loss = 0.1754
2025-12-26 19:18:32,760 - INFO -   Batch 1150/1264: Loss = 0.1233
2025-12-26 19:19:05,403 - INFO -   Batch 1200/1264: Loss = 0.1100
2025-12-26 19:19:38,048 - INFO -   Batch 1250/1264: Loss = 0.1506
2025-12-26 19:22:37,057 - INFO - Epoch 72/160: Train Loss: 0.1481, Val Loss: 0.9937, Val mIoU: 43.15%, Val PixelAcc: 79.86%
2025-12-26 19:23:11,189 - INFO -   Batch 50/1264: Loss = 0.1550
2025-12-26 19:23:43,834 - INFO -   Batch 100/1264: Loss = 0.1341
2025-12-26 19:24:16,478 - INFO -   Batch 150/1264: Loss = 0.1791
2025-12-26 19:24:49,122 - INFO -   Batch 200/1264: Loss = 0.1426
2025-12-26 19:25:21,769 - INFO -   Batch 250/1264: Loss = 0.1520
2025-12-26 19:25:54,416 - INFO -   Batch 300/1264: Loss = 0.1232
2025-12-26 19:26:27,062 - INFO -   Batch 350/1264: Loss = 0.1853
2025-12-26 19:26:59,709 - INFO -   Batch 400/1264: Loss = 0.1542
2025-12-26 19:27:32,352 - INFO -   Batch 450/1264: Loss = 0.1181
2025-12-26 19:28:04,998 - INFO -   Batch 500/1264: Loss = 0.1406
2025-12-26 19:28:37,642 - INFO -   Batch 550/1264: Loss = 0.1634
2025-12-26 19:29:10,287 - INFO -   Batch 600/1264: Loss = 0.1143
2025-12-26 19:29:42,927 - INFO -   Batch 650/1264: Loss = 0.1278
2025-12-26 19:30:15,565 - INFO -   Batch 700/1264: Loss = 0.1719
2025-12-26 19:30:48,201 - INFO -   Batch 750/1264: Loss = 0.1473
2025-12-26 19:31:20,839 - INFO -   Batch 800/1264: Loss = 0.1828
2025-12-26 19:31:53,481 - INFO -   Batch 850/1264: Loss = 0.1744
2025-12-26 19:32:26,121 - INFO -   Batch 900/1264: Loss = 0.1748
2025-12-26 19:32:58,766 - INFO -   Batch 950/1264: Loss = 0.1339
2025-12-26 19:33:31,409 - INFO -   Batch 1000/1264: Loss = 0.1555
2025-12-26 19:34:04,050 - INFO -   Batch 1050/1264: Loss = 0.1413
2025-12-26 19:34:36,695 - INFO -   Batch 1100/1264: Loss = 0.1462
2025-12-26 19:35:09,340 - INFO -   Batch 1150/1264: Loss = 0.1461
2025-12-26 19:35:41,987 - INFO -   Batch 1200/1264: Loss = 0.1561
2025-12-26 19:36:14,628 - INFO -   Batch 1250/1264: Loss = 0.1208
2025-12-26 19:39:13,163 - INFO - Epoch 73/160: Train Loss: 0.1471, Val Loss: 0.9952, Val mIoU: 43.21%, Val PixelAcc: 80.02%
2025-12-26 19:39:47,895 - INFO -   Batch 50/1264: Loss = 0.1348
2025-12-26 19:40:20,540 - INFO -   Batch 100/1264: Loss = 0.1531
2025-12-26 19:40:53,182 - INFO -   Batch 150/1264: Loss = 0.1301
2025-12-26 19:41:25,825 - INFO -   Batch 200/1264: Loss = 0.1410
2025-12-26 19:41:58,472 - INFO -   Batch 250/1264: Loss = 0.1906
2025-12-26 19:42:31,115 - INFO -   Batch 300/1264: Loss = 0.1245
2025-12-26 19:43:03,764 - INFO -   Batch 350/1264: Loss = 0.1465
2025-12-26 19:43:36,414 - INFO -   Batch 400/1264: Loss = 0.1779
2025-12-26 19:44:09,059 - INFO -   Batch 450/1264: Loss = 0.1616
2025-12-26 19:44:41,701 - INFO -   Batch 500/1264: Loss = 0.1450
2025-12-26 19:45:14,343 - INFO -   Batch 550/1264: Loss = 0.1345
2025-12-26 19:45:46,990 - INFO -   Batch 600/1264: Loss = 0.1813
2025-12-26 19:46:19,635 - INFO -   Batch 650/1264: Loss = 0.1376
2025-12-26 19:46:52,281 - INFO -   Batch 700/1264: Loss = 0.1319
2025-12-26 19:47:24,927 - INFO -   Batch 750/1264: Loss = 0.1899
2025-12-26 19:47:57,571 - INFO -   Batch 800/1264: Loss = 0.1392
2025-12-26 19:48:30,217 - INFO -   Batch 850/1264: Loss = 0.1383
2025-12-26 19:49:02,861 - INFO -   Batch 900/1264: Loss = 0.1597
2025-12-26 19:49:35,503 - INFO -   Batch 950/1264: Loss = 0.1116
2025-12-26 19:50:08,144 - INFO -   Batch 1000/1264: Loss = 0.1602
2025-12-26 19:50:40,790 - INFO -   Batch 1050/1264: Loss = 0.1568
2025-12-26 19:51:13,441 - INFO -   Batch 1100/1264: Loss = 0.1338
2025-12-26 19:51:46,095 - INFO -   Batch 1150/1264: Loss = 0.1246
2025-12-26 19:52:18,742 - INFO -   Batch 1200/1264: Loss = 0.1470
2025-12-26 19:52:51,390 - INFO -   Batch 1250/1264: Loss = 0.0944
2025-12-26 19:55:50,259 - INFO - Epoch 74/160: Train Loss: 0.1444, Val Loss: 0.9804, Val mIoU: 43.65%, Val PixelAcc: 80.20%
2025-12-26 19:56:26,029 - INFO -   Batch 50/1264: Loss = 0.1100
2025-12-26 19:56:58,680 - INFO -   Batch 100/1264: Loss = 0.1232
2025-12-26 19:57:31,329 - INFO -   Batch 150/1264: Loss = 0.1302
2025-12-26 19:58:03,981 - INFO -   Batch 200/1264: Loss = 0.1027
2025-12-26 19:58:36,632 - INFO -   Batch 250/1264: Loss = 0.1363
2025-12-26 19:59:09,281 - INFO -   Batch 300/1264: Loss = 0.1398
2025-12-26 19:59:41,931 - INFO -   Batch 350/1264: Loss = 0.1234
2025-12-26 20:00:14,583 - INFO -   Batch 400/1264: Loss = 0.1253
2025-12-26 20:00:47,231 - INFO -   Batch 450/1264: Loss = 0.1502
2025-12-26 20:01:19,877 - INFO -   Batch 500/1264: Loss = 0.1565
2025-12-26 20:01:52,524 - INFO -   Batch 550/1264: Loss = 0.1153
2025-12-26 20:02:25,170 - INFO -   Batch 600/1264: Loss = 0.1458
2025-12-26 20:02:57,814 - INFO -   Batch 650/1264: Loss = 0.1414
2025-12-26 20:03:30,460 - INFO -   Batch 700/1264: Loss = 0.1228
2025-12-26 20:04:03,105 - INFO -   Batch 750/1264: Loss = 0.1597
2025-12-26 20:04:35,750 - INFO -   Batch 800/1264: Loss = 0.1647
2025-12-26 20:05:08,392 - INFO -   Batch 850/1264: Loss = 0.0979
2025-12-26 20:05:41,036 - INFO -   Batch 900/1264: Loss = 0.1263
2025-12-26 20:06:13,680 - INFO -   Batch 950/1264: Loss = 0.1311
2025-12-26 20:06:46,321 - INFO -   Batch 1000/1264: Loss = 0.1959
2025-12-26 20:07:18,962 - INFO -   Batch 1050/1264: Loss = 0.2014
2025-12-26 20:07:51,604 - INFO -   Batch 1100/1264: Loss = 0.1810
2025-12-26 20:08:24,247 - INFO -   Batch 1150/1264: Loss = 0.1343
2025-12-26 20:08:56,892 - INFO -   Batch 1200/1264: Loss = 0.1839
2025-12-26 20:09:29,536 - INFO -   Batch 1250/1264: Loss = 0.1614
2025-12-26 20:12:28,371 - INFO - Epoch 75/160: Train Loss: 0.1417, Val Loss: 0.9845, Val mIoU: 43.70%, Val PixelAcc: 80.01%
2025-12-26 20:13:02,730 - INFO -   Batch 50/1264: Loss = 0.1275
2025-12-26 20:13:35,375 - INFO -   Batch 100/1264: Loss = 0.0996
2025-12-26 20:14:08,021 - INFO -   Batch 150/1264: Loss = 0.1143
2025-12-26 20:14:40,666 - INFO -   Batch 200/1264: Loss = 0.1376
2025-12-26 20:15:13,310 - INFO -   Batch 250/1264: Loss = 0.1284
2025-12-26 20:15:45,958 - INFO -   Batch 300/1264: Loss = 0.1184
2025-12-26 20:16:18,602 - INFO -   Batch 350/1264: Loss = 0.1851
2025-12-26 20:16:51,244 - INFO -   Batch 400/1264: Loss = 0.1036
2025-12-26 20:17:23,889 - INFO -   Batch 450/1264: Loss = 0.1442
2025-12-26 20:17:56,533 - INFO -   Batch 500/1264: Loss = 0.2117
2025-12-26 20:18:29,175 - INFO -   Batch 550/1264: Loss = 0.1282
2025-12-26 20:19:01,820 - INFO -   Batch 600/1264: Loss = 0.1635
2025-12-26 20:19:34,466 - INFO -   Batch 650/1264: Loss = 0.2043
2025-12-26 20:20:07,112 - INFO -   Batch 700/1264: Loss = 0.1670
2025-12-26 20:20:39,757 - INFO -   Batch 750/1264: Loss = 0.1937
2025-12-26 20:21:12,401 - INFO -   Batch 800/1264: Loss = 0.0958
2025-12-26 20:21:45,048 - INFO -   Batch 850/1264: Loss = 0.1159
2025-12-26 20:22:17,691 - INFO -   Batch 900/1264: Loss = 0.1475
2025-12-26 20:22:50,332 - INFO -   Batch 950/1264: Loss = 0.1297
2025-12-26 20:23:22,974 - INFO -   Batch 1000/1264: Loss = 0.0916
2025-12-26 20:23:55,616 - INFO -   Batch 1050/1264: Loss = 0.1386
2025-12-26 20:24:28,261 - INFO -   Batch 1100/1264: Loss = 0.1900
2025-12-26 20:25:00,908 - INFO -   Batch 1150/1264: Loss = 0.1732
2025-12-26 20:25:33,555 - INFO -   Batch 1200/1264: Loss = 0.1485
2025-12-26 20:26:06,204 - INFO -   Batch 1250/1264: Loss = 0.1313
2025-12-26 20:29:05,169 - INFO - Epoch 76/160: Train Loss: 0.1398, Val Loss: 0.9925, Val mIoU: 43.33%, Val PixelAcc: 80.27%
2025-12-26 20:29:39,558 - INFO -   Batch 50/1264: Loss = 0.1265
2025-12-26 20:30:12,210 - INFO -   Batch 100/1264: Loss = 0.1350
2025-12-26 20:30:44,859 - INFO -   Batch 150/1264: Loss = 0.1466
2025-12-26 20:31:17,507 - INFO -   Batch 200/1264: Loss = 0.1536
2025-12-26 20:31:50,154 - INFO -   Batch 250/1264: Loss = 0.1270
2025-12-26 20:32:22,798 - INFO -   Batch 300/1264: Loss = 0.0794
2025-12-26 20:32:55,445 - INFO -   Batch 350/1264: Loss = 0.1503
2025-12-26 20:33:28,090 - INFO -   Batch 400/1264: Loss = 0.1264
2025-12-26 20:34:00,732 - INFO -   Batch 450/1264: Loss = 0.1399
2025-12-26 20:34:33,378 - INFO -   Batch 500/1264: Loss = 0.1389
2025-12-26 20:35:06,025 - INFO -   Batch 550/1264: Loss = 0.1302
2025-12-26 20:35:38,668 - INFO -   Batch 600/1264: Loss = 0.1386
2025-12-26 20:36:11,310 - INFO -   Batch 650/1264: Loss = 0.1248
2025-12-26 20:36:43,950 - INFO -   Batch 700/1264: Loss = 0.1095
2025-12-26 20:37:16,590 - INFO -   Batch 750/1264: Loss = 0.1377
2025-12-26 20:37:49,230 - INFO -   Batch 800/1264: Loss = 0.1316
2025-12-26 20:38:21,869 - INFO -   Batch 850/1264: Loss = 0.0937
2025-12-26 20:38:54,508 - INFO -   Batch 900/1264: Loss = 0.1291
2025-12-26 20:39:27,148 - INFO -   Batch 950/1264: Loss = 0.1639
2025-12-26 20:39:59,788 - INFO -   Batch 1000/1264: Loss = 0.1435
2025-12-26 20:40:32,428 - INFO -   Batch 1050/1264: Loss = 0.1620
2025-12-26 20:41:05,068 - INFO -   Batch 1100/1264: Loss = 0.1388
2025-12-26 20:41:37,704 - INFO -   Batch 1150/1264: Loss = 0.1659
2025-12-26 20:42:10,341 - INFO -   Batch 1200/1264: Loss = 0.1650
2025-12-26 20:42:42,980 - INFO -   Batch 1250/1264: Loss = 0.1729
2025-12-26 20:45:41,337 - INFO - Epoch 77/160: Train Loss: 0.1383, Val Loss: 0.9833, Val mIoU: 43.56%, Val PixelAcc: 80.28%
2025-12-26 20:46:15,884 - INFO -   Batch 50/1264: Loss = 0.1460
2025-12-26 20:46:48,526 - INFO -   Batch 100/1264: Loss = 0.1079
2025-12-26 20:47:21,171 - INFO -   Batch 150/1264: Loss = 0.1147
2025-12-26 20:47:53,812 - INFO -   Batch 200/1264: Loss = 0.1635
2025-12-26 20:48:26,453 - INFO -   Batch 250/1264: Loss = 0.1477
2025-12-26 20:48:59,101 - INFO -   Batch 300/1264: Loss = 0.1497
2025-12-26 20:49:31,746 - INFO -   Batch 350/1264: Loss = 0.1109
2025-12-26 20:50:04,390 - INFO -   Batch 400/1264: Loss = 0.1274
2025-12-26 20:50:37,034 - INFO -   Batch 450/1264: Loss = 0.1372
2025-12-26 20:51:09,682 - INFO -   Batch 500/1264: Loss = 0.1547
2025-12-26 20:51:42,328 - INFO -   Batch 550/1264: Loss = 0.1760
2025-12-26 20:52:14,976 - INFO -   Batch 600/1264: Loss = 0.1327
2025-12-26 20:52:47,620 - INFO -   Batch 650/1264: Loss = 0.1531
2025-12-26 20:53:20,262 - INFO -   Batch 700/1264: Loss = 0.1166
2025-12-26 20:53:52,901 - INFO -   Batch 750/1264: Loss = 0.1619
2025-12-26 20:54:25,541 - INFO -   Batch 800/1264: Loss = 0.1184
2025-12-26 20:54:58,182 - INFO -   Batch 850/1264: Loss = 0.1845
2025-12-26 20:55:30,822 - INFO -   Batch 900/1264: Loss = 0.1192
2025-12-26 20:56:03,462 - INFO -   Batch 950/1264: Loss = 0.1605
2025-12-26 20:56:36,103 - INFO -   Batch 1000/1264: Loss = 0.0765
2025-12-26 20:57:08,744 - INFO -   Batch 1050/1264: Loss = 0.1134
2025-12-26 20:57:41,383 - INFO -   Batch 1100/1264: Loss = 0.1169
2025-12-26 20:58:14,022 - INFO -   Batch 1150/1264: Loss = 0.1374
2025-12-26 20:58:46,661 - INFO -   Batch 1200/1264: Loss = 0.1483
2025-12-26 20:59:19,303 - INFO -   Batch 1250/1264: Loss = 0.1169
2025-12-26 21:02:18,050 - INFO - Epoch 78/160: Train Loss: 0.1395, Val Loss: 1.0108, Val mIoU: 43.53%, Val PixelAcc: 80.03%
2025-12-26 21:02:52,562 - INFO -   Batch 50/1264: Loss = 0.1066
2025-12-26 21:03:25,205 - INFO -   Batch 100/1264: Loss = 0.0965
2025-12-26 21:03:57,848 - INFO -   Batch 150/1264: Loss = 0.1412
2025-12-26 21:04:30,489 - INFO -   Batch 200/1264: Loss = 0.1184
2025-12-26 21:05:03,130 - INFO -   Batch 250/1264: Loss = 0.1487
2025-12-26 21:05:35,769 - INFO -   Batch 300/1264: Loss = 0.1210
2025-12-26 21:06:08,410 - INFO -   Batch 350/1264: Loss = 0.1605
2025-12-26 21:06:41,047 - INFO -   Batch 400/1264: Loss = 0.1236
2025-12-26 21:07:13,685 - INFO -   Batch 450/1264: Loss = 0.1315
2025-12-26 21:07:46,326 - INFO -   Batch 500/1264: Loss = 0.1454
2025-12-26 21:08:18,970 - INFO -   Batch 550/1264: Loss = 0.1312
2025-12-26 21:08:51,614 - INFO -   Batch 600/1264: Loss = 0.1569
2025-12-26 21:09:24,258 - INFO -   Batch 650/1264: Loss = 0.2155
2025-12-26 21:09:56,903 - INFO -   Batch 700/1264: Loss = 0.1177
2025-12-26 21:10:29,547 - INFO -   Batch 750/1264: Loss = 0.1516
2025-12-26 21:11:02,191 - INFO -   Batch 800/1264: Loss = 0.1374
2025-12-26 21:11:34,833 - INFO -   Batch 850/1264: Loss = 0.1123
2025-12-26 21:12:07,475 - INFO -   Batch 900/1264: Loss = 0.1306
2025-12-26 21:12:40,116 - INFO -   Batch 950/1264: Loss = 0.1414
2025-12-26 21:13:12,756 - INFO -   Batch 1000/1264: Loss = 0.1097
2025-12-26 21:13:45,397 - INFO -   Batch 1050/1264: Loss = 0.1322
2025-12-26 21:14:18,035 - INFO -   Batch 1100/1264: Loss = 0.1385
2025-12-26 21:14:50,677 - INFO -   Batch 1150/1264: Loss = 0.1805
2025-12-26 21:15:23,315 - INFO -   Batch 1200/1264: Loss = 0.1523
2025-12-26 21:15:55,957 - INFO -   Batch 1250/1264: Loss = 0.1683
2025-12-26 21:18:54,711 - INFO - Epoch 79/160: Train Loss: 0.1371, Val Loss: 1.0277, Val mIoU: 43.05%, Val PixelAcc: 79.98%
2025-12-26 21:19:29,045 - INFO -   Batch 50/1264: Loss = 0.1339
2025-12-26 21:20:01,691 - INFO -   Batch 100/1264: Loss = 0.1650
2025-12-26 21:20:34,334 - INFO -   Batch 150/1264: Loss = 0.1545
2025-12-26 21:21:06,980 - INFO -   Batch 200/1264: Loss = 0.1115
2025-12-26 21:21:39,624 - INFO -   Batch 250/1264: Loss = 0.0987
2025-12-26 21:22:12,265 - INFO -   Batch 300/1264: Loss = 0.1688
2025-12-26 21:22:44,907 - INFO -   Batch 350/1264: Loss = 0.1566
2025-12-26 21:23:17,549 - INFO -   Batch 400/1264: Loss = 0.1417
2025-12-26 21:23:50,193 - INFO -   Batch 450/1264: Loss = 0.1385
2025-12-26 21:24:22,834 - INFO -   Batch 500/1264: Loss = 0.1552
2025-12-26 21:24:55,476 - INFO -   Batch 550/1264: Loss = 0.1213
2025-12-26 21:25:28,117 - INFO -   Batch 600/1264: Loss = 0.1548
2025-12-26 21:26:00,760 - INFO -   Batch 650/1264: Loss = 0.1432
2025-12-26 21:26:33,404 - INFO -   Batch 700/1264: Loss = 0.1691
2025-12-26 21:27:06,051 - INFO -   Batch 750/1264: Loss = 0.1337
2025-12-26 21:27:38,700 - INFO -   Batch 800/1264: Loss = 0.1445
2025-12-26 21:28:11,346 - INFO -   Batch 850/1264: Loss = 0.1256
2025-12-26 21:28:43,992 - INFO -   Batch 900/1264: Loss = 0.1598
2025-12-26 21:29:16,633 - INFO -   Batch 950/1264: Loss = 0.1572
2025-12-26 21:29:49,276 - INFO -   Batch 1000/1264: Loss = 0.1317
2025-12-26 21:30:21,926 - INFO -   Batch 1050/1264: Loss = 0.1569
2025-12-26 21:30:54,570 - INFO -   Batch 1100/1264: Loss = 0.1343
2025-12-26 21:31:27,216 - INFO -   Batch 1150/1264: Loss = 0.1222
2025-12-26 21:31:59,862 - INFO -   Batch 1200/1264: Loss = 0.1374
2025-12-26 21:32:32,505 - INFO -   Batch 1250/1264: Loss = 0.1589
2025-12-26 21:35:31,891 - INFO - Epoch 80/160: Train Loss: 0.1376, Val Loss: 1.0197, Val mIoU: 43.64%, Val PixelAcc: 80.12%
2025-12-26 21:35:33,472 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_80.pth
2025-12-26 21:35:33,473 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_80.pth
2025-12-26 21:36:07,578 - INFO -   Batch 50/1264: Loss = 0.1227
2025-12-26 21:36:40,273 - INFO -   Batch 100/1264: Loss = 0.1314
2025-12-26 21:37:12,958 - INFO -   Batch 150/1264: Loss = 0.1120
2025-12-26 21:37:45,598 - INFO -   Batch 200/1264: Loss = 0.1252
2025-12-26 21:38:18,248 - INFO -   Batch 250/1264: Loss = 0.1302
2025-12-26 21:38:50,894 - INFO -   Batch 300/1264: Loss = 0.1188
2025-12-26 21:39:23,568 - INFO -   Batch 350/1264: Loss = 0.1575
2025-12-26 21:39:56,233 - INFO -   Batch 400/1264: Loss = 0.1336
2025-12-26 21:40:28,879 - INFO -   Batch 450/1264: Loss = 0.1443
2025-12-26 21:41:01,524 - INFO -   Batch 500/1264: Loss = 0.1216
2025-12-26 21:41:34,171 - INFO -   Batch 550/1264: Loss = 0.1326
2025-12-26 21:42:06,829 - INFO -   Batch 600/1264: Loss = 0.1230
2025-12-26 21:42:39,478 - INFO -   Batch 650/1264: Loss = 0.1642
2025-12-26 21:43:12,125 - INFO -   Batch 700/1264: Loss = 0.1473
2025-12-26 21:43:44,773 - INFO -   Batch 750/1264: Loss = 0.1127
2025-12-26 21:44:17,418 - INFO -   Batch 800/1264: Loss = 0.1503
2025-12-26 21:44:50,139 - INFO -   Batch 850/1264: Loss = 0.1601
2025-12-26 21:45:22,787 - INFO -   Batch 900/1264: Loss = 0.1271
2025-12-26 21:45:55,435 - INFO -   Batch 950/1264: Loss = 0.1322
2025-12-26 21:46:28,074 - INFO -   Batch 1000/1264: Loss = 0.1146
2025-12-26 21:47:00,718 - INFO -   Batch 1050/1264: Loss = 0.1281
2025-12-26 21:47:33,362 - INFO -   Batch 1100/1264: Loss = 0.1110
2025-12-26 21:48:06,022 - INFO -   Batch 1150/1264: Loss = 0.1438
2025-12-26 21:48:38,682 - INFO -   Batch 1200/1264: Loss = 0.1352
2025-12-26 21:49:11,323 - INFO -   Batch 1250/1264: Loss = 0.1232
2025-12-26 21:52:10,908 - INFO - Epoch 81/160: Train Loss: 0.1352, Val Loss: 1.0042, Val mIoU: 43.37%, Val PixelAcc: 80.08%
2025-12-26 21:52:44,867 - INFO -   Batch 50/1264: Loss = 0.1525
2025-12-26 21:53:17,506 - INFO -   Batch 100/1264: Loss = 0.1290
2025-12-26 21:53:50,148 - INFO -   Batch 150/1264: Loss = 0.1668
2025-12-26 21:54:22,793 - INFO -   Batch 200/1264: Loss = 0.1686
2025-12-26 21:54:55,444 - INFO -   Batch 250/1264: Loss = 0.1230
2025-12-26 21:55:28,087 - INFO -   Batch 300/1264: Loss = 0.1183
2025-12-26 21:56:00,744 - INFO -   Batch 350/1264: Loss = 0.1672
2025-12-26 21:56:33,381 - INFO -   Batch 400/1264: Loss = 0.1526
2025-12-26 21:57:06,050 - INFO -   Batch 450/1264: Loss = 0.1138
2025-12-26 21:57:38,781 - INFO -   Batch 500/1264: Loss = 0.1041
2025-12-26 21:58:11,465 - INFO -   Batch 550/1264: Loss = 0.1323
2025-12-26 21:58:44,106 - INFO -   Batch 600/1264: Loss = 0.1065
2025-12-26 21:59:16,757 - INFO -   Batch 650/1264: Loss = 0.1196
2025-12-26 21:59:49,440 - INFO -   Batch 700/1264: Loss = 0.1063
2025-12-26 22:00:22,088 - INFO -   Batch 750/1264: Loss = 0.1241
2025-12-26 22:00:54,726 - INFO -   Batch 800/1264: Loss = 0.1203
2025-12-26 22:01:27,365 - INFO -   Batch 850/1264: Loss = 0.1081
2025-12-26 22:02:00,039 - INFO -   Batch 900/1264: Loss = 0.1055
2025-12-26 22:02:32,767 - INFO -   Batch 950/1264: Loss = 0.1258
2025-12-26 22:03:05,410 - INFO -   Batch 1000/1264: Loss = 0.1639
2025-12-26 22:03:38,054 - INFO -   Batch 1050/1264: Loss = 0.0926
2025-12-26 22:04:10,695 - INFO -   Batch 1100/1264: Loss = 0.1178
2025-12-26 22:04:43,352 - INFO -   Batch 1150/1264: Loss = 0.1473
2025-12-26 22:05:16,090 - INFO -   Batch 1200/1264: Loss = 0.1716
2025-12-26 22:05:48,745 - INFO -   Batch 1250/1264: Loss = 0.1327
2025-12-26 22:08:47,459 - INFO - Epoch 82/160: Train Loss: 0.1322, Val Loss: 1.0234, Val mIoU: 43.52%, Val PixelAcc: 80.20%
2025-12-26 22:09:21,502 - INFO -   Batch 50/1264: Loss = 0.2496
2025-12-26 22:09:54,153 - INFO -   Batch 100/1264: Loss = 0.1331
2025-12-26 22:10:26,800 - INFO -   Batch 150/1264: Loss = 0.1345
2025-12-26 22:10:59,444 - INFO -   Batch 200/1264: Loss = 0.0862
2025-12-26 22:11:32,084 - INFO -   Batch 250/1264: Loss = 0.1189
2025-12-26 22:12:04,721 - INFO -   Batch 300/1264: Loss = 0.1357
2025-12-26 22:12:37,362 - INFO -   Batch 350/1264: Loss = 0.1294
2025-12-26 22:13:10,004 - INFO -   Batch 400/1264: Loss = 0.1352
2025-12-26 22:13:42,644 - INFO -   Batch 450/1264: Loss = 0.1224
2025-12-26 22:14:15,285 - INFO -   Batch 500/1264: Loss = 0.0928
2025-12-26 22:14:47,925 - INFO -   Batch 550/1264: Loss = 0.1447
2025-12-26 22:15:20,567 - INFO -   Batch 600/1264: Loss = 0.1762
2025-12-26 22:15:53,206 - INFO -   Batch 650/1264: Loss = 0.1334
2025-12-26 22:16:25,849 - INFO -   Batch 700/1264: Loss = 0.1856
2025-12-26 22:16:58,490 - INFO -   Batch 750/1264: Loss = 0.1600
2025-12-26 22:17:31,132 - INFO -   Batch 800/1264: Loss = 0.1206
2025-12-26 22:18:03,776 - INFO -   Batch 850/1264: Loss = 0.1322
2025-12-26 22:18:36,416 - INFO -   Batch 900/1264: Loss = 0.1036
2025-12-26 22:19:09,062 - INFO -   Batch 950/1264: Loss = 0.1552
2025-12-26 22:19:41,713 - INFO -   Batch 1000/1264: Loss = 0.1037
2025-12-26 22:20:14,362 - INFO -   Batch 1050/1264: Loss = 0.1232
2025-12-26 22:20:47,008 - INFO -   Batch 1100/1264: Loss = 0.1235
2025-12-26 22:21:19,654 - INFO -   Batch 1150/1264: Loss = 0.1293
2025-12-26 22:21:52,297 - INFO -   Batch 1200/1264: Loss = 0.0949
2025-12-26 22:22:24,942 - INFO -   Batch 1250/1264: Loss = 0.1030
2025-12-26 22:25:23,947 - INFO - Epoch 83/160: Train Loss: 0.1317, Val Loss: 1.0181, Val mIoU: 43.69%, Val PixelAcc: 80.12%
2025-12-26 22:25:59,419 - INFO -   Batch 50/1264: Loss = 0.1361
2025-12-26 22:26:32,065 - INFO -   Batch 100/1264: Loss = 0.1260
2025-12-26 22:27:04,711 - INFO -   Batch 150/1264: Loss = 0.1649
2025-12-26 22:27:37,356 - INFO -   Batch 200/1264: Loss = 0.1121
2025-12-26 22:28:10,001 - INFO -   Batch 250/1264: Loss = 0.0995
2025-12-26 22:28:42,648 - INFO -   Batch 300/1264: Loss = 0.1526
2025-12-26 22:29:15,301 - INFO -   Batch 350/1264: Loss = 0.1351
2025-12-26 22:29:47,947 - INFO -   Batch 400/1264: Loss = 0.1220
2025-12-26 22:30:20,613 - INFO -   Batch 450/1264: Loss = 0.1691
2025-12-26 22:30:53,258 - INFO -   Batch 500/1264: Loss = 0.1460
2025-12-26 22:31:25,902 - INFO -   Batch 550/1264: Loss = 0.1253
2025-12-26 22:31:58,547 - INFO -   Batch 600/1264: Loss = 0.1198
2025-12-26 22:32:31,199 - INFO -   Batch 650/1264: Loss = 0.1248
2025-12-26 22:33:03,843 - INFO -   Batch 700/1264: Loss = 0.1440
2025-12-26 22:33:36,492 - INFO -   Batch 750/1264: Loss = 0.1354
2025-12-26 22:34:09,144 - INFO -   Batch 800/1264: Loss = 0.1067
2025-12-26 22:34:41,796 - INFO -   Batch 850/1264: Loss = 0.1395
2025-12-26 22:35:14,445 - INFO -   Batch 900/1264: Loss = 0.1344
2025-12-26 22:35:47,095 - INFO -   Batch 950/1264: Loss = 0.1127
2025-12-26 22:36:19,745 - INFO -   Batch 1000/1264: Loss = 0.1244
2025-12-26 22:36:52,398 - INFO -   Batch 1050/1264: Loss = 0.1648
2025-12-26 22:37:25,047 - INFO -   Batch 1100/1264: Loss = 0.1230
2025-12-26 22:37:57,692 - INFO -   Batch 1150/1264: Loss = 0.1420
2025-12-26 22:38:30,339 - INFO -   Batch 1200/1264: Loss = 0.1686
2025-12-26 22:39:02,985 - INFO -   Batch 1250/1264: Loss = 0.1375
2025-12-26 22:42:02,242 - INFO - Epoch 84/160: Train Loss: 0.1285, Val Loss: 1.0171, Val mIoU: 43.47%, Val PixelAcc: 80.07%
2025-12-26 22:42:36,616 - INFO -   Batch 50/1264: Loss = 0.1138
2025-12-26 22:43:09,264 - INFO -   Batch 100/1264: Loss = 0.1215
2025-12-26 22:43:41,916 - INFO -   Batch 150/1264: Loss = 0.1381
2025-12-26 22:44:14,565 - INFO -   Batch 200/1264: Loss = 0.1693
2025-12-26 22:44:47,213 - INFO -   Batch 250/1264: Loss = 0.1274
2025-12-26 22:45:19,859 - INFO -   Batch 300/1264: Loss = 0.1497
2025-12-26 22:45:52,503 - INFO -   Batch 350/1264: Loss = 0.1275
2025-12-26 22:46:25,151 - INFO -   Batch 400/1264: Loss = 0.1005
2025-12-26 22:46:57,806 - INFO -   Batch 450/1264: Loss = 0.1211
2025-12-26 22:47:30,451 - INFO -   Batch 500/1264: Loss = 0.1143
2025-12-26 22:48:03,103 - INFO -   Batch 550/1264: Loss = 0.1406
2025-12-26 22:48:35,754 - INFO -   Batch 600/1264: Loss = 0.1504
2025-12-26 22:49:08,406 - INFO -   Batch 650/1264: Loss = 0.1344
2025-12-26 22:49:41,068 - INFO -   Batch 700/1264: Loss = 0.1292
2025-12-26 22:50:13,716 - INFO -   Batch 750/1264: Loss = 0.1133
2025-12-26 22:50:46,367 - INFO -   Batch 800/1264: Loss = 0.1516
2025-12-26 22:51:19,016 - INFO -   Batch 850/1264: Loss = 0.1322
2025-12-26 22:51:51,668 - INFO -   Batch 900/1264: Loss = 0.1188
2025-12-26 22:52:24,323 - INFO -   Batch 950/1264: Loss = 0.1139
2025-12-26 22:52:56,974 - INFO -   Batch 1000/1264: Loss = 0.1439
2025-12-26 22:53:29,626 - INFO -   Batch 1050/1264: Loss = 0.1275
2025-12-26 22:54:02,273 - INFO -   Batch 1100/1264: Loss = 0.1290
2025-12-26 22:54:34,920 - INFO -   Batch 1150/1264: Loss = 0.1464
2025-12-26 22:55:07,563 - INFO -   Batch 1200/1264: Loss = 0.1747
2025-12-26 22:55:40,205 - INFO -   Batch 1250/1264: Loss = 0.1060
2025-12-26 22:58:39,334 - INFO - Epoch 85/160: Train Loss: 0.1300, Val Loss: 1.0379, Val mIoU: 43.26%, Val PixelAcc: 80.04%
2025-12-26 22:59:13,637 - INFO -   Batch 50/1264: Loss = 0.1281
2025-12-26 22:59:46,279 - INFO -   Batch 100/1264: Loss = 0.1299
2025-12-26 23:00:18,922 - INFO -   Batch 150/1264: Loss = 0.1472
2025-12-26 23:00:51,564 - INFO -   Batch 200/1264: Loss = 0.1453
2025-12-26 23:01:24,209 - INFO -   Batch 250/1264: Loss = 0.1296
2025-12-26 23:01:56,868 - INFO -   Batch 300/1264: Loss = 0.1616
2025-12-26 23:02:29,512 - INFO -   Batch 350/1264: Loss = 0.1108
2025-12-26 23:03:02,154 - INFO -   Batch 400/1264: Loss = 0.1056
2025-12-26 23:03:34,796 - INFO -   Batch 450/1264: Loss = 0.1201
2025-12-26 23:04:07,441 - INFO -   Batch 500/1264: Loss = 0.1276
2025-12-26 23:04:40,081 - INFO -   Batch 550/1264: Loss = 0.1387
2025-12-26 23:05:12,720 - INFO -   Batch 600/1264: Loss = 0.1317
2025-12-26 23:05:45,355 - INFO -   Batch 650/1264: Loss = 0.1093
2025-12-26 23:06:17,991 - INFO -   Batch 700/1264: Loss = 0.1255
2025-12-26 23:06:50,629 - INFO -   Batch 750/1264: Loss = 0.1228
2025-12-26 23:07:23,269 - INFO -   Batch 800/1264: Loss = 0.1276
2025-12-26 23:07:55,908 - INFO -   Batch 850/1264: Loss = 0.1557
2025-12-26 23:08:28,549 - INFO -   Batch 900/1264: Loss = 0.1577
2025-12-26 23:09:01,188 - INFO -   Batch 950/1264: Loss = 0.1271
2025-12-26 23:09:33,828 - INFO -   Batch 1000/1264: Loss = 0.1269
2025-12-26 23:10:06,464 - INFO -   Batch 1050/1264: Loss = 0.0939
2025-12-26 23:10:39,105 - INFO -   Batch 1100/1264: Loss = 0.0922
2025-12-26 23:11:11,742 - INFO -   Batch 1150/1264: Loss = 0.1067
2025-12-26 23:11:44,380 - INFO -   Batch 1200/1264: Loss = 0.1470
2025-12-26 23:12:17,018 - INFO -   Batch 1250/1264: Loss = 0.1355
2025-12-26 23:15:15,697 - INFO - Epoch 86/160: Train Loss: 0.1280, Val Loss: 1.0285, Val mIoU: 43.50%, Val PixelAcc: 80.07%
2025-12-26 23:15:50,022 - INFO -   Batch 50/1264: Loss = 0.0987
2025-12-26 23:16:22,665 - INFO -   Batch 100/1264: Loss = 0.1036
2025-12-26 23:16:55,305 - INFO -   Batch 150/1264: Loss = 0.1209
2025-12-26 23:17:27,945 - INFO -   Batch 200/1264: Loss = 0.1428
2025-12-26 23:18:00,587 - INFO -   Batch 250/1264: Loss = 0.0901
2025-12-26 23:18:33,227 - INFO -   Batch 300/1264: Loss = 0.1258
2025-12-26 23:19:05,866 - INFO -   Batch 350/1264: Loss = 0.1173
2025-12-26 23:19:38,503 - INFO -   Batch 400/1264: Loss = 0.1103
2025-12-26 23:20:11,140 - INFO -   Batch 450/1264: Loss = 0.1278
2025-12-26 23:20:43,779 - INFO -   Batch 500/1264: Loss = 0.1130
2025-12-26 23:21:16,417 - INFO -   Batch 550/1264: Loss = 0.1163
2025-12-26 23:21:49,053 - INFO -   Batch 600/1264: Loss = 0.1469
2025-12-26 23:22:21,689 - INFO -   Batch 650/1264: Loss = 0.1114
2025-12-26 23:22:54,327 - INFO -   Batch 700/1264: Loss = 0.1075
2025-12-26 23:23:26,964 - INFO -   Batch 750/1264: Loss = 0.1521
2025-12-26 23:23:59,599 - INFO -   Batch 800/1264: Loss = 0.1402
2025-12-26 23:24:32,234 - INFO -   Batch 850/1264: Loss = 0.1141
2025-12-26 23:25:04,872 - INFO -   Batch 900/1264: Loss = 0.1194
2025-12-26 23:25:37,510 - INFO -   Batch 950/1264: Loss = 0.1117
2025-12-26 23:26:10,148 - INFO -   Batch 1000/1264: Loss = 0.1118
2025-12-26 23:26:42,786 - INFO -   Batch 1050/1264: Loss = 0.1105
2025-12-26 23:27:15,428 - INFO -   Batch 1100/1264: Loss = 0.1288
2025-12-26 23:27:48,068 - INFO -   Batch 1150/1264: Loss = 0.1180
2025-12-26 23:28:20,707 - INFO -   Batch 1200/1264: Loss = 0.1350
2025-12-26 23:28:53,342 - INFO -   Batch 1250/1264: Loss = 0.1191
2025-12-26 23:31:52,096 - INFO - Epoch 87/160: Train Loss: 0.1260, Val Loss: 1.0232, Val mIoU: 43.55%, Val PixelAcc: 80.22%
2025-12-26 23:32:26,402 - INFO -   Batch 50/1264: Loss = 0.2057
2025-12-26 23:32:59,045 - INFO -   Batch 100/1264: Loss = 0.1238
2025-12-26 23:33:31,688 - INFO -   Batch 150/1264: Loss = 0.1996
2025-12-26 23:34:04,331 - INFO -   Batch 200/1264: Loss = 0.1548
2025-12-26 23:34:36,972 - INFO -   Batch 250/1264: Loss = 0.1475
2025-12-26 23:35:09,611 - INFO -   Batch 300/1264: Loss = 0.0998
2025-12-26 23:35:42,251 - INFO -   Batch 350/1264: Loss = 0.2030
2025-12-26 23:36:14,890 - INFO -   Batch 400/1264: Loss = 0.1591
2025-12-26 23:36:47,527 - INFO -   Batch 450/1264: Loss = 0.1764
2025-12-26 23:37:20,163 - INFO -   Batch 500/1264: Loss = 0.1266
2025-12-26 23:37:52,798 - INFO -   Batch 550/1264: Loss = 0.1204
2025-12-26 23:38:25,429 - INFO -   Batch 600/1264: Loss = 0.0949
2025-12-26 23:38:58,058 - INFO -   Batch 650/1264: Loss = 0.1162
2025-12-26 23:39:30,690 - INFO -   Batch 700/1264: Loss = 0.1341
2025-12-26 23:40:03,322 - INFO -   Batch 750/1264: Loss = 0.1017
2025-12-26 23:40:35,953 - INFO -   Batch 800/1264: Loss = 0.1528
2025-12-26 23:41:08,587 - INFO -   Batch 850/1264: Loss = 0.1353
2025-12-26 23:41:41,219 - INFO -   Batch 900/1264: Loss = 0.1237
2025-12-26 23:42:13,855 - INFO -   Batch 950/1264: Loss = 0.1465
2025-12-26 23:42:46,487 - INFO -   Batch 1000/1264: Loss = 0.1658
2025-12-26 23:43:19,120 - INFO -   Batch 1050/1264: Loss = 0.1377
2025-12-26 23:43:51,753 - INFO -   Batch 1100/1264: Loss = 0.0982
2025-12-26 23:44:24,389 - INFO -   Batch 1150/1264: Loss = 0.0944
2025-12-26 23:44:57,026 - INFO -   Batch 1200/1264: Loss = 0.1275
2025-12-26 23:45:29,663 - INFO -   Batch 1250/1264: Loss = 0.1342
2025-12-26 23:48:28,083 - INFO - Epoch 88/160: Train Loss: 0.1247, Val Loss: 1.0304, Val mIoU: 43.51%, Val PixelAcc: 80.29%
2025-12-26 23:49:03,072 - INFO -   Batch 50/1264: Loss = 0.1123
2025-12-26 23:49:35,712 - INFO -   Batch 100/1264: Loss = 0.1173
2025-12-26 23:50:08,352 - INFO -   Batch 150/1264: Loss = 0.1136
2025-12-26 23:50:40,996 - INFO -   Batch 200/1264: Loss = 0.1173
2025-12-26 23:51:13,639 - INFO -   Batch 250/1264: Loss = 0.0932
2025-12-26 23:51:46,282 - INFO -   Batch 300/1264: Loss = 0.1100
2025-12-26 23:52:18,926 - INFO -   Batch 350/1264: Loss = 0.1051
2025-12-26 23:52:51,568 - INFO -   Batch 400/1264: Loss = 0.1166
2025-12-26 23:53:24,210 - INFO -   Batch 450/1264: Loss = 0.1273
2025-12-26 23:53:56,854 - INFO -   Batch 500/1264: Loss = 0.1348
2025-12-26 23:54:29,494 - INFO -   Batch 550/1264: Loss = 0.1048
2025-12-26 23:55:02,134 - INFO -   Batch 600/1264: Loss = 0.1232
2025-12-26 23:55:34,770 - INFO -   Batch 650/1264: Loss = 0.1539
2025-12-26 23:56:07,410 - INFO -   Batch 700/1264: Loss = 0.1214
2025-12-26 23:56:40,051 - INFO -   Batch 750/1264: Loss = 0.1015
2025-12-26 23:57:12,693 - INFO -   Batch 800/1264: Loss = 0.0952
2025-12-26 23:57:45,330 - INFO -   Batch 850/1264: Loss = 0.1064
2025-12-26 23:58:17,967 - INFO -   Batch 900/1264: Loss = 0.1258
2025-12-26 23:58:50,609 - INFO -   Batch 950/1264: Loss = 0.1381
2025-12-26 23:59:23,249 - INFO -   Batch 1000/1264: Loss = 0.1267
2025-12-26 23:59:55,888 - INFO -   Batch 1050/1264: Loss = 0.1153
2025-12-27 00:00:28,529 - INFO -   Batch 1100/1264: Loss = 0.1150
2025-12-27 00:01:01,168 - INFO -   Batch 1150/1264: Loss = 0.1924
2025-12-27 00:01:33,807 - INFO -   Batch 1200/1264: Loss = 0.0986
2025-12-27 00:02:06,447 - INFO -   Batch 1250/1264: Loss = 0.1190
2025-12-27 00:05:05,065 - INFO - Epoch 89/160: Train Loss: 0.1233, Val Loss: 1.0324, Val mIoU: 43.91%, Val PixelAcc: 80.34%
2025-12-27 00:05:05,067 - INFO -   → New best mIoU: 43.91%
2025-12-27 00:05:07,060 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-27 00:05:07,062 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-27 00:05:41,611 - INFO -   Batch 50/1264: Loss = 0.1209
2025-12-27 00:06:14,250 - INFO -   Batch 100/1264: Loss = 0.1156
2025-12-27 00:06:46,887 - INFO -   Batch 150/1264: Loss = 0.1569
2025-12-27 00:07:19,527 - INFO -   Batch 200/1264: Loss = 0.1030
2025-12-27 00:07:52,166 - INFO -   Batch 250/1264: Loss = 0.0751
2025-12-27 00:08:24,805 - INFO -   Batch 300/1264: Loss = 0.1104
2025-12-27 00:08:57,446 - INFO -   Batch 350/1264: Loss = 0.1261
2025-12-27 00:09:30,088 - INFO -   Batch 400/1264: Loss = 0.1170
2025-12-27 00:10:02,729 - INFO -   Batch 450/1264: Loss = 0.1276
2025-12-27 00:10:35,367 - INFO -   Batch 500/1264: Loss = 0.1298
2025-12-27 00:11:08,008 - INFO -   Batch 550/1264: Loss = 0.1105
2025-12-27 00:11:40,646 - INFO -   Batch 600/1264: Loss = 0.1202
2025-12-27 00:12:13,286 - INFO -   Batch 650/1264: Loss = 0.1312
2025-12-27 00:12:45,927 - INFO -   Batch 700/1264: Loss = 0.1232
2025-12-27 00:13:18,568 - INFO -   Batch 750/1264: Loss = 0.0980
2025-12-27 00:13:51,207 - INFO -   Batch 800/1264: Loss = 0.1161
2025-12-27 00:14:23,846 - INFO -   Batch 850/1264: Loss = 0.1243
2025-12-27 00:14:56,483 - INFO -   Batch 900/1264: Loss = 0.1079
2025-12-27 00:15:29,122 - INFO -   Batch 950/1264: Loss = 0.1038
2025-12-27 00:16:01,761 - INFO -   Batch 1000/1264: Loss = 0.1077
2025-12-27 00:16:34,402 - INFO -   Batch 1050/1264: Loss = 0.0934
2025-12-27 00:17:07,045 - INFO -   Batch 1100/1264: Loss = 0.1454
2025-12-27 00:17:39,688 - INFO -   Batch 1150/1264: Loss = 0.0856
2025-12-27 00:18:12,326 - INFO -   Batch 1200/1264: Loss = 0.1335
2025-12-27 00:18:44,967 - INFO -   Batch 1250/1264: Loss = 0.1193
2025-12-27 00:21:44,200 - INFO - Epoch 90/160: Train Loss: 0.1222, Val Loss: 1.0258, Val mIoU: 44.06%, Val PixelAcc: 80.27%
2025-12-27 00:21:44,202 - INFO -   → New best mIoU: 44.06%
2025-12-27 00:21:45,747 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-27 00:21:45,748 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-27 00:21:47,223 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_90.pth
2025-12-27 00:21:47,224 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_90.pth
2025-12-27 00:22:22,264 - INFO -   Batch 50/1264: Loss = 0.1258
2025-12-27 00:22:54,904 - INFO -   Batch 100/1264: Loss = 0.1155
2025-12-27 00:23:27,543 - INFO -   Batch 150/1264: Loss = 0.1342
2025-12-27 00:24:00,185 - INFO -   Batch 200/1264: Loss = 0.1157
2025-12-27 00:24:32,825 - INFO -   Batch 250/1264: Loss = 0.1287
2025-12-27 00:25:05,463 - INFO -   Batch 300/1264: Loss = 0.1265
2025-12-27 00:25:38,101 - INFO -   Batch 350/1264: Loss = 0.1143
2025-12-27 00:26:10,737 - INFO -   Batch 400/1264: Loss = 0.1519
2025-12-27 00:26:43,376 - INFO -   Batch 450/1264: Loss = 0.1021
2025-12-27 00:27:16,015 - INFO -   Batch 500/1264: Loss = 0.1552
2025-12-27 00:27:48,654 - INFO -   Batch 550/1264: Loss = 0.1411
2025-12-27 00:28:21,293 - INFO -   Batch 600/1264: Loss = 0.1129
2025-12-27 00:28:53,937 - INFO -   Batch 650/1264: Loss = 0.1265
2025-12-27 00:29:26,574 - INFO -   Batch 700/1264: Loss = 0.1159
2025-12-27 00:29:59,213 - INFO -   Batch 750/1264: Loss = 0.1148
2025-12-27 00:30:31,854 - INFO -   Batch 800/1264: Loss = 0.1187
2025-12-27 00:31:04,494 - INFO -   Batch 850/1264: Loss = 0.1375
2025-12-27 00:31:37,135 - INFO -   Batch 900/1264: Loss = 0.0989
2025-12-27 00:32:09,777 - INFO -   Batch 950/1264: Loss = 0.1291
2025-12-27 00:32:42,416 - INFO -   Batch 1000/1264: Loss = 0.0976
2025-12-27 00:33:15,055 - INFO -   Batch 1050/1264: Loss = 0.1455
2025-12-27 00:33:47,697 - INFO -   Batch 1100/1264: Loss = 0.1330
2025-12-27 00:34:20,342 - INFO -   Batch 1150/1264: Loss = 0.1199
2025-12-27 00:34:52,983 - INFO -   Batch 1200/1264: Loss = 0.1417
2025-12-27 00:35:25,623 - INFO -   Batch 1250/1264: Loss = 0.1114
2025-12-27 00:38:24,351 - INFO - Epoch 91/160: Train Loss: 0.1215, Val Loss: 1.0260, Val mIoU: 44.08%, Val PixelAcc: 80.47%
2025-12-27 00:38:24,353 - INFO -   → New best mIoU: 44.08%
2025-12-27 00:38:25,906 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-27 00:38:25,908 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-27 00:39:00,146 - INFO -   Batch 50/1264: Loss = 0.1345
2025-12-27 00:39:32,782 - INFO -   Batch 100/1264: Loss = 0.0912
2025-12-27 00:40:05,418 - INFO -   Batch 150/1264: Loss = 0.0909
2025-12-27 00:40:38,057 - INFO -   Batch 200/1264: Loss = 0.1106
2025-12-27 00:41:10,702 - INFO -   Batch 250/1264: Loss = 0.1183
2025-12-27 00:41:43,344 - INFO -   Batch 300/1264: Loss = 0.1246
2025-12-27 00:42:15,985 - INFO -   Batch 350/1264: Loss = 0.1224
2025-12-27 00:42:48,623 - INFO -   Batch 400/1264: Loss = 0.1087
2025-12-27 00:43:21,262 - INFO -   Batch 450/1264: Loss = 0.1110
2025-12-27 00:43:53,905 - INFO -   Batch 500/1264: Loss = 0.1024
2025-12-27 00:44:26,546 - INFO -   Batch 550/1264: Loss = 0.0944
2025-12-27 00:44:59,191 - INFO -   Batch 600/1264: Loss = 0.1041
2025-12-27 00:45:31,833 - INFO -   Batch 650/1264: Loss = 0.0933
2025-12-27 00:46:04,471 - INFO -   Batch 700/1264: Loss = 0.1209
2025-12-27 00:46:37,116 - INFO -   Batch 750/1264: Loss = 0.1060
2025-12-27 00:47:09,761 - INFO -   Batch 800/1264: Loss = 0.1319
2025-12-27 00:47:42,405 - INFO -   Batch 850/1264: Loss = 0.0974
2025-12-27 00:48:15,050 - INFO -   Batch 900/1264: Loss = 0.1044
2025-12-27 00:48:47,692 - INFO -   Batch 950/1264: Loss = 0.1542
2025-12-27 00:49:20,334 - INFO -   Batch 1000/1264: Loss = 0.0990
2025-12-27 00:49:52,979 - INFO -   Batch 1050/1264: Loss = 0.0974
2025-12-27 00:50:25,625 - INFO -   Batch 1100/1264: Loss = 0.1201
2025-12-27 00:50:58,273 - INFO -   Batch 1150/1264: Loss = 0.1361
2025-12-27 00:51:30,921 - INFO -   Batch 1200/1264: Loss = 0.1488
2025-12-27 00:52:03,571 - INFO -   Batch 1250/1264: Loss = 0.1446
2025-12-27 00:55:02,379 - INFO - Epoch 92/160: Train Loss: 0.1204, Val Loss: 1.0216, Val mIoU: 43.86%, Val PixelAcc: 80.31%
2025-12-27 00:55:36,898 - INFO -   Batch 50/1264: Loss = 0.1054
2025-12-27 00:56:09,543 - INFO -   Batch 100/1264: Loss = 0.1017
2025-12-27 00:56:42,191 - INFO -   Batch 150/1264: Loss = 0.0989
2025-12-27 00:57:14,835 - INFO -   Batch 200/1264: Loss = 0.1097
2025-12-27 00:57:47,479 - INFO -   Batch 250/1264: Loss = 0.1266
2025-12-27 00:58:20,121 - INFO -   Batch 300/1264: Loss = 0.1134
2025-12-27 00:58:52,763 - INFO -   Batch 350/1264: Loss = 0.0966
2025-12-27 00:59:25,409 - INFO -   Batch 400/1264: Loss = 0.1715
2025-12-27 00:59:58,054 - INFO -   Batch 450/1264: Loss = 0.1096
2025-12-27 01:00:30,701 - INFO -   Batch 500/1264: Loss = 0.1067
2025-12-27 01:01:03,346 - INFO -   Batch 550/1264: Loss = 0.1077
2025-12-27 01:01:35,989 - INFO -   Batch 600/1264: Loss = 0.1412
2025-12-27 01:02:08,632 - INFO -   Batch 650/1264: Loss = 0.1069
2025-12-27 01:02:41,279 - INFO -   Batch 700/1264: Loss = 0.1062
2025-12-27 01:03:13,923 - INFO -   Batch 750/1264: Loss = 0.0925
2025-12-27 01:03:46,565 - INFO -   Batch 800/1264: Loss = 0.0995
2025-12-27 01:04:19,207 - INFO -   Batch 850/1264: Loss = 0.1135
2025-12-27 01:04:51,850 - INFO -   Batch 900/1264: Loss = 0.1208
2025-12-27 01:05:24,498 - INFO -   Batch 950/1264: Loss = 0.1319
2025-12-27 01:05:57,144 - INFO -   Batch 1000/1264: Loss = 0.1171
2025-12-27 01:06:29,788 - INFO -   Batch 1050/1264: Loss = 0.0869
2025-12-27 01:07:02,435 - INFO -   Batch 1100/1264: Loss = 0.1019
2025-12-27 01:07:35,076 - INFO -   Batch 1150/1264: Loss = 0.1208
2025-12-27 01:08:07,718 - INFO -   Batch 1200/1264: Loss = 0.1008
2025-12-27 01:08:40,361 - INFO -   Batch 1250/1264: Loss = 0.0860
2025-12-27 01:11:38,998 - INFO - Epoch 93/160: Train Loss: 0.1187, Val Loss: 1.0211, Val mIoU: 44.11%, Val PixelAcc: 80.37%
2025-12-27 01:11:39,000 - INFO -   → New best mIoU: 44.11%
2025-12-27 01:11:40,542 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-27 01:11:40,544 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-27 01:12:14,943 - INFO -   Batch 50/1264: Loss = 0.0906
2025-12-27 01:12:47,589 - INFO -   Batch 100/1264: Loss = 0.1378
2025-12-27 01:13:20,236 - INFO -   Batch 150/1264: Loss = 0.1246
2025-12-27 01:13:52,882 - INFO -   Batch 200/1264: Loss = 0.1282
2025-12-27 01:14:25,524 - INFO -   Batch 250/1264: Loss = 0.1190
2025-12-27 01:14:58,169 - INFO -   Batch 300/1264: Loss = 0.1507
2025-12-27 01:15:30,813 - INFO -   Batch 350/1264: Loss = 0.1283
2025-12-27 01:16:03,460 - INFO -   Batch 400/1264: Loss = 0.0890
2025-12-27 01:16:36,108 - INFO -   Batch 450/1264: Loss = 0.1202
2025-12-27 01:17:08,750 - INFO -   Batch 500/1264: Loss = 0.0947
2025-12-27 01:17:41,390 - INFO -   Batch 550/1264: Loss = 0.1065
2025-12-27 01:18:14,036 - INFO -   Batch 600/1264: Loss = 0.1180
2025-12-27 01:18:46,684 - INFO -   Batch 650/1264: Loss = 0.1223
2025-12-27 01:19:19,330 - INFO -   Batch 700/1264: Loss = 0.1114
2025-12-27 01:19:51,974 - INFO -   Batch 750/1264: Loss = 0.0764
2025-12-27 01:20:24,618 - INFO -   Batch 800/1264: Loss = 0.1408
2025-12-27 01:20:57,263 - INFO -   Batch 850/1264: Loss = 0.1100
2025-12-27 01:21:29,907 - INFO -   Batch 900/1264: Loss = 0.0870
2025-12-27 01:22:02,550 - INFO -   Batch 950/1264: Loss = 0.1464
2025-12-27 01:22:35,193 - INFO -   Batch 1000/1264: Loss = 0.0957
2025-12-27 01:23:07,839 - INFO -   Batch 1050/1264: Loss = 0.0903
2025-12-27 01:23:40,485 - INFO -   Batch 1100/1264: Loss = 0.0969
2025-12-27 01:24:13,128 - INFO -   Batch 1150/1264: Loss = 0.1424
2025-12-27 01:24:45,770 - INFO -   Batch 1200/1264: Loss = 0.0987
2025-12-27 01:25:18,411 - INFO -   Batch 1250/1264: Loss = 0.1241
2025-12-27 01:28:17,175 - INFO - Epoch 94/160: Train Loss: 0.1180, Val Loss: 1.0444, Val mIoU: 43.99%, Val PixelAcc: 80.40%
2025-12-27 01:28:52,054 - INFO -   Batch 50/1264: Loss = 0.1093
2025-12-27 01:29:24,692 - INFO -   Batch 100/1264: Loss = 0.1444
2025-12-27 01:29:57,332 - INFO -   Batch 150/1264: Loss = 0.1202
2025-12-27 01:30:29,970 - INFO -   Batch 200/1264: Loss = 0.0899
2025-12-27 01:31:02,612 - INFO -   Batch 250/1264: Loss = 0.1243
2025-12-27 01:31:35,252 - INFO -   Batch 300/1264: Loss = 0.0908
2025-12-27 01:32:07,887 - INFO -   Batch 350/1264: Loss = 0.0915
2025-12-27 01:32:40,524 - INFO -   Batch 400/1264: Loss = 0.1060
2025-12-27 01:33:13,164 - INFO -   Batch 450/1264: Loss = 0.1176
2025-12-27 01:33:45,804 - INFO -   Batch 500/1264: Loss = 0.1204
2025-12-27 01:34:18,444 - INFO -   Batch 550/1264: Loss = 0.0961
2025-12-27 01:34:51,084 - INFO -   Batch 600/1264: Loss = 0.1179
2025-12-27 01:35:23,723 - INFO -   Batch 650/1264: Loss = 0.0997
2025-12-27 01:35:56,364 - INFO -   Batch 700/1264: Loss = 0.0917
2025-12-27 01:36:29,003 - INFO -   Batch 750/1264: Loss = 0.1312
2025-12-27 01:37:01,644 - INFO -   Batch 800/1264: Loss = 0.1156
2025-12-27 01:37:34,284 - INFO -   Batch 850/1264: Loss = 0.1084
2025-12-27 01:38:06,926 - INFO -   Batch 900/1264: Loss = 0.0975
2025-12-27 01:38:39,568 - INFO -   Batch 950/1264: Loss = 0.1242
2025-12-27 01:39:12,213 - INFO -   Batch 1000/1264: Loss = 0.1247
2025-12-27 01:39:44,859 - INFO -   Batch 1050/1264: Loss = 0.1034
2025-12-27 01:40:17,503 - INFO -   Batch 1100/1264: Loss = 0.1506
2025-12-27 01:40:50,147 - INFO -   Batch 1150/1264: Loss = 0.1077
2025-12-27 01:41:22,789 - INFO -   Batch 1200/1264: Loss = 0.0870
2025-12-27 01:41:55,430 - INFO -   Batch 1250/1264: Loss = 0.1315
2025-12-27 01:44:54,244 - INFO - Epoch 95/160: Train Loss: 0.1177, Val Loss: 1.0311, Val mIoU: 43.74%, Val PixelAcc: 80.37%
2025-12-27 01:45:28,602 - INFO -   Batch 50/1264: Loss = 0.0906
2025-12-27 01:46:01,245 - INFO -   Batch 100/1264: Loss = 0.1247
2025-12-27 01:46:33,890 - INFO -   Batch 150/1264: Loss = 0.1304
2025-12-27 01:47:06,530 - INFO -   Batch 200/1264: Loss = 0.1473
2025-12-27 01:47:39,177 - INFO -   Batch 250/1264: Loss = 0.1052
2025-12-27 01:48:11,826 - INFO -   Batch 300/1264: Loss = 0.1494
2025-12-27 01:48:44,472 - INFO -   Batch 350/1264: Loss = 0.0979
2025-12-27 01:49:17,117 - INFO -   Batch 400/1264: Loss = 0.1243
2025-12-27 01:49:49,755 - INFO -   Batch 450/1264: Loss = 0.1173
2025-12-27 01:50:22,394 - INFO -   Batch 500/1264: Loss = 0.1123
2025-12-27 01:50:55,034 - INFO -   Batch 550/1264: Loss = 0.1231
2025-12-27 01:51:27,677 - INFO -   Batch 600/1264: Loss = 0.1323
2025-12-27 01:52:00,319 - INFO -   Batch 650/1264: Loss = 0.0836
2025-12-27 01:52:32,958 - INFO -   Batch 700/1264: Loss = 0.1074
2025-12-27 01:53:05,598 - INFO -   Batch 750/1264: Loss = 0.1250
2025-12-27 01:53:38,244 - INFO -   Batch 800/1264: Loss = 0.1138
2025-12-27 01:54:10,883 - INFO -   Batch 850/1264: Loss = 0.1604
2025-12-27 01:54:43,518 - INFO -   Batch 900/1264: Loss = 0.1236
2025-12-27 01:55:16,150 - INFO -   Batch 950/1264: Loss = 0.0846
2025-12-27 01:55:48,787 - INFO -   Batch 1000/1264: Loss = 0.0999
2025-12-27 01:56:21,426 - INFO -   Batch 1050/1264: Loss = 0.1005
2025-12-27 01:56:54,066 - INFO -   Batch 1100/1264: Loss = 0.1020
2025-12-27 01:57:26,708 - INFO -   Batch 1150/1264: Loss = 0.1219
2025-12-27 01:57:59,348 - INFO -   Batch 1200/1264: Loss = 0.1002
2025-12-27 01:58:31,988 - INFO -   Batch 1250/1264: Loss = 0.1477
2025-12-27 02:01:30,727 - INFO - Epoch 96/160: Train Loss: 0.1146, Val Loss: 1.0388, Val mIoU: 43.99%, Val PixelAcc: 80.37%
2025-12-27 02:02:05,584 - INFO -   Batch 50/1264: Loss = 0.1450
2025-12-27 02:02:38,226 - INFO -   Batch 100/1264: Loss = 0.1197
2025-12-27 02:03:10,866 - INFO -   Batch 150/1264: Loss = 0.1220
2025-12-27 02:03:43,507 - INFO -   Batch 200/1264: Loss = 0.1076
2025-12-27 02:04:16,145 - INFO -   Batch 250/1264: Loss = 0.1145
2025-12-27 02:04:48,784 - INFO -   Batch 300/1264: Loss = 0.1300
2025-12-27 02:05:21,428 - INFO -   Batch 350/1264: Loss = 0.1213
2025-12-27 02:05:54,072 - INFO -   Batch 400/1264: Loss = 0.1636
2025-12-27 02:06:26,717 - INFO -   Batch 450/1264: Loss = 0.1037
2025-12-27 02:06:59,362 - INFO -   Batch 500/1264: Loss = 0.1010
2025-12-27 02:07:32,006 - INFO -   Batch 550/1264: Loss = 0.1217
2025-12-27 02:08:04,648 - INFO -   Batch 600/1264: Loss = 0.0943
2025-12-27 02:08:37,287 - INFO -   Batch 650/1264: Loss = 0.1161
2025-12-27 02:09:09,928 - INFO -   Batch 700/1264: Loss = 0.1243
2025-12-27 02:09:43,171 - INFO -   Batch 750/1264: Loss = 0.0976
2025-12-27 02:10:15,807 - INFO -   Batch 800/1264: Loss = 0.1038
2025-12-27 02:10:48,445 - INFO -   Batch 850/1264: Loss = 0.1159
2025-12-27 02:11:21,087 - INFO -   Batch 900/1264: Loss = 0.1306
2025-12-27 02:11:53,730 - INFO -   Batch 950/1264: Loss = 0.0897
2025-12-27 02:12:26,371 - INFO -   Batch 1000/1264: Loss = 0.1241
2025-12-27 02:12:59,011 - INFO -   Batch 1050/1264: Loss = 0.1346
2025-12-27 02:13:31,650 - INFO -   Batch 1100/1264: Loss = 0.1181
2025-12-27 02:14:04,292 - INFO -   Batch 1150/1264: Loss = 0.1277
2025-12-27 02:14:36,929 - INFO -   Batch 1200/1264: Loss = 0.0897
2025-12-27 02:15:09,564 - INFO -   Batch 1250/1264: Loss = 0.1064
2025-12-27 02:18:08,079 - INFO - Epoch 97/160: Train Loss: 0.1151, Val Loss: 1.0427, Val mIoU: 44.12%, Val PixelAcc: 80.44%
2025-12-27 02:18:08,081 - INFO -   → New best mIoU: 44.12%
2025-12-27 02:18:10,635 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-27 02:18:10,637 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-27 02:18:45,463 - INFO -   Batch 50/1264: Loss = 0.1102
2025-12-27 02:19:18,103 - INFO -   Batch 100/1264: Loss = 0.1114
2025-12-27 02:19:50,744 - INFO -   Batch 150/1264: Loss = 0.0987
2025-12-27 02:20:23,387 - INFO -   Batch 200/1264: Loss = 0.0935
2025-12-27 02:20:56,034 - INFO -   Batch 250/1264: Loss = 0.0815
2025-12-27 02:21:28,678 - INFO -   Batch 300/1264: Loss = 0.0957
2025-12-27 02:22:01,318 - INFO -   Batch 350/1264: Loss = 0.1345
2025-12-27 02:22:33,959 - INFO -   Batch 400/1264: Loss = 0.1104
2025-12-27 02:23:06,599 - INFO -   Batch 450/1264: Loss = 0.1018
2025-12-27 02:23:39,239 - INFO -   Batch 500/1264: Loss = 0.0864
2025-12-27 02:24:11,881 - INFO -   Batch 550/1264: Loss = 0.1004
2025-12-27 02:24:44,525 - INFO -   Batch 600/1264: Loss = 0.1108
2025-12-27 02:25:17,164 - INFO -   Batch 650/1264: Loss = 0.1098
2025-12-27 02:25:49,800 - INFO -   Batch 700/1264: Loss = 0.1338
2025-12-27 02:26:22,439 - INFO -   Batch 750/1264: Loss = 0.1070
2025-12-27 02:26:55,080 - INFO -   Batch 800/1264: Loss = 0.1883
2025-12-27 02:27:27,721 - INFO -   Batch 850/1264: Loss = 0.1277
2025-12-27 02:28:00,360 - INFO -   Batch 900/1264: Loss = 0.1135
2025-12-27 02:28:32,998 - INFO -   Batch 950/1264: Loss = 0.1169
2025-12-27 02:29:05,637 - INFO -   Batch 1000/1264: Loss = 0.1123
2025-12-27 02:29:38,276 - INFO -   Batch 1050/1264: Loss = 0.0994
2025-12-27 02:30:10,914 - INFO -   Batch 1100/1264: Loss = 0.1236
2025-12-27 02:30:43,549 - INFO -   Batch 1150/1264: Loss = 0.1229
2025-12-27 02:31:16,190 - INFO -   Batch 1200/1264: Loss = 0.1519
2025-12-27 02:31:48,830 - INFO -   Batch 1250/1264: Loss = 0.1213
2025-12-27 02:34:47,750 - INFO - Epoch 98/160: Train Loss: 0.1139, Val Loss: 1.0492, Val mIoU: 43.88%, Val PixelAcc: 80.40%
2025-12-27 02:35:21,833 - INFO -   Batch 50/1264: Loss = 0.1030
2025-12-27 02:35:54,477 - INFO -   Batch 100/1264: Loss = 0.1098
2025-12-27 02:36:27,117 - INFO -   Batch 150/1264: Loss = 0.0893
2025-12-27 02:36:59,758 - INFO -   Batch 200/1264: Loss = 0.1171
2025-12-27 02:37:32,401 - INFO -   Batch 250/1264: Loss = 0.0787
2025-12-27 02:38:05,042 - INFO -   Batch 300/1264: Loss = 0.1202
2025-12-27 02:38:37,687 - INFO -   Batch 350/1264: Loss = 0.1230
2025-12-27 02:39:10,329 - INFO -   Batch 400/1264: Loss = 0.0976
2025-12-27 02:39:42,973 - INFO -   Batch 450/1264: Loss = 0.1041
2025-12-27 02:40:15,617 - INFO -   Batch 500/1264: Loss = 0.1344
2025-12-27 02:40:48,260 - INFO -   Batch 550/1264: Loss = 0.1083
2025-12-27 02:41:20,899 - INFO -   Batch 600/1264: Loss = 0.1153
2025-12-27 02:41:53,540 - INFO -   Batch 650/1264: Loss = 0.0980
2025-12-27 02:42:26,181 - INFO -   Batch 700/1264: Loss = 0.1300
2025-12-27 02:42:58,825 - INFO -   Batch 750/1264: Loss = 0.1016
2025-12-27 02:43:31,469 - INFO -   Batch 800/1264: Loss = 0.1097
2025-12-27 02:44:04,109 - INFO -   Batch 850/1264: Loss = 0.1060
2025-12-27 02:44:36,751 - INFO -   Batch 900/1264: Loss = 0.0969
2025-12-27 02:45:09,391 - INFO -   Batch 950/1264: Loss = 0.1121
2025-12-27 02:45:42,034 - INFO -   Batch 1000/1264: Loss = 0.0913
2025-12-27 02:46:14,677 - INFO -   Batch 1050/1264: Loss = 0.1108
2025-12-27 02:46:47,317 - INFO -   Batch 1100/1264: Loss = 0.1245
2025-12-27 02:47:19,957 - INFO -   Batch 1150/1264: Loss = 0.1107
2025-12-27 02:47:52,597 - INFO -   Batch 1200/1264: Loss = 0.1308
2025-12-27 02:48:25,241 - INFO -   Batch 1250/1264: Loss = 0.1703
2025-12-27 02:51:24,010 - INFO - Epoch 99/160: Train Loss: 0.1143, Val Loss: 1.0573, Val mIoU: 43.93%, Val PixelAcc: 80.38%
2025-12-27 02:51:58,728 - INFO -   Batch 50/1264: Loss = 0.0954
2025-12-27 02:52:31,367 - INFO -   Batch 100/1264: Loss = 0.1312
2025-12-27 02:53:04,010 - INFO -   Batch 150/1264: Loss = 0.0966
2025-12-27 02:53:36,653 - INFO -   Batch 200/1264: Loss = 0.1082
2025-12-27 02:54:09,293 - INFO -   Batch 250/1264: Loss = 0.1346
2025-12-27 02:54:41,936 - INFO -   Batch 300/1264: Loss = 0.0812
2025-12-27 02:55:14,581 - INFO -   Batch 350/1264: Loss = 0.1261
2025-12-27 02:55:47,228 - INFO -   Batch 400/1264: Loss = 0.0974
2025-12-27 02:56:19,868 - INFO -   Batch 450/1264: Loss = 0.1043
2025-12-27 02:56:52,508 - INFO -   Batch 500/1264: Loss = 0.1211
2025-12-27 02:57:25,150 - INFO -   Batch 550/1264: Loss = 0.1082
2025-12-27 02:57:57,803 - INFO -   Batch 600/1264: Loss = 0.1140
2025-12-27 02:58:30,453 - INFO -   Batch 650/1264: Loss = 0.0969
2025-12-27 02:59:03,097 - INFO -   Batch 700/1264: Loss = 0.0855
2025-12-27 02:59:35,741 - INFO -   Batch 750/1264: Loss = 0.0938
2025-12-27 03:00:08,380 - INFO -   Batch 800/1264: Loss = 0.1064
2025-12-27 03:00:41,023 - INFO -   Batch 850/1264: Loss = 0.1259
2025-12-27 03:01:13,671 - INFO -   Batch 900/1264: Loss = 0.0941
2025-12-27 03:01:46,314 - INFO -   Batch 950/1264: Loss = 0.1071
2025-12-27 03:02:18,959 - INFO -   Batch 1000/1264: Loss = 0.1061
2025-12-27 03:02:51,603 - INFO -   Batch 1050/1264: Loss = 0.1006
2025-12-27 03:03:24,245 - INFO -   Batch 1100/1264: Loss = 0.1244
2025-12-27 03:03:56,890 - INFO -   Batch 1150/1264: Loss = 0.0974
2025-12-27 03:04:29,534 - INFO -   Batch 1200/1264: Loss = 0.1048
2025-12-27 03:05:02,177 - INFO -   Batch 1250/1264: Loss = 0.1123
2025-12-27 03:08:00,708 - INFO - Epoch 100/160: Train Loss: 0.1112, Val Loss: 1.0536, Val mIoU: 43.88%, Val PixelAcc: 80.37%
2025-12-27 03:08:02,281 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_100.pth
2025-12-27 03:08:02,282 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_100.pth
2025-12-27 03:08:37,289 - INFO -   Batch 50/1264: Loss = 0.1254
2025-12-27 03:09:09,931 - INFO -   Batch 100/1264: Loss = 0.1110
2025-12-27 03:09:42,573 - INFO -   Batch 150/1264: Loss = 0.1210
2025-12-27 03:10:15,217 - INFO -   Batch 200/1264: Loss = 0.0857
2025-12-27 03:10:47,861 - INFO -   Batch 250/1264: Loss = 0.1040
2025-12-27 03:11:20,502 - INFO -   Batch 300/1264: Loss = 0.1026
2025-12-27 03:11:53,145 - INFO -   Batch 350/1264: Loss = 0.0835
2025-12-27 03:12:25,786 - INFO -   Batch 400/1264: Loss = 0.0982
2025-12-27 03:12:58,427 - INFO -   Batch 450/1264: Loss = 0.0886
2025-12-27 03:13:31,072 - INFO -   Batch 500/1264: Loss = 0.0699
2025-12-27 03:14:03,714 - INFO -   Batch 550/1264: Loss = 0.1233
2025-12-27 03:14:36,357 - INFO -   Batch 600/1264: Loss = 0.1442
2025-12-27 03:15:09,000 - INFO -   Batch 650/1264: Loss = 0.0754
2025-12-27 03:15:41,641 - INFO -   Batch 700/1264: Loss = 0.1216
2025-12-27 03:16:14,281 - INFO -   Batch 750/1264: Loss = 0.1212
2025-12-27 03:16:46,918 - INFO -   Batch 800/1264: Loss = 0.1214
2025-12-27 03:17:19,555 - INFO -   Batch 850/1264: Loss = 0.0976
2025-12-27 03:17:52,193 - INFO -   Batch 900/1264: Loss = 0.0847
2025-12-27 03:18:24,830 - INFO -   Batch 950/1264: Loss = 0.0999
2025-12-27 03:18:57,473 - INFO -   Batch 1000/1264: Loss = 0.0801
2025-12-27 03:19:30,117 - INFO -   Batch 1050/1264: Loss = 0.1226
2025-12-27 03:20:02,762 - INFO -   Batch 1100/1264: Loss = 0.1067
2025-12-27 03:20:35,406 - INFO -   Batch 1150/1264: Loss = 0.1427
2025-12-27 03:21:08,052 - INFO -   Batch 1200/1264: Loss = 0.0955
2025-12-27 03:21:40,695 - INFO -   Batch 1250/1264: Loss = 0.0987
2025-12-27 03:24:38,874 - INFO - Epoch 101/160: Train Loss: 0.1104, Val Loss: 1.0451, Val mIoU: 43.87%, Val PixelAcc: 80.36%
2025-12-27 03:25:13,867 - INFO -   Batch 50/1264: Loss = 0.1470
2025-12-27 03:25:46,508 - INFO -   Batch 100/1264: Loss = 0.1183
2025-12-27 03:26:19,151 - INFO -   Batch 150/1264: Loss = 0.1068
2025-12-27 03:26:51,793 - INFO -   Batch 200/1264: Loss = 0.0959
2025-12-27 03:27:24,436 - INFO -   Batch 250/1264: Loss = 0.0943
2025-12-27 03:27:57,080 - INFO -   Batch 300/1264: Loss = 0.1155
2025-12-27 03:28:29,719 - INFO -   Batch 350/1264: Loss = 0.1280
2025-12-27 03:29:02,360 - INFO -   Batch 400/1264: Loss = 0.0978
2025-12-27 03:29:34,998 - INFO -   Batch 450/1264: Loss = 0.1030
2025-12-27 03:30:07,635 - INFO -   Batch 500/1264: Loss = 0.1174
2025-12-27 03:30:40,277 - INFO -   Batch 550/1264: Loss = 0.1151
2025-12-27 03:31:12,923 - INFO -   Batch 600/1264: Loss = 0.1122
2025-12-27 03:31:45,564 - INFO -   Batch 650/1264: Loss = 0.1154
2025-12-27 03:32:18,205 - INFO -   Batch 700/1264: Loss = 0.1123
2025-12-27 03:32:50,844 - INFO -   Batch 750/1264: Loss = 0.1153
2025-12-27 03:33:23,482 - INFO -   Batch 800/1264: Loss = 0.0867
2025-12-27 03:33:56,122 - INFO -   Batch 850/1264: Loss = 0.0777
2025-12-27 03:34:28,765 - INFO -   Batch 900/1264: Loss = 0.0973
2025-12-27 03:35:01,409 - INFO -   Batch 950/1264: Loss = 0.1180
2025-12-27 03:35:34,052 - INFO -   Batch 1000/1264: Loss = 0.0908
2025-12-27 03:36:06,694 - INFO -   Batch 1050/1264: Loss = 0.1286
2025-12-27 03:36:39,338 - INFO -   Batch 1100/1264: Loss = 0.0932
2025-12-27 03:37:11,978 - INFO -   Batch 1150/1264: Loss = 0.1225
2025-12-27 03:37:44,616 - INFO -   Batch 1200/1264: Loss = 0.1362
2025-12-27 03:38:17,254 - INFO -   Batch 1250/1264: Loss = 0.1101
2025-12-27 03:41:15,897 - INFO - Epoch 102/160: Train Loss: 0.1096, Val Loss: 1.0561, Val mIoU: 43.92%, Val PixelAcc: 80.34%
2025-12-27 03:41:50,544 - INFO -   Batch 50/1264: Loss = 0.1092
2025-12-27 03:42:23,187 - INFO -   Batch 100/1264: Loss = 0.1024
2025-12-27 03:42:55,829 - INFO -   Batch 150/1264: Loss = 0.1140
2025-12-27 03:43:28,473 - INFO -   Batch 200/1264: Loss = 0.1203
2025-12-27 03:44:01,122 - INFO -   Batch 250/1264: Loss = 0.1449
2025-12-27 03:44:33,773 - INFO -   Batch 300/1264: Loss = 0.1022
2025-12-27 03:45:06,422 - INFO -   Batch 350/1264: Loss = 0.1093
2025-12-27 03:45:39,071 - INFO -   Batch 400/1264: Loss = 0.1383
2025-12-27 03:46:11,712 - INFO -   Batch 450/1264: Loss = 0.0973
2025-12-27 03:46:44,354 - INFO -   Batch 500/1264: Loss = 0.1272
2025-12-27 03:47:16,994 - INFO -   Batch 550/1264: Loss = 0.1176
2025-12-27 03:47:49,634 - INFO -   Batch 600/1264: Loss = 0.1122
2025-12-27 03:48:22,275 - INFO -   Batch 650/1264: Loss = 0.0850
2025-12-27 03:48:54,912 - INFO -   Batch 700/1264: Loss = 0.1341
2025-12-27 03:49:27,554 - INFO -   Batch 750/1264: Loss = 0.0917
2025-12-27 03:50:00,196 - INFO -   Batch 800/1264: Loss = 0.1135
2025-12-27 03:50:32,836 - INFO -   Batch 850/1264: Loss = 0.1139
2025-12-27 03:51:05,477 - INFO -   Batch 900/1264: Loss = 0.1058
2025-12-27 03:51:38,120 - INFO -   Batch 950/1264: Loss = 0.1297
2025-12-27 03:52:10,763 - INFO -   Batch 1000/1264: Loss = 0.1074
2025-12-27 03:52:43,405 - INFO -   Batch 1050/1264: Loss = 0.1036
2025-12-27 03:53:16,049 - INFO -   Batch 1100/1264: Loss = 0.0753
2025-12-27 03:53:48,690 - INFO -   Batch 1150/1264: Loss = 0.1191
2025-12-27 03:54:21,333 - INFO -   Batch 1200/1264: Loss = 0.0929
2025-12-27 03:54:53,973 - INFO -   Batch 1250/1264: Loss = 0.1139
2025-12-27 03:57:52,476 - INFO - Epoch 103/160: Train Loss: 0.1091, Val Loss: 1.0518, Val mIoU: 44.02%, Val PixelAcc: 80.42%
2025-12-27 03:58:27,265 - INFO -   Batch 50/1264: Loss = 0.0759
2025-12-27 03:58:59,912 - INFO -   Batch 100/1264: Loss = 0.1106
2025-12-27 03:59:32,557 - INFO -   Batch 150/1264: Loss = 0.1468
2025-12-27 04:00:05,201 - INFO -   Batch 200/1264: Loss = 0.0837
2025-12-27 04:00:37,840 - INFO -   Batch 250/1264: Loss = 0.1048
2025-12-27 04:01:10,480 - INFO -   Batch 300/1264: Loss = 0.1255
2025-12-27 04:01:43,121 - INFO -   Batch 350/1264: Loss = 0.0850
2025-12-27 04:02:15,761 - INFO -   Batch 400/1264: Loss = 0.1097
2025-12-27 04:02:48,401 - INFO -   Batch 450/1264: Loss = 0.0844
2025-12-27 04:03:21,044 - INFO -   Batch 500/1264: Loss = 0.1026
2025-12-27 04:03:53,691 - INFO -   Batch 550/1264: Loss = 0.1045
2025-12-27 04:04:26,334 - INFO -   Batch 600/1264: Loss = 0.0876
2025-12-27 04:04:58,974 - INFO -   Batch 650/1264: Loss = 0.1034
2025-12-27 04:05:31,614 - INFO -   Batch 700/1264: Loss = 0.1311
2025-12-27 04:06:04,258 - INFO -   Batch 750/1264: Loss = 0.1059
2025-12-27 04:06:36,904 - INFO -   Batch 800/1264: Loss = 0.1359
2025-12-27 04:07:09,550 - INFO -   Batch 850/1264: Loss = 0.1030
2025-12-27 04:07:42,195 - INFO -   Batch 900/1264: Loss = 0.1136
2025-12-27 04:08:14,839 - INFO -   Batch 950/1264: Loss = 0.1475
2025-12-27 04:08:47,487 - INFO -   Batch 1000/1264: Loss = 0.1080
2025-12-27 04:09:20,131 - INFO -   Batch 1050/1264: Loss = 0.0911
2025-12-27 04:09:52,777 - INFO -   Batch 1100/1264: Loss = 0.1029
2025-12-27 04:10:25,419 - INFO -   Batch 1150/1264: Loss = 0.0906
2025-12-27 04:10:58,059 - INFO -   Batch 1200/1264: Loss = 0.1203
2025-12-27 04:11:30,704 - INFO -   Batch 1250/1264: Loss = 0.1404
2025-12-27 04:14:29,175 - INFO - Epoch 104/160: Train Loss: 0.1088, Val Loss: 1.0713, Val mIoU: 43.94%, Val PixelAcc: 80.40%
2025-12-27 04:15:03,789 - INFO -   Batch 50/1264: Loss = 0.0634
2025-12-27 04:15:36,435 - INFO -   Batch 100/1264: Loss = 0.0985
2025-12-27 04:16:09,076 - INFO -   Batch 150/1264: Loss = 0.1197
2025-12-27 04:16:41,720 - INFO -   Batch 200/1264: Loss = 0.1040
2025-12-27 04:17:14,361 - INFO -   Batch 250/1264: Loss = 0.1023
2025-12-27 04:17:47,002 - INFO -   Batch 300/1264: Loss = 0.0692
2025-12-27 04:18:19,642 - INFO -   Batch 350/1264: Loss = 0.1091
2025-12-27 04:18:52,284 - INFO -   Batch 400/1264: Loss = 0.1274
2025-12-27 04:19:24,927 - INFO -   Batch 450/1264: Loss = 0.1098
2025-12-27 04:19:57,570 - INFO -   Batch 500/1264: Loss = 0.1500
2025-12-27 04:20:30,207 - INFO -   Batch 550/1264: Loss = 0.1224
2025-12-27 04:21:02,846 - INFO -   Batch 600/1264: Loss = 0.1249
2025-12-27 04:21:35,488 - INFO -   Batch 650/1264: Loss = 0.1014
2025-12-27 04:22:08,131 - INFO -   Batch 700/1264: Loss = 0.0825
2025-12-27 04:22:40,768 - INFO -   Batch 750/1264: Loss = 0.0823
2025-12-27 04:23:13,408 - INFO -   Batch 800/1264: Loss = 0.1073
2025-12-27 04:23:46,048 - INFO -   Batch 850/1264: Loss = 0.1004
2025-12-27 04:24:18,690 - INFO -   Batch 900/1264: Loss = 0.1050
2025-12-27 04:24:51,333 - INFO -   Batch 950/1264: Loss = 0.1173
2025-12-27 04:25:23,972 - INFO -   Batch 1000/1264: Loss = 0.1029
2025-12-27 04:25:56,614 - INFO -   Batch 1050/1264: Loss = 0.1079
2025-12-27 04:26:29,257 - INFO -   Batch 1100/1264: Loss = 0.0975
2025-12-27 04:27:01,898 - INFO -   Batch 1150/1264: Loss = 0.0878
2025-12-27 04:27:34,537 - INFO -   Batch 1200/1264: Loss = 0.1054
2025-12-27 04:28:07,177 - INFO -   Batch 1250/1264: Loss = 0.1346
2025-12-27 04:31:05,751 - INFO - Epoch 105/160: Train Loss: 0.1088, Val Loss: 1.0587, Val mIoU: 44.17%, Val PixelAcc: 80.36%
2025-12-27 04:31:05,752 - INFO -   → New best mIoU: 44.17%
2025-12-27 04:31:07,321 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-27 04:31:07,323 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-27 04:31:42,019 - INFO -   Batch 50/1264: Loss = 0.0984
2025-12-27 04:32:14,663 - INFO -   Batch 100/1264: Loss = 0.1061
2025-12-27 04:32:47,305 - INFO -   Batch 150/1264: Loss = 0.0940
2025-12-27 04:33:19,951 - INFO -   Batch 200/1264: Loss = 0.0930
2025-12-27 04:33:52,594 - INFO -   Batch 250/1264: Loss = 0.1306
2025-12-27 04:34:25,236 - INFO -   Batch 300/1264: Loss = 0.0949
2025-12-27 04:34:57,878 - INFO -   Batch 350/1264: Loss = 0.1189
2025-12-27 04:35:30,521 - INFO -   Batch 400/1264: Loss = 0.1098
2025-12-27 04:36:03,163 - INFO -   Batch 450/1264: Loss = 0.1012
2025-12-27 04:36:35,805 - INFO -   Batch 500/1264: Loss = 0.0973
2025-12-27 04:37:08,444 - INFO -   Batch 550/1264: Loss = 0.1014
2025-12-27 04:37:41,088 - INFO -   Batch 600/1264: Loss = 0.1174
2025-12-27 04:38:13,728 - INFO -   Batch 650/1264: Loss = 0.0971
2025-12-27 04:38:46,373 - INFO -   Batch 700/1264: Loss = 0.1085
2025-12-27 04:39:19,017 - INFO -   Batch 750/1264: Loss = 0.1083
2025-12-27 04:39:51,662 - INFO -   Batch 800/1264: Loss = 0.0847
2025-12-27 04:40:24,308 - INFO -   Batch 850/1264: Loss = 0.1199
2025-12-27 04:40:56,953 - INFO -   Batch 900/1264: Loss = 0.0971
2025-12-27 04:41:29,592 - INFO -   Batch 950/1264: Loss = 0.1232
2025-12-27 04:42:02,237 - INFO -   Batch 1000/1264: Loss = 0.1097
2025-12-27 04:42:34,884 - INFO -   Batch 1050/1264: Loss = 0.1021
2025-12-27 04:43:07,530 - INFO -   Batch 1100/1264: Loss = 0.0974
2025-12-27 04:43:40,177 - INFO -   Batch 1150/1264: Loss = 0.1076
2025-12-27 04:44:12,823 - INFO -   Batch 1200/1264: Loss = 0.1144
2025-12-27 04:44:45,469 - INFO -   Batch 1250/1264: Loss = 0.0771
2025-12-27 04:47:44,190 - INFO - Epoch 106/160: Train Loss: 0.1073, Val Loss: 1.0748, Val mIoU: 43.91%, Val PixelAcc: 80.35%
2025-12-27 04:48:18,928 - INFO -   Batch 50/1264: Loss = 0.0889
2025-12-27 04:48:51,576 - INFO -   Batch 100/1264: Loss = 0.1040
2025-12-27 04:49:24,225 - INFO -   Batch 150/1264: Loss = 0.1085
2025-12-27 04:49:56,872 - INFO -   Batch 200/1264: Loss = 0.0933
2025-12-27 04:50:29,516 - INFO -   Batch 250/1264: Loss = 0.1519
2025-12-27 04:51:02,159 - INFO -   Batch 300/1264: Loss = 0.1211
2025-12-27 04:51:34,800 - INFO -   Batch 350/1264: Loss = 0.1037
2025-12-27 04:52:07,440 - INFO -   Batch 400/1264: Loss = 0.0997
2025-12-27 04:52:40,081 - INFO -   Batch 450/1264: Loss = 0.1634
2025-12-27 04:53:12,720 - INFO -   Batch 500/1264: Loss = 0.0967
2025-12-27 04:53:45,361 - INFO -   Batch 550/1264: Loss = 0.0641
2025-12-27 04:54:18,003 - INFO -   Batch 600/1264: Loss = 0.1088
2025-12-27 04:54:50,644 - INFO -   Batch 650/1264: Loss = 0.1042
2025-12-27 04:55:23,288 - INFO -   Batch 700/1264: Loss = 0.0932
2025-12-27 04:55:55,930 - INFO -   Batch 750/1264: Loss = 0.0956
2025-12-27 04:56:28,569 - INFO -   Batch 800/1264: Loss = 0.1053
2025-12-27 04:57:01,210 - INFO -   Batch 850/1264: Loss = 0.0882
2025-12-27 04:57:33,853 - INFO -   Batch 900/1264: Loss = 0.1022
2025-12-27 04:58:06,497 - INFO -   Batch 950/1264: Loss = 0.0785
2025-12-27 04:58:39,143 - INFO -   Batch 1000/1264: Loss = 0.0889
2025-12-27 04:59:11,785 - INFO -   Batch 1050/1264: Loss = 0.1031
2025-12-27 04:59:44,428 - INFO -   Batch 1100/1264: Loss = 0.1352
2025-12-27 05:00:17,070 - INFO -   Batch 1150/1264: Loss = 0.1182
2025-12-27 05:00:49,710 - INFO -   Batch 1200/1264: Loss = 0.1125
2025-12-27 05:01:22,352 - INFO -   Batch 1250/1264: Loss = 0.1044
2025-12-27 05:04:20,985 - INFO - Epoch 107/160: Train Loss: 0.1069, Val Loss: 1.0704, Val mIoU: 43.94%, Val PixelAcc: 80.29%
2025-12-27 05:04:55,390 - INFO -   Batch 50/1264: Loss = 0.0953
2025-12-27 05:05:28,036 - INFO -   Batch 100/1264: Loss = 0.1304
2025-12-27 05:06:00,682 - INFO -   Batch 150/1264: Loss = 0.0768
2025-12-27 05:06:33,328 - INFO -   Batch 200/1264: Loss = 0.1132
2025-12-27 05:07:05,972 - INFO -   Batch 250/1264: Loss = 0.0941
2025-12-27 05:07:38,614 - INFO -   Batch 300/1264: Loss = 0.1088
2025-12-27 05:08:11,255 - INFO -   Batch 350/1264: Loss = 0.0965
2025-12-27 05:08:43,896 - INFO -   Batch 400/1264: Loss = 0.0650
2025-12-27 05:09:16,540 - INFO -   Batch 450/1264: Loss = 0.1065
2025-12-27 05:09:49,180 - INFO -   Batch 500/1264: Loss = 0.1049
2025-12-27 05:10:21,818 - INFO -   Batch 550/1264: Loss = 0.0942
2025-12-27 05:10:54,461 - INFO -   Batch 600/1264: Loss = 0.1017
2025-12-27 05:11:27,106 - INFO -   Batch 650/1264: Loss = 0.0957
2025-12-27 05:11:59,750 - INFO -   Batch 700/1264: Loss = 0.1039
2025-12-27 05:12:32,391 - INFO -   Batch 750/1264: Loss = 0.1007
2025-12-27 05:13:05,033 - INFO -   Batch 800/1264: Loss = 0.0796
2025-12-27 05:13:37,680 - INFO -   Batch 850/1264: Loss = 0.1184
2025-12-27 05:14:10,325 - INFO -   Batch 900/1264: Loss = 0.0827
2025-12-27 05:14:42,969 - INFO -   Batch 950/1264: Loss = 0.1164
2025-12-27 05:15:15,614 - INFO -   Batch 1000/1264: Loss = 0.0986
2025-12-27 05:15:48,258 - INFO -   Batch 1050/1264: Loss = 0.0676
2025-12-27 05:16:20,902 - INFO -   Batch 1100/1264: Loss = 0.1435
2025-12-27 05:16:53,543 - INFO -   Batch 1150/1264: Loss = 0.0963
2025-12-27 05:17:26,184 - INFO -   Batch 1200/1264: Loss = 0.0805
2025-12-27 05:17:58,825 - INFO -   Batch 1250/1264: Loss = 0.1106
2025-12-27 05:20:57,122 - INFO - Epoch 108/160: Train Loss: 0.1050, Val Loss: 1.0737, Val mIoU: 43.84%, Val PixelAcc: 80.27%
2025-12-27 05:21:32,117 - INFO -   Batch 50/1264: Loss = 0.0993
2025-12-27 05:22:04,759 - INFO -   Batch 100/1264: Loss = 0.1173
2025-12-27 05:22:37,398 - INFO -   Batch 150/1264: Loss = 0.1510
2025-12-27 05:23:10,041 - INFO -   Batch 200/1264: Loss = 0.1538
2025-12-27 05:23:42,680 - INFO -   Batch 250/1264: Loss = 0.0857
2025-12-27 05:24:15,318 - INFO -   Batch 300/1264: Loss = 0.0642
2025-12-27 05:24:47,960 - INFO -   Batch 350/1264: Loss = 0.1274
2025-12-27 05:25:20,600 - INFO -   Batch 400/1264: Loss = 0.1089
2025-12-27 05:25:53,239 - INFO -   Batch 450/1264: Loss = 0.1156
2025-12-27 05:26:25,880 - INFO -   Batch 500/1264: Loss = 0.0919
2025-12-27 05:26:58,519 - INFO -   Batch 550/1264: Loss = 0.0718
2025-12-27 05:27:31,154 - INFO -   Batch 600/1264: Loss = 0.0947
2025-12-27 05:28:03,794 - INFO -   Batch 650/1264: Loss = 0.0875
2025-12-27 05:28:36,437 - INFO -   Batch 700/1264: Loss = 0.1016
2025-12-27 05:29:09,080 - INFO -   Batch 750/1264: Loss = 0.0867
2025-12-27 05:29:41,722 - INFO -   Batch 800/1264: Loss = 0.0682
2025-12-27 05:30:14,368 - INFO -   Batch 850/1264: Loss = 0.0727
2025-12-27 05:30:47,011 - INFO -   Batch 900/1264: Loss = 0.1298
2025-12-27 05:31:19,653 - INFO -   Batch 950/1264: Loss = 0.1109
2025-12-27 05:31:52,295 - INFO -   Batch 1000/1264: Loss = 0.0860
2025-12-27 05:32:24,934 - INFO -   Batch 1050/1264: Loss = 0.1170
2025-12-27 05:32:57,570 - INFO -   Batch 1100/1264: Loss = 0.0928
2025-12-27 05:33:30,205 - INFO -   Batch 1150/1264: Loss = 0.1036
2025-12-27 05:34:02,839 - INFO -   Batch 1200/1264: Loss = 0.0989
2025-12-27 05:34:35,473 - INFO -   Batch 1250/1264: Loss = 0.0726
2025-12-27 05:37:33,849 - INFO - Epoch 109/160: Train Loss: 0.1043, Val Loss: 1.0679, Val mIoU: 44.15%, Val PixelAcc: 80.44%
2025-12-27 05:38:07,935 - INFO -   Batch 50/1264: Loss = 0.1174
2025-12-27 05:38:40,576 - INFO -   Batch 100/1264: Loss = 0.0773
2025-12-27 05:39:13,214 - INFO -   Batch 150/1264: Loss = 0.0901
2025-12-27 05:39:45,853 - INFO -   Batch 200/1264: Loss = 0.1068
2025-12-27 05:40:18,488 - INFO -   Batch 250/1264: Loss = 0.1005
2025-12-27 05:40:51,124 - INFO -   Batch 300/1264: Loss = 0.1271
2025-12-27 05:41:23,762 - INFO -   Batch 350/1264: Loss = 0.1111
2025-12-27 05:41:56,399 - INFO -   Batch 400/1264: Loss = 0.1036
2025-12-27 05:42:29,041 - INFO -   Batch 450/1264: Loss = 0.1033
2025-12-27 05:43:01,681 - INFO -   Batch 500/1264: Loss = 0.1087
2025-12-27 05:43:34,318 - INFO -   Batch 550/1264: Loss = 0.1220
2025-12-27 05:44:06,953 - INFO -   Batch 600/1264: Loss = 0.0986
2025-12-27 05:44:39,592 - INFO -   Batch 650/1264: Loss = 0.1215
2025-12-27 05:45:12,228 - INFO -   Batch 700/1264: Loss = 0.0841
2025-12-27 05:45:44,867 - INFO -   Batch 750/1264: Loss = 0.0972
2025-12-27 05:46:17,505 - INFO -   Batch 800/1264: Loss = 0.0929
2025-12-27 05:46:50,146 - INFO -   Batch 850/1264: Loss = 0.0824
2025-12-27 05:47:22,788 - INFO -   Batch 900/1264: Loss = 0.0977
2025-12-27 05:47:55,428 - INFO -   Batch 950/1264: Loss = 0.1200
2025-12-27 05:48:28,068 - INFO -   Batch 1000/1264: Loss = 0.0854
2025-12-27 05:49:00,713 - INFO -   Batch 1050/1264: Loss = 0.1163
2025-12-27 05:49:33,357 - INFO -   Batch 1100/1264: Loss = 0.1143
2025-12-27 05:50:06,001 - INFO -   Batch 1150/1264: Loss = 0.1093
2025-12-27 05:50:38,646 - INFO -   Batch 1200/1264: Loss = 0.1034
2025-12-27 05:51:11,288 - INFO -   Batch 1250/1264: Loss = 0.1136
2025-12-27 05:54:10,206 - INFO - Epoch 110/160: Train Loss: 0.1049, Val Loss: 1.0716, Val mIoU: 44.28%, Val PixelAcc: 80.38%
2025-12-27 05:54:10,208 - INFO -   → New best mIoU: 44.28%
2025-12-27 05:54:11,737 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-27 05:54:11,739 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-27 05:54:13,191 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_110.pth
2025-12-27 05:54:13,192 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_110.pth
2025-12-27 05:54:48,222 - INFO -   Batch 50/1264: Loss = 0.1188
2025-12-27 05:55:20,865 - INFO -   Batch 100/1264: Loss = 0.0918
2025-12-27 05:55:53,509 - INFO -   Batch 150/1264: Loss = 0.1272
2025-12-27 05:56:26,152 - INFO -   Batch 200/1264: Loss = 0.0940
2025-12-27 05:56:58,794 - INFO -   Batch 250/1264: Loss = 0.0925
2025-12-27 05:57:31,437 - INFO -   Batch 300/1264: Loss = 0.0751
2025-12-27 05:58:04,075 - INFO -   Batch 350/1264: Loss = 0.0931
2025-12-27 05:58:36,713 - INFO -   Batch 400/1264: Loss = 0.0834
2025-12-27 05:59:09,354 - INFO -   Batch 450/1264: Loss = 0.1131
2025-12-27 05:59:41,991 - INFO -   Batch 500/1264: Loss = 0.1124
2025-12-27 06:00:14,628 - INFO -   Batch 550/1264: Loss = 0.0789
2025-12-27 06:00:47,263 - INFO -   Batch 600/1264: Loss = 0.1064
2025-12-27 06:01:19,902 - INFO -   Batch 650/1264: Loss = 0.0724
2025-12-27 06:01:52,543 - INFO -   Batch 700/1264: Loss = 0.0829
2025-12-27 06:02:25,185 - INFO -   Batch 750/1264: Loss = 0.0831
2025-12-27 06:02:57,827 - INFO -   Batch 800/1264: Loss = 0.1036
2025-12-27 06:03:30,468 - INFO -   Batch 850/1264: Loss = 0.1376
2025-12-27 06:04:03,111 - INFO -   Batch 900/1264: Loss = 0.1353
2025-12-27 06:04:35,750 - INFO -   Batch 950/1264: Loss = 0.0977
2025-12-27 06:05:08,387 - INFO -   Batch 1000/1264: Loss = 0.1051
2025-12-27 06:05:41,028 - INFO -   Batch 1050/1264: Loss = 0.0931
2025-12-27 06:06:13,666 - INFO -   Batch 1100/1264: Loss = 0.0942
2025-12-27 06:06:46,309 - INFO -   Batch 1150/1264: Loss = 0.0866
2025-12-27 06:07:18,948 - INFO -   Batch 1200/1264: Loss = 0.0909
2025-12-27 06:07:51,588 - INFO -   Batch 1250/1264: Loss = 0.1191
2025-12-27 06:10:50,306 - INFO - Epoch 111/160: Train Loss: 0.1041, Val Loss: 1.0706, Val mIoU: 44.01%, Val PixelAcc: 80.40%
2025-12-27 06:11:25,169 - INFO -   Batch 50/1264: Loss = 0.1315
2025-12-27 06:11:57,814 - INFO -   Batch 100/1264: Loss = 0.1234
2025-12-27 06:12:30,461 - INFO -   Batch 150/1264: Loss = 0.1226
2025-12-27 06:13:03,104 - INFO -   Batch 200/1264: Loss = 0.0675
2025-12-27 06:13:35,747 - INFO -   Batch 250/1264: Loss = 0.1182
2025-12-27 06:14:08,389 - INFO -   Batch 300/1264: Loss = 0.1447
2025-12-27 06:14:41,033 - INFO -   Batch 350/1264: Loss = 0.1044
2025-12-27 06:15:13,677 - INFO -   Batch 400/1264: Loss = 0.0925
2025-12-27 06:15:46,321 - INFO -   Batch 450/1264: Loss = 0.1121
2025-12-27 06:16:18,963 - INFO -   Batch 500/1264: Loss = 0.0757
2025-12-27 06:16:51,603 - INFO -   Batch 550/1264: Loss = 0.1125
2025-12-27 06:17:24,245 - INFO -   Batch 600/1264: Loss = 0.0809
2025-12-27 06:17:56,894 - INFO -   Batch 650/1264: Loss = 0.1163
2025-12-27 06:18:29,542 - INFO -   Batch 700/1264: Loss = 0.1073
2025-12-27 06:19:02,188 - INFO -   Batch 750/1264: Loss = 0.1403
2025-12-27 06:19:34,835 - INFO -   Batch 800/1264: Loss = 0.1012
2025-12-27 06:20:07,479 - INFO -   Batch 850/1264: Loss = 0.0738
2025-12-27 06:20:40,120 - INFO -   Batch 900/1264: Loss = 0.1080
2025-12-27 06:21:12,760 - INFO -   Batch 950/1264: Loss = 0.0774
2025-12-27 06:21:45,399 - INFO -   Batch 1000/1264: Loss = 0.0918
2025-12-27 06:22:18,039 - INFO -   Batch 1050/1264: Loss = 0.0980
2025-12-27 06:22:50,682 - INFO -   Batch 1100/1264: Loss = 0.0895
2025-12-27 06:23:23,323 - INFO -   Batch 1150/1264: Loss = 0.0907
2025-12-27 06:23:55,962 - INFO -   Batch 1200/1264: Loss = 0.0839
2025-12-27 06:24:28,604 - INFO -   Batch 1250/1264: Loss = 0.0926
2025-12-27 06:27:27,134 - INFO - Epoch 112/160: Train Loss: 0.1019, Val Loss: 1.0823, Val mIoU: 43.79%, Val PixelAcc: 80.35%
2025-12-27 06:28:01,574 - INFO -   Batch 50/1264: Loss = 0.1120
2025-12-27 06:28:34,217 - INFO -   Batch 100/1264: Loss = 0.1121
2025-12-27 06:29:06,859 - INFO -   Batch 150/1264: Loss = 0.0957
2025-12-27 06:29:39,498 - INFO -   Batch 200/1264: Loss = 0.1003
2025-12-27 06:30:12,138 - INFO -   Batch 250/1264: Loss = 0.1322
2025-12-27 06:30:44,780 - INFO -   Batch 300/1264: Loss = 0.0682
2025-12-27 06:31:17,424 - INFO -   Batch 350/1264: Loss = 0.1002
2025-12-27 06:31:50,067 - INFO -   Batch 400/1264: Loss = 0.0752
2025-12-27 06:32:22,711 - INFO -   Batch 450/1264: Loss = 0.1209
2025-12-27 06:32:55,349 - INFO -   Batch 500/1264: Loss = 0.0799
2025-12-27 06:33:27,986 - INFO -   Batch 550/1264: Loss = 0.0836
2025-12-27 06:34:00,622 - INFO -   Batch 600/1264: Loss = 0.1087
2025-12-27 06:34:33,265 - INFO -   Batch 650/1264: Loss = 0.0969
2025-12-27 06:35:05,906 - INFO -   Batch 700/1264: Loss = 0.0968
2025-12-27 06:35:38,548 - INFO -   Batch 750/1264: Loss = 0.0870
2025-12-27 06:36:11,190 - INFO -   Batch 800/1264: Loss = 0.1296
2025-12-27 06:36:46,382 - INFO -   Batch 850/1264: Loss = 0.1175
2025-12-27 06:37:19,022 - INFO -   Batch 900/1264: Loss = 0.0980
2025-12-27 06:37:51,665 - INFO -   Batch 950/1264: Loss = 0.1275
2025-12-27 06:38:24,308 - INFO -   Batch 1000/1264: Loss = 0.1024
2025-12-27 06:38:56,949 - INFO -   Batch 1050/1264: Loss = 0.1164
2025-12-27 06:39:29,590 - INFO -   Batch 1100/1264: Loss = 0.0725
2025-12-27 06:40:02,228 - INFO -   Batch 1150/1264: Loss = 0.0977
2025-12-27 06:40:34,865 - INFO -   Batch 1200/1264: Loss = 0.1072
2025-12-27 06:41:07,504 - INFO -   Batch 1250/1264: Loss = 0.0972
2025-12-27 06:44:05,815 - INFO - Epoch 113/160: Train Loss: 0.1009, Val Loss: 1.0762, Val mIoU: 44.12%, Val PixelAcc: 80.36%
2025-12-27 06:44:40,785 - INFO -   Batch 50/1264: Loss = 0.0769
2025-12-27 06:45:13,427 - INFO -   Batch 100/1264: Loss = 0.1084
2025-12-27 06:45:46,065 - INFO -   Batch 150/1264: Loss = 0.0966
2025-12-27 06:46:18,707 - INFO -   Batch 200/1264: Loss = 0.0846
2025-12-27 06:46:51,348 - INFO -   Batch 250/1264: Loss = 0.0993
2025-12-27 06:47:23,985 - INFO -   Batch 300/1264: Loss = 0.1154
2025-12-27 06:47:56,625 - INFO -   Batch 350/1264: Loss = 0.0738
2025-12-27 06:48:29,261 - INFO -   Batch 400/1264: Loss = 0.0947
2025-12-27 06:49:01,897 - INFO -   Batch 450/1264: Loss = 0.0838
2025-12-27 06:49:34,533 - INFO -   Batch 500/1264: Loss = 0.0764
2025-12-27 06:50:07,171 - INFO -   Batch 550/1264: Loss = 0.1157
2025-12-27 06:50:39,809 - INFO -   Batch 600/1264: Loss = 0.1214
2025-12-27 06:51:12,452 - INFO -   Batch 650/1264: Loss = 0.0972
2025-12-27 06:51:45,095 - INFO -   Batch 700/1264: Loss = 0.0922
2025-12-27 06:52:17,734 - INFO -   Batch 750/1264: Loss = 0.1088
2025-12-27 06:52:50,372 - INFO -   Batch 800/1264: Loss = 0.1276
2025-12-27 06:53:23,013 - INFO -   Batch 850/1264: Loss = 0.1235
2025-12-27 06:53:55,652 - INFO -   Batch 900/1264: Loss = 0.1433
2025-12-27 06:54:28,293 - INFO -   Batch 950/1264: Loss = 0.1023
2025-12-27 06:55:00,934 - INFO -   Batch 1000/1264: Loss = 0.0870
2025-12-27 06:55:33,576 - INFO -   Batch 1050/1264: Loss = 0.1233
2025-12-27 06:56:06,216 - INFO -   Batch 1100/1264: Loss = 0.0923
2025-12-27 06:56:38,858 - INFO -   Batch 1150/1264: Loss = 0.0678
2025-12-27 06:57:11,496 - INFO -   Batch 1200/1264: Loss = 0.0871
2025-12-27 06:57:44,137 - INFO -   Batch 1250/1264: Loss = 0.0989
2025-12-27 07:00:43,040 - INFO - Epoch 114/160: Train Loss: 0.1014, Val Loss: 1.0848, Val mIoU: 43.84%, Val PixelAcc: 80.37%
2025-12-27 07:01:17,074 - INFO -   Batch 50/1264: Loss = 0.0904
2025-12-27 07:01:49,717 - INFO -   Batch 100/1264: Loss = 0.0911
2025-12-27 07:02:22,357 - INFO -   Batch 150/1264: Loss = 0.0831
2025-12-27 07:02:54,999 - INFO -   Batch 200/1264: Loss = 0.1039
2025-12-27 07:03:27,637 - INFO -   Batch 250/1264: Loss = 0.0855
2025-12-27 07:04:00,275 - INFO -   Batch 300/1264: Loss = 0.1116
2025-12-27 07:04:32,916 - INFO -   Batch 350/1264: Loss = 0.1015
2025-12-27 07:05:05,554 - INFO -   Batch 400/1264: Loss = 0.1179
2025-12-27 07:05:38,192 - INFO -   Batch 450/1264: Loss = 0.0679
2025-12-27 07:06:10,833 - INFO -   Batch 500/1264: Loss = 0.1356
2025-12-27 07:06:43,472 - INFO -   Batch 550/1264: Loss = 0.0821
2025-12-27 07:07:16,111 - INFO -   Batch 600/1264: Loss = 0.0840
2025-12-27 07:07:48,749 - INFO -   Batch 650/1264: Loss = 0.0917
2025-12-27 07:08:21,389 - INFO -   Batch 700/1264: Loss = 0.1207
2025-12-27 07:08:54,033 - INFO -   Batch 750/1264: Loss = 0.0891
2025-12-27 07:09:26,678 - INFO -   Batch 800/1264: Loss = 0.1108
2025-12-27 07:09:59,321 - INFO -   Batch 850/1264: Loss = 0.0838
2025-12-27 07:10:31,966 - INFO -   Batch 900/1264: Loss = 0.1017
2025-12-27 07:11:04,609 - INFO -   Batch 950/1264: Loss = 0.0894
2025-12-27 07:11:37,254 - INFO -   Batch 1000/1264: Loss = 0.0758
2025-12-27 07:12:09,899 - INFO -   Batch 1050/1264: Loss = 0.0839
2025-12-27 07:12:42,545 - INFO -   Batch 1100/1264: Loss = 0.1014
2025-12-27 07:13:15,195 - INFO -   Batch 1150/1264: Loss = 0.1092
2025-12-27 07:13:47,836 - INFO -   Batch 1200/1264: Loss = 0.0994
2025-12-27 07:14:20,478 - INFO -   Batch 1250/1264: Loss = 0.1019
2025-12-27 07:17:18,924 - INFO - Epoch 115/160: Train Loss: 0.1013, Val Loss: 1.0935, Val mIoU: 43.70%, Val PixelAcc: 80.29%
2025-12-27 07:17:53,899 - INFO -   Batch 50/1264: Loss = 0.0858
2025-12-27 07:18:26,541 - INFO -   Batch 100/1264: Loss = 0.1049
2025-12-27 07:18:59,183 - INFO -   Batch 150/1264: Loss = 0.1004
2025-12-27 07:19:31,822 - INFO -   Batch 200/1264: Loss = 0.0756
2025-12-27 07:20:04,461 - INFO -   Batch 250/1264: Loss = 0.0587
2025-12-27 07:20:37,106 - INFO -   Batch 300/1264: Loss = 0.1235
2025-12-27 07:21:09,754 - INFO -   Batch 350/1264: Loss = 0.1205
2025-12-27 07:21:42,402 - INFO -   Batch 400/1264: Loss = 0.0867
2025-12-27 07:22:15,053 - INFO -   Batch 450/1264: Loss = 0.0709
2025-12-27 07:22:47,701 - INFO -   Batch 500/1264: Loss = 0.0716
2025-12-27 07:23:20,350 - INFO -   Batch 550/1264: Loss = 0.1055
2025-12-27 07:23:53,000 - INFO -   Batch 600/1264: Loss = 0.1060
2025-12-27 07:24:25,646 - INFO -   Batch 650/1264: Loss = 0.1047
2025-12-27 07:24:58,291 - INFO -   Batch 700/1264: Loss = 0.0956
2025-12-27 07:25:30,937 - INFO -   Batch 750/1264: Loss = 0.1038
2025-12-27 07:26:03,580 - INFO -   Batch 800/1264: Loss = 0.0900
2025-12-27 07:26:36,225 - INFO -   Batch 850/1264: Loss = 0.0855
2025-12-27 07:27:08,868 - INFO -   Batch 900/1264: Loss = 0.1108
2025-12-27 07:27:41,509 - INFO -   Batch 950/1264: Loss = 0.0934
2025-12-27 07:28:14,150 - INFO -   Batch 1000/1264: Loss = 0.0715
2025-12-27 07:28:46,788 - INFO -   Batch 1050/1264: Loss = 0.0958
2025-12-27 07:29:19,426 - INFO -   Batch 1100/1264: Loss = 0.1044
2025-12-27 07:29:52,063 - INFO -   Batch 1150/1264: Loss = 0.1009
2025-12-27 07:30:24,700 - INFO -   Batch 1200/1264: Loss = 0.0946
2025-12-27 07:30:57,337 - INFO -   Batch 1250/1264: Loss = 0.0758
2025-12-27 07:33:55,870 - INFO - Epoch 116/160: Train Loss: 0.1003, Val Loss: 1.0968, Val mIoU: 43.99%, Val PixelAcc: 80.31%
2025-12-27 07:34:30,611 - INFO -   Batch 50/1264: Loss = 0.0895
2025-12-27 07:35:03,256 - INFO -   Batch 100/1264: Loss = 0.0903
2025-12-27 07:35:35,901 - INFO -   Batch 150/1264: Loss = 0.1118
2025-12-27 07:36:08,538 - INFO -   Batch 200/1264: Loss = 0.1069
2025-12-27 07:36:41,177 - INFO -   Batch 250/1264: Loss = 0.1357
2025-12-27 07:37:13,816 - INFO -   Batch 300/1264: Loss = 0.0896
2025-12-27 07:37:46,457 - INFO -   Batch 350/1264: Loss = 0.0990
2025-12-27 07:38:19,099 - INFO -   Batch 400/1264: Loss = 0.0952
2025-12-27 07:38:51,739 - INFO -   Batch 450/1264: Loss = 0.0775
2025-12-27 07:39:24,381 - INFO -   Batch 500/1264: Loss = 0.0916
2025-12-27 07:39:57,022 - INFO -   Batch 550/1264: Loss = 0.1002
2025-12-27 07:40:29,663 - INFO -   Batch 600/1264: Loss = 0.1173
2025-12-27 07:41:02,303 - INFO -   Batch 650/1264: Loss = 0.1139
2025-12-27 07:41:34,944 - INFO -   Batch 700/1264: Loss = 0.1095
2025-12-27 07:42:07,591 - INFO -   Batch 750/1264: Loss = 0.0977
2025-12-27 07:42:40,237 - INFO -   Batch 800/1264: Loss = 0.1153
2025-12-27 07:43:12,879 - INFO -   Batch 850/1264: Loss = 0.0939
2025-12-27 07:43:45,518 - INFO -   Batch 900/1264: Loss = 0.1131
2025-12-27 07:44:18,158 - INFO -   Batch 950/1264: Loss = 0.0988
2025-12-27 07:44:50,796 - INFO -   Batch 1000/1264: Loss = 0.0979
2025-12-27 07:45:23,437 - INFO -   Batch 1050/1264: Loss = 0.1149
2025-12-27 07:45:56,077 - INFO -   Batch 1100/1264: Loss = 0.0981
2025-12-27 07:46:28,716 - INFO -   Batch 1150/1264: Loss = 0.0849
2025-12-27 07:47:01,359 - INFO -   Batch 1200/1264: Loss = 0.1383
2025-12-27 07:47:34,002 - INFO -   Batch 1250/1264: Loss = 0.0876
2025-12-27 07:50:32,539 - INFO - Epoch 117/160: Train Loss: 0.1001, Val Loss: 1.0939, Val mIoU: 43.80%, Val PixelAcc: 80.42%
2025-12-27 07:51:07,517 - INFO -   Batch 50/1264: Loss = 0.1134
2025-12-27 07:51:40,164 - INFO -   Batch 100/1264: Loss = 0.0793
2025-12-27 07:52:12,812 - INFO -   Batch 150/1264: Loss = 0.0959
2025-12-27 07:52:45,457 - INFO -   Batch 200/1264: Loss = 0.0837
2025-12-27 07:53:18,104 - INFO -   Batch 250/1264: Loss = 0.1229
2025-12-27 07:53:50,748 - INFO -   Batch 300/1264: Loss = 0.0965
2025-12-27 07:54:23,387 - INFO -   Batch 350/1264: Loss = 0.1119
2025-12-27 07:54:56,028 - INFO -   Batch 400/1264: Loss = 0.0895
2025-12-27 07:55:28,673 - INFO -   Batch 450/1264: Loss = 0.0942
2025-12-27 07:56:01,313 - INFO -   Batch 500/1264: Loss = 0.0934
2025-12-27 07:56:33,950 - INFO -   Batch 550/1264: Loss = 0.1109
2025-12-27 07:57:06,591 - INFO -   Batch 600/1264: Loss = 0.0936
2025-12-27 07:57:39,234 - INFO -   Batch 650/1264: Loss = 0.1133
2025-12-27 07:58:11,876 - INFO -   Batch 700/1264: Loss = 0.0942
2025-12-27 07:58:44,517 - INFO -   Batch 750/1264: Loss = 0.0855
2025-12-27 07:59:17,156 - INFO -   Batch 800/1264: Loss = 0.1193
2025-12-27 07:59:49,796 - INFO -   Batch 850/1264: Loss = 0.0905
2025-12-27 08:00:22,433 - INFO -   Batch 900/1264: Loss = 0.1083
2025-12-27 08:00:55,082 - INFO -   Batch 950/1264: Loss = 0.0993
2025-12-27 08:01:27,717 - INFO -   Batch 1000/1264: Loss = 0.1064
2025-12-27 08:02:00,352 - INFO -   Batch 1050/1264: Loss = 0.0647
2025-12-27 08:02:32,988 - INFO -   Batch 1100/1264: Loss = 0.1448
2025-12-27 08:03:05,625 - INFO -   Batch 1150/1264: Loss = 0.0711
2025-12-27 08:03:38,258 - INFO -   Batch 1200/1264: Loss = 0.0968
2025-12-27 08:04:10,895 - INFO -   Batch 1250/1264: Loss = 0.0926
2025-12-27 08:07:10,056 - INFO - Epoch 118/160: Train Loss: 0.0991, Val Loss: 1.0858, Val mIoU: 43.97%, Val PixelAcc: 80.40%
2025-12-27 08:07:44,881 - INFO -   Batch 50/1264: Loss = 0.1064
2025-12-27 08:08:17,523 - INFO -   Batch 100/1264: Loss = 0.1290
2025-12-27 08:08:50,170 - INFO -   Batch 150/1264: Loss = 0.0902
2025-12-27 08:09:22,811 - INFO -   Batch 200/1264: Loss = 0.0890
2025-12-27 08:09:55,448 - INFO -   Batch 250/1264: Loss = 0.0771
2025-12-27 08:10:28,094 - INFO -   Batch 300/1264: Loss = 0.0991
2025-12-27 08:11:00,738 - INFO -   Batch 350/1264: Loss = 0.1500
2025-12-27 08:11:33,384 - INFO -   Batch 400/1264: Loss = 0.0834
2025-12-27 08:12:06,030 - INFO -   Batch 450/1264: Loss = 0.1188
2025-12-27 08:12:38,675 - INFO -   Batch 500/1264: Loss = 0.1013
2025-12-27 08:13:11,314 - INFO -   Batch 550/1264: Loss = 0.0833
2025-12-27 08:13:43,959 - INFO -   Batch 600/1264: Loss = 0.0716
2025-12-27 08:14:16,603 - INFO -   Batch 650/1264: Loss = 0.0960
2025-12-27 08:14:49,248 - INFO -   Batch 700/1264: Loss = 0.1308
2025-12-27 08:15:21,897 - INFO -   Batch 750/1264: Loss = 0.1169
2025-12-27 08:15:54,547 - INFO -   Batch 800/1264: Loss = 0.1119
2025-12-27 08:16:27,191 - INFO -   Batch 850/1264: Loss = 0.0893
2025-12-27 08:16:59,835 - INFO -   Batch 900/1264: Loss = 0.0961
2025-12-27 08:17:32,478 - INFO -   Batch 950/1264: Loss = 0.0951
2025-12-27 08:18:05,121 - INFO -   Batch 1000/1264: Loss = 0.0637
2025-12-27 08:18:37,766 - INFO -   Batch 1050/1264: Loss = 0.0862
2025-12-27 08:19:10,410 - INFO -   Batch 1100/1264: Loss = 0.1068
2025-12-27 08:19:43,056 - INFO -   Batch 1150/1264: Loss = 0.1212
2025-12-27 08:20:15,698 - INFO -   Batch 1200/1264: Loss = 0.0737
2025-12-27 08:20:48,341 - INFO -   Batch 1250/1264: Loss = 0.0938
2025-12-27 08:23:47,205 - INFO - Epoch 119/160: Train Loss: 0.0990, Val Loss: 1.0916, Val mIoU: 44.29%, Val PixelAcc: 80.46%
2025-12-27 08:23:47,206 - INFO -   → New best mIoU: 44.29%
2025-12-27 08:23:49,222 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-27 08:23:49,224 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-27 08:24:23,993 - INFO -   Batch 50/1264: Loss = 0.1063
2025-12-27 08:24:56,633 - INFO -   Batch 100/1264: Loss = 0.1094
2025-12-27 08:25:29,278 - INFO -   Batch 150/1264: Loss = 0.0918
2025-12-27 08:26:01,922 - INFO -   Batch 200/1264: Loss = 0.0776
2025-12-27 08:26:34,567 - INFO -   Batch 250/1264: Loss = 0.1090
2025-12-27 08:27:07,211 - INFO -   Batch 300/1264: Loss = 0.0882
2025-12-27 08:27:39,856 - INFO -   Batch 350/1264: Loss = 0.0668
2025-12-27 08:28:12,500 - INFO -   Batch 400/1264: Loss = 0.1280
2025-12-27 08:28:45,147 - INFO -   Batch 450/1264: Loss = 0.1020
2025-12-27 08:29:17,795 - INFO -   Batch 500/1264: Loss = 0.1204
2025-12-27 08:29:50,441 - INFO -   Batch 550/1264: Loss = 0.0990
2025-12-27 08:30:23,086 - INFO -   Batch 600/1264: Loss = 0.1106
2025-12-27 08:30:55,738 - INFO -   Batch 650/1264: Loss = 0.0841
2025-12-27 08:31:28,390 - INFO -   Batch 700/1264: Loss = 0.1071
2025-12-27 08:32:01,040 - INFO -   Batch 750/1264: Loss = 0.0888
2025-12-27 08:32:33,688 - INFO -   Batch 800/1264: Loss = 0.1029
2025-12-27 08:33:06,337 - INFO -   Batch 850/1264: Loss = 0.0926
2025-12-27 08:33:38,987 - INFO -   Batch 900/1264: Loss = 0.0773
2025-12-27 08:34:11,633 - INFO -   Batch 950/1264: Loss = 0.0906
2025-12-27 08:34:44,283 - INFO -   Batch 1000/1264: Loss = 0.1014
2025-12-27 08:35:16,933 - INFO -   Batch 1050/1264: Loss = 0.1281
2025-12-27 08:35:49,581 - INFO -   Batch 1100/1264: Loss = 0.0855
2025-12-27 08:36:22,479 - INFO -   Batch 1150/1264: Loss = 0.1372
2025-12-27 08:36:55,661 - INFO -   Batch 1200/1264: Loss = 0.0864
2025-12-27 08:37:28,835 - INFO -   Batch 1250/1264: Loss = 0.1172
2025-12-27 08:40:28,088 - INFO - Epoch 120/160: Train Loss: 0.0980, Val Loss: 1.1094, Val mIoU: 43.95%, Val PixelAcc: 80.42%
2025-12-27 08:40:29,598 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_120.pth
2025-12-27 08:40:29,600 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_120.pth
2025-12-27 08:41:04,748 - INFO -   Batch 50/1264: Loss = 0.0745
2025-12-27 08:41:38,206 - INFO -   Batch 100/1264: Loss = 0.0834
2025-12-27 08:42:11,648 - INFO -   Batch 150/1264: Loss = 0.1077
2025-12-27 08:42:45,119 - INFO -   Batch 200/1264: Loss = 0.0979
2025-12-27 08:43:18,595 - INFO -   Batch 250/1264: Loss = 0.0975
2025-12-27 08:43:52,119 - INFO -   Batch 300/1264: Loss = 0.0947
2025-12-27 08:44:25,721 - INFO -   Batch 350/1264: Loss = 0.1020
2025-12-27 08:44:59,301 - INFO -   Batch 400/1264: Loss = 0.1203
2025-12-27 08:45:33,022 - INFO -   Batch 450/1264: Loss = 0.0957
2025-12-27 08:46:06,808 - INFO -   Batch 500/1264: Loss = 0.1306
2025-12-27 08:46:40,690 - INFO -   Batch 550/1264: Loss = 0.1045
2025-12-27 08:47:14,586 - INFO -   Batch 600/1264: Loss = 0.0739
2025-12-27 08:47:48,359 - INFO -   Batch 650/1264: Loss = 0.1010
2025-12-27 08:48:22,158 - INFO -   Batch 700/1264: Loss = 0.0980
2025-12-27 08:48:55,960 - INFO -   Batch 750/1264: Loss = 0.0807
2025-12-27 08:49:29,892 - INFO -   Batch 800/1264: Loss = 0.0877
2025-12-27 08:50:03,783 - INFO -   Batch 850/1264: Loss = 0.1069
2025-12-27 08:50:37,575 - INFO -   Batch 900/1264: Loss = 0.0897
2025-12-27 08:51:11,441 - INFO -   Batch 950/1264: Loss = 0.0964
2025-12-27 08:51:45,251 - INFO -   Batch 1000/1264: Loss = 0.1001
2025-12-27 08:52:19,026 - INFO -   Batch 1050/1264: Loss = 0.0840
2025-12-27 08:52:52,701 - INFO -   Batch 1100/1264: Loss = 0.0783
2025-12-27 08:53:26,368 - INFO -   Batch 1150/1264: Loss = 0.0859
2025-12-27 08:54:00,127 - INFO -   Batch 1200/1264: Loss = 0.0751
2025-12-27 08:54:33,872 - INFO -   Batch 1250/1264: Loss = 0.0955
2025-12-27 08:57:32,824 - INFO - Epoch 121/160: Train Loss: 0.0976, Val Loss: 1.0892, Val mIoU: 44.02%, Val PixelAcc: 80.42%
2025-12-27 08:58:07,667 - INFO -   Batch 50/1264: Loss = 0.1082
2025-12-27 08:58:40,306 - INFO -   Batch 100/1264: Loss = 0.1091
2025-12-27 08:59:12,948 - INFO -   Batch 150/1264: Loss = 0.0659
2025-12-27 08:59:45,589 - INFO -   Batch 200/1264: Loss = 0.1071
2025-12-27 09:00:18,233 - INFO -   Batch 250/1264: Loss = 0.0977
2025-12-27 09:00:50,877 - INFO -   Batch 300/1264: Loss = 0.1222
2025-12-27 09:01:23,520 - INFO -   Batch 350/1264: Loss = 0.1039
2025-12-27 09:01:56,167 - INFO -   Batch 400/1264: Loss = 0.1079
2025-12-27 09:02:28,810 - INFO -   Batch 450/1264: Loss = 0.0818
2025-12-27 09:03:01,456 - INFO -   Batch 500/1264: Loss = 0.1083
2025-12-27 09:03:34,107 - INFO -   Batch 550/1264: Loss = 0.0985
2025-12-27 09:04:06,752 - INFO -   Batch 600/1264: Loss = 0.0665
2025-12-27 09:04:39,398 - INFO -   Batch 650/1264: Loss = 0.0754
2025-12-27 09:05:12,040 - INFO -   Batch 700/1264: Loss = 0.0687
2025-12-27 09:05:44,683 - INFO -   Batch 750/1264: Loss = 0.1054
2025-12-27 09:06:17,328 - INFO -   Batch 800/1264: Loss = 0.0807
2025-12-27 09:06:49,969 - INFO -   Batch 850/1264: Loss = 0.1129
2025-12-27 09:07:22,617 - INFO -   Batch 900/1264: Loss = 0.0859
2025-12-27 09:07:55,262 - INFO -   Batch 950/1264: Loss = 0.1108
2025-12-27 09:08:27,904 - INFO -   Batch 1000/1264: Loss = 0.1310
2025-12-27 09:09:00,549 - INFO -   Batch 1050/1264: Loss = 0.0775
2025-12-27 09:09:33,191 - INFO -   Batch 1100/1264: Loss = 0.1666
2025-12-27 09:10:05,833 - INFO -   Batch 1150/1264: Loss = 0.1013
2025-12-27 09:10:38,477 - INFO -   Batch 1200/1264: Loss = 0.1342
2025-12-27 09:11:11,128 - INFO -   Batch 1250/1264: Loss = 0.0841
2025-12-27 09:14:09,964 - INFO - Epoch 122/160: Train Loss: 0.0973, Val Loss: 1.0938, Val mIoU: 44.04%, Val PixelAcc: 80.50%
2025-12-27 09:14:45,533 - INFO -   Batch 50/1264: Loss = 0.0837
2025-12-27 09:15:18,970 - INFO -   Batch 100/1264: Loss = 0.0941
2025-12-27 09:15:52,377 - INFO -   Batch 150/1264: Loss = 0.1106
2025-12-27 09:16:25,801 - INFO -   Batch 200/1264: Loss = 0.1353
2025-12-27 09:16:59,176 - INFO -   Batch 250/1264: Loss = 0.0907
2025-12-27 09:17:32,584 - INFO -   Batch 300/1264: Loss = 0.1211
2025-12-27 09:18:05,955 - INFO -   Batch 350/1264: Loss = 0.1359
2025-12-27 09:18:39,268 - INFO -   Batch 400/1264: Loss = 0.1398
2025-12-27 09:19:12,537 - INFO -   Batch 450/1264: Loss = 0.0818
2025-12-27 09:19:45,844 - INFO -   Batch 500/1264: Loss = 0.1338
2025-12-27 09:20:19,176 - INFO -   Batch 550/1264: Loss = 0.0928
2025-12-27 09:20:52,477 - INFO -   Batch 600/1264: Loss = 0.1002
2025-12-27 09:21:25,831 - INFO -   Batch 650/1264: Loss = 0.0752
2025-12-27 09:21:59,141 - INFO -   Batch 700/1264: Loss = 0.0913
2025-12-27 09:22:32,423 - INFO -   Batch 750/1264: Loss = 0.0640
2025-12-27 09:23:05,627 - INFO -   Batch 800/1264: Loss = 0.1004
2025-12-27 09:23:38,891 - INFO -   Batch 850/1264: Loss = 0.0821
2025-12-27 09:24:12,169 - INFO -   Batch 900/1264: Loss = 0.0895
2025-12-27 09:24:45,455 - INFO -   Batch 950/1264: Loss = 0.0796
2025-12-27 09:25:18,656 - INFO -   Batch 1000/1264: Loss = 0.0749
2025-12-27 09:25:51,939 - INFO -   Batch 1050/1264: Loss = 0.1306
2025-12-27 09:26:25,269 - INFO -   Batch 1100/1264: Loss = 0.0899
2025-12-27 09:26:58,620 - INFO -   Batch 1150/1264: Loss = 0.1052
2025-12-27 09:27:31,877 - INFO -   Batch 1200/1264: Loss = 0.1219
2025-12-27 09:28:05,190 - INFO -   Batch 1250/1264: Loss = 0.0912
2025-12-27 09:31:04,432 - INFO - Epoch 123/160: Train Loss: 0.0969, Val Loss: 1.0955, Val mIoU: 43.97%, Val PixelAcc: 80.43%
2025-12-27 09:31:39,612 - INFO -   Batch 50/1264: Loss = 0.0891
2025-12-27 09:32:12,861 - INFO -   Batch 100/1264: Loss = 0.0793
2025-12-27 09:32:46,042 - INFO -   Batch 150/1264: Loss = 0.1102
2025-12-27 09:33:19,308 - INFO -   Batch 200/1264: Loss = 0.0733
2025-12-27 09:33:52,578 - INFO -   Batch 250/1264: Loss = 0.0894
2025-12-27 09:34:25,886 - INFO -   Batch 300/1264: Loss = 0.0682
2025-12-27 09:34:59,131 - INFO -   Batch 350/1264: Loss = 0.0950
2025-12-27 09:35:32,443 - INFO -   Batch 400/1264: Loss = 0.1082
2025-12-27 09:36:05,756 - INFO -   Batch 450/1264: Loss = 0.0819
2025-12-27 09:36:39,078 - INFO -   Batch 500/1264: Loss = 0.0974
2025-12-27 09:37:12,318 - INFO -   Batch 550/1264: Loss = 0.0895
2025-12-27 09:37:45,612 - INFO -   Batch 600/1264: Loss = 0.1041
2025-12-27 09:38:18,593 - INFO -   Batch 650/1264: Loss = 0.0703
2025-12-27 09:38:51,258 - INFO -   Batch 700/1264: Loss = 0.0978
2025-12-27 09:39:23,903 - INFO -   Batch 750/1264: Loss = 0.1357
2025-12-27 09:39:56,556 - INFO -   Batch 800/1264: Loss = 0.1059
2025-12-27 09:40:29,280 - INFO -   Batch 850/1264: Loss = 0.0874
2025-12-27 09:41:02,081 - INFO -   Batch 900/1264: Loss = 0.1265
2025-12-27 09:41:34,937 - INFO -   Batch 950/1264: Loss = 0.0838
2025-12-27 09:42:07,809 - INFO -   Batch 1000/1264: Loss = 0.0972
2025-12-27 09:42:40,451 - INFO -   Batch 1050/1264: Loss = 0.0964
2025-12-27 09:43:13,095 - INFO -   Batch 1100/1264: Loss = 0.1017
2025-12-27 09:43:45,751 - INFO -   Batch 1150/1264: Loss = 0.1427
2025-12-27 09:44:18,490 - INFO -   Batch 1200/1264: Loss = 0.1178
2025-12-27 09:44:51,302 - INFO -   Batch 1250/1264: Loss = 0.0803
2025-12-27 09:47:49,999 - INFO - Epoch 124/160: Train Loss: 0.0992, Val Loss: 1.1246, Val mIoU: 43.86%, Val PixelAcc: 80.28%
2025-12-27 09:48:24,918 - INFO -   Batch 50/1264: Loss = 0.1005
2025-12-27 09:48:57,568 - INFO -   Batch 100/1264: Loss = 0.1074
2025-12-27 09:49:30,214 - INFO -   Batch 150/1264: Loss = 0.0817
2025-12-27 09:50:02,866 - INFO -   Batch 200/1264: Loss = 0.0848
2025-12-27 09:50:35,526 - INFO -   Batch 250/1264: Loss = 0.0813
2025-12-27 09:51:08,176 - INFO -   Batch 300/1264: Loss = 0.0755
2025-12-27 09:51:40,857 - INFO -   Batch 350/1264: Loss = 0.0929
2025-12-27 09:52:13,630 - INFO -   Batch 400/1264: Loss = 0.1261
2025-12-27 09:52:46,465 - INFO -   Batch 450/1264: Loss = 0.1043
2025-12-27 09:53:19,365 - INFO -   Batch 500/1264: Loss = 0.1094
2025-12-27 09:53:52,132 - INFO -   Batch 550/1264: Loss = 0.1069
2025-12-27 09:54:24,781 - INFO -   Batch 600/1264: Loss = 0.0713
2025-12-27 09:54:57,483 - INFO -   Batch 650/1264: Loss = 0.1243
2025-12-27 09:55:30,139 - INFO -   Batch 700/1264: Loss = 0.1134
2025-12-27 09:56:02,786 - INFO -   Batch 750/1264: Loss = 0.0851
2025-12-27 09:56:35,454 - INFO -   Batch 800/1264: Loss = 0.1659
2025-12-27 09:57:08,204 - INFO -   Batch 850/1264: Loss = 0.0878
2025-12-27 09:57:41,032 - INFO -   Batch 900/1264: Loss = 0.0837
2025-12-27 09:58:13,834 - INFO -   Batch 950/1264: Loss = 0.0750
2025-12-27 09:58:46,486 - INFO -   Batch 1000/1264: Loss = 0.0934
2025-12-27 09:59:19,147 - INFO -   Batch 1050/1264: Loss = 0.0973
2025-12-27 09:59:51,794 - INFO -   Batch 1100/1264: Loss = 0.0929
2025-12-27 10:00:24,477 - INFO -   Batch 1150/1264: Loss = 0.0977
2025-12-27 10:00:57,258 - INFO -   Batch 1200/1264: Loss = 0.1140
2025-12-27 10:01:30,085 - INFO -   Batch 1250/1264: Loss = 0.0753
2025-12-27 10:04:28,819 - INFO - Epoch 125/160: Train Loss: 0.0966, Val Loss: 1.0967, Val mIoU: 44.23%, Val PixelAcc: 80.39%
2025-12-27 10:05:04,147 - INFO -   Batch 50/1264: Loss = 0.0724
2025-12-27 10:05:37,142 - INFO -   Batch 100/1264: Loss = 0.0708
2025-12-27 10:06:10,174 - INFO -   Batch 150/1264: Loss = 0.0964
2025-12-27 10:06:43,232 - INFO -   Batch 200/1264: Loss = 0.0882
2025-12-27 10:07:16,314 - INFO -   Batch 250/1264: Loss = 0.0958
2025-12-27 10:07:49,090 - INFO -   Batch 300/1264: Loss = 0.0756
2025-12-27 10:08:21,740 - INFO -   Batch 350/1264: Loss = 0.0909
2025-12-27 10:08:54,420 - INFO -   Batch 400/1264: Loss = 0.0933
2025-12-27 10:09:27,075 - INFO -   Batch 450/1264: Loss = 0.0824
2025-12-27 10:09:59,727 - INFO -   Batch 500/1264: Loss = 0.1047
2025-12-27 10:10:32,378 - INFO -   Batch 550/1264: Loss = 0.0912
2025-12-27 10:11:05,035 - INFO -   Batch 600/1264: Loss = 0.0922
2025-12-27 10:11:37,687 - INFO -   Batch 650/1264: Loss = 0.1236
2025-12-27 10:12:10,338 - INFO -   Batch 700/1264: Loss = 0.1007
2025-12-27 10:12:42,996 - INFO -   Batch 750/1264: Loss = 0.0911
2025-12-27 10:13:15,682 - INFO -   Batch 800/1264: Loss = 0.0798
2025-12-27 10:13:48,446 - INFO -   Batch 850/1264: Loss = 0.0908
2025-12-27 10:14:21,271 - INFO -   Batch 900/1264: Loss = 0.0943
2025-12-27 10:14:54,151 - INFO -   Batch 950/1264: Loss = 0.0824
2025-12-27 10:15:27,095 - INFO -   Batch 1000/1264: Loss = 0.0953
2025-12-27 10:16:00,113 - INFO -   Batch 1050/1264: Loss = 0.0817
2025-12-27 10:16:33,220 - INFO -   Batch 1100/1264: Loss = 0.1052
2025-12-27 10:17:06,333 - INFO -   Batch 1150/1264: Loss = 0.0960
2025-12-27 10:17:39,449 - INFO -   Batch 1200/1264: Loss = 0.0922
2025-12-27 10:18:12,569 - INFO -   Batch 1250/1264: Loss = 0.1277
2025-12-27 10:21:11,348 - INFO - Epoch 126/160: Train Loss: 0.0952, Val Loss: 1.0986, Val mIoU: 44.37%, Val PixelAcc: 80.55%
2025-12-27 10:21:11,351 - INFO -   → New best mIoU: 44.37%
2025-12-27 10:21:12,900 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-27 10:21:12,901 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-27 10:21:47,846 - INFO -   Batch 50/1264: Loss = 0.0892
2025-12-27 10:22:20,518 - INFO -   Batch 100/1264: Loss = 0.0980
2025-12-27 10:22:53,182 - INFO -   Batch 150/1264: Loss = 0.1115
2025-12-27 10:23:25,862 - INFO -   Batch 200/1264: Loss = 0.0924
2025-12-27 10:23:58,534 - INFO -   Batch 250/1264: Loss = 0.0926
2025-12-27 10:24:31,244 - INFO -   Batch 300/1264: Loss = 0.0824
2025-12-27 10:25:04,030 - INFO -   Batch 350/1264: Loss = 0.0999
2025-12-27 10:25:36,874 - INFO -   Batch 400/1264: Loss = 0.1129
2025-12-27 10:26:09,871 - INFO -   Batch 450/1264: Loss = 0.0880
2025-12-27 10:26:42,808 - INFO -   Batch 500/1264: Loss = 0.0900
2025-12-27 10:27:15,766 - INFO -   Batch 550/1264: Loss = 0.0867
2025-12-27 10:27:48,746 - INFO -   Batch 600/1264: Loss = 0.0986
2025-12-27 10:28:21,626 - INFO -   Batch 650/1264: Loss = 0.0866
2025-12-27 10:28:54,292 - INFO -   Batch 700/1264: Loss = 0.1009
2025-12-27 10:29:26,951 - INFO -   Batch 750/1264: Loss = 0.0871
2025-12-27 10:29:59,628 - INFO -   Batch 800/1264: Loss = 0.0886
2025-12-27 10:30:32,332 - INFO -   Batch 850/1264: Loss = 0.1031
2025-12-27 10:31:05,068 - INFO -   Batch 900/1264: Loss = 0.0927
2025-12-27 10:31:37,727 - INFO -   Batch 950/1264: Loss = 0.0892
2025-12-27 10:32:10,387 - INFO -   Batch 1000/1264: Loss = 0.0657
2025-12-27 10:32:43,078 - INFO -   Batch 1050/1264: Loss = 0.0929
2025-12-27 10:33:15,809 - INFO -   Batch 1100/1264: Loss = 0.0979
2025-12-27 10:33:48,631 - INFO -   Batch 1150/1264: Loss = 0.0801
2025-12-27 10:34:21,482 - INFO -   Batch 1200/1264: Loss = 0.1175
2025-12-27 10:34:54,403 - INFO -   Batch 1250/1264: Loss = 0.1355
2025-12-27 10:37:54,495 - INFO - Epoch 127/160: Train Loss: 0.0953, Val Loss: 1.0883, Val mIoU: 44.41%, Val PixelAcc: 80.51%
2025-12-27 10:37:54,843 - INFO -   → New best mIoU: 44.41%
2025-12-27 10:37:57,728 - INFO - ✅ Checkpoint saved: runs/run_221/best_model.pth
2025-12-27 10:37:57,729 - INFO -   → Best model saved to runs/run_221/best_model.pth
2025-12-27 10:38:32,418 - INFO -   Batch 50/1264: Loss = 0.1119
2025-12-27 10:39:05,275 - INFO -   Batch 100/1264: Loss = 0.0997
2025-12-27 10:39:38,218 - INFO -   Batch 150/1264: Loss = 0.0908
2025-12-27 10:40:11,188 - INFO -   Batch 200/1264: Loss = 0.1194
2025-12-27 10:40:43,859 - INFO -   Batch 250/1264: Loss = 0.1346
2025-12-27 10:41:16,521 - INFO -   Batch 300/1264: Loss = 0.1139
2025-12-27 10:41:49,176 - INFO -   Batch 350/1264: Loss = 0.0592
2025-12-27 10:42:21,828 - INFO -   Batch 400/1264: Loss = 0.0794
2025-12-27 10:42:54,488 - INFO -   Batch 450/1264: Loss = 0.0957
2025-12-27 10:43:27,148 - INFO -   Batch 500/1264: Loss = 0.0919
2025-12-27 10:43:59,898 - INFO -   Batch 550/1264: Loss = 0.1180
2025-12-27 10:44:32,717 - INFO -   Batch 600/1264: Loss = 0.0929
2025-12-27 10:45:05,600 - INFO -   Batch 650/1264: Loss = 0.0808
2025-12-27 10:45:38,312 - INFO -   Batch 700/1264: Loss = 0.0864
2025-12-27 10:46:10,969 - INFO -   Batch 750/1264: Loss = 0.1024
2025-12-27 10:46:43,636 - INFO -   Batch 800/1264: Loss = 0.0902
2025-12-27 10:47:16,295 - INFO -   Batch 850/1264: Loss = 0.0840
2025-12-27 10:47:48,956 - INFO -   Batch 900/1264: Loss = 0.1010
2025-12-27 10:48:21,624 - INFO -   Batch 950/1264: Loss = 0.0740
2025-12-27 10:48:54,273 - INFO -   Batch 1000/1264: Loss = 0.1058
2025-12-27 10:49:26,929 - INFO -   Batch 1050/1264: Loss = 0.0812
2025-12-27 10:49:59,592 - INFO -   Batch 1100/1264: Loss = 0.0923
2025-12-27 10:50:32,252 - INFO -   Batch 1150/1264: Loss = 0.1116
2025-12-27 10:51:04,912 - INFO -   Batch 1200/1264: Loss = 0.1188
2025-12-27 10:51:37,647 - INFO -   Batch 1250/1264: Loss = 0.0803
2025-12-27 10:54:37,910 - INFO - Epoch 128/160: Train Loss: 0.0946, Val Loss: 1.1010, Val mIoU: 44.17%, Val PixelAcc: 80.48%
2025-12-27 10:55:12,407 - INFO -   Batch 50/1264: Loss = 0.0803
2025-12-27 10:55:45,065 - INFO -   Batch 100/1264: Loss = 0.1099
2025-12-27 10:56:17,726 - INFO -   Batch 150/1264: Loss = 0.0776
2025-12-27 10:56:50,421 - INFO -   Batch 200/1264: Loss = 0.1027
2025-12-27 10:57:23,084 - INFO -   Batch 250/1264: Loss = 0.1070
2025-12-27 10:57:55,748 - INFO -   Batch 300/1264: Loss = 0.1080
2025-12-27 10:58:28,504 - INFO -   Batch 350/1264: Loss = 0.0844
2025-12-27 10:59:01,340 - INFO -   Batch 400/1264: Loss = 0.0962
2025-12-27 10:59:34,255 - INFO -   Batch 450/1264: Loss = 0.0810
2025-12-27 11:00:07,217 - INFO -   Batch 500/1264: Loss = 0.0891
2025-12-27 11:00:40,219 - INFO -   Batch 550/1264: Loss = 0.0799
2025-12-27 11:01:13,264 - INFO -   Batch 600/1264: Loss = 0.0990
2025-12-27 11:01:46,351 - INFO -   Batch 650/1264: Loss = 0.0908
2025-12-27 11:02:19,454 - INFO -   Batch 700/1264: Loss = 0.0874
2025-12-27 11:02:52,145 - INFO -   Batch 750/1264: Loss = 0.1083
2025-12-27 11:03:24,803 - INFO -   Batch 800/1264: Loss = 0.1084
2025-12-27 11:03:57,527 - INFO -   Batch 850/1264: Loss = 0.0994
2025-12-27 11:04:30,313 - INFO -   Batch 900/1264: Loss = 0.0811
2025-12-27 11:05:03,057 - INFO -   Batch 950/1264: Loss = 0.0937
2025-12-27 11:05:35,715 - INFO -   Batch 1000/1264: Loss = 0.0999
2025-12-27 11:06:08,367 - INFO -   Batch 1050/1264: Loss = 0.1032
2025-12-27 11:06:41,028 - INFO -   Batch 1100/1264: Loss = 0.1580
2025-12-27 11:07:13,714 - INFO -   Batch 1150/1264: Loss = 0.0777
2025-12-27 11:07:46,490 - INFO -   Batch 1200/1264: Loss = 0.0795
2025-12-27 11:08:19,327 - INFO -   Batch 1250/1264: Loss = 0.0817
2025-12-27 11:11:19,796 - INFO - Epoch 129/160: Train Loss: 0.0944, Val Loss: 1.1051, Val mIoU: 44.26%, Val PixelAcc: 80.50%
2025-12-27 11:11:54,757 - INFO -   Batch 50/1264: Loss = 0.0868
2025-12-27 11:12:27,602 - INFO -   Batch 100/1264: Loss = 0.0918
2025-12-27 11:13:00,526 - INFO -   Batch 150/1264: Loss = 0.1055
2025-12-27 11:13:33,509 - INFO -   Batch 200/1264: Loss = 0.0880
2025-12-27 11:14:06,517 - INFO -   Batch 250/1264: Loss = 0.0965
2025-12-27 11:14:39,547 - INFO -   Batch 300/1264: Loss = 0.1004
2025-12-27 11:15:12,489 - INFO -   Batch 350/1264: Loss = 0.0924
2025-12-27 11:15:45,146 - INFO -   Batch 400/1264: Loss = 0.1069
2025-12-27 11:16:17,804 - INFO -   Batch 450/1264: Loss = 0.0893
2025-12-27 11:16:50,458 - INFO -   Batch 500/1264: Loss = 0.0752
2025-12-27 11:17:23,112 - INFO -   Batch 550/1264: Loss = 0.1165
2025-12-27 11:17:55,768 - INFO -   Batch 600/1264: Loss = 0.1203
2025-12-27 11:18:28,516 - INFO -   Batch 650/1264: Loss = 0.1281
2025-12-27 11:19:01,304 - INFO -   Batch 700/1264: Loss = 0.0810
2025-12-27 11:19:34,156 - INFO -   Batch 750/1264: Loss = 0.0978
2025-12-27 11:20:07,088 - INFO -   Batch 800/1264: Loss = 0.0944
2025-12-27 11:20:40,100 - INFO -   Batch 850/1264: Loss = 0.0939
2025-12-27 11:21:13,130 - INFO -   Batch 900/1264: Loss = 0.0990
2025-12-27 11:21:46,206 - INFO -   Batch 950/1264: Loss = 0.0666
2025-12-27 11:22:19,298 - INFO -   Batch 1000/1264: Loss = 0.1349
2025-12-27 11:22:52,158 - INFO -   Batch 1050/1264: Loss = 0.0843
2025-12-27 11:23:24,822 - INFO -   Batch 1100/1264: Loss = 0.0877
2025-12-27 11:23:57,491 - INFO -   Batch 1150/1264: Loss = 0.0971
2025-12-27 11:24:30,221 - INFO -   Batch 1200/1264: Loss = 0.1037
2025-12-27 11:25:02,990 - INFO -   Batch 1250/1264: Loss = 0.0907
2025-12-27 11:28:03,655 - INFO - Epoch 130/160: Train Loss: 0.0953, Val Loss: 1.1062, Val mIoU: 44.24%, Val PixelAcc: 80.52%
2025-12-27 11:28:05,144 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_130.pth
2025-12-27 11:28:05,146 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_130.pth
2025-12-27 11:28:40,006 - INFO -   Batch 50/1264: Loss = 0.0896
2025-12-27 11:29:12,791 - INFO -   Batch 100/1264: Loss = 0.0881
2025-12-27 11:29:45,642 - INFO -   Batch 150/1264: Loss = 0.0700
2025-12-27 11:30:18,570 - INFO -   Batch 200/1264: Loss = 0.1104
2025-12-27 11:30:51,553 - INFO -   Batch 250/1264: Loss = 0.1083
2025-12-27 11:31:24,581 - INFO -   Batch 300/1264: Loss = 0.0998
2025-12-27 11:31:57,478 - INFO -   Batch 350/1264: Loss = 0.1456
2025-12-27 11:32:30,139 - INFO -   Batch 400/1264: Loss = 0.1815
2025-12-27 11:33:02,815 - INFO -   Batch 450/1264: Loss = 0.1330
2025-12-27 11:33:35,484 - INFO -   Batch 500/1264: Loss = 0.1017
2025-12-27 11:34:08,156 - INFO -   Batch 550/1264: Loss = 0.0844
2025-12-27 11:34:40,814 - INFO -   Batch 600/1264: Loss = 0.1105
2025-12-27 11:35:13,477 - INFO -   Batch 650/1264: Loss = 0.0866
2025-12-27 11:35:46,190 - INFO -   Batch 700/1264: Loss = 0.0878
2025-12-27 11:36:18,981 - INFO -   Batch 750/1264: Loss = 0.0771
2025-12-27 11:36:51,843 - INFO -   Batch 800/1264: Loss = 0.1004
2025-12-27 11:37:24,804 - INFO -   Batch 850/1264: Loss = 0.1070
2025-12-27 11:37:57,792 - INFO -   Batch 900/1264: Loss = 0.0703
2025-12-27 11:38:30,849 - INFO -   Batch 950/1264: Loss = 0.0936
2025-12-27 11:39:03,616 - INFO -   Batch 1000/1264: Loss = 0.0899
2025-12-27 11:39:36,288 - INFO -   Batch 1050/1264: Loss = 0.0909
2025-12-27 11:40:09,012 - INFO -   Batch 1100/1264: Loss = 0.1004
2025-12-27 11:40:41,751 - INFO -   Batch 1150/1264: Loss = 0.0710
2025-12-27 11:41:14,410 - INFO -   Batch 1200/1264: Loss = 0.0624
2025-12-27 11:41:47,136 - INFO -   Batch 1250/1264: Loss = 0.0783
2025-12-27 11:44:47,852 - INFO - Epoch 131/160: Train Loss: 0.0939, Val Loss: 1.1040, Val mIoU: 44.06%, Val PixelAcc: 80.51%
2025-12-27 11:45:22,693 - INFO -   Batch 50/1264: Loss = 0.1079
2025-12-27 11:45:55,703 - INFO -   Batch 100/1264: Loss = 0.1101
2025-12-27 11:46:28,747 - INFO -   Batch 150/1264: Loss = 0.0735
2025-12-27 11:47:01,399 - INFO -   Batch 200/1264: Loss = 0.0814
2025-12-27 11:47:34,074 - INFO -   Batch 250/1264: Loss = 0.0794
2025-12-27 11:48:06,743 - INFO -   Batch 300/1264: Loss = 0.1110
2025-12-27 11:48:39,412 - INFO -   Batch 350/1264: Loss = 0.0738
2025-12-27 11:49:12,081 - INFO -   Batch 400/1264: Loss = 0.0654
2025-12-27 11:49:44,750 - INFO -   Batch 450/1264: Loss = 0.0842
2025-12-27 11:50:17,433 - INFO -   Batch 500/1264: Loss = 0.0937
2025-12-27 11:50:50,172 - INFO -   Batch 550/1264: Loss = 0.1204
2025-12-27 11:51:22,890 - INFO -   Batch 600/1264: Loss = 0.1071
2025-12-27 11:51:55,549 - INFO -   Batch 650/1264: Loss = 0.0899
2025-12-27 11:52:28,210 - INFO -   Batch 700/1264: Loss = 0.0887
2025-12-27 11:53:00,954 - INFO -   Batch 750/1264: Loss = 0.0985
2025-12-27 11:53:33,766 - INFO -   Batch 800/1264: Loss = 0.0941
2025-12-27 11:54:06,647 - INFO -   Batch 850/1264: Loss = 0.0831
2025-12-27 11:54:39,605 - INFO -   Batch 900/1264: Loss = 0.0676
2025-12-27 11:55:12,635 - INFO -   Batch 950/1264: Loss = 0.0855
2025-12-27 11:55:45,735 - INFO -   Batch 1000/1264: Loss = 0.0906
2025-12-27 11:56:18,859 - INFO -   Batch 1050/1264: Loss = 0.0825
2025-12-27 11:56:51,993 - INFO -   Batch 1100/1264: Loss = 0.0918
2025-12-27 11:57:24,900 - INFO -   Batch 1150/1264: Loss = 0.0960
2025-12-27 11:57:57,599 - INFO -   Batch 1200/1264: Loss = 0.1164
2025-12-27 11:58:30,300 - INFO -   Batch 1250/1264: Loss = 0.0758
2025-12-27 12:01:30,596 - INFO - Epoch 132/160: Train Loss: 0.0933, Val Loss: 1.1020, Val mIoU: 44.09%, Val PixelAcc: 80.49%
2025-12-27 12:02:05,261 - INFO -   Batch 50/1264: Loss = 0.0991
2025-12-27 12:02:38,044 - INFO -   Batch 100/1264: Loss = 0.0747
2025-12-27 12:03:10,893 - INFO -   Batch 150/1264: Loss = 0.0876
2025-12-27 12:03:43,838 - INFO -   Batch 200/1264: Loss = 0.0890
2025-12-27 12:04:16,907 - INFO -   Batch 250/1264: Loss = 0.0877
2025-12-27 12:04:50,014 - INFO -   Batch 300/1264: Loss = 0.1226
2025-12-27 12:05:23,125 - INFO -   Batch 350/1264: Loss = 0.0959
2025-12-27 12:05:56,279 - INFO -   Batch 400/1264: Loss = 0.0941
2025-12-27 12:06:29,032 - INFO -   Batch 450/1264: Loss = 0.0845
2025-12-27 12:07:01,692 - INFO -   Batch 500/1264: Loss = 0.0808
2025-12-27 12:07:34,389 - INFO -   Batch 550/1264: Loss = 0.0974
2025-12-27 12:08:07,162 - INFO -   Batch 600/1264: Loss = 0.1007
2025-12-27 12:08:39,991 - INFO -   Batch 650/1264: Loss = 0.1056
2025-12-27 12:09:12,887 - INFO -   Batch 700/1264: Loss = 0.1112
2025-12-27 12:09:45,849 - INFO -   Batch 750/1264: Loss = 0.1160
2025-12-27 12:10:18,858 - INFO -   Batch 800/1264: Loss = 0.1044
2025-12-27 12:10:51,809 - INFO -   Batch 850/1264: Loss = 0.1447
2025-12-27 12:11:24,470 - INFO -   Batch 900/1264: Loss = 0.0684
2025-12-27 12:11:57,125 - INFO -   Batch 950/1264: Loss = 0.1004
2025-12-27 12:12:29,834 - INFO -   Batch 1000/1264: Loss = 0.1059
2025-12-27 12:13:02,605 - INFO -   Batch 1050/1264: Loss = 0.1085
2025-12-27 12:13:35,423 - INFO -   Batch 1100/1264: Loss = 0.1091
2025-12-27 12:14:08,324 - INFO -   Batch 1150/1264: Loss = 0.0858
2025-12-27 12:14:41,260 - INFO -   Batch 1200/1264: Loss = 0.0912
2025-12-27 12:15:13,910 - INFO -   Batch 1250/1264: Loss = 0.0843
2025-12-27 12:18:14,204 - INFO - Epoch 133/160: Train Loss: 0.0933, Val Loss: 1.1135, Val mIoU: 44.30%, Val PixelAcc: 80.47%
2025-12-27 12:18:49,097 - INFO -   Batch 50/1264: Loss = 0.0902
2025-12-27 12:19:21,759 - INFO -   Batch 100/1264: Loss = 0.0828
2025-12-27 12:19:54,473 - INFO -   Batch 150/1264: Loss = 0.0933
2025-12-27 12:20:27,272 - INFO -   Batch 200/1264: Loss = 0.0993
2025-12-27 12:20:59,943 - INFO -   Batch 250/1264: Loss = 0.0917
2025-12-27 12:21:32,599 - INFO -   Batch 300/1264: Loss = 0.1148
2025-12-27 12:22:05,284 - INFO -   Batch 350/1264: Loss = 0.0767
2025-12-27 12:22:38,044 - INFO -   Batch 400/1264: Loss = 0.0815
2025-12-27 12:23:10,845 - INFO -   Batch 450/1264: Loss = 0.0816
2025-12-27 12:23:43,716 - INFO -   Batch 500/1264: Loss = 0.0965
2025-12-27 12:24:16,655 - INFO -   Batch 550/1264: Loss = 0.0889
2025-12-27 12:24:49,635 - INFO -   Batch 600/1264: Loss = 0.0966
2025-12-27 12:25:22,676 - INFO -   Batch 650/1264: Loss = 0.1012
2025-12-27 12:25:55,734 - INFO -   Batch 700/1264: Loss = 0.1092
2025-12-27 12:26:28,819 - INFO -   Batch 750/1264: Loss = 0.0665
2025-12-27 12:27:01,608 - INFO -   Batch 800/1264: Loss = 0.0985
2025-12-27 12:27:34,266 - INFO -   Batch 850/1264: Loss = 0.0843
2025-12-27 12:28:06,950 - INFO -   Batch 900/1264: Loss = 0.0799
2025-12-27 12:28:39,698 - INFO -   Batch 950/1264: Loss = 0.0911
2025-12-27 12:29:12,479 - INFO -   Batch 1000/1264: Loss = 0.0749
2025-12-27 12:29:45,327 - INFO -   Batch 1050/1264: Loss = 0.0854
2025-12-27 12:30:18,244 - INFO -   Batch 1100/1264: Loss = 0.1027
2025-12-27 12:30:51,882 - INFO -   Batch 1150/1264: Loss = 0.0878
2025-12-27 12:31:24,875 - INFO -   Batch 1200/1264: Loss = 0.0693
2025-12-27 12:31:57,893 - INFO -   Batch 1250/1264: Loss = 0.0719
2025-12-27 12:34:58,141 - INFO - Epoch 134/160: Train Loss: 0.0927, Val Loss: 1.1039, Val mIoU: 44.00%, Val PixelAcc: 80.41%
2025-12-27 12:35:33,087 - INFO -   Batch 50/1264: Loss = 0.0854
2025-12-27 12:36:05,760 - INFO -   Batch 100/1264: Loss = 0.0863
2025-12-27 12:36:38,564 - INFO -   Batch 150/1264: Loss = 0.0730
2025-12-27 12:37:11,413 - INFO -   Batch 200/1264: Loss = 0.0663
2025-12-27 12:37:44,293 - INFO -   Batch 250/1264: Loss = 0.1125
2025-12-27 12:38:17,231 - INFO -   Batch 300/1264: Loss = 0.0588
2025-12-27 12:38:50,221 - INFO -   Batch 350/1264: Loss = 0.0809
2025-12-27 12:39:23,002 - INFO -   Batch 400/1264: Loss = 0.1004
2025-12-27 12:39:55,661 - INFO -   Batch 450/1264: Loss = 0.1107
2025-12-27 12:40:28,318 - INFO -   Batch 500/1264: Loss = 0.0680
2025-12-27 12:41:00,983 - INFO -   Batch 550/1264: Loss = 0.0742
2025-12-27 12:41:33,705 - INFO -   Batch 600/1264: Loss = 0.0913
2025-12-27 12:42:06,502 - INFO -   Batch 650/1264: Loss = 0.0837
2025-12-27 12:42:39,336 - INFO -   Batch 700/1264: Loss = 0.0925
2025-12-27 12:43:12,239 - INFO -   Batch 750/1264: Loss = 0.1277
2025-12-27 12:43:45,132 - INFO -   Batch 800/1264: Loss = 0.0713
2025-12-27 12:44:17,787 - INFO -   Batch 850/1264: Loss = 0.1157
2025-12-27 12:44:50,445 - INFO -   Batch 900/1264: Loss = 0.0783
2025-12-27 12:45:23,103 - INFO -   Batch 950/1264: Loss = 0.0946
2025-12-27 12:45:55,758 - INFO -   Batch 1000/1264: Loss = 0.0857
2025-12-27 12:46:28,418 - INFO -   Batch 1050/1264: Loss = 0.1009
2025-12-27 12:47:01,095 - INFO -   Batch 1100/1264: Loss = 0.0900
2025-12-27 12:47:33,861 - INFO -   Batch 1150/1264: Loss = 0.0902
2025-12-27 12:48:06,675 - INFO -   Batch 1200/1264: Loss = 0.1004
2025-12-27 12:48:39,545 - INFO -   Batch 1250/1264: Loss = 0.1187
2025-12-27 12:51:39,793 - INFO - Epoch 135/160: Train Loss: 0.0923, Val Loss: 1.1004, Val mIoU: 44.19%, Val PixelAcc: 80.53%
2025-12-27 12:52:14,636 - INFO -   Batch 50/1264: Loss = 0.0871
2025-12-27 12:52:47,305 - INFO -   Batch 100/1264: Loss = 0.0710
2025-12-27 12:53:20,010 - INFO -   Batch 150/1264: Loss = 0.0783
2025-12-27 12:53:52,811 - INFO -   Batch 200/1264: Loss = 0.0808
2025-12-27 12:54:25,660 - INFO -   Batch 250/1264: Loss = 0.1201
2025-12-27 12:54:58,563 - INFO -   Batch 300/1264: Loss = 0.1022
2025-12-27 12:55:31,500 - INFO -   Batch 350/1264: Loss = 0.0965
2025-12-27 12:56:04,450 - INFO -   Batch 400/1264: Loss = 0.0877
2025-12-27 12:56:37,406 - INFO -   Batch 450/1264: Loss = 0.1530
2025-12-27 12:57:10,171 - INFO -   Batch 500/1264: Loss = 0.0673
2025-12-27 12:57:42,823 - INFO -   Batch 550/1264: Loss = 0.0664
2025-12-27 12:58:15,477 - INFO -   Batch 600/1264: Loss = 0.0929
2025-12-27 12:58:48,134 - INFO -   Batch 650/1264: Loss = 0.0787
2025-12-27 12:59:20,862 - INFO -   Batch 700/1264: Loss = 0.0749
2025-12-27 12:59:53,648 - INFO -   Batch 750/1264: Loss = 0.1202
2025-12-27 13:00:26,495 - INFO -   Batch 800/1264: Loss = 0.1331
2025-12-27 13:00:59,392 - INFO -   Batch 850/1264: Loss = 0.0934
2025-12-27 13:01:32,317 - INFO -   Batch 900/1264: Loss = 0.0724
2025-12-27 13:02:05,274 - INFO -   Batch 950/1264: Loss = 0.1082
2025-12-27 13:02:38,230 - INFO -   Batch 1000/1264: Loss = 0.1086
2025-12-27 13:03:11,033 - INFO -   Batch 1050/1264: Loss = 0.0697
2025-12-27 13:03:43,697 - INFO -   Batch 1100/1264: Loss = 0.0861
2025-12-27 13:04:16,363 - INFO -   Batch 1150/1264: Loss = 0.0865
2025-12-27 13:04:49,024 - INFO -   Batch 1200/1264: Loss = 0.0943
2025-12-27 13:05:21,694 - INFO -   Batch 1250/1264: Loss = 0.0839
2025-12-27 13:08:22,888 - INFO - Epoch 136/160: Train Loss: 0.0919, Val Loss: 1.1128, Val mIoU: 44.22%, Val PixelAcc: 80.48%
2025-12-27 13:08:57,643 - INFO -   Batch 50/1264: Loss = 0.1062
2025-12-27 13:09:30,545 - INFO -   Batch 100/1264: Loss = 0.0779
2025-12-27 13:10:03,477 - INFO -   Batch 150/1264: Loss = 0.1027
2025-12-27 13:10:36,128 - INFO -   Batch 200/1264: Loss = 0.0827
2025-12-27 13:11:08,787 - INFO -   Batch 250/1264: Loss = 0.1000
2025-12-27 13:11:41,462 - INFO -   Batch 300/1264: Loss = 0.0824
2025-12-27 13:12:14,110 - INFO -   Batch 350/1264: Loss = 0.1192
2025-12-27 13:12:46,764 - INFO -   Batch 400/1264: Loss = 0.0988
2025-12-27 13:13:19,419 - INFO -   Batch 450/1264: Loss = 0.1062
2025-12-27 13:13:52,079 - INFO -   Batch 500/1264: Loss = 0.0765
2025-12-27 13:14:24,812 - INFO -   Batch 550/1264: Loss = 0.0799
2025-12-27 13:14:57,614 - INFO -   Batch 600/1264: Loss = 0.0725
2025-12-27 13:15:30,485 - INFO -   Batch 650/1264: Loss = 0.1040
2025-12-27 13:16:03,240 - INFO -   Batch 700/1264: Loss = 0.0867
2025-12-27 13:16:35,892 - INFO -   Batch 750/1264: Loss = 0.1122
2025-12-27 13:17:08,541 - INFO -   Batch 800/1264: Loss = 0.0910
2025-12-27 13:17:41,195 - INFO -   Batch 850/1264: Loss = 0.1005
2025-12-27 13:18:13,849 - INFO -   Batch 900/1264: Loss = 0.0795
2025-12-27 13:18:46,510 - INFO -   Batch 950/1264: Loss = 0.0858
2025-12-27 13:19:19,255 - INFO -   Batch 1000/1264: Loss = 0.0905
2025-12-27 13:19:53,893 - INFO -   Batch 1050/1264: Loss = 0.0764
2025-12-27 13:20:26,770 - INFO -   Batch 1100/1264: Loss = 0.0687
2025-12-27 13:20:59,710 - INFO -   Batch 1150/1264: Loss = 0.1057
2025-12-27 13:21:32,646 - INFO -   Batch 1200/1264: Loss = 0.0930
2025-12-27 13:22:05,598 - INFO -   Batch 1250/1264: Loss = 0.0800
2025-12-27 13:25:10,798 - INFO - Epoch 137/160: Train Loss: 0.0918, Val Loss: 1.1075, Val mIoU: 44.24%, Val PixelAcc: 80.48%
2025-12-27 13:25:45,980 - INFO -   Batch 50/1264: Loss = 0.0862
2025-12-27 13:26:18,821 - INFO -   Batch 100/1264: Loss = 0.0707
2025-12-27 13:26:51,775 - INFO -   Batch 150/1264: Loss = 0.0751
2025-12-27 13:27:24,541 - INFO -   Batch 200/1264: Loss = 0.0815
2025-12-27 13:27:57,190 - INFO -   Batch 250/1264: Loss = 0.0736
2025-12-27 13:28:29,848 - INFO -   Batch 300/1264: Loss = 0.0693
2025-12-27 13:29:02,511 - INFO -   Batch 350/1264: Loss = 0.1164
2025-12-27 13:29:35,234 - INFO -   Batch 400/1264: Loss = 0.0802
2025-12-27 13:30:07,884 - INFO -   Batch 450/1264: Loss = 0.0744
2025-12-27 13:30:40,534 - INFO -   Batch 500/1264: Loss = 0.0909
2025-12-27 13:31:13,192 - INFO -   Batch 550/1264: Loss = 0.0663
2025-12-27 13:31:45,917 - INFO -   Batch 600/1264: Loss = 0.1013
2025-12-27 13:32:18,718 - INFO -   Batch 650/1264: Loss = 0.0741
2025-12-27 13:32:51,582 - INFO -   Batch 700/1264: Loss = 0.0786
2025-12-27 13:33:24,520 - INFO -   Batch 750/1264: Loss = 0.0698
2025-12-27 13:33:57,479 - INFO -   Batch 800/1264: Loss = 0.1246
2025-12-27 13:34:30,452 - INFO -   Batch 850/1264: Loss = 0.0767
2025-12-27 13:35:03,446 - INFO -   Batch 900/1264: Loss = 0.1245
2025-12-27 13:35:36,337 - INFO -   Batch 950/1264: Loss = 0.0748
2025-12-27 13:36:08,980 - INFO -   Batch 1000/1264: Loss = 0.1041
2025-12-27 13:36:41,833 - INFO -   Batch 1050/1264: Loss = 0.0925
2025-12-27 13:37:15,249 - INFO -   Batch 1100/1264: Loss = 0.0678
2025-12-27 13:37:47,939 - INFO -   Batch 1150/1264: Loss = 0.1150
2025-12-27 13:38:20,580 - INFO -   Batch 1200/1264: Loss = 0.0721
2025-12-27 13:38:53,225 - INFO -   Batch 1250/1264: Loss = 0.0910
2025-12-27 13:41:56,363 - INFO - Epoch 138/160: Train Loss: 0.0923, Val Loss: 1.1106, Val mIoU: 44.16%, Val PixelAcc: 80.52%
2025-12-27 13:42:31,302 - INFO -   Batch 50/1264: Loss = 0.0797
2025-12-27 13:43:03,984 - INFO -   Batch 100/1264: Loss = 0.0792
2025-12-27 13:43:36,754 - INFO -   Batch 150/1264: Loss = 0.0827
2025-12-27 13:44:09,602 - INFO -   Batch 200/1264: Loss = 0.1271
2025-12-27 13:44:42,526 - INFO -   Batch 250/1264: Loss = 0.0682
2025-12-27 13:45:15,492 - INFO -   Batch 300/1264: Loss = 0.0976
2025-12-27 13:45:48,487 - INFO -   Batch 350/1264: Loss = 0.0789
2025-12-27 13:46:21,493 - INFO -   Batch 400/1264: Loss = 0.1091
2025-12-27 13:46:54,515 - INFO -   Batch 450/1264: Loss = 0.1182
2025-12-27 13:47:27,202 - INFO -   Batch 500/1264: Loss = 0.0984
2025-12-27 13:47:59,847 - INFO -   Batch 550/1264: Loss = 0.0645
2025-12-27 13:48:32,550 - INFO -   Batch 600/1264: Loss = 0.1136
2025-12-27 13:49:05,238 - INFO -   Batch 650/1264: Loss = 0.1015
2025-12-27 13:49:37,881 - INFO -   Batch 700/1264: Loss = 0.0780
2025-12-27 13:50:10,531 - INFO -   Batch 750/1264: Loss = 0.1059
2025-12-27 13:50:43,214 - INFO -   Batch 800/1264: Loss = 0.0719
2025-12-27 13:51:15,874 - INFO -   Batch 850/1264: Loss = 0.0918
2025-12-27 13:51:48,544 - INFO -   Batch 900/1264: Loss = 0.0501
2025-12-27 13:52:21,276 - INFO -   Batch 950/1264: Loss = 0.0609
2025-12-27 13:52:54,062 - INFO -   Batch 1000/1264: Loss = 0.0968
2025-12-27 13:53:26,899 - INFO -   Batch 1050/1264: Loss = 0.0721
2025-12-27 13:53:59,786 - INFO -   Batch 1100/1264: Loss = 0.1154
2025-12-27 13:54:32,741 - INFO -   Batch 1150/1264: Loss = 0.1118
2025-12-27 13:55:05,732 - INFO -   Batch 1200/1264: Loss = 0.1133
2025-12-27 13:55:38,717 - INFO -   Batch 1250/1264: Loss = 0.0708
2025-12-27 13:58:43,803 - INFO - Epoch 139/160: Train Loss: 0.0913, Val Loss: 1.1210, Val mIoU: 44.18%, Val PixelAcc: 80.46%
2025-12-27 13:59:18,875 - INFO -   Batch 50/1264: Loss = 0.0767
2025-12-27 13:59:51,543 - INFO -   Batch 100/1264: Loss = 0.0664
2025-12-27 14:00:24,212 - INFO -   Batch 150/1264: Loss = 0.0561
2025-12-27 14:00:56,919 - INFO -   Batch 200/1264: Loss = 0.0888
2025-12-27 14:01:29,583 - INFO -   Batch 250/1264: Loss = 0.0802
2025-12-27 14:02:02,256 - INFO -   Batch 300/1264: Loss = 0.1021
2025-12-27 14:02:34,925 - INFO -   Batch 350/1264: Loss = 0.1139
2025-12-27 14:03:07,598 - INFO -   Batch 400/1264: Loss = 0.0612
2025-12-27 14:03:40,273 - INFO -   Batch 450/1264: Loss = 0.0849
2025-12-27 14:04:12,941 - INFO -   Batch 500/1264: Loss = 0.1002
2025-12-27 14:04:45,618 - INFO -   Batch 550/1264: Loss = 0.0883
2025-12-27 14:05:18,291 - INFO -   Batch 600/1264: Loss = 0.0937
2025-12-27 14:05:50,963 - INFO -   Batch 650/1264: Loss = 0.0941
2025-12-27 14:06:23,625 - INFO -   Batch 700/1264: Loss = 0.0800
2025-12-27 14:06:56,303 - INFO -   Batch 750/1264: Loss = 0.0884
2025-12-27 14:07:28,979 - INFO -   Batch 800/1264: Loss = 0.0765
2025-12-27 14:08:01,642 - INFO -   Batch 850/1264: Loss = 0.1058
2025-12-27 14:08:34,321 - INFO -   Batch 900/1264: Loss = 0.0750
2025-12-27 14:09:06,994 - INFO -   Batch 950/1264: Loss = 0.0986
2025-12-27 14:09:39,665 - INFO -   Batch 1000/1264: Loss = 0.0889
2025-12-27 14:10:12,332 - INFO -   Batch 1050/1264: Loss = 0.0889
2025-12-27 14:10:44,992 - INFO -   Batch 1100/1264: Loss = 0.0822
2025-12-27 14:11:17,658 - INFO -   Batch 1150/1264: Loss = 0.0646
2025-12-27 14:11:50,322 - INFO -   Batch 1200/1264: Loss = 0.1003
2025-12-27 14:12:22,993 - INFO -   Batch 1250/1264: Loss = 0.0794
2025-12-27 14:15:28,530 - INFO - Epoch 140/160: Train Loss: 0.0914, Val Loss: 1.1134, Val mIoU: 43.89%, Val PixelAcc: 80.47%
2025-12-27 14:15:30,090 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_140.pth
2025-12-27 14:15:30,092 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_140.pth
2025-12-27 14:16:04,689 - INFO -   Batch 50/1264: Loss = 0.0923
2025-12-27 14:16:37,359 - INFO -   Batch 100/1264: Loss = 0.0963
2025-12-27 14:17:10,032 - INFO -   Batch 150/1264: Loss = 0.1063
2025-12-27 14:17:42,700 - INFO -   Batch 200/1264: Loss = 0.0687
2025-12-27 14:18:15,375 - INFO -   Batch 250/1264: Loss = 0.0965
2025-12-27 14:18:48,048 - INFO -   Batch 300/1264: Loss = 0.0955
2025-12-27 14:19:20,757 - INFO -   Batch 350/1264: Loss = 0.1016
2025-12-27 14:19:53,438 - INFO -   Batch 400/1264: Loss = 0.0966
2025-12-27 14:20:26,095 - INFO -   Batch 450/1264: Loss = 0.0839
2025-12-27 14:20:58,756 - INFO -   Batch 500/1264: Loss = 0.0783
2025-12-27 14:21:31,900 - INFO -   Batch 550/1264: Loss = 0.0934
2025-12-27 14:22:04,567 - INFO -   Batch 600/1264: Loss = 0.0944
2025-12-27 14:22:37,229 - INFO -   Batch 650/1264: Loss = 0.0833
2025-12-27 14:23:09,897 - INFO -   Batch 700/1264: Loss = 0.0924
2025-12-27 14:23:42,632 - INFO -   Batch 750/1264: Loss = 0.0679
2025-12-27 14:24:15,420 - INFO -   Batch 800/1264: Loss = 0.0883
2025-12-27 14:24:48,067 - INFO -   Batch 850/1264: Loss = 0.1249
2025-12-27 14:25:20,721 - INFO -   Batch 900/1264: Loss = 0.1103
2025-12-27 14:25:53,379 - INFO -   Batch 950/1264: Loss = 0.0917
2025-12-27 14:26:26,097 - INFO -   Batch 1000/1264: Loss = 0.1005
2025-12-27 14:26:58,879 - INFO -   Batch 1050/1264: Loss = 0.0857
2025-12-27 14:27:31,727 - INFO -   Batch 1100/1264: Loss = 0.1089
2025-12-27 14:28:04,655 - INFO -   Batch 1150/1264: Loss = 0.1448
2025-12-27 14:28:37,620 - INFO -   Batch 1200/1264: Loss = 0.0770
2025-12-27 14:29:10,612 - INFO -   Batch 1250/1264: Loss = 0.1125
2025-12-27 14:32:15,732 - INFO - Epoch 141/160: Train Loss: 0.0914, Val Loss: 1.1167, Val mIoU: 44.03%, Val PixelAcc: 80.41%
2025-12-27 14:32:50,560 - INFO -   Batch 50/1264: Loss = 0.0907
2025-12-27 14:33:23,269 - INFO -   Batch 100/1264: Loss = 0.0985
2025-12-27 14:33:56,058 - INFO -   Batch 150/1264: Loss = 0.0801
2025-12-27 14:34:28,912 - INFO -   Batch 200/1264: Loss = 0.1237
2025-12-27 14:35:01,846 - INFO -   Batch 250/1264: Loss = 0.1102
2025-12-27 14:35:34,812 - INFO -   Batch 300/1264: Loss = 0.1005
2025-12-27 14:36:07,793 - INFO -   Batch 350/1264: Loss = 0.0823
2025-12-27 14:36:40,785 - INFO -   Batch 400/1264: Loss = 0.0682
2025-12-27 14:37:13,525 - INFO -   Batch 450/1264: Loss = 0.1039
2025-12-27 14:37:46,188 - INFO -   Batch 500/1264: Loss = 0.1107
2025-12-27 14:38:18,865 - INFO -   Batch 550/1264: Loss = 0.1124
2025-12-27 14:38:51,533 - INFO -   Batch 600/1264: Loss = 0.0862
2025-12-27 14:39:24,215 - INFO -   Batch 650/1264: Loss = 0.1040
2025-12-27 14:39:56,958 - INFO -   Batch 700/1264: Loss = 0.1221
2025-12-27 14:40:29,771 - INFO -   Batch 750/1264: Loss = 0.1032
2025-12-27 14:41:02,651 - INFO -   Batch 800/1264: Loss = 0.0910
2025-12-27 14:41:35,594 - INFO -   Batch 850/1264: Loss = 0.1192
2025-12-27 14:42:08,422 - INFO -   Batch 900/1264: Loss = 0.0825
2025-12-27 14:42:41,084 - INFO -   Batch 950/1264: Loss = 0.0957
2025-12-27 14:43:13,752 - INFO -   Batch 1000/1264: Loss = 0.0830
2025-12-27 14:43:46,435 - INFO -   Batch 1050/1264: Loss = 0.0768
2025-12-27 14:44:19,107 - INFO -   Batch 1100/1264: Loss = 0.0710
2025-12-27 14:44:51,786 - INFO -   Batch 1150/1264: Loss = 0.0771
2025-12-27 14:45:24,539 - INFO -   Batch 1200/1264: Loss = 0.1032
2025-12-27 14:45:57,245 - INFO -   Batch 1250/1264: Loss = 0.0947
2025-12-27 14:49:01,264 - INFO - Epoch 142/160: Train Loss: 0.0911, Val Loss: 1.1029, Val mIoU: 43.93%, Val PixelAcc: 80.48%
2025-12-27 14:49:35,920 - INFO -   Batch 50/1264: Loss = 0.0719
2025-12-27 14:50:08,589 - INFO -   Batch 100/1264: Loss = 0.0690
2025-12-27 14:50:41,307 - INFO -   Batch 150/1264: Loss = 0.0846
2025-12-27 14:51:14,111 - INFO -   Batch 200/1264: Loss = 0.0883
2025-12-27 14:51:46,980 - INFO -   Batch 250/1264: Loss = 0.1083
2025-12-27 14:52:19,916 - INFO -   Batch 300/1264: Loss = 0.1032
2025-12-27 14:52:52,894 - INFO -   Batch 350/1264: Loss = 0.0960
2025-12-27 14:53:25,854 - INFO -   Batch 400/1264: Loss = 0.1091
2025-12-27 14:53:58,509 - INFO -   Batch 450/1264: Loss = 0.1123
2025-12-27 14:54:31,186 - INFO -   Batch 500/1264: Loss = 0.0903
2025-12-27 14:55:03,855 - INFO -   Batch 550/1264: Loss = 0.0714
2025-12-27 14:55:36,514 - INFO -   Batch 600/1264: Loss = 0.0538
2025-12-27 14:56:09,174 - INFO -   Batch 650/1264: Loss = 0.0789
2025-12-27 14:56:41,847 - INFO -   Batch 700/1264: Loss = 0.0952
2025-12-27 14:57:14,508 - INFO -   Batch 750/1264: Loss = 0.0814
2025-12-27 14:57:47,171 - INFO -   Batch 800/1264: Loss = 0.0857
2025-12-27 14:58:19,837 - INFO -   Batch 850/1264: Loss = 0.0984
2025-12-27 14:58:52,507 - INFO -   Batch 900/1264: Loss = 0.1219
2025-12-27 14:59:25,198 - INFO -   Batch 950/1264: Loss = 0.0899
2025-12-27 14:59:57,856 - INFO -   Batch 1000/1264: Loss = 0.0855
2025-12-27 15:00:30,516 - INFO -   Batch 1050/1264: Loss = 0.0788
2025-12-27 15:01:03,216 - INFO -   Batch 1100/1264: Loss = 0.1010
2025-12-27 15:01:35,919 - INFO -   Batch 1150/1264: Loss = 0.0901
2025-12-27 15:02:08,698 - INFO -   Batch 1200/1264: Loss = 0.0839
2025-12-27 15:02:41,537 - INFO -   Batch 1250/1264: Loss = 0.0845
2025-12-27 15:05:45,869 - INFO - Epoch 143/160: Train Loss: 0.0911, Val Loss: 1.1173, Val mIoU: 44.07%, Val PixelAcc: 80.54%
2025-12-27 15:06:20,733 - INFO -   Batch 50/1264: Loss = 0.0874
2025-12-27 15:06:53,409 - INFO -   Batch 100/1264: Loss = 0.1508
2025-12-27 15:07:26,135 - INFO -   Batch 150/1264: Loss = 0.0912
2025-12-27 15:07:58,939 - INFO -   Batch 200/1264: Loss = 0.1089
2025-12-27 15:08:31,792 - INFO -   Batch 250/1264: Loss = 0.1187
2025-12-27 15:09:04,721 - INFO -   Batch 300/1264: Loss = 0.1119
2025-12-27 15:09:37,686 - INFO -   Batch 350/1264: Loss = 0.1083
2025-12-27 15:10:10,669 - INFO -   Batch 400/1264: Loss = 0.0932
2025-12-27 15:10:43,689 - INFO -   Batch 450/1264: Loss = 0.1206
2025-12-27 15:11:16,609 - INFO -   Batch 500/1264: Loss = 0.1291
2025-12-27 15:11:49,261 - INFO -   Batch 550/1264: Loss = 0.0910
2025-12-27 15:12:21,915 - INFO -   Batch 600/1264: Loss = 0.1068
2025-12-27 15:12:54,587 - INFO -   Batch 650/1264: Loss = 0.0885
2025-12-27 15:13:27,254 - INFO -   Batch 700/1264: Loss = 0.0830
2025-12-27 15:13:59,936 - INFO -   Batch 750/1264: Loss = 0.0922
2025-12-27 15:14:32,608 - INFO -   Batch 800/1264: Loss = 0.1004
2025-12-27 15:15:05,325 - INFO -   Batch 850/1264: Loss = 0.0977
2025-12-27 15:15:38,102 - INFO -   Batch 900/1264: Loss = 0.0865
2025-12-27 15:16:10,941 - INFO -   Batch 950/1264: Loss = 0.0825
2025-12-27 15:16:43,853 - INFO -   Batch 1000/1264: Loss = 0.0799
2025-12-27 15:17:16,542 - INFO -   Batch 1050/1264: Loss = 0.0893
2025-12-27 15:17:49,208 - INFO -   Batch 1100/1264: Loss = 0.1107
2025-12-27 15:18:21,875 - INFO -   Batch 1150/1264: Loss = 0.0629
2025-12-27 15:18:54,576 - INFO -   Batch 1200/1264: Loss = 0.0851
2025-12-27 15:19:27,370 - INFO -   Batch 1250/1264: Loss = 0.0606
2025-12-27 15:22:31,656 - INFO - Epoch 144/160: Train Loss: 0.0908, Val Loss: 1.1091, Val mIoU: 44.06%, Val PixelAcc: 80.50%
2025-12-27 15:23:06,419 - INFO -   Batch 50/1264: Loss = 0.0867
2025-12-27 15:23:39,098 - INFO -   Batch 100/1264: Loss = 0.0885
2025-12-27 15:24:11,767 - INFO -   Batch 150/1264: Loss = 0.0818
2025-12-27 15:24:44,455 - INFO -   Batch 200/1264: Loss = 0.0936
2025-12-27 15:25:17,122 - INFO -   Batch 250/1264: Loss = 0.0966
2025-12-27 15:25:49,789 - INFO -   Batch 300/1264: Loss = 0.0959
2025-12-27 15:26:22,475 - INFO -   Batch 350/1264: Loss = 0.0829
2025-12-27 15:26:55,142 - INFO -   Batch 400/1264: Loss = 0.0783
2025-12-27 15:27:27,811 - INFO -   Batch 450/1264: Loss = 0.1019
2025-12-27 15:28:00,487 - INFO -   Batch 500/1264: Loss = 0.0949
2025-12-27 15:28:33,209 - INFO -   Batch 550/1264: Loss = 0.1111
2025-12-27 15:29:05,991 - INFO -   Batch 600/1264: Loss = 0.0804
2025-12-27 15:29:38,827 - INFO -   Batch 650/1264: Loss = 0.0867
2025-12-27 15:30:11,753 - INFO -   Batch 700/1264: Loss = 0.0594
2025-12-27 15:30:44,442 - INFO -   Batch 750/1264: Loss = 0.0840
2025-12-27 15:31:17,102 - INFO -   Batch 800/1264: Loss = 0.0815
2025-12-27 15:31:49,776 - INFO -   Batch 850/1264: Loss = 0.0918
2025-12-27 15:32:22,520 - INFO -   Batch 900/1264: Loss = 0.0937
2025-12-27 15:32:55,322 - INFO -   Batch 950/1264: Loss = 0.0968
2025-12-27 15:33:28,148 - INFO -   Batch 1000/1264: Loss = 0.1013
2025-12-27 15:34:00,807 - INFO -   Batch 1050/1264: Loss = 0.0779
2025-12-27 15:34:33,467 - INFO -   Batch 1100/1264: Loss = 0.0911
2025-12-27 15:35:06,140 - INFO -   Batch 1150/1264: Loss = 0.1098
2025-12-27 15:35:38,814 - INFO -   Batch 1200/1264: Loss = 0.0890
2025-12-27 15:36:11,489 - INFO -   Batch 1250/1264: Loss = 0.0957
2025-12-27 15:39:16,208 - INFO - Epoch 145/160: Train Loss: 0.0906, Val Loss: 1.1183, Val mIoU: 44.20%, Val PixelAcc: 80.52%
2025-12-27 15:39:51,509 - INFO -   Batch 50/1264: Loss = 0.0782
2025-12-27 15:40:24,484 - INFO -   Batch 100/1264: Loss = 0.1188
2025-12-27 15:40:57,474 - INFO -   Batch 150/1264: Loss = 0.0863
2025-12-27 15:41:30,330 - INFO -   Batch 200/1264: Loss = 0.0761
2025-12-27 15:42:02,986 - INFO -   Batch 250/1264: Loss = 0.0923
2025-12-27 15:42:35,678 - INFO -   Batch 300/1264: Loss = 0.0824
2025-12-27 15:43:08,367 - INFO -   Batch 350/1264: Loss = 0.0817
2025-12-27 15:43:41,110 - INFO -   Batch 400/1264: Loss = 0.0692
2025-12-27 15:44:13,920 - INFO -   Batch 450/1264: Loss = 0.1360
2025-12-27 15:44:46,783 - INFO -   Batch 500/1264: Loss = 0.1018
2025-12-27 15:45:19,714 - INFO -   Batch 550/1264: Loss = 0.1026
2025-12-27 15:45:52,679 - INFO -   Batch 600/1264: Loss = 0.0819
2025-12-27 15:46:25,663 - INFO -   Batch 650/1264: Loss = 0.0985
2025-12-27 15:46:58,666 - INFO -   Batch 700/1264: Loss = 0.0562
2025-12-27 15:47:31,484 - INFO -   Batch 750/1264: Loss = 0.0818
2025-12-27 15:48:04,161 - INFO -   Batch 800/1264: Loss = 0.1016
2025-12-27 15:48:36,836 - INFO -   Batch 850/1264: Loss = 0.0867
2025-12-27 15:49:09,508 - INFO -   Batch 900/1264: Loss = 0.1180
2025-12-27 15:49:42,244 - INFO -   Batch 950/1264: Loss = 0.1285
2025-12-27 15:50:15,056 - INFO -   Batch 1000/1264: Loss = 0.0913
2025-12-27 15:50:47,933 - INFO -   Batch 1050/1264: Loss = 0.0971
2025-12-27 15:51:20,861 - INFO -   Batch 1100/1264: Loss = 0.0636
2025-12-27 15:51:53,816 - INFO -   Batch 1150/1264: Loss = 0.0691
2025-12-27 15:52:26,794 - INFO -   Batch 1200/1264: Loss = 0.1242
2025-12-27 15:52:59,777 - INFO -   Batch 1250/1264: Loss = 0.1022
2025-12-27 15:56:05,103 - INFO - Epoch 146/160: Train Loss: 0.0905, Val Loss: 1.1100, Val mIoU: 43.98%, Val PixelAcc: 80.46%
2025-12-27 15:56:40,363 - INFO -   Batch 50/1264: Loss = 0.0995
2025-12-27 15:57:13,256 - INFO -   Batch 100/1264: Loss = 0.0886
2025-12-27 15:57:46,206 - INFO -   Batch 150/1264: Loss = 0.0795
2025-12-27 15:58:18,943 - INFO -   Batch 200/1264: Loss = 0.0691
2025-12-27 15:58:51,611 - INFO -   Batch 250/1264: Loss = 0.0779
2025-12-27 15:59:24,288 - INFO -   Batch 300/1264: Loss = 0.0915
2025-12-27 15:59:56,984 - INFO -   Batch 350/1264: Loss = 0.0861
2025-12-27 16:00:29,742 - INFO -   Batch 400/1264: Loss = 0.1055
2025-12-27 16:01:02,560 - INFO -   Batch 450/1264: Loss = 0.1349
2025-12-27 16:01:35,451 - INFO -   Batch 500/1264: Loss = 0.1002
2025-12-27 16:02:08,420 - INFO -   Batch 550/1264: Loss = 0.0842
2025-12-27 16:02:41,420 - INFO -   Batch 600/1264: Loss = 0.0715
2025-12-27 16:03:14,421 - INFO -   Batch 650/1264: Loss = 0.0828
2025-12-27 16:03:47,438 - INFO -   Batch 700/1264: Loss = 0.0882
2025-12-27 16:04:20,199 - INFO -   Batch 750/1264: Loss = 0.0636
2025-12-27 16:04:52,859 - INFO -   Batch 800/1264: Loss = 0.1016
2025-12-27 16:05:25,528 - INFO -   Batch 850/1264: Loss = 0.1332
2025-12-27 16:05:58,190 - INFO -   Batch 900/1264: Loss = 0.0617
2025-12-27 16:06:30,851 - INFO -   Batch 950/1264: Loss = 0.0966
2025-12-27 16:07:03,524 - INFO -   Batch 1000/1264: Loss = 0.0912
2025-12-27 16:07:36,193 - INFO -   Batch 1050/1264: Loss = 0.1019
2025-12-27 16:08:08,917 - INFO -   Batch 1100/1264: Loss = 0.0913
2025-12-27 16:08:41,717 - INFO -   Batch 1150/1264: Loss = 0.0774
2025-12-27 16:09:14,580 - INFO -   Batch 1200/1264: Loss = 0.1085
2025-12-27 16:09:47,512 - INFO -   Batch 1250/1264: Loss = 0.0872
2025-12-27 16:12:52,058 - INFO - Epoch 147/160: Train Loss: 0.0904, Val Loss: 1.1256, Val mIoU: 43.96%, Val PixelAcc: 80.45%
2025-12-27 16:13:27,293 - INFO -   Batch 50/1264: Loss = 0.0802
2025-12-27 16:13:59,961 - INFO -   Batch 100/1264: Loss = 0.0800
2025-12-27 16:14:32,641 - INFO -   Batch 150/1264: Loss = 0.0944
2025-12-27 16:15:05,383 - INFO -   Batch 200/1264: Loss = 0.0821
2025-12-27 16:15:38,196 - INFO -   Batch 250/1264: Loss = 0.0812
2025-12-27 16:16:11,051 - INFO -   Batch 300/1264: Loss = 0.0740
2025-12-27 16:16:43,996 - INFO -   Batch 350/1264: Loss = 0.1082
2025-12-27 16:17:16,967 - INFO -   Batch 400/1264: Loss = 0.0964
2025-12-27 16:17:49,947 - INFO -   Batch 450/1264: Loss = 0.0982
2025-12-27 16:18:22,800 - INFO -   Batch 500/1264: Loss = 0.1027
2025-12-27 16:18:55,465 - INFO -   Batch 550/1264: Loss = 0.0943
2025-12-27 16:19:28,131 - INFO -   Batch 600/1264: Loss = 0.1557
2025-12-27 16:20:00,797 - INFO -   Batch 650/1264: Loss = 0.0874
2025-12-27 16:20:33,460 - INFO -   Batch 700/1264: Loss = 0.0700
2025-12-27 16:21:06,133 - INFO -   Batch 750/1264: Loss = 0.0772
2025-12-27 16:21:38,802 - INFO -   Batch 800/1264: Loss = 0.0922
2025-12-27 16:22:11,467 - INFO -   Batch 850/1264: Loss = 0.1411
2025-12-27 16:22:44,149 - INFO -   Batch 900/1264: Loss = 0.0812
2025-12-27 16:23:16,829 - INFO -   Batch 950/1264: Loss = 0.0832
2025-12-27 16:23:49,514 - INFO -   Batch 1000/1264: Loss = 0.0810
2025-12-27 16:24:22,186 - INFO -   Batch 1050/1264: Loss = 0.0959
2025-12-27 16:24:54,852 - INFO -   Batch 1100/1264: Loss = 0.1138
2025-12-27 16:25:27,526 - INFO -   Batch 1150/1264: Loss = 0.0809
2025-12-27 16:26:00,202 - INFO -   Batch 1200/1264: Loss = 0.0947
2025-12-27 16:26:32,926 - INFO -   Batch 1250/1264: Loss = 0.1113
2025-12-27 16:29:38,091 - INFO - Epoch 148/160: Train Loss: 0.0899, Val Loss: 1.1309, Val mIoU: 44.08%, Val PixelAcc: 80.42%
2025-12-27 16:30:13,229 - INFO -   Batch 50/1264: Loss = 0.0789
2025-12-27 16:30:45,904 - INFO -   Batch 100/1264: Loss = 0.0859
2025-12-27 16:31:18,570 - INFO -   Batch 150/1264: Loss = 0.0966
2025-12-27 16:31:51,244 - INFO -   Batch 200/1264: Loss = 0.0958
2025-12-27 16:32:23,921 - INFO -   Batch 250/1264: Loss = 0.0731
2025-12-27 16:32:56,586 - INFO -   Batch 300/1264: Loss = 0.0918
2025-12-27 16:33:29,254 - INFO -   Batch 350/1264: Loss = 0.0921
2025-12-27 16:34:01,921 - INFO -   Batch 400/1264: Loss = 0.0832
2025-12-27 16:34:34,595 - INFO -   Batch 450/1264: Loss = 0.0792
2025-12-27 16:35:07,267 - INFO -   Batch 500/1264: Loss = 0.0649
2025-12-27 16:35:39,926 - INFO -   Batch 550/1264: Loss = 0.0767
2025-12-27 16:36:12,592 - INFO -   Batch 600/1264: Loss = 0.1015
2025-12-27 16:36:45,251 - INFO -   Batch 650/1264: Loss = 0.0900
2025-12-27 16:37:17,907 - INFO -   Batch 700/1264: Loss = 0.1155
2025-12-27 16:37:50,572 - INFO -   Batch 750/1264: Loss = 0.1034
2025-12-27 16:38:23,266 - INFO -   Batch 800/1264: Loss = 0.0924
2025-12-27 16:38:55,957 - INFO -   Batch 850/1264: Loss = 0.0845
2025-12-27 16:39:28,619 - INFO -   Batch 900/1264: Loss = 0.1160
2025-12-27 16:40:01,288 - INFO -   Batch 950/1264: Loss = 0.1008
2025-12-27 16:40:33,949 - INFO -   Batch 1000/1264: Loss = 0.0786
2025-12-27 16:41:06,620 - INFO -   Batch 1050/1264: Loss = 0.0683
2025-12-27 16:41:39,292 - INFO -   Batch 1100/1264: Loss = 0.0907
2025-12-27 16:42:11,995 - INFO -   Batch 1150/1264: Loss = 0.0848
2025-12-27 16:42:44,784 - INFO -   Batch 1200/1264: Loss = 0.0734
2025-12-27 16:43:17,575 - INFO -   Batch 1250/1264: Loss = 0.0886
2025-12-27 16:46:23,965 - INFO - Epoch 149/160: Train Loss: 0.0896, Val Loss: 1.1215, Val mIoU: 44.18%, Val PixelAcc: 80.53%
2025-12-27 16:46:59,911 - INFO -   Batch 50/1264: Loss = 0.0657
2025-12-27 16:47:32,741 - INFO -   Batch 100/1264: Loss = 0.0881
2025-12-27 16:48:05,657 - INFO -   Batch 150/1264: Loss = 0.0933
2025-12-27 16:48:38,622 - INFO -   Batch 200/1264: Loss = 0.1062
2025-12-27 16:49:11,590 - INFO -   Batch 250/1264: Loss = 0.1168
2025-12-27 16:49:44,648 - INFO -   Batch 300/1264: Loss = 0.0831
2025-12-27 16:50:17,469 - INFO -   Batch 350/1264: Loss = 0.0765
2025-12-27 16:50:50,134 - INFO -   Batch 400/1264: Loss = 0.0816
2025-12-27 16:51:22,820 - INFO -   Batch 450/1264: Loss = 0.0942
2025-12-27 16:51:55,495 - INFO -   Batch 500/1264: Loss = 0.0866
2025-12-27 16:52:28,241 - INFO -   Batch 550/1264: Loss = 0.0972
2025-12-27 16:53:01,064 - INFO -   Batch 600/1264: Loss = 0.0997
2025-12-27 16:53:33,940 - INFO -   Batch 650/1264: Loss = 0.0957
2025-12-27 16:54:06,884 - INFO -   Batch 700/1264: Loss = 0.0812
2025-12-27 16:54:39,877 - INFO -   Batch 750/1264: Loss = 0.1057
2025-12-27 16:55:12,756 - INFO -   Batch 800/1264: Loss = 0.0991
2025-12-27 16:55:45,416 - INFO -   Batch 850/1264: Loss = 0.0702
2025-12-27 16:56:18,104 - INFO -   Batch 900/1264: Loss = 0.0856
2025-12-27 16:56:50,774 - INFO -   Batch 950/1264: Loss = 0.0546
2025-12-27 16:57:23,534 - INFO -   Batch 1000/1264: Loss = 0.1288
2025-12-27 16:57:56,350 - INFO -   Batch 1050/1264: Loss = 0.0539
2025-12-27 16:58:29,252 - INFO -   Batch 1100/1264: Loss = 0.0978
2025-12-27 16:59:02,238 - INFO -   Batch 1150/1264: Loss = 0.0790
2025-12-27 16:59:35,261 - INFO -   Batch 1200/1264: Loss = 0.0617
2025-12-27 17:00:08,366 - INFO -   Batch 1250/1264: Loss = 0.0680
2025-12-27 17:03:10,073 - INFO - Epoch 150/160: Train Loss: 0.0902, Val Loss: 1.1089, Val mIoU: 44.13%, Val PixelAcc: 80.54%
2025-12-27 17:03:11,844 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_150.pth
2025-12-27 17:03:11,846 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_150.pth
2025-12-27 17:03:46,631 - INFO -   Batch 50/1264: Loss = 0.0651
2025-12-27 17:04:19,472 - INFO -   Batch 100/1264: Loss = 0.0686
2025-12-27 17:04:52,400 - INFO -   Batch 150/1264: Loss = 0.0984
2025-12-27 17:05:25,405 - INFO -   Batch 200/1264: Loss = 0.0703
2025-12-27 17:05:58,363 - INFO -   Batch 250/1264: Loss = 0.0913
2025-12-27 17:06:31,027 - INFO -   Batch 300/1264: Loss = 0.0862
2025-12-27 17:07:03,685 - INFO -   Batch 350/1264: Loss = 0.1132
2025-12-27 17:07:36,334 - INFO -   Batch 400/1264: Loss = 0.0780
2025-12-27 17:08:08,997 - INFO -   Batch 450/1264: Loss = 0.1105
2025-12-27 17:08:41,753 - INFO -   Batch 500/1264: Loss = 0.0755
2025-12-27 17:09:14,552 - INFO -   Batch 550/1264: Loss = 0.0783
2025-12-27 17:09:47,409 - INFO -   Batch 600/1264: Loss = 0.1103
2025-12-27 17:10:20,330 - INFO -   Batch 650/1264: Loss = 0.0801
2025-12-27 17:10:53,308 - INFO -   Batch 700/1264: Loss = 0.0791
2025-12-27 17:11:26,317 - INFO -   Batch 750/1264: Loss = 0.1120
2025-12-27 17:11:59,356 - INFO -   Batch 800/1264: Loss = 0.1019
2025-12-27 17:12:32,291 - INFO -   Batch 850/1264: Loss = 0.0809
2025-12-27 17:13:04,955 - INFO -   Batch 900/1264: Loss = 0.0951
2025-12-27 17:13:37,627 - INFO -   Batch 950/1264: Loss = 0.0909
2025-12-27 17:14:10,386 - INFO -   Batch 1000/1264: Loss = 0.0819
2025-12-27 17:14:43,176 - INFO -   Batch 1050/1264: Loss = 0.0892
2025-12-27 17:15:15,877 - INFO -   Batch 1100/1264: Loss = 0.0774
2025-12-27 17:15:48,557 - INFO -   Batch 1150/1264: Loss = 0.0631
2025-12-27 17:16:21,244 - INFO -   Batch 1200/1264: Loss = 0.0765
2025-12-27 17:16:53,894 - INFO -   Batch 1250/1264: Loss = 0.0919
2025-12-27 17:19:56,239 - INFO - Epoch 151/160: Train Loss: 0.0895, Val Loss: 1.1140, Val mIoU: 44.12%, Val PixelAcc: 80.51%
2025-12-27 17:20:30,639 - INFO -   Batch 50/1264: Loss = 0.0922
2025-12-27 17:21:03,573 - INFO -   Batch 100/1264: Loss = 0.0976
2025-12-27 17:21:36,553 - INFO -   Batch 150/1264: Loss = 0.0862
2025-12-27 17:22:09,585 - INFO -   Batch 200/1264: Loss = 0.0953
2025-12-27 17:22:42,621 - INFO -   Batch 250/1264: Loss = 0.1097
2025-12-27 17:23:15,684 - INFO -   Batch 300/1264: Loss = 0.0836
2025-12-27 17:23:48,440 - INFO -   Batch 350/1264: Loss = 0.0910
2025-12-27 17:24:21,109 - INFO -   Batch 400/1264: Loss = 0.0856
2025-12-27 17:24:54,231 - INFO -   Batch 450/1264: Loss = 0.0991
2025-12-27 17:25:26,925 - INFO -   Batch 500/1264: Loss = 0.0988
2025-12-27 17:26:01,047 - INFO -   Batch 550/1264: Loss = 0.0794
2025-12-27 17:26:33,713 - INFO -   Batch 600/1264: Loss = 0.1110
2025-12-27 17:27:06,418 - INFO -   Batch 650/1264: Loss = 0.0716
2025-12-27 17:27:39,148 - INFO -   Batch 700/1264: Loss = 0.0837
2025-12-27 17:28:11,842 - INFO -   Batch 750/1264: Loss = 0.1023
2025-12-27 17:28:44,509 - INFO -   Batch 800/1264: Loss = 0.0874
2025-12-27 17:29:17,169 - INFO -   Batch 850/1264: Loss = 0.1002
2025-12-27 17:29:49,848 - INFO -   Batch 900/1264: Loss = 0.0709
2025-12-27 17:30:22,585 - INFO -   Batch 950/1264: Loss = 0.0812
2025-12-27 17:30:55,383 - INFO -   Batch 1000/1264: Loss = 0.0667
2025-12-27 17:31:28,071 - INFO -   Batch 1050/1264: Loss = 0.1132
2025-12-27 17:32:00,737 - INFO -   Batch 1100/1264: Loss = 0.0786
2025-12-27 17:32:33,420 - INFO -   Batch 1150/1264: Loss = 0.0929
2025-12-27 17:33:06,198 - INFO -   Batch 1200/1264: Loss = 0.0843
2025-12-27 17:33:39,009 - INFO -   Batch 1250/1264: Loss = 0.1241
2025-12-27 17:36:41,742 - INFO - Epoch 152/160: Train Loss: 0.0898, Val Loss: 1.1097, Val mIoU: 44.12%, Val PixelAcc: 80.54%
2025-12-27 17:37:16,327 - INFO -   Batch 50/1264: Loss = 0.0913
2025-12-27 17:37:49,017 - INFO -   Batch 100/1264: Loss = 0.0933
2025-12-27 17:38:21,812 - INFO -   Batch 150/1264: Loss = 0.0691
2025-12-27 17:38:54,542 - INFO -   Batch 200/1264: Loss = 0.0797
2025-12-27 17:39:27,208 - INFO -   Batch 250/1264: Loss = 0.0875
2025-12-27 17:39:59,897 - INFO -   Batch 300/1264: Loss = 0.0951
2025-12-27 17:40:32,589 - INFO -   Batch 350/1264: Loss = 0.0800
2025-12-27 17:41:05,257 - INFO -   Batch 400/1264: Loss = 0.0606
2025-12-27 17:41:37,939 - INFO -   Batch 450/1264: Loss = 0.0784
2025-12-27 17:42:10,809 - INFO -   Batch 500/1264: Loss = 0.1097
2025-12-27 17:42:43,627 - INFO -   Batch 550/1264: Loss = 0.0941
2025-12-27 17:43:16,664 - INFO -   Batch 600/1264: Loss = 0.0702
2025-12-27 17:43:49,650 - INFO -   Batch 650/1264: Loss = 0.0840
2025-12-27 17:44:22,756 - INFO -   Batch 700/1264: Loss = 0.0915
2025-12-27 17:44:55,800 - INFO -   Batch 750/1264: Loss = 0.1161
2025-12-27 17:45:28,838 - INFO -   Batch 800/1264: Loss = 0.0740
2025-12-27 17:46:01,978 - INFO -   Batch 850/1264: Loss = 0.0899
2025-12-27 17:46:34,804 - INFO -   Batch 900/1264: Loss = 0.0747
2025-12-27 17:47:07,474 - INFO -   Batch 950/1264: Loss = 0.0780
2025-12-27 17:47:40,145 - INFO -   Batch 1000/1264: Loss = 0.0777
2025-12-27 17:48:12,988 - INFO -   Batch 1050/1264: Loss = 0.0818
2025-12-27 17:48:45,749 - INFO -   Batch 1100/1264: Loss = 0.0980
2025-12-27 17:49:18,566 - INFO -   Batch 1150/1264: Loss = 0.0812
2025-12-27 17:49:51,423 - INFO -   Batch 1200/1264: Loss = 0.0861
2025-12-27 17:50:24,387 - INFO -   Batch 1250/1264: Loss = 0.0760
2025-12-27 17:53:33,647 - INFO - Epoch 153/160: Train Loss: 0.0894, Val Loss: 1.1143, Val mIoU: 44.24%, Val PixelAcc: 80.57%
2025-12-27 17:54:08,467 - INFO -   Batch 50/1264: Loss = 0.0758
2025-12-27 17:54:41,160 - INFO -   Batch 100/1264: Loss = 0.0964
2025-12-27 17:55:13,923 - INFO -   Batch 150/1264: Loss = 0.0939
2025-12-27 17:55:46,752 - INFO -   Batch 200/1264: Loss = 0.1114
2025-12-27 17:56:20,169 - INFO -   Batch 250/1264: Loss = 0.0602
2025-12-27 17:56:53,137 - INFO -   Batch 300/1264: Loss = 0.0997
2025-12-27 17:57:26,128 - INFO -   Batch 350/1264: Loss = 0.0880
2025-12-27 17:57:59,150 - INFO -   Batch 400/1264: Loss = 0.0926
2025-12-27 17:58:32,180 - INFO -   Batch 450/1264: Loss = 0.0627
2025-12-27 17:59:04,928 - INFO -   Batch 500/1264: Loss = 0.0793
2025-12-27 17:59:37,580 - INFO -   Batch 550/1264: Loss = 0.1351
2025-12-27 18:00:10,248 - INFO -   Batch 600/1264: Loss = 0.0820
2025-12-27 18:00:42,974 - INFO -   Batch 650/1264: Loss = 0.1150
2025-12-27 18:01:15,681 - INFO -   Batch 700/1264: Loss = 0.1036
2025-12-27 18:01:48,336 - INFO -   Batch 750/1264: Loss = 0.0792
2025-12-27 18:02:21,018 - INFO -   Batch 800/1264: Loss = 0.0767
2025-12-27 18:02:53,883 - INFO -   Batch 850/1264: Loss = 0.0760
2025-12-27 18:03:26,551 - INFO -   Batch 900/1264: Loss = 0.0804
2025-12-27 18:03:59,216 - INFO -   Batch 950/1264: Loss = 0.0665
2025-12-27 18:04:31,863 - INFO -   Batch 1000/1264: Loss = 0.0901
2025-12-27 18:05:04,520 - INFO -   Batch 1050/1264: Loss = 0.0628
2025-12-27 18:05:37,199 - INFO -   Batch 1100/1264: Loss = 0.1259
2025-12-27 18:06:09,923 - INFO -   Batch 1150/1264: Loss = 0.1207
2025-12-27 18:06:42,579 - INFO -   Batch 1200/1264: Loss = 0.0959
2025-12-27 18:07:15,236 - INFO -   Batch 1250/1264: Loss = 0.0717
2025-12-27 18:10:17,131 - INFO - Epoch 154/160: Train Loss: 0.0899, Val Loss: 1.1116, Val mIoU: 44.13%, Val PixelAcc: 80.50%
2025-12-27 18:10:51,925 - INFO -   Batch 50/1264: Loss = 0.0840
2025-12-27 18:11:24,899 - INFO -   Batch 100/1264: Loss = 0.0901
2025-12-27 18:11:57,908 - INFO -   Batch 150/1264: Loss = 0.0799
2025-12-27 18:12:30,675 - INFO -   Batch 200/1264: Loss = 0.0829
2025-12-27 18:13:03,329 - INFO -   Batch 250/1264: Loss = 0.0654
2025-12-27 18:13:35,983 - INFO -   Batch 300/1264: Loss = 0.0820
2025-12-27 18:14:08,633 - INFO -   Batch 350/1264: Loss = 0.0691
2025-12-27 18:14:41,286 - INFO -   Batch 400/1264: Loss = 0.0828
2025-12-27 18:15:13,969 - INFO -   Batch 450/1264: Loss = 0.0680
2025-12-27 18:15:46,653 - INFO -   Batch 500/1264: Loss = 0.0968
2025-12-27 18:16:19,300 - INFO -   Batch 550/1264: Loss = 0.0955
2025-12-27 18:16:51,967 - INFO -   Batch 600/1264: Loss = 0.0996
2025-12-27 18:17:24,658 - INFO -   Batch 650/1264: Loss = 0.0852
2025-12-27 18:17:57,414 - INFO -   Batch 700/1264: Loss = 0.0860
2025-12-27 18:18:30,220 - INFO -   Batch 750/1264: Loss = 0.1095
2025-12-27 18:19:03,070 - INFO -   Batch 800/1264: Loss = 0.1098
2025-12-27 18:19:35,986 - INFO -   Batch 850/1264: Loss = 0.1389
2025-12-27 18:20:08,973 - INFO -   Batch 900/1264: Loss = 0.0754
2025-12-27 18:20:41,983 - INFO -   Batch 950/1264: Loss = 0.1002
2025-12-27 18:21:14,690 - INFO -   Batch 1000/1264: Loss = 0.1045
2025-12-27 18:21:47,347 - INFO -   Batch 1050/1264: Loss = 0.0706
2025-12-27 18:22:20,021 - INFO -   Batch 1100/1264: Loss = 0.1294
2025-12-27 18:22:52,754 - INFO -   Batch 1150/1264: Loss = 0.0965
2025-12-27 18:23:25,554 - INFO -   Batch 1200/1264: Loss = 0.0725
2025-12-27 18:23:58,387 - INFO -   Batch 1250/1264: Loss = 0.0712
2025-12-27 18:27:00,124 - INFO - Epoch 155/160: Train Loss: 0.0899, Val Loss: 1.1121, Val mIoU: 44.33%, Val PixelAcc: 80.48%
2025-12-27 18:27:35,250 - INFO -   Batch 50/1264: Loss = 0.0813
2025-12-27 18:28:08,111 - INFO -   Batch 100/1264: Loss = 0.0690
2025-12-27 18:28:41,058 - INFO -   Batch 150/1264: Loss = 0.0759
2025-12-27 18:29:14,018 - INFO -   Batch 200/1264: Loss = 0.0944
2025-12-27 18:29:46,702 - INFO -   Batch 250/1264: Loss = 0.0870
2025-12-27 18:30:19,371 - INFO -   Batch 300/1264: Loss = 0.0875
2025-12-27 18:30:52,028 - INFO -   Batch 350/1264: Loss = 0.0919
2025-12-27 18:31:24,679 - INFO -   Batch 400/1264: Loss = 0.0756
2025-12-27 18:31:57,351 - INFO -   Batch 450/1264: Loss = 0.0599
2025-12-27 18:32:30,102 - INFO -   Batch 500/1264: Loss = 0.0795
2025-12-27 18:33:02,881 - INFO -   Batch 550/1264: Loss = 0.1137
2025-12-27 18:33:35,703 - INFO -   Batch 600/1264: Loss = 0.0956
2025-12-27 18:34:08,599 - INFO -   Batch 650/1264: Loss = 0.0978
2025-12-27 18:34:41,552 - INFO -   Batch 700/1264: Loss = 0.0891
2025-12-27 18:35:14,558 - INFO -   Batch 750/1264: Loss = 0.0849
2025-12-27 18:35:47,626 - INFO -   Batch 800/1264: Loss = 0.0624
2025-12-27 18:36:20,710 - INFO -   Batch 850/1264: Loss = 0.1246
2025-12-27 18:36:53,589 - INFO -   Batch 900/1264: Loss = 0.0757
2025-12-27 18:37:26,253 - INFO -   Batch 950/1264: Loss = 0.0602
2025-12-27 18:37:58,911 - INFO -   Batch 1000/1264: Loss = 0.1050
2025-12-27 18:38:31,618 - INFO -   Batch 1050/1264: Loss = 0.0946
2025-12-27 18:39:04,395 - INFO -   Batch 1100/1264: Loss = 0.1185
2025-12-27 18:39:37,208 - INFO -   Batch 1150/1264: Loss = 0.1053
2025-12-27 18:40:10,084 - INFO -   Batch 1200/1264: Loss = 0.0961
2025-12-27 18:40:43,010 - INFO -   Batch 1250/1264: Loss = 0.0857
2025-12-27 18:43:45,111 - INFO - Epoch 156/160: Train Loss: 0.0895, Val Loss: 1.1064, Val mIoU: 44.14%, Val PixelAcc: 80.49%
2025-12-27 18:44:19,509 - INFO -   Batch 50/1264: Loss = 0.0814
2025-12-27 18:44:52,186 - INFO -   Batch 100/1264: Loss = 0.0816
2025-12-27 18:45:24,938 - INFO -   Batch 150/1264: Loss = 0.0812
2025-12-27 18:45:57,745 - INFO -   Batch 200/1264: Loss = 0.0925
2025-12-27 18:46:30,627 - INFO -   Batch 250/1264: Loss = 0.0750
2025-12-27 18:47:03,505 - INFO -   Batch 300/1264: Loss = 0.0872
2025-12-27 18:47:36,170 - INFO -   Batch 350/1264: Loss = 0.0625
2025-12-27 18:48:08,828 - INFO -   Batch 400/1264: Loss = 0.0998
2025-12-27 18:48:41,478 - INFO -   Batch 450/1264: Loss = 0.1002
2025-12-27 18:49:14,132 - INFO -   Batch 500/1264: Loss = 0.0855
2025-12-27 18:49:46,794 - INFO -   Batch 550/1264: Loss = 0.0844
2025-12-27 18:50:19,454 - INFO -   Batch 600/1264: Loss = 0.0762
2025-12-27 18:50:52,116 - INFO -   Batch 650/1264: Loss = 0.1092
2025-12-27 18:51:24,839 - INFO -   Batch 700/1264: Loss = 0.0861
2025-12-27 18:51:57,610 - INFO -   Batch 750/1264: Loss = 0.0802
2025-12-27 18:52:30,453 - INFO -   Batch 800/1264: Loss = 0.1161
2025-12-27 18:53:03,349 - INFO -   Batch 850/1264: Loss = 0.0845
2025-12-27 18:53:36,290 - INFO -   Batch 900/1264: Loss = 0.1035
2025-12-27 18:54:09,275 - INFO -   Batch 950/1264: Loss = 0.0801
2025-12-27 18:54:42,288 - INFO -   Batch 1000/1264: Loss = 0.0872
2025-12-27 18:55:15,351 - INFO -   Batch 1050/1264: Loss = 0.0756
2025-12-27 18:55:48,419 - INFO -   Batch 1100/1264: Loss = 0.1065
2025-12-27 18:56:21,129 - INFO -   Batch 1150/1264: Loss = 0.0944
2025-12-27 18:56:53,793 - INFO -   Batch 1200/1264: Loss = 0.1439
2025-12-27 18:57:26,465 - INFO -   Batch 1250/1264: Loss = 0.0848
2025-12-27 19:00:28,046 - INFO - Epoch 157/160: Train Loss: 0.0891, Val Loss: 1.1137, Val mIoU: 44.14%, Val PixelAcc: 80.46%
2025-12-27 19:01:02,985 - INFO -   Batch 50/1264: Loss = 0.0880
2025-12-27 19:01:35,990 - INFO -   Batch 100/1264: Loss = 0.0848
2025-12-27 19:02:09,042 - INFO -   Batch 150/1264: Loss = 0.0895
2025-12-27 19:02:41,785 - INFO -   Batch 200/1264: Loss = 0.0883
2025-12-27 19:03:14,454 - INFO -   Batch 250/1264: Loss = 0.0597
2025-12-27 19:03:47,118 - INFO -   Batch 300/1264: Loss = 0.0879
2025-12-27 19:04:19,778 - INFO -   Batch 350/1264: Loss = 0.0904
2025-12-27 19:04:52,445 - INFO -   Batch 400/1264: Loss = 0.0900
2025-12-27 19:05:25,144 - INFO -   Batch 450/1264: Loss = 0.0788
2025-12-27 19:05:57,924 - INFO -   Batch 500/1264: Loss = 0.1017
2025-12-27 19:06:30,748 - INFO -   Batch 550/1264: Loss = 0.0833
2025-12-27 19:07:03,609 - INFO -   Batch 600/1264: Loss = 0.0970
2025-12-27 19:07:36,540 - INFO -   Batch 650/1264: Loss = 0.1163
2025-12-27 19:08:09,544 - INFO -   Batch 700/1264: Loss = 0.1066
2025-12-27 19:08:42,586 - INFO -   Batch 750/1264: Loss = 0.0876
2025-12-27 19:09:15,637 - INFO -   Batch 800/1264: Loss = 0.0962
2025-12-27 19:09:48,715 - INFO -   Batch 850/1264: Loss = 0.0787
2025-12-27 19:10:21,408 - INFO -   Batch 900/1264: Loss = 0.0716
2025-12-27 19:10:54,072 - INFO -   Batch 950/1264: Loss = 0.0844
2025-12-27 19:11:26,748 - INFO -   Batch 1000/1264: Loss = 0.1006
2025-12-27 19:11:59,495 - INFO -   Batch 1050/1264: Loss = 0.0711
2025-12-27 19:12:32,288 - INFO -   Batch 1100/1264: Loss = 0.1104
2025-12-27 19:13:05,119 - INFO -   Batch 1150/1264: Loss = 0.0803
2025-12-27 19:13:38,015 - INFO -   Batch 1200/1264: Loss = 0.0984
2025-12-27 19:14:10,992 - INFO -   Batch 1250/1264: Loss = 0.1058
2025-12-27 19:17:13,599 - INFO - Epoch 158/160: Train Loss: 0.0896, Val Loss: 1.1262, Val mIoU: 43.86%, Val PixelAcc: 80.46%
2025-12-27 19:17:49,214 - INFO -   Batch 50/1264: Loss = 0.1019
2025-12-27 19:18:21,891 - INFO -   Batch 100/1264: Loss = 0.0896
2025-12-27 19:18:54,572 - INFO -   Batch 150/1264: Loss = 0.0847
2025-12-27 19:19:27,249 - INFO -   Batch 200/1264: Loss = 0.0855
2025-12-27 19:19:59,941 - INFO -   Batch 250/1264: Loss = 0.0962
2025-12-27 19:20:32,715 - INFO -   Batch 300/1264: Loss = 0.1076
2025-12-27 19:21:05,557 - INFO -   Batch 350/1264: Loss = 0.0691
2025-12-27 19:21:38,446 - INFO -   Batch 400/1264: Loss = 0.0846
2025-12-27 19:22:11,378 - INFO -   Batch 450/1264: Loss = 0.0543
2025-12-27 19:22:44,336 - INFO -   Batch 500/1264: Loss = 0.0905
2025-12-27 19:23:17,329 - INFO -   Batch 550/1264: Loss = 0.0839
2025-12-27 19:23:50,334 - INFO -   Batch 600/1264: Loss = 0.0921
2025-12-27 19:24:23,077 - INFO -   Batch 650/1264: Loss = 0.1056
2025-12-27 19:24:55,738 - INFO -   Batch 700/1264: Loss = 0.0956
2025-12-27 19:25:28,410 - INFO -   Batch 750/1264: Loss = 0.1171
2025-12-27 19:26:01,166 - INFO -   Batch 800/1264: Loss = 0.0925
2025-12-27 19:26:33,950 - INFO -   Batch 850/1264: Loss = 0.1125
2025-12-27 19:27:06,792 - INFO -   Batch 900/1264: Loss = 0.0808
2025-12-27 19:27:39,780 - INFO -   Batch 950/1264: Loss = 0.0846
2025-12-27 19:28:12,830 - INFO -   Batch 1000/1264: Loss = 0.0567
2025-12-27 19:28:45,847 - INFO -   Batch 1050/1264: Loss = 0.0738
2025-12-27 19:29:18,769 - INFO -   Batch 1100/1264: Loss = 0.0720
2025-12-27 19:29:51,498 - INFO -   Batch 1150/1264: Loss = 0.0710
2025-12-27 19:30:24,264 - INFO -   Batch 1200/1264: Loss = 0.1138
2025-12-27 19:30:56,990 - INFO -   Batch 1250/1264: Loss = 0.0663
2025-12-27 19:34:06,967 - INFO - Epoch 159/160: Train Loss: 0.0894, Val Loss: 1.1205, Val mIoU: 43.99%, Val PixelAcc: 80.51%
2025-12-27 19:34:41,648 - INFO -   Batch 50/1264: Loss = 0.0726
2025-12-27 19:35:14,776 - INFO -   Batch 100/1264: Loss = 0.0903
2025-12-27 19:35:47,577 - INFO -   Batch 150/1264: Loss = 0.0951
2025-12-27 19:36:20,267 - INFO -   Batch 200/1264: Loss = 0.1086
2025-12-27 19:36:52,946 - INFO -   Batch 250/1264: Loss = 0.0961
2025-12-27 19:37:25,690 - INFO -   Batch 300/1264: Loss = 0.1066
2025-12-27 19:37:58,484 - INFO -   Batch 350/1264: Loss = 0.0981
2025-12-27 19:38:31,329 - INFO -   Batch 400/1264: Loss = 0.0844
2025-12-27 19:39:04,007 - INFO -   Batch 450/1264: Loss = 0.0717
2025-12-27 19:39:36,689 - INFO -   Batch 500/1264: Loss = 0.0926
2025-12-27 19:40:09,431 - INFO -   Batch 550/1264: Loss = 0.0693
2025-12-27 19:40:42,209 - INFO -   Batch 600/1264: Loss = 0.0774
2025-12-27 19:41:15,034 - INFO -   Batch 650/1264: Loss = 0.0982
2025-12-27 19:41:47,916 - INFO -   Batch 700/1264: Loss = 0.1177
2025-12-27 19:42:20,875 - INFO -   Batch 750/1264: Loss = 0.0962
2025-12-27 19:42:53,845 - INFO -   Batch 800/1264: Loss = 0.0961
2025-12-27 19:43:26,847 - INFO -   Batch 850/1264: Loss = 0.0837
2025-12-27 19:43:59,958 - INFO -   Batch 900/1264: Loss = 0.0857
2025-12-27 19:44:33,078 - INFO -   Batch 950/1264: Loss = 0.1048
2025-12-27 19:45:05,936 - INFO -   Batch 1000/1264: Loss = 0.0959
2025-12-27 19:45:38,592 - INFO -   Batch 1050/1264: Loss = 0.0703
2025-12-27 19:46:11,276 - INFO -   Batch 1100/1264: Loss = 0.0818
2025-12-27 19:46:43,991 - INFO -   Batch 1150/1264: Loss = 0.1041
2025-12-27 19:47:16,839 - INFO -   Batch 1200/1264: Loss = 0.0862
2025-12-27 19:47:49,697 - INFO -   Batch 1250/1264: Loss = 0.1012
2025-12-27 19:50:51,961 - INFO - Epoch 160/160: Train Loss: 0.0897, Val Loss: 1.1216, Val mIoU: 44.11%, Val PixelAcc: 80.54%
2025-12-27 19:50:53,496 - INFO - ✅ Checkpoint saved: runs/run_221/checkpoint_epoch_160.pth
2025-12-27 19:50:53,497 - INFO - Checkpoint saved: runs/run_221/checkpoint_epoch_160.pth
2025-12-27 19:50:53,498 - INFO - Training complete! Best mIoU: 44.41%
2025-12-27 19:50:53,499 - INFO - Training complete!
2025-12-27 19:50:53,500 - INFO - Generating reports...
2025-12-27 19:50:53,502 - INFO - Generating segmentation training curves...
2025-12-27 19:50:54,477 - INFO - Segmentation training curves saved with base name 'runs/run_221/segmentation_curves_swin_upernet'
2025-12-27 19:50:54,479 - INFO - Performing final evaluation on validation set...
2025-12-27 19:53:48,541 - INFO - Generating per-class IoU plot...
2025-12-27 19:53:49,182 - INFO - Per-class IoU plot saved to 'runs/run_221/iou_per_class_swin_upernet.png'
2025-12-27 19:53:49,184 - INFO - Generating LR schedule plot...
2025-12-27 19:53:49,542 - INFO - LR schedule plot saved to 'runs/run_221/lr_schedule_swin_upernet.png'
2025-12-27 19:53:49,544 - INFO - 
==================================================
2025-12-27 19:53:49,545 - INFO - Final Segmentation Results (swin_upernet)
2025-12-27 19:53:49,547 - INFO - ==================================================
2025-12-27 19:53:49,548 - INFO - Loss: 1.1216
2025-12-27 19:53:49,550 - INFO - mIoU: 44.11%
2025-12-27 19:53:49,551 - INFO - Pixel Accuracy: 80.54%
2025-12-27 19:53:49,553 - INFO - Classes Present: 150/150
2025-12-27 19:53:49,558 - INFO - Final metrics saved to runs/run_221/final_metrics_swin_upernet.json
2025-12-27 19:53:50,064 - INFO - ✅ Model weights saved: runs/run_221/final_model_swin_upernet_weights.pth
2025-12-27 19:53:50,068 - INFO - Final model saved: runs/run_221/final_model_swin_upernet_weights.pth
2025-12-27 19:53:50,069 - INFO - Final model metadata saved: runs/run_221/final_model_swin_upernet_metadata.json
2025-12-27 19:53:50,094 - INFO - Experiment completed. Results saved to runs/run_221/results_swin_upernet.json
2025-12-27 19:53:50,127 - INFO - Experiment completed. Metadata saved to runs/run_221/metadata_swin_upernet.json
2025-12-27 19:53:50,129 - INFO - ============================================================
2025-12-27 19:53:50,130 - INFO - SEGMENTATION PIPELINE COMPLETE
2025-12-27 19:53:50,131 - INFO - Final mIoU: 44.11%
2025-12-27 19:53:50,132 - INFO - Final Pixel Acc: 80.54%
2025-12-27 19:53:50,133 - INFO - ============================================================
2025-12-27 19:53:50,137 - INFO - Segmentation training completed successfully!
2025-12-27 19:53:50,138 - INFO - Final mIoU: 44.11%
