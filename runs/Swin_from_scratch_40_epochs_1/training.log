2025-12-28 11:16:56,731 - INFO - Logging initialized. Log file: runs/run_241/training.log
2025-12-28 11:17:00,218 - INFO - Experiment directory: runs/run_241
2025-12-28 11:17:00,220 - INFO - Setting random seeds for reproducibility (seed: 42)...
2025-12-28 11:17:00,246 - INFO - ✅ All seeds set to 42 (deterministic=False)
2025-12-28 11:17:00,735 - INFO - GPU memory cleared at startup. Available: 23.6GB
2025-12-28 11:17:00,736 - INFO - Using device: cuda
2025-12-28 11:17:00,737 - INFO - Training configuration: epochs=40, warmup=3, lr=0.0005
2025-12-28 11:17:00,738 - INFO - SWIN configuration: variant=tiny, use_shifted_window=True, use_relative_bias=True, use_absolute_pos_embed=False, use_hierarchical_merge=False, use_gradient_checkpointing=True
2025-12-28 11:17:00,739 - INFO - SWIN details: embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], window_size=7
2025-12-28 11:17:00,740 - INFO - Loading dataset...
2025-12-28 11:17:00,743 - INFO - Starting ImageNet data loading from root: /
2025-12-28 11:17:00,745 - INFO - Resolved root path: /
2025-12-28 11:17:00,746 - INFO - Root path exists: True
2025-12-28 11:17:00,747 - INFO - Root path is dir: True
2025-12-28 11:17:00,755 - INFO - Contents of /: ['/.exec', '/.run', '/.shell', '/.singularity.d', '/.test', '/bin', '/boot', '/dev', '/environment', '/etc', '/home', '/lib', '/lib64', '/media', '/mnt', '/opt', '/proc', '/requirements.txt', '/root', '/run', '/sbin', '/singularity', '/srv', '/sys', '/tmp', '/usr', '/var', '/meta', '/train_set', '/train_set_small', '/val_set', '/val_set_unlabeled']
2025-12-28 11:17:00,757 - INFO - Expected train_dir: /train_set, exists: True
2025-12-28 11:17:00,758 - INFO - Expected val_dir: /val_set, exists: True
2025-12-28 11:17:23,321 - INFO - Loaded ImageNet data from /: train=1281167, val=50000
2025-12-28 11:17:23,703 - INFO - Dataset loaded: train=100000 samples (782 batches), val=50000 samples (391 batches), test=50000 samples (391 batches)
2025-12-28 11:17:27,782 - INFO - Training SWIN from scratch
2025-12-28 11:17:27,788 - INFO - Initializing SWIN model from scratch...
2025-12-28 11:17:27,789 - INFO - Model architecture: SWIN
2025-12-28 11:17:27,791 - INFO - Model config: {'type': 'swin', 'variant': 'tiny', 'patch_size': 4, 'embed_dim': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24], 'window_size': 7, 'mlp_ratio': 4.0, 'dropout': 0.0, 'attention_dropout': 0.0, 'projection_dropout': 0.0, 'drop_path_rate': 0.08, 'use_shifted_window': True, 'use_relative_bias': True, 'use_absolute_pos_embed': False, 'use_hierarchical_merge': False, 'use_gradient_checkpointing': True}
2025-12-28 11:17:28,778 - INFO - Model created with random initialization
2025-12-28 11:17:28,780 - INFO - Total parameters: 28,288,354
2025-12-28 11:17:28,781 - INFO - Trainable parameters: 28,288,354
2025-12-28 11:17:28,880 - INFO - Starting from-scratch training...
2025-12-28 11:17:28,881 - INFO - Optimizer: training all model parameters
2025-12-28 11:17:28,885 - INFO - LR Scheduler: cosine with 3 warmup epochs
2025-12-28 11:17:28,887 - INFO - Starting training...
2025-12-28 12:11:09,374 - INFO - Epoch 1/40: LR: 0.000170
2025-12-28 12:11:09,384 - INFO - Epoch 1/40: Train Loss: 6.8849, Val Loss: 6.7706, Val Acc: 0.46%
2025-12-28 12:55:59,425 - INFO - Epoch 2/40: LR: 0.000335
2025-12-28 12:55:59,444 - INFO - Epoch 2/40: Train Loss: 6.7451, Val Loss: 6.4989, Val Acc: 0.88%
2025-12-28 13:40:49,764 - INFO - Epoch 3/40: LR: 0.000500
2025-12-28 13:40:51,239 - INFO - Epoch 3/40: Train Loss: 6.4621, Val Loss: 6.2041, Val Acc: 1.49%
2025-12-28 14:25:31,465 - INFO - Epoch 4/40: LR: 0.000499
2025-12-28 14:25:31,481 - INFO - Epoch 4/40: Train Loss: 6.2694, Val Loss: 5.9979, Val Acc: 2.16%
2025-12-28 15:10:29,594 - INFO - Epoch 5/40: LR: 0.000497
2025-12-28 15:25:08,376 - INFO - Epoch 5/40: Train Loss: 6.0915, Val Loss: 5.8338, Val Acc: 2.78%, Test Loss: 5.8338, Test Acc: 2.78%
2025-12-28 16:10:27,049 - INFO - Epoch 6/40: LR: 0.000493
2025-12-28 16:10:27,177 - INFO - Epoch 6/40: Train Loss: 5.9455, Val Loss: 5.6602, Val Acc: 3.95%
2025-12-28 16:55:47,595 - INFO - Epoch 7/40: LR: 0.000487
2025-12-28 16:55:47,863 - INFO - Epoch 7/40: Train Loss: 5.8202, Val Loss: 5.5100, Val Acc: 4.79%
2025-12-28 17:40:57,946 - INFO - Epoch 8/40: LR: 0.000480
2025-12-28 17:40:58,033 - INFO - Epoch 8/40: Train Loss: 5.7139, Val Loss: 5.3542, Val Acc: 5.98%
2025-12-28 18:26:29,834 - INFO - Epoch 9/40: LR: 0.000471
2025-12-28 18:26:29,979 - INFO - Epoch 9/40: Train Loss: 5.6064, Val Loss: 5.2640, Val Acc: 6.69%
2025-12-28 19:11:10,515 - INFO - Epoch 10/40: LR: 0.000461
2025-12-28 19:25:38,373 - INFO - Epoch 10/40: Train Loss: 5.4980, Val Loss: 5.1691, Val Acc: 7.62%, Test Loss: 5.1691, Test Acc: 7.62%
2025-12-28 19:25:38,462 - INFO - Saving checkpoint for epoch 10...
2025-12-28 19:25:39,735 - INFO - ✅ Checkpoint saved: runs/run_241/checkpoint_epoch_10.pth
2025-12-28 20:10:56,478 - INFO - Epoch 11/40: LR: 0.000450
2025-12-28 20:10:56,597 - INFO - Epoch 11/40: Train Loss: 5.4016, Val Loss: 5.0265, Val Acc: 9.01%
2025-12-28 20:55:31,580 - INFO - Epoch 12/40: LR: 0.000437
2025-12-28 20:55:31,630 - INFO - Epoch 12/40: Train Loss: 5.3020, Val Loss: 4.9162, Val Acc: 10.00%
2025-12-28 21:40:23,851 - INFO - Epoch 13/40: LR: 0.000424
2025-12-28 21:40:24,824 - INFO - Epoch 13/40: Train Loss: 5.2032, Val Loss: 4.8560, Val Acc: 10.77%
2025-12-28 22:24:58,503 - INFO - Epoch 14/40: LR: 0.000409
2025-12-28 22:24:58,597 - INFO - Epoch 14/40: Train Loss: 5.1112, Val Loss: 4.7501, Val Acc: 11.84%
2025-12-28 23:10:04,298 - INFO - Epoch 15/40: LR: 0.000393
2025-12-28 23:24:32,844 - INFO - Epoch 15/40: Train Loss: 5.0267, Val Loss: 4.6294, Val Acc: 13.01%, Test Loss: 4.6294, Test Acc: 13.01%
2025-12-29 00:09:23,831 - INFO - Epoch 16/40: LR: 0.000376
2025-12-29 00:09:23,963 - INFO - Epoch 16/40: Train Loss: 4.9269, Val Loss: 4.5254, Val Acc: 14.30%
2025-12-29 00:54:18,907 - INFO - Epoch 17/40: LR: 0.000359
2025-12-29 00:54:19,192 - INFO - Epoch 17/40: Train Loss: 4.8470, Val Loss: 4.4531, Val Acc: 14.75%
2025-12-29 01:38:38,288 - INFO - Epoch 18/40: LR: 0.000341
2025-12-29 01:38:38,443 - INFO - Epoch 18/40: Train Loss: 4.7606, Val Loss: 4.4604, Val Acc: 14.94%
2025-12-29 02:23:29,134 - INFO - Epoch 19/40: LR: 0.000322
2025-12-29 02:23:29,205 - INFO - Epoch 19/40: Train Loss: 4.6724, Val Loss: 4.3161, Val Acc: 16.50%
2025-12-29 03:08:18,969 - INFO - Epoch 20/40: LR: 0.000304
2025-12-29 03:22:55,817 - INFO - Epoch 20/40: Train Loss: 4.5909, Val Loss: 4.3025, Val Acc: 16.73%, Test Loss: 4.3025, Test Acc: 16.73%
2025-12-29 03:22:55,842 - INFO - Saving checkpoint for epoch 20...
2025-12-29 03:22:57,430 - INFO - ✅ Checkpoint saved: runs/run_241/checkpoint_epoch_20.pth
2025-12-29 04:08:24,498 - INFO - Epoch 21/40: LR: 0.000285
2025-12-29 04:08:24,608 - INFO - Epoch 21/40: Train Loss: 4.5064, Val Loss: 4.2314, Val Acc: 17.75%
2025-12-29 04:53:30,908 - INFO - Epoch 22/40: LR: 0.000265
2025-12-29 04:53:30,996 - INFO - Epoch 22/40: Train Loss: 4.4291, Val Loss: 4.1547, Val Acc: 18.71%
2025-12-29 05:38:44,330 - INFO - Epoch 23/40: LR: 0.000246
2025-12-29 05:38:45,567 - INFO - Epoch 23/40: Train Loss: 4.3425, Val Loss: 4.1186, Val Acc: 19.45%
2025-12-29 06:23:46,031 - INFO - Epoch 24/40: LR: 0.000228
2025-12-29 06:23:46,074 - INFO - Epoch 24/40: Train Loss: 4.2684, Val Loss: 4.0491, Val Acc: 20.32%
2025-12-29 07:09:12,991 - INFO - Epoch 25/40: LR: 0.000209
2025-12-29 07:23:38,393 - INFO - Epoch 25/40: Train Loss: 4.2033, Val Loss: 3.9951, Val Acc: 20.97%, Test Loss: 3.9951, Test Acc: 20.97%
2025-12-29 08:08:56,029 - INFO - Epoch 26/40: LR: 0.000191
2025-12-29 08:08:56,073 - INFO - Epoch 26/40: Train Loss: 4.1338, Val Loss: 3.9284, Val Acc: 21.76%
2025-12-29 08:54:06,644 - INFO - Epoch 27/40: LR: 0.000174
2025-12-29 08:54:06,721 - INFO - Epoch 27/40: Train Loss: 4.0608, Val Loss: 3.8802, Val Acc: 22.43%
2025-12-29 09:38:31,126 - INFO - Epoch 28/40: LR: 0.000157
2025-12-29 09:38:31,144 - INFO - Epoch 28/40: Train Loss: 3.9906, Val Loss: 3.8319, Val Acc: 23.33%
2025-12-29 10:23:03,687 - INFO - Epoch 29/40: LR: 0.000141
2025-12-29 10:23:03,701 - INFO - Epoch 29/40: Train Loss: 3.9246, Val Loss: 3.8218, Val Acc: 23.61%
2025-12-29 11:07:42,085 - INFO - Epoch 30/40: LR: 0.000126
2025-12-29 11:22:14,073 - INFO - Epoch 30/40: Train Loss: 3.8601, Val Loss: 3.7673, Val Acc: 24.34%, Test Loss: 3.7673, Test Acc: 24.34%
2025-12-29 11:22:14,075 - INFO - Saving checkpoint for epoch 30...
2025-12-29 11:22:15,836 - INFO - ✅ Checkpoint saved: runs/run_241/checkpoint_epoch_30.pth
2025-12-29 12:07:04,495 - INFO - Epoch 31/40: LR: 0.000113
2025-12-29 12:07:04,514 - INFO - Epoch 31/40: Train Loss: 3.8089, Val Loss: 3.7563, Val Acc: 24.60%
2025-12-29 12:51:40,450 - INFO - Epoch 32/40: LR: 0.000100
2025-12-29 12:51:40,540 - INFO - Epoch 32/40: Train Loss: 3.7592, Val Loss: 3.6893, Val Acc: 25.45%
2025-12-29 13:36:15,574 - INFO - Epoch 33/40: LR: 0.000089
2025-12-29 13:36:15,681 - INFO - Epoch 33/40: Train Loss: 3.7047, Val Loss: 3.6827, Val Acc: 25.66%
2025-12-29 14:20:53,429 - INFO - Epoch 34/40: LR: 0.000079
2025-12-29 14:20:53,490 - INFO - Epoch 34/40: Train Loss: 3.6761, Val Loss: 3.6415, Val Acc: 26.29%
2025-12-29 15:05:41,493 - INFO - Epoch 35/40: LR: 0.000070
2025-12-29 15:20:11,998 - INFO - Epoch 35/40: Train Loss: 3.6304, Val Loss: 3.6492, Val Acc: 26.22%, Test Loss: 3.6492, Test Acc: 26.22%
2025-12-29 16:09:32,153 - INFO - Epoch 36/40: LR: 0.000063
2025-12-29 16:09:32,251 - INFO - Epoch 36/40: Train Loss: 3.5881, Val Loss: 3.6224, Val Acc: 26.65%
2025-12-29 16:56:13,682 - INFO - Epoch 37/40: LR: 0.000057
2025-12-29 16:56:13,737 - INFO - Epoch 37/40: Train Loss: 3.5564, Val Loss: 3.6160, Val Acc: 26.66%
2025-12-29 17:42:11,331 - INFO - Epoch 38/40: LR: 0.000053
2025-12-29 17:42:11,461 - INFO - Epoch 38/40: Train Loss: 3.5281, Val Loss: 3.5950, Val Acc: 26.98%
2025-12-29 18:28:46,011 - INFO - Epoch 39/40: LR: 0.000051
2025-12-29 18:28:46,129 - INFO - Epoch 39/40: Train Loss: 3.5120, Val Loss: 3.5928, Val Acc: 27.17%
2025-12-29 19:13:27,886 - INFO - Epoch 40/40: LR: 0.000050
2025-12-29 19:28:01,752 - INFO - Epoch 40/40: Train Loss: 3.4804, Val Loss: 3.5810, Val Acc: 27.22%, Test Loss: 3.5810, Test Acc: 27.22%
2025-12-29 19:28:01,866 - INFO - Saving checkpoint for epoch 40...
2025-12-29 19:28:05,221 - INFO - ✅ Checkpoint saved: runs/run_241/checkpoint_epoch_40.pth
2025-12-29 19:28:05,235 - INFO - Training completed!
2025-12-29 19:28:05,249 - INFO - Training completed!
2025-12-29 19:28:05,250 - INFO - Generating training curves...
2025-12-29 19:28:10,611 - INFO - Too many classes (1000) for per-class plotting, skipping
2025-12-29 19:28:10,614 - INFO - Training curves saved as separate images with base name 'runs/run_241/training_curves_from_scratch'
2025-12-29 19:28:10,625 - INFO - Generating confusion matrix on test set...
2025-12-29 20:34:05,938 - INFO - Performing final evaluation of from_scratch model on test set...
2025-12-29 20:49:54,876 - INFO - Final Test Results of from_scratch model: Loss: 3.5810, Accuracy: 27.22%
2025-12-29 20:49:54,920 - INFO - Generating LR schedule plot of from_scratch model...
2025-12-29 20:49:55,735 - INFO - LR schedule plot saved to 'runs/run_241/lr_schedule_from_scratch.png'
2025-12-29 20:49:55,736 - INFO - 
Final Test Results:
2025-12-29 20:49:55,738 - INFO - Loss: 3.5810
2025-12-29 20:49:55,739 - INFO - Accuracy: 27.22%
2025-12-29 20:49:55,740 - INFO - Precision: 26.67%
2025-12-29 20:49:55,741 - INFO - Recall: 27.22%
2025-12-29 20:49:55,742 - INFO - F1 Score: 25.81%
2025-12-29 20:49:56,507 - INFO - Experiment completed. Results saved to runs/run_241/results_from_scratch.json
2025-12-29 20:49:56,512 - INFO - Experiment completed. Metadata saved to runs/run_241/metadata_from_scratch.json
2025-12-29 20:49:57,024 - INFO - ✅ Model weights saved: runs/run_241/final_model_from_scratch_weights.pth
2025-12-29 20:49:57,029 - INFO - Final model saved: runs/run_241/final_model_from_scratch_weights.pth
2025-12-29 20:49:57,030 - INFO - Final model metadata saved: runs/run_241/final_model_from_scratch_metadata.json
2025-12-29 20:49:57,031 - INFO - === FROM-SCRATCH RESULTS (IMAGENET) ===
2025-12-29 20:49:57,033 - INFO - Final Accuracy: 27.22%
2025-12-29 20:49:57,034 - INFO - Final F1 Score: 25.81%
2025-12-29 20:49:57,041 - INFO - from_scratch experiment completed successfully!
